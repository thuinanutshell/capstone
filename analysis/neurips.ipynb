{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449cd6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/thananhthu/capstone/output/run_20250623_115120/all_papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8883d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>proceeding</th>\n",
       "      <th>url</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MicroAdam: Accurate Adaptive Optimization with...</td>\n",
       "      <td>Ionut-Vlad Modoranu, Mher Safaryan, Grigory Ma...</td>\n",
       "      <td>We propose a new variant of the Adam optimizer...</td>\n",
       "      <td>https://papers.nips.cc//paper_files/paper/2024</td>\n",
       "      <td>https://papers.nips.cc//paper_files/paper/2024...</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GITA: Graph to Visual and Textual Integration ...</td>\n",
       "      <td>Yanbin Wei, Shuai Fu, Weisen Jiang, Zejian Zha...</td>\n",
       "      <td>Large Language Models (LLMs) are increasingly ...</td>\n",
       "      <td>https://papers.nips.cc//paper_files/paper/2024</td>\n",
       "      <td>https://papers.nips.cc//paper_files/paper/2024...</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does PDE order affect the convergence of P...</td>\n",
       "      <td>Changhoon Song, Yesom Park, Myungjoo Kang</td>\n",
       "      <td>This paper analyzes the inverse relationship b...</td>\n",
       "      <td>https://papers.nips.cc//paper_files/paper/2024</td>\n",
       "      <td>https://papers.nips.cc//paper_files/paper/2024...</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fair Wasserstein Coresets</td>\n",
       "      <td>Zikai Xiong, Niccolò Dalmasso, Shubham Sharma,...</td>\n",
       "      <td>Data distillation and coresets have emerged as...</td>\n",
       "      <td>https://papers.nips.cc//paper_files/paper/2024</td>\n",
       "      <td>https://papers.nips.cc//paper_files/paper/2024...</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Improved Regret for Bandit Convex Optimization...</td>\n",
       "      <td>Yuanyu Wan, Chang Yao, Mingli Song, Lijun Zhang</td>\n",
       "      <td>We investigate bandit convex optimization (BCO...</td>\n",
       "      <td>https://papers.nips.cc//paper_files/paper/2024</td>\n",
       "      <td>https://papers.nips.cc//paper_files/paper/2024...</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  MicroAdam: Accurate Adaptive Optimization with...   \n",
       "1  GITA: Graph to Visual and Textual Integration ...   \n",
       "2  How does PDE order affect the convergence of P...   \n",
       "3                          Fair Wasserstein Coresets   \n",
       "4  Improved Regret for Bandit Convex Optimization...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Ionut-Vlad Modoranu, Mher Safaryan, Grigory Ma...   \n",
       "1  Yanbin Wei, Shuai Fu, Weisen Jiang, Zejian Zha...   \n",
       "2          Changhoon Song, Yesom Park, Myungjoo Kang   \n",
       "3  Zikai Xiong, Niccolò Dalmasso, Shubham Sharma,...   \n",
       "4    Yuanyu Wan, Chang Yao, Mingli Song, Lijun Zhang   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We propose a new variant of the Adam optimizer...   \n",
       "1  Large Language Models (LLMs) are increasingly ...   \n",
       "2  This paper analyzes the inverse relationship b...   \n",
       "3  Data distillation and coresets have emerged as...   \n",
       "4  We investigate bandit convex optimization (BCO...   \n",
       "\n",
       "                                       proceeding  \\\n",
       "0  https://papers.nips.cc//paper_files/paper/2024   \n",
       "1  https://papers.nips.cc//paper_files/paper/2024   \n",
       "2  https://papers.nips.cc//paper_files/paper/2024   \n",
       "3  https://papers.nips.cc//paper_files/paper/2024   \n",
       "4  https://papers.nips.cc//paper_files/paper/2024   \n",
       "\n",
       "                                                 url  year  \n",
       "0  https://papers.nips.cc//paper_files/paper/2024...  2024  \n",
       "1  https://papers.nips.cc//paper_files/paper/2024...  2024  \n",
       "2  https://papers.nips.cc//paper_files/paper/2024...  2024  \n",
       "3  https://papers.nips.cc//paper_files/paper/2024...  2024  \n",
       "4  https://papers.nips.cc//paper_files/paper/2024...  2024  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8fd824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0224fb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': {0: 'MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable Convergence',\n",
       "  1: 'GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning',\n",
       "  2: 'How does PDE order affect the convergence of PINNs?',\n",
       "  3: 'Fair Wasserstein Coresets',\n",
       "  4: 'Improved Regret for Bandit Convex Optimization with Delayed Feedback',\n",
       "  5: 'Enhancing Chess Reinforcement Learning with Graph Representation',\n",
       "  6: 'Mixtures of Experts for Audio-Visual Learning',\n",
       "  7: 'Learning Place Cell Representations and Context-Dependent Remapping',\n",
       "  8: 'Robust Sparse Regression with Non-Isotropic Designs',\n",
       "  9: 'Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy',\n",
       "  10: 'Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs',\n",
       "  11: 'Decompose, Analyze and Rethink: Solving Intricate Problems with Human-like Reasoning Cycle',\n",
       "  12: 'UQ-Guided Hyperparameter Optimization for Iterative Learners',\n",
       "  13: 'Occupancy-based Policy Gradient: Estimation, Convergence, and Optimality',\n",
       "  14: 'TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables',\n",
       "  15: 'DrivAerNet++: A Large-Scale Multimodal Car Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks',\n",
       "  16: 'DomainGallery: Few-shot Domain-driven Image Generation by Attribute-centric Finetuning',\n",
       "  17: 'Collaborative Cognitive Diagnosis with Disentangled Representation Learning for Learner Modeling',\n",
       "  18: 'AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning',\n",
       "  19: 'Slot-VLM: Object-Event Slots for Video-Language Modeling',\n",
       "  20: 'VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time',\n",
       "  21: 'Community Detection Guarantees using Embeddings Learned by Node2Vec',\n",
       "  22: 'Physics-Informed Regularization for Domain-Agnostic Dynamical System Modeling',\n",
       "  23: 'Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders',\n",
       "  24: 'Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving',\n",
       "  25: 'Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers',\n",
       "  26: 'Towards Understanding How Transformers Learn In-context Through a Representation Learning Lens',\n",
       "  27: 'FINALLY: fast and universal speech enhancement with studio-like quality',\n",
       "  28: 'Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate',\n",
       "  29: 'Prospective Representation Learning for Non-Exemplar Class-Incremental Learning',\n",
       "  30: 'SPO: Sequential Monte Carlo Policy Optimisation',\n",
       "  31: 'Differentiable Modal Synthesis for Physical Modeling of Planar String Sound and Motion Simulation',\n",
       "  32: 'FEDMEKI: A Benchmark for Scaling Medical Foundation Models via Federated Knowledge Injection',\n",
       "  33: 'GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music Scores for All Singing Tasks',\n",
       "  34: 'Unified Lexical Representation for Interpretable Visual-Language Alignment',\n",
       "  35: 'LLaNA: Large Language and NeRF Assistant',\n",
       "  36: 'DiTFastAttn: Attention Compression for Diffusion Transformer Models',\n",
       "  37: 'Open LLMs are Necessary for Current Private Adaptations and Outperform their Closed Alternatives',\n",
       "  38: 'SpelsNet: Surface Primitive Elements Segmentation by B-Rep Graph Structure Supervision',\n",
       "  39: 'KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization',\n",
       "  40: 'DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion',\n",
       "  41: 'Causal Contrastive Learning for Counterfactual Regression Over Time',\n",
       "  42: 'Certified Robustness for Deep Equilibrium Models via Serialized Random Smoothing',\n",
       "  43: 'Causal vs. Anticausal merging of predictors',\n",
       "  44: 'Sim2Real-Fire: A Multi-modal Simulation Dataset for Forecast and Backtracking of Real-world Forest Fire',\n",
       "  45: 'SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models',\n",
       "  46: 'HelpSteer 2: Open-source dataset for training top-performing reward models',\n",
       "  47: 'Efficient Adversarial Training in LLMs with Continuous Attacks',\n",
       "  48: 'Context and Geometry Aware Voxel Transformer for Semantic Scene Completion',\n",
       "  49: 'MIDGArD: Modular Interpretable Diffusion over Graphs for Articulated Designs',\n",
       "  50: 'Provably Safe Neural Network Controllers via Differential Dynamic Logic',\n",
       "  51: 'Learning Image Priors Through Patch-Based Diffusion Models for Solving Inverse Problems',\n",
       "  52: 'Seek Commonality but Preserve Differences: Dissected Dynamics Modeling for Multi-modal Visual RL',\n",
       "  53: 'Reciprocal Learning',\n",
       "  54: 'D-LLM: A Token Adaptive Computing Resource Allocation Strategy for Large Language Models',\n",
       "  55: 'Near-Optimal Distributionally Robust Reinforcement Learning with General $L_p$ Norms',\n",
       "  56: 'AdaNovo: Towards Robust \\\\emph{De Novo} Peptide Sequencing in Proteomics against Data Biases',\n",
       "  57: 'MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes',\n",
       "  58: 'JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models',\n",
       "  59: \"Voila-A: Aligning Vision-Language Models with User's Gaze Attention\",\n",
       "  60: 'einspace: Searching for Neural Architectures from Fundamental Operations',\n",
       "  61: 'Structured flexibility in recurrent neural networks via neuromodulation',\n",
       "  62: 'DU-Shapley: A Shapley Value Proxy for Efficient Dataset Valuation',\n",
       "  63: 'Enhancing Zero-Shot Vision Models by Label-Free Prompt Distribution Learning and Bias Correcting',\n",
       "  64: 'LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch',\n",
       "  65: 'Diffusion PID: Interpreting Diffusion via Partial Information Decomposition',\n",
       "  66: 'Test-Time Dynamic Image Fusion',\n",
       "  67: 'An End-To-End Graph Attention Network Hashing for Cross-Modal Retrieval',\n",
       "  68: 'Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba',\n",
       "  69: 'Omnigrasp: Grasping Diverse Objects with Simulated Humanoids',\n",
       "  70: 'AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data',\n",
       "  71: 'Gliding over the Pareto Front with Uniform Designs',\n",
       "  72: 'WildPPG: A Real-World PPG Dataset of Long Continuous Recordings',\n",
       "  73: 'SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization',\n",
       "  74: 'Online Learning of Delayed Choices',\n",
       "  75: 'Overcoming Common Flaws in the Evaluation of Selective Classification Systems',\n",
       "  76: 'Connecting Joint-Embedding Predictive Architecture with Contrastive Self-supervised Learning',\n",
       "  77: 'Multi-Label Learning with Stronger Consistency Guarantees',\n",
       "  78: 'Data Distribution Valuation',\n",
       "  79: 'AdjointDEIS: Efficient Gradients for Diffusion Models',\n",
       "  80: 'Using Unity to Help Solve Reinforcement Learning',\n",
       "  81: 'Differentially Private Equivalence Testing for Continuous Distributions and Applications',\n",
       "  82: 'Reinforcement Learning with Adaptive Regularization for Safe Control of Critical Systems',\n",
       "  83: 'BERTs are Generative In-Context Learners',\n",
       "  84: 'Faster Local Solvers for Graph Diffusion Equations',\n",
       "  85: 'Beyond Accuracy: Tracking more like Human via Visual Search',\n",
       "  86: 'Credit Attribution and Stable Compression',\n",
       "  87: 'Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration',\n",
       "  88: 'A Walsh Hadamard Derived Linear Vector Symbolic Architecture',\n",
       "  89: 'Autonomous Agents for Collaborative Task under Information Asymmetry',\n",
       "  90: 'Retrieval-Augmented Diffusion Models for Time Series Forecasting',\n",
       "  91: 'The Surprising Effectiveness of SP Voting with Partial Preferences',\n",
       "  92: 'Flatten Anything: Unsupervised Neural Surface Parameterization',\n",
       "  93: 'BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling',\n",
       "  94: \"ReMoDetect: Reward Models Recognize Aligned LLM's Generations\",\n",
       "  95: 'Analyzing & Reducing the Need for Learning Rate Warmup in GPT Training',\n",
       "  96: \"A Hitchhiker's Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning\",\n",
       "  97: 'Meta-Reinforcement Learning with Universal Policy Adaptation: Provable Near-Optimality under All-task Optimum Comparator',\n",
       "  98: 'Mutual Information Estimation via Normalizing Flows',\n",
       "  99: 'UltraEdit: Instruction-based Fine-Grained Image Editing at Scale',\n",
       "  100: 'CARE: a Benchmark Suite for the Classification and Retrieval of Enzymes',\n",
       "  101: 'Few-Shot Adversarial Prompt Learning on Vision-Language Models',\n",
       "  102: 'UniSDF: Unifying Neural Representations for High-Fidelity 3D Reconstruction of Complex Scenes with Reflections',\n",
       "  103: 'Goal Reduction with Loop-Removal Accelerates RL and Models Human Brain Activity in Goal-Directed Learning',\n",
       "  104: 'QBB: Quantization with Binary Bases for LLMs',\n",
       "  105: 'VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding',\n",
       "  106: 'Disentangling and mitigating the impact of task similarity for continual learning',\n",
       "  107: 'Warm-up Free Policy Optimization: Improved Regret in Linear Markov Decision Processes',\n",
       "  108: 'KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization',\n",
       "  109: 'Unified Insights: Harnessing Multi-modal Data for Phenotype Imputation via View Decoupling',\n",
       "  110: 'Optimal Private and Communication Constraint Distributed Goodness-of-Fit Testing for Discrete Distributions in the Large Sample Regime',\n",
       "  111: 'Concentrate Attention: Towards Domain-Generalizable Prompt Optimization for Language Models',\n",
       "  112: 'Training for Stable Explanation for Free',\n",
       "  113: 'NeuralSolver: Learning Algorithms For Consistent and Efficient Extrapolation Across General Tasks',\n",
       "  114: 'SolarCube: An Integrative Benchmark Dataset Harnessing Satellite and In-situ Observations for Large-scale Solar Energy Forecasting',\n",
       "  115: 'Feint Behaviors and Strategies: Formalization, Implementation and Evaluation',\n",
       "  116: 'LT-Defense: Searching-free Backdoor Defense via Exploiting the Long-tailed Effect',\n",
       "  117: 'HyperLogic: Enhancing Diversity and Accuracy in Rule Learning with HyperNets',\n",
       "  118: 'CAT: Coordinating Anatomical-Textual Prompts for Multi-Organ and Tumor Segmentation',\n",
       "  119: 'Optimizing Automatic Differentiation with Deep Reinforcement Learning',\n",
       "  120: 'DiGRAF: Diffeomorphic Graph-Adaptive Activation Function',\n",
       "  121: 'Fair Secretaries with Unfair Predictions',\n",
       "  122: 'Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity',\n",
       "  123: 'Testably Learning Polynomial Threshold Functions',\n",
       "  124: 'Flexible Context-Driven Sensory Processing in Dynamical Vision Models',\n",
       "  125: 'Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices',\n",
       "  126: 'Diffusion-Inspired Truncated Sampler for Text-Video Retrieval',\n",
       "  127: 'LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models',\n",
       "  128: 'ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization',\n",
       "  129: 'A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health',\n",
       "  130: 'FUG: Feature-Universal Graph Contrastive Pre-training for Graphs with Diverse Node Features',\n",
       "  131: 'ReGS: Reference-based Controllable Scene Stylization with Gaussian Splatting',\n",
       "  132: 'QueST: Self-Supervised Skill Abstractions for Learning Continuous Control',\n",
       "  133: 'DiffPhyCon: A Generative Approach to Control Complex Physical Systems',\n",
       "  134: 'FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity Refiner',\n",
       "  135: 'MADiff: Offline Multi-agent Learning with Diffusion Models',\n",
       "  136: 'Polynomial-Time Computation of Exact $\\\\Phi$-Equilibria in Polyhedral Games',\n",
       "  137: 'Data Attribution for Text-to-Image Models by Unlearning Synthesized Images',\n",
       "  138: 'CLIPCEIL: Domain Generalization through CLIP via Channel rEfinement and Image-text aLignment',\n",
       "  139: 'Verifiably Robust Conformal Prediction',\n",
       "  140: 'Ordering-Based Causal Discovery for Linear and Nonlinear Relations',\n",
       "  141: 'Revisiting Score Propagation in Graph Out-of-Distribution Detection',\n",
       "  142: 'Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution',\n",
       "  143: 'Explicit Eigenvalue Regularization Improves Sharpness-Aware Minimization',\n",
       "  144: 'LAM3D: Large Image-Point Clouds Alignment Model for 3D Reconstruction from Single Image',\n",
       "  145: 'Weight decay induces low-rank attention layers',\n",
       "  146: 'Association Pattern-aware Fusion for Biological Entity Relationship Prediction',\n",
       "  147: 'TaskBench: Benchmarking Large Language Models for Task Automation',\n",
       "  148: 'Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning',\n",
       "  149: 'On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models',\n",
       "  150: 'Sparse Bayesian Generative Modeling for Compressive Sensing',\n",
       "  151: 'Generative Semi-supervised Graph Anomaly Detection',\n",
       "  152: 'Building on Efficient Foundations: Effective Training of LLMs with Structured Feedforward Layers',\n",
       "  153: 'Optimizing over Multiple Distributions under Generalized Quasar-Convexity Condition',\n",
       "  154: 'Topological Generalization Bounds for Discrete-Time Stochastic Optimization Algorithms',\n",
       "  155: 'Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models',\n",
       "  156: 'HORSE: Hierarchical Representation for Large-Scale Neural Subset Selection',\n",
       "  157: 'Segment, Shuffle, and Stitch: A Simple Layer for Improving Time-Series Representations',\n",
       "  158: 'TARSS-Net: Temporal-Aware Radar Semantic Segmentation Network',\n",
       "  159: 'ProEdit: Simple Progression is All You Need for High-Quality 3D Scene Editing',\n",
       "  160: 'FindingEmo: An Image Dataset for Emotion Recognition in the Wild',\n",
       "  161: 'IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons',\n",
       "  162: 'Utilizing Human Behavior Modeling to Manipulate Explanations in AI-Assisted Decision Making: The Good, the Bad, and the Scary',\n",
       "  163: 'HairDiffusion: Vivid Multi-Colored Hair Editing via Latent Diffusion',\n",
       "  164: 'PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression',\n",
       "  165: 'Feedback control guides credit assignment in recurrent neural networks',\n",
       "  166: 'COLD: Causal reasOning in cLosed Daily activities',\n",
       "  167: 'SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models',\n",
       "  168: 'BackdoorAlign: Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment',\n",
       "  169: 'AGILE: A Novel Reinforcement Learning Framework of LLM Agents',\n",
       "  170: 'SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation',\n",
       "  171: 'Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound',\n",
       "  172: 'Can Graph Learning Improve Planning in LLM-based Agents?',\n",
       "  173: 'Fully Explicit Dynamic Gaussian Splatting',\n",
       "  174: 'Graph Neural Networks and Arithmetic Circuits',\n",
       "  175: 'Self-Refining Diffusion Samplers: Enabling Parallelization via Parareal Iterations',\n",
       "  176: 'Can Learned Optimization Make Reinforcement Learning Less Difficult?',\n",
       "  177: 'Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling',\n",
       "  178: 'Schur Nets: exploiting local structure for equivariance in higher order graph neural networks',\n",
       "  179: 'Visual Fourier Prompt Tuning',\n",
       "  180: 'Learning Representations for Hierarchies with Minimal Support',\n",
       "  181: 'Continuous Partitioning for Graph-Based Semi-Supervised Learning',\n",
       "  182: 'Expressive Gaussian Human Avatars from Monocular RGB Video',\n",
       "  183: 'Hardness of Learning Neural Networks under the Manifold Hypothesis',\n",
       "  184: 'TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment',\n",
       "  185: 'Multi-Label Open Set Recognition',\n",
       "  186: 'Med-Real2Sim: Non-Invasive Medical Digital Twins using Physics-Informed Self-Supervised Learning',\n",
       "  187: 'SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition',\n",
       "  188: 'Towards Scalable and Stable Parallelization of Nonlinear RNNs',\n",
       "  189: '3D Gaussian Rendering Can Be Sparser: Efficient Rendering via Learned Fragment Pruning',\n",
       "  190: 'Trade-Offs of Diagonal Fisher Information Matrix Estimators',\n",
       "  191: 'End-to-end Learnable Clustering for Intent Learning in Recommendation',\n",
       "  192: 'LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings',\n",
       "  193: 'Exploiting Representation Curvature for Boundary Detection in Time Series',\n",
       "  194: 'WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks',\n",
       "  195: 'KnowGPT: Knowledge Graph based Prompting for Large Language Models',\n",
       "  196: 'UDC: A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems',\n",
       "  197: 'Eye-gaze Guided Multi-modal Alignment for Medical Representation Learning',\n",
       "  198: 'Federated Ensemble-Directed Offline Reinforcement Learning',\n",
       "  199: 'Linking In-context Learning in Transformers to Human Episodic Memory',\n",
       "  200: 'Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning',\n",
       "  201: 'IMAGPose: A Unified Conditional Framework for Pose-Guided Person Generation',\n",
       "  202: 'Supervised Kernel Thinning',\n",
       "  203: 'Cell ontology guided transcriptome foundation model',\n",
       "  204: 'Weight Diffusion for Future: Learn to Generalize in Non-Stationary Environments',\n",
       "  205: 'Interpretable Lightweight Transformer via Unrolling of Learned Graph Smoothness Priors',\n",
       "  206: 'Sketching for Distributed Deep Learning: A Sharper Analysis',\n",
       "  207: 'Unified Covariate Adjustment for Causal Inference',\n",
       "  208: 'The High Line: Exact Risk and Learning Rate Curves of Stochastic Adaptive Learning Rate Algorithms',\n",
       "  209: 'Exploring Adversarial Robustness of Deep State Space Models',\n",
       "  210: 'DMC-VB: A Benchmark for Representation Learning for Control with Visual Distractors',\n",
       "  211: 'EASI: Evolutionary Adversarial Simulator Identification for Sim-to-Real Transfer',\n",
       "  212: 'BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays',\n",
       "  213: 'On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games',\n",
       "  214: 'On $f$-Divergence Principled Domain Adaptation: An Improved Framework',\n",
       "  215: 'LLM-based Skill Diffusion for Zero-shot Policy Adaptation',\n",
       "  216: 'TARP-VP: Towards Evaluation of Transferred  Adversarial Robustness and Privacy on Label  Mapping Visual Prompting Models',\n",
       "  217: 'Learning diffusion at lightspeed',\n",
       "  218: 'One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos',\n",
       "  219: 'Rethinking Imbalance in Image Super-Resolution for Efficient Inference',\n",
       "  220: 'How Control Information Influences Multilingual Text Image Generation and Editing?',\n",
       "  221: 'SplitNeRF: Split Sum Approximation Neural Field for Joint Geometry, Illumination, and Material Estimation',\n",
       "  222: 'Flexible task abstractions emerge in linear networks with fast and bounded units',\n",
       "  223: 'Adapting Diffusion Models for Improved Prompt Compliance and Controllable Image Synthesis',\n",
       "  224: 'On the Saturation Effects of Spectral Algorithms in Large Dimensions',\n",
       "  225: 'Cross-Scale Self-Supervised Blind Image Deblurring via Implicit Neural Representation',\n",
       "  226: 'A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning',\n",
       "  227: 'Learning to be Smooth: An End-to-End Differentiable Particle Smoother',\n",
       "  228: 'Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation',\n",
       "  229: 'TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy',\n",
       "  230: 'HonestLLM: Toward an Honest and Helpful Large Language Model',\n",
       "  231: 'MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models',\n",
       "  232: 'UrbanDataLayer: A Unified Data Pipeline for Urban Science',\n",
       "  233: 'Neural Isometries: Taming Transformations for Equivariant ML',\n",
       "  234: 'You Only Cache Once: Decoder-Decoder Architectures for Language Models',\n",
       "  235: 'Locally Private and Robust Multi-Armed Bandits',\n",
       "  236: 'Understanding Information Storage and Transfer in Multi-Modal Large Language Models',\n",
       "  237: 'Sharing Key Semantics in Transformer Makes Efficient Image Restoration',\n",
       "  238: 'Text-space Graph Foundation Models: Comprehensive Benchmarks and New Insights',\n",
       "  239: 'Breaking Long-Tailed Learning Bottlenecks: A Controllable Paradigm with Hypernetwork-Generated Diverse Experts',\n",
       "  240: 'Optimal Multiclass U-Calibration Error and Beyond',\n",
       "  241: 'Text2CAD: Generating Sequential CAD Designs from Beginner-to-Expert Level Text Prompts',\n",
       "  242: 'MVGamba: Unify 3D Content Generation as State Space Sequence Modeling',\n",
       "  243: 'Make Continual Learning Stronger via C-Flat',\n",
       "  244: 'Limits of Transformer Language Models on Learning to Compose Algorithms',\n",
       "  245: 'Stability and Generalization of Asynchronous SGD: Sharper Bounds Beyond Lipschitz and Smoothness',\n",
       "  246: 'On the Identifiability of Hybrid Deep Generative Models: Meta-Learning as a Solution',\n",
       "  247: 'MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models',\n",
       "  248: 'Understanding the Gains from Repeated Self-Distillation',\n",
       "  249: 'In-Context Learning State Vector with Inner and Momentum Optimization',\n",
       "  250: 'DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving',\n",
       "  251: 'CiteME: Can Language Models Accurately Cite Scientific Claims?',\n",
       "  252: 'Visual Prompt Tuning in Null Space for Continual Learning',\n",
       "  253: 'The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better',\n",
       "  254: 'Reproducibility of predictive networks for mouse visual cortex',\n",
       "  255: 'Kraken: Inherently Parallel Transformers For Efficient Multi-Device Inference',\n",
       "  256: 'DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection',\n",
       "  257: 'Incorporating Surrogate Gradient Norm to Improve Offline Optimization Techniques',\n",
       "  258: 'Revisiting Adversarial Patches for Designing Camera-Agnostic Attacks against Person Detection',\n",
       "  259: 'Graph Diffusion Transformers for Multi-Conditional Molecular Generation',\n",
       "  260: 'WildGuard: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs',\n",
       "  261: 'Reparameterization invariance in approximate Bayesian inference',\n",
       "  262: 'Localized Adaptive Risk Control',\n",
       "  263: 'Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks',\n",
       "  264: 'Federated Learning under Periodic Client Participation and Heterogeneous Data: A New Communication-Efficient Algorithm and Analysis',\n",
       "  265: 'xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology',\n",
       "  266: 'Generalized Linear Bandits with Limited Adaptivity',\n",
       "  267: 'Image-aware Evaluation of Generated Medical Reports',\n",
       "  268: 'Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning',\n",
       "  269: 'An effective framework for estimating individualized treatment rules',\n",
       "  270: 'A Simple and Adaptive Learning Rate for FTRL in Online Learning with Minimax Regret of $\\\\Theta(T^{2/3})$ and its Application to Best-of-Both-Worlds',\n",
       "  271: 'Rethinking Fourier Transform from A Basis Functions Perspective for Long-term Time Series Forecasting',\n",
       "  272: 'Beyond Primal-Dual Methods in Bandits with Stochastic and Adversarial Constraints',\n",
       "  273: 'Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity',\n",
       "  274: 'Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning',\n",
       "  275: 'Invisible Image Watermarks Are Provably Removable Using Generative AI',\n",
       "  276: 'Stochastic Optimization Schemes for Performative Prediction with Nonconvex Loss',\n",
       "  277: 'MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs',\n",
       "  278: 'SDP4Bit: Toward 4-bit Communication Quantization in Sharded Data Parallelism for LLM Training',\n",
       "  279: 'How Does Message Passing Improve Collaborative Filtering?',\n",
       "  280: 'An Adaptive Approach for Infinitely Many-armed Bandits under Generalized Rotting Constraints',\n",
       "  281: 'Near-Optimal Streaming Heavy-Tailed Statistical Estimation with Clipped SGD',\n",
       "  282: 'Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities',\n",
       "  283: 'Fine Tuning Out-of-Vocabulary Item Recommendation with User Sequence Imagination',\n",
       "  284: 'Data Mixture Inference Attack: BPE Tokenizers Reveal Training Data Compositions',\n",
       "  285: 'Improving Deep Learning Optimization through Constrained Parameter Regularization',\n",
       "  286: 'A generalized neural tangent kernel for surrogate gradient learning',\n",
       "  287: 'Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space',\n",
       "  288: 'AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models',\n",
       "  289: 'Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels',\n",
       "  290: 'Sample Efficient Bayesian Learning of Causal Graphs from Interventions',\n",
       "  291: 'StreamFlow: Streamlined Multi-Frame Optical Flow Estimation for Video Sequences',\n",
       "  292: 'Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models',\n",
       "  293: 'VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark',\n",
       "  294: 'A Unified Principle of Pessimism for Offline Reinforcement Learning under Model Mismatch',\n",
       "  295: 'Overcoming Brittleness in Pareto-Optimal Learning Augmented Algorithms',\n",
       "  296: 'GraphMETRO: Mitigating Complex Graph Distribution Shifts via Mixture of Aligned Experts',\n",
       "  297: 'CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based Multi-Entity Action Recognition',\n",
       "  298: 'Continual Counting with Gradual Privacy Expiration',\n",
       "  299: 'Clustering then Propagation: Select Better Anchors for Knowledge Graph Embedding',\n",
       "  300: 'LoFiT: Localized Fine-tuning on LLM Representations',\n",
       "  301: 'Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering',\n",
       "  302: 'Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials',\n",
       "  303: 'HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning',\n",
       "  304: 'Graph Diffusion Policy Optimization',\n",
       "  305: 'UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training',\n",
       "  306: 'Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective',\n",
       "  307: 'PrivAuditor: Benchmarking Data Protection Vulnerabilities in LLM Adaptation Techniques',\n",
       "  308: 'A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics',\n",
       "  309: 'A Recipe for Charge Density Prediction',\n",
       "  310: 'Deep Graph Mating',\n",
       "  311: 'LIVE: Learnable In-Context Vector for Visual Question Answering',\n",
       "  312: 'Online Relational Inference for Evolving Multi-agent Interacting Systems',\n",
       "  313: 'Open-Book Neural Algorithmic Reasoning',\n",
       "  314: 'Classification Diffusion Models: Revitalizing Density Ratio Estimation',\n",
       "  315: 'Text-Infused Attention and Foreground-Aware Modeling for Zero-Shot Temporal Action Detection',\n",
       "  316: 'Online Budgeted Matching with General Bids',\n",
       "  317: 'Marginal Causal Flows for Validation and Inference',\n",
       "  318: 'CRAYM: Neural Field Optimization via Camera RAY Matching',\n",
       "  319: 'The Road Less Scheduled',\n",
       "  320: 'Resource-Aware Federated Self-Supervised Learning with Global Class Representations',\n",
       "  321: 'Spiking Transformer with Experts Mixture',\n",
       "  322: 'Semantic Routing via Autoregressive Modeling',\n",
       "  323: 'DiscoveryWorld: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents',\n",
       "  324: 'TSDS: Data Selection for Task-Specific Model Finetuning',\n",
       "  325: 'Fully Unconstrained Online Learning',\n",
       "  326: 'Masked Hard-Attention Transformers Recognize Exactly the Star-Free Languages',\n",
       "  327: 'CosAE: Learnable Fourier Series for Image Restoration',\n",
       "  328: 'Exactly Minimax-Optimal Locally Differentially Private Sampling',\n",
       "  329: 'Exploring Context Window of Large Language Models via Decomposed Positional Vectors',\n",
       "  330: 'Spectral Graph Pruning Against Over-Squashing and Over-Smoothing',\n",
       "  331: 'Fairness in Social Influence Maximization via Optimal Transport',\n",
       "  332: 'ECLipsE: Efficient Compositional Lipschitz Constant Estimation for Deep Neural Networks',\n",
       "  333: 'FairJob: A Real-World Dataset for Fairness in Online Systems',\n",
       "  334: 'CRAG - Comprehensive RAG Benchmark',\n",
       "  335: 'Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training',\n",
       "  336: 'Annealed Multiple Choice Learning: Overcoming limitations of Winner-takes-all with annealing',\n",
       "  337: 'Are Multiple Instance Learning Algorithms Learnable for Instances?',\n",
       "  338: 'Layer-Adaptive State Pruning for Deep State Space Models',\n",
       "  339: 'Score-based 3D molecule generation with neural fields',\n",
       "  340: 'PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations',\n",
       "  341: 'On Sparse Canonical Correlation Analysis',\n",
       "  342: 'A Pairwise Pseudo-likelihood Approach for Matrix Completion with Informative Missingness',\n",
       "  343: 'Toward Global Convergence of Gradient EM for Over-Paramterized Gaussian Mixture Models',\n",
       "  344: 'Conditioning non-linear and infinite-dimensional diffusion processes',\n",
       "  345: 'On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions',\n",
       "  346: 'Quantum algorithm for large-scale market equilibrium computation',\n",
       "  347: 'Adversarially Robust Dense-Sparse Tradeoffs via Heavy-Hitters',\n",
       "  348: \"I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token\",\n",
       "  349: 'Scaling Continuous Latent Variable Models as Probabilistic Integral Circuits',\n",
       "  350: 'What do Graph Neural Networks learn? Insights from Tropical Geometry',\n",
       "  351: 'Understanding the Expressivity and Trainability of Fourier Neural Operator: A Mean-Field Perspective',\n",
       "  352: 'Selective Attention: Enhancing Transformer through Principled Context Control',\n",
       "  353: 'Differentially Private Stochastic Gradient Descent with Fixed-Size Minibatches: Tighter RDP Guarantees with or without Replacement',\n",
       "  354: 'A Simple and Optimal Approach for Universal Online Learning with Gradient Variations',\n",
       "  355: 'LESS: Label-Efficient and Single-Stage Referring 3D Segmentation',\n",
       "  356: 'Optimal Algorithms for Augmented Testing of Discrete Distributions',\n",
       "  357: 'Personalized Instance-based Navigation Toward User-Specific Objects in Realistic Environments',\n",
       "  358: 'Mind the Gap Between Prototypes and Images in Cross-domain Finetuning',\n",
       "  359: 'Robust Reinforcement Learning with General Utility',\n",
       "  360: 'Addressing Bias in Online Selection with Limited Budget of Comparisons',\n",
       "  361: 'MonoMAE: Enhancing Monocular 3D Detection through Depth-Aware Masked Autoencoders',\n",
       "  362: 'Measuring Goal-Directedness',\n",
       "  363: 'Long-Range Feedback Spiking Network Captures Dynamic and Static Representations of the Visual Cortex under Movie Stimuli',\n",
       "  364: 'Typicalness-Aware Learning for Failure Detection',\n",
       "  365: 'CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark',\n",
       "  366: 'EffiBench: Benchmarking the Efficiency of Automatically Generated Code',\n",
       "  367: 'PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition',\n",
       "  368: 'Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning',\n",
       "  369: 'Slot State Space Models',\n",
       "  370: 'Incorporating Test-Time Optimization into Training with Dual Networks for Human Mesh Recovery',\n",
       "  371: 'On the Identifiability of Poisson Branching Structural Causal Model Using Probability Generating Function',\n",
       "  372: 'AsEP: Benchmarking Deep Learning Methods for Antibody-specific Epitope Prediction',\n",
       "  373: 'Variational Flow Matching for Graph Generation',\n",
       "  374: 'A Gradient Accumulation Method for Dense Retriever under Memory Constraint',\n",
       "  375: 'Road Network Representation Learning with the Third Law of  Geography',\n",
       "  376: 'HyperPrism: An Adaptive Non-linear Aggregation Framework for Distributed Machine Learning over Non-IID Data and Time-varying Communication Links',\n",
       "  377: 'A Versatile Diffusion Transformer with Mixture of Noise Levels for Audiovisual Generation',\n",
       "  378: 'GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation',\n",
       "  379: 'Interpretable Generalized Additive Models for Datasets with Missing Values',\n",
       "  380: 'Kangaroo: Lossless Self-Speculative Decoding for Accelerating LLMs via Double Early Exiting',\n",
       "  381: 'Identifying Selections for Unsupervised Subtask Discovery',\n",
       "  382: 'Bandits with Preference Feedback: A Stackelberg Game Perspective',\n",
       "  383: 'DMesh: A Differentiable Mesh Representation',\n",
       "  384: 'UniTox: Leveraging LLMs to Curate a Unified Dataset of Drug-Induced Toxicity from FDA Labels',\n",
       "  385: 'MoE Jetpack: From Dense Checkpoints to Adaptive Mixture of Experts for Vision Tasks',\n",
       "  386: 'Differentially Private Graph Diffusion with Applications in Personalized PageRanks',\n",
       "  387: 'MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing  Dataset and Benchmark for Text-to-Image Generation',\n",
       "  388: 'Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation',\n",
       "  389: 'Action Imitation in Common Action Space for Customized Action Image Synthesis',\n",
       "  390: 'PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging',\n",
       "  391: 'The Importance of Online Data: Understanding Preference Fine-tuning via Coverage',\n",
       "  392: 'LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Control and Rendering',\n",
       "  393: 'What makes unlearning hard and what to do about it',\n",
       "  394: 'The Power of Resets in Online Reinforcement Learning',\n",
       "  395: 'Gaussian Approximation and Multiplier Bootstrap for Polyak-Ruppert Averaged Linear Stochastic Approximation with Applications to TD Learning',\n",
       "  396: 'DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning',\n",
       "  397: 'Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps',\n",
       "  398: 'Distributionally Robust Reinforcement Learning with Interactive Data Collection: Fundamental Hardness and Near-Optimal Algorithms',\n",
       "  399: 'Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference',\n",
       "  400: 'Scaling the Codebook Size of VQ-GAN to 100,000 with a Utilization Rate of 99%',\n",
       "  401: 'Representation Noising: A Defence Mechanism Against Harmful Finetuning',\n",
       "  402: 'Towards Calibrated Robust Fine-Tuning of Vision-Language Models',\n",
       "  403: 'Towards Safe Concept Transfer of Multi-Modal Diffusion via Causal Representation Editing',\n",
       "  404: 'Variational Distillation of Diffusion Policies into Mixture of Experts',\n",
       "  405: 'Handling Learnwares from Heterogeneous Feature Spaces with Explicit Label Exploitation',\n",
       "  406: 'ROIDICE: Offline Return on Investment Maximization for Efficient Decision Making',\n",
       "  407: 'Generalization Bound and Learning Methods for Data-Driven Projections in Linear Programming',\n",
       "  408: 'CV-VAE: A Compatible Video VAE for Latent Generative Video Models',\n",
       "  409: 'Historical Test-time Prompt Tuning for Vision Foundation Models',\n",
       "  410: 'Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation',\n",
       "  411: 'Regret Minimization in Stackelberg Games with Side Information',\n",
       "  412: 'Navigating the Effect of Parametrization for Dimensionality Reduction',\n",
       "  413: 'DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model',\n",
       "  414: 'Peri-midFormer: Periodic Pyramid Transformer for Time Series Analysis',\n",
       "  415: 'QKFormer: Hierarchical Spiking Transformer using Q-K Attention',\n",
       "  416: 'DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices',\n",
       "  417: 'Policy-shaped prediction: avoiding distractions in model-based reinforcement learning',\n",
       "  418: 'Fundamental Convergence Analysis of Sharpness-Aware Minimization',\n",
       "  419: 'Honor Among Bandits: No-Regret Learning for Online Fair Division',\n",
       "  420: 'BLURD: Benchmarking and Learning using a Unified Rendering and Diffusion Model',\n",
       "  421: 'Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks',\n",
       "  422: 'Zero-Shot Event-Intensity Asymmetric Stereo via Visual Prompting from Image Domain',\n",
       "  423: 'Towards Comprehensive Detection of Chinese Harmful Memes',\n",
       "  424: 'On improved Conditioning Mechanisms and Pre-training Strategies for Diffusion Models',\n",
       "  425: '$\\\\texttt{Model-GLUE}$: Democratized LLM Scaling for A Large Model Zoo in the Wild',\n",
       "  426: 'A Full-duplex Speech Dialogue Scheme Based On Large Language Model',\n",
       "  427: 'MultiPull: Detailing Signed Distance Functions by Pulling Multi-Level Queries at Multi-Step',\n",
       "  428: 'Learning Cooperative Trajectory Representations for Motion Forecasting',\n",
       "  429: 'eXponential FAmily Dynamical Systems (XFADS): Large-scale nonlinear Gaussian state-space modeling',\n",
       "  430: 'AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields',\n",
       "  431: 'Parameter Efficient Adaptation for Image Restoration with Heterogeneous Mixture-of-Experts',\n",
       "  432: 'DiffCut: Catalyzing Zero-Shot Semantic Segmentation with Diffusion Features and Recursive Normalized Cut',\n",
       "  433: 'BitDelta: Your Fine-Tune May Only Be Worth One Bit',\n",
       "  434: 'LION: Linear Group RNN for 3D Object Detection in Point Clouds',\n",
       "  435: '$SE(3)$ Equivariant Ray Embeddings for Implicit Multi-View Depth Estimation',\n",
       "  436: 'Fast and Memory-Efficient Video Diffusion Using Streamlined Inference',\n",
       "  437: 'Sparse High Rank Adapters',\n",
       "  438: 'Beyond Aesthetics: Cultural Competence in Text-to-Image Models',\n",
       "  439: 'Pretraining with Random Noise for Fast and Robust Learning without Weight Transport',\n",
       "  440: 'High-Resolution Image Harmonization with Adaptive-Interval Color Transformation',\n",
       "  441: 'Pseudo-Siamese Blind-spot Transformers for Self-Supervised Real-World Denoising',\n",
       "  442: 'Expected Probabilistic Hierarchies',\n",
       "  443: 'Video Token Merging for Long Video Understanding',\n",
       "  444: 'Alignment at Pre-training! Towards Native Alignment for Arabic LLMs',\n",
       "  445: 'FSP-Laplace: Function-Space Priors for the Laplace Approximation in Bayesian Deep Learning',\n",
       "  446: 'SCAFFLSA: Taming Heterogeneity in Federated Linear Stochastic Approximation and TD Learning',\n",
       "  447: 'Sequential Probability Assignment with Contexts: Minimax Regret, Contextual Shtarkov Sums, and Contextual Normalized Maximum Likelihood',\n",
       "  448: 'Functional Bilevel Optimization for Machine Learning',\n",
       "  449: 'Emergence of heavy tails in homogenized stochastic gradient descent',\n",
       "  450: 'Initialization is Critical to Whether Transformers Fit Composite Functions by Reasoning or Memorizing',\n",
       "  451: 'Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand Avatars',\n",
       "  452: 'Learning-Augmented Dynamic Submodular Maximization',\n",
       "  453: 'IF-Font: Ideographic Description Sequence-Following Font Generation',\n",
       "  454: 'DataComp-LM: In search of the next generation of training sets for language models',\n",
       "  455: 'Pretrained Optimization Model for Zero-Shot Black Box Optimization',\n",
       "  456: 'Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning',\n",
       "  457: 'Boundary Decomposition for Nadir Objective Vector Estimation',\n",
       "  458: 'Recurrent Reinforcement Learning with Memoroids',\n",
       "  459: 'Image Copy Detection for Diffusion Models',\n",
       "  460: 'Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources',\n",
       "  461: 'Interaction-Force Transport Gradient Flows',\n",
       "  462: 'TreeVI: Reparameterizable Tree-structured Variational Inference for Instance-level Correlation Capturing',\n",
       "  463: 'Optimal Parallelization of Boosting',\n",
       "  464: 'ProgressGym: Alignment with a Millennium of Moral Progress',\n",
       "  465: 'Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification',\n",
       "  466: 'Segmenting Watermarked Texts From Language Models',\n",
       "  467: 'Data-Efficient Learning with Neural Programs',\n",
       "  468: 'Language Models as Hierarchy Encoders',\n",
       "  469: 'Unconditional stability of a recurrent neural circuit implementing divisive normalization',\n",
       "  470: 'G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training',\n",
       "  471: 'On the Role of Attention Masks and LayerNorm in Transformers',\n",
       "  472: 'Directional Smoothness and Gradient Methods: Convergence and Adaptivity',\n",
       "  473: 'OVT-B: A New Large-Scale Benchmark for Open-Vocabulary Multi-Object Tracking',\n",
       "  474: 'The Implicit Bias of Heterogeneity towards Invariance: A Study of Multi-Environment Matrix Sensing',\n",
       "  475: 'MemVLT: Vision-Language Tracking with Adaptive Memory-based Prompts',\n",
       "  476: 'Towards Universal Mesh Movement Networks',\n",
       "  477: 'Natural Counterfactuals With Necessary Backtracking',\n",
       "  478: 'BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep Neural Network Inference',\n",
       "  479: 'CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning',\n",
       "  480: 'ReLIZO: Sample Reusable Linear Interpolation-based Zeroth-order Optimization',\n",
       "  481: 'Entity Alignment with Noisy Annotations from Large Language Models',\n",
       "  482: 'How Diffusion Models Learn to Factorize and Compose',\n",
       "  483: 'Learning to Mitigate Externalities: the Coase Theorem with Hindsight Rationality',\n",
       "  484: 'Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?',\n",
       "  485: 'Lower Bounds of Uniform Stability in Gradient-Based Bilevel Algorithms for Hyperparameter Optimization',\n",
       "  486: 'Improving self-training under distribution shifts via anchored confidence with theoretical guarantees',\n",
       "  487: '4Diffusion: Multi-view Video Diffusion Model for 4D Generation',\n",
       "  488: 'How do Large Language Models Handle Multilingualism?',\n",
       "  489: 'Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn',\n",
       "  490: 'Benchmarking LLMs via Uncertainty Quantification',\n",
       "  491: 'No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations',\n",
       "  492: 'ChatQA: Surpassing GPT-4 on Conversational QA and RAG',\n",
       "  493: 'Improving Linear System Solvers for Hyperparameter Optimisation in Iterative Gaussian Processes',\n",
       "  494: 'Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning',\n",
       "  495: 'FT-AED: Benchmark Dataset for Early Freeway Traffic Anomalous Event Detection',\n",
       "  496: 'CoBo: Collaborative Learning via Bilevel Optimization',\n",
       "  497: 'FasMe: Fast and Sample-efficient Meta Estimator for Precision Matrix Learning in Small Sample Settings',\n",
       "  498: 'HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis',\n",
       "  499: 'MTGS: A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction',\n",
       "  500: 'Can Large Language Model Agents Simulate Human Trust Behavior?',\n",
       "  501: 'Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control',\n",
       "  502: 'EEVR: A Dataset of Paired Physiological Signals and Textual Descriptions for Joint Emotion Representation Learning',\n",
       "  503: \"Evaluating Large Vision-and-Language Models on Children's Mathematical Olympiads\",\n",
       "  504: 'Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack',\n",
       "  505: 'Observational Scaling Laws and the Predictability of Langauge Model Performance',\n",
       "  506: 'Embedding-Aligned Language Models',\n",
       "  507: 'How to Boost Any Loss Function',\n",
       "  508: 'Pearls from Pebbles: Improved Confidence Functions for Auto-labeling',\n",
       "  509: 'SHMT: Self-supervised Hierarchical Makeup Transfer via Latent Diffusion Models',\n",
       "  510: 'Mixture of Link Predictors on Graphs',\n",
       "  511: 'No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery',\n",
       "  512: 'Automated Efficient Estimation using Monte Carlo Efficient Influence Functions',\n",
       "  513: 'AR-Pro: Counterfactual Explanations for Anomaly Repair with Formal Properties',\n",
       "  514: 'Stability and Generalization of Adversarial Training for Shallow Neural Networks with Smooth Activation',\n",
       "  515: 'DPIC: Decoupling Prompt and Intrinsic Characteristics for LLM Generated Text Detection',\n",
       "  516: 'Melting Pot Contest: Charting the Future of Generalized Cooperative Intelligence',\n",
       "  517: 'Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control',\n",
       "  518: 'Text to Blind Motion',\n",
       "  519: 'Cryptographic Hardness of Score Estimation',\n",
       "  520: 'The Secretary Problem with Predicted Additive Gap',\n",
       "  521: 'SpecExec: Massively Parallel Speculative Decoding For Interactive LLM Inference on Consumer Devices',\n",
       "  522: 'From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When',\n",
       "  523: 'Are Large-scale Soft Labels Necessary for Large-scale Dataset Distillation?',\n",
       "  524: 'Linear Uncertainty Quantification of Graphical Model Inference',\n",
       "  525: '4+3 Phases of Compute-Optimal Neural Scaling Laws',\n",
       "  526: 'Learning Mixtures of Unknown Causal Interventions',\n",
       "  527: 'On the Noise Robustness of In-Context Learning for Text Generation',\n",
       "  528: 'Mercury: A Code Efficiency Benchmark for Code Large Language Models',\n",
       "  529: 'LVD-2M: A Long-take Video Dataset with Temporally Dense Captions',\n",
       "  530: 'Implicit Regularization of Decentralized Gradient Descent for Sparse Regression',\n",
       "  531: 'Loki: Low-rank Keys for Efficient Sparse Attention',\n",
       "  532: 'VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks',\n",
       "  533: 'ST$_k$: A Scalable Module for Solving Top-k Problems',\n",
       "  534: 'Non-geodesically-convex optimization in the Wasserstein space',\n",
       "  535: 'Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search',\n",
       "  536: 'Enhancing Preference-based Linear Bandits via Human Response Time',\n",
       "  537: 'Zero-Shot Reinforcement Learning from Low Quality Data',\n",
       "  538: 'Ordered Momentum for Asynchronous SGD',\n",
       "  539: 'GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages',\n",
       "  540: 'Real-Time Recurrent Learning using Trace Units in Reinforcement Learning',\n",
       "  541: 'NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples',\n",
       "  542: 'Abstract Reward Processes: Leveraging State Abstraction for Consistent Off-Policy Evaluation',\n",
       "  543: 'Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models',\n",
       "  544: 'A Globally Optimal Portfolio for m-Sparse Sharpe Ratio Maximization',\n",
       "  545: 'SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset',\n",
       "  546: 'Supra-Laplacian Encoding for Transformer on Dynamic Graphs',\n",
       "  547: 'Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation',\n",
       "  548: 'Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation',\n",
       "  549: 'Preference-based Pure Exploration',\n",
       "  550: 'Policy Optimization for Robust Average Reward MDPs',\n",
       "  551: 'Beyond Euclidean: Dual-Space Representation Learning for Weakly Supervised Video Violence Detection',\n",
       "  552: 'Learning Discrete Latent Variable Structures with Tensor Rank Conditions',\n",
       "  553: 'The Map Equation Goes Neural: Mapping Network Flows with Graph Neural Networks',\n",
       "  554: 'Locating What You Need: Towards Adapting Diffusion Models to OOD Concepts In-the-Wild',\n",
       "  555: 'ESPACE: Dimensionality Reduction of Activations for Model Compression',\n",
       "  556: 'Toxicity Detection for Free',\n",
       "  557: 'PERIA: Perceive, Reason, Imagine, Act via Holistic Language and Vision Planning for Manipulation',\n",
       "  558: 'CLIPAway: Harmonizing focused embeddings for removing objects via diffusion models',\n",
       "  559: 'Banded Square Root Matrix Factorization for Differentially Private Model Training',\n",
       "  560: 'Bayesian-guided Label Mapping for Visual Reprogramming',\n",
       "  561: 'Visual Anchors Are Strong Information Aggregators For Multimodal Large Language Model',\n",
       "  562: 'Combining Observational Data and Language for Species Range Estimation',\n",
       "  563: 'From an Image to a Scene: Learning to Imagine the World from a Million 360° Videos',\n",
       "  564: 'Multiview Scene Graph',\n",
       "  565: 'Fixed Confidence Best Arm Identification in the Bayesian Setting',\n",
       "  566: 'Mars: Situated Inductive Reasoning in an Open-World Environment',\n",
       "  567: 'Fairness-Aware Estimation of Graphical Models',\n",
       "  568: 'Addressing Spatial-Temporal Heterogeneity: General Mixed Time Series Analysis via Latent Continuity Recovery and Alignment',\n",
       "  569: 'Recursive PAC-Bayes: A Frequentist Approach to Sequential Prior Updates with No Information Loss',\n",
       "  570: 'SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations',\n",
       "  571: 'Towards Learning Group-Equivariant Features for Domain Adaptive 3D Detection',\n",
       "  572: 'Beyond Efficiency: Molecular Data Pruning for Enhanced Generalization',\n",
       "  573: 'Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models',\n",
       "  574: 'FedAvP: Augment Local Data via Shared Policy in Federated Learning',\n",
       "  575: 'The Representation Landscape of Few-Shot Learning and Fine-Tuning in Large Language Models',\n",
       "  576: 'Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification',\n",
       "  577: 'Curriculum Fine-tuning of Vision Foundation Model for Medical Image Classification Under Label Noise',\n",
       "  578: 'Inferring stochastic low-rank recurrent neural networks from neural data',\n",
       "  579: 'Cardinality-Aware Set Prediction and Top-$k$ Classification',\n",
       "  580: 'In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization',\n",
       "  581: 'Learning 3D Equivariant Implicit Function with Patch-Level Pose-Invariant Representation',\n",
       "  582: 'Communication Efficient Distributed Training with Distributed Lion',\n",
       "  583: 'Fairness and Efficiency in Online Class Matching',\n",
       "  584: 'Ad Auctions for LLMs via Retrieval Augmented Generation',\n",
       "  585: 'ReVideo: Remake a Video with Motion and Content Control',\n",
       "  586: 'Finding Transformer Circuits With Edge Pruning',\n",
       "  587: 'DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception',\n",
       "  588: 'Globally Convergent Variational Inference',\n",
       "  589: 'Auditing Local Explanations is Hard',\n",
       "  590: 'HumanVLA: Towards Vision-Language Directed Object Rearrangement by Physical Humanoid',\n",
       "  591: 'Efficient Reinforcement Learning by Discovering Neural Pathways',\n",
       "  592: 'SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation',\n",
       "  593: 'MeMo: Meaningful, Modular Controllers via Noise Injection',\n",
       "  594: 'FERERO: A Flexible Framework for Preference-Guided Multi-Objective Learning',\n",
       "  595: 'RGMDT: Return-Gap-Minimizing Decision Tree Extraction in Non-Euclidean Metric Space',\n",
       "  596: 'WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off',\n",
       "  597: 'Gene-Gene Relationship Modeling Based on Genetic Evidence for Single-Cell RNA-Seq Data Imputation',\n",
       "  598: 'Suppress Content Shift: Better Diffusion Features via Off-the-Shelf Generation Techniques',\n",
       "  599: 'Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting',\n",
       "  600: 'Image Reconstruction Via Autoencoding Sequential Deep Image Prior',\n",
       "  601: 'Invariant subspaces and PCA in nearly matrix multiplication time',\n",
       "  602: 'Boosting Graph Pooling with Persistent Homology',\n",
       "  603: 'OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary Understanding',\n",
       "  604: 'Proportional Fairness in Non-Centroid Clustering',\n",
       "  605: 'PaGoDA: Progressive Growing of a One-Step Generator from a Low-Resolution Diffusion Teacher',\n",
       "  606: 'OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI',\n",
       "  607: 'Interpretable Concept-Based Memory Reasoning',\n",
       "  608: 'Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion',\n",
       "  609: 'Evaluating the design space of diffusion-based generative models',\n",
       "  610: 'SOI: Scaling Down Computational Complexity by Estimating Partial States of the Model',\n",
       "  611: 'Robust Neural Contextual Bandit against Adversarial Corruptions',\n",
       "  612: 'An Expectation-Maximization Algorithm for Training Clean Diffusion Models from Corrupted Observations',\n",
       "  613: 'ShareGPT4Video: Improving Video Understanding and Generation with Better Captions',\n",
       "  614: 'Beyond Slow Signs in High-fidelity Model Extraction',\n",
       "  615: 'FUGAL: Feature-fortified Unrestricted Graph Alignment',\n",
       "  616: 'Revisiting Few-Shot Object Detection with Vision-Language Models',\n",
       "  617: 'Progressive Entropic Optimal Transport Solvers',\n",
       "  618: 'Optimizing the coalition gain in Online Auctions with Greedy Structured Bandits',\n",
       "  619: 'DEFT: Efficient Fine-tuning of Diffusion Models by Learning the Generalised $h$-transform',\n",
       "  620: 'LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate',\n",
       "  621: 'Achieving Near-Optimal Convergence for Distributed Minimax Optimization with Adaptive Stepsizes',\n",
       "  622: 'Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving',\n",
       "  623: 'Complete Graphical Criterion for Sequential Covariate Adjustment in Causal Inference',\n",
       "  624: 'RETR: Multi-View Radar Detection Transformer for Indoor Perception',\n",
       "  625: 'On the Scalability of GNNs for Molecular Graphs',\n",
       "  626: 'Improving Viewpoint-Independent Object-Centric Representations through Active Viewpoint Selection',\n",
       "  627: 'Aggregating Quantitative Relative Judgments: From Social Choice to Ranking Prediction',\n",
       "  628: 'Task Me Anything',\n",
       "  629: 'Vision Foundation Model Enables Generalizable Object Pose Estimation',\n",
       "  630: 'Curvature Clues: Decoding Deep Learning Privacy with Input Loss Curvature',\n",
       "  631: 'Exploring Fixed Point in Image Editing: Theoretical Support and Convergence Optimization',\n",
       "  632: 'MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models',\n",
       "  633: 'Efficient Streaming Algorithms for Graphlet Sampling',\n",
       "  634: 'HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation',\n",
       "  635: 'Semi-Supervised Sparse Gaussian Classification: Provable Benefits of Unlabeled Data',\n",
       "  636: 'DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation',\n",
       "  637: 'Grounding Multimodal Large Language Models in Actions',\n",
       "  638: 'Optimal Algorithms for Learning Partitions with Faulty Oracles',\n",
       "  639: 'MemoryFormer : Minimize Transformer Computation by Removing Fully-Connected Layers',\n",
       "  640: 'LCGen: Mining in Low-Certainty Generation for View-consistent Text-to-3D',\n",
       "  641: 'Learning and Transferring Sparse Contextual Bigrams with Linear Transformers',\n",
       "  642: 'Predictor-Corrector Enhanced Transformers with Exponential Moving Average Coefficient Learning',\n",
       "  643: 'Public-data Assisted Private Stochastic Optimization: Power and Limitations',\n",
       "  644: 'Globally Q-linear Gauss-Newton Method for Overparameterized Non-convex Matrix Sensing',\n",
       "  645: 'Implicit Curriculum in Procgen Made Explicit',\n",
       "  646: 'Easy Regional Contrastive Learning of Expressive Fashion Representations',\n",
       "  647: 'Gradient Methods for Online DR-Submodular Maximization with Stochastic Long-Term Constraints',\n",
       "  648: 'Needle In A Multimodal Haystack',\n",
       "  649: 'A benchmark for prediction of transcriptomic responses to chemical perturbations across cell types',\n",
       "  650: 'General Articulated Objects Manipulation in Real Images via Part-Aware Diffusion Process',\n",
       "  651: 'Edit Distance Robust Watermarks via Indexing Pseudorandom Codes',\n",
       "  652: 'Nesterov acceleration despite very noisy gradients',\n",
       "  653: 'DiffAug: A Diffuse-and-Denoise Augmentation for Training Robust Classifiers',\n",
       "  654: 'Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective',\n",
       "  655: 'Active Learning with LLMs for Partially Observed and Cost-Aware Scenarios',\n",
       "  656: 'Local Superior Soups: A Catalyst for Model Merging in Cross-Silo Federated Learning',\n",
       "  657: 'Boosting the Transferability of Adversarial Attack on Vision Transformer with Adaptive Token Tuning',\n",
       "  658: 'Maia-2: A Unified Model for Human-AI Alignment in Chess',\n",
       "  659: 'Derivative-enhanced Deep Operator Network',\n",
       "  660: 'No-Regret Bandit Exploration based on Soft Tree Ensemble Model',\n",
       "  661: 'A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks',\n",
       "  662: 'Bridging semantics and pragmatics in information-theoretic emergent communication',\n",
       "  663: 'Watermarking Makes Language Models Radioactive',\n",
       "  664: 'Approximation-Aware Bayesian Optimization',\n",
       "  665: 'Differential Privacy in Scalable General Kernel Learning via $K$-means Nystr{\\\\\"o}m Random Features',\n",
       "  666: 'Taming Generative Diffusion Prior for Universal Blind Image Restoration',\n",
       "  667: 'Discrete Dictionary-based Decomposition Layer for Structured Representation Learning',\n",
       "  668: 'ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation',\n",
       "  669: 'WildGaussians: 3D Gaussian Splatting In the Wild',\n",
       "  670: 'What If the Input is Expanded in OOD Detection?',\n",
       "  671: 'RelBench: A Benchmark for Deep Learning on Relational Databases',\n",
       "  672: 'PageRank Bandits for Link Prediction',\n",
       "  673: 'DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation',\n",
       "  674: 'Spatio-Temporal Interactive Learning for Efficient Image Reconstruction of Spiking Cameras',\n",
       "  675: 'Acceleration Exists! Optimization Problems When Oracle Can Only Compare Objective Function Values',\n",
       "  676: 'Towards the Transferability of Rewards Recovered via Regularized Inverse Reinforcement Learning',\n",
       "  677: 'OneActor: Consistent Subject Generation via Cluster-Conditioned Guidance',\n",
       "  678: 'Injecting Undetectable Backdoors in Obfuscated Neural Networks and Language Models',\n",
       "  679: 'Nimbus: Secure and Efficient Two-Party Inference for Transformers',\n",
       "  680: 'SureMap: Simultaneous mean estimation for single-task and multi-task disaggregated evaluation',\n",
       "  681: 'Is Function Similarity Over-Engineered? Building a Benchmark',\n",
       "  682: 'FUSE: Fast Unified Simulation and Estimation for PDEs',\n",
       "  683: 'The Collusion of Memory and Nonlinearity in Stochastic Approximation With Constant Stepsize',\n",
       "  684: 'BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices',\n",
       "  685: 'Interventionally Consistent Surrogates for Complex Simulation Models',\n",
       "  686: 'PEACE: A Dataset of Pharmaceutical Care for Cancer Pain Analgesia Evaluation and Medication Decision',\n",
       "  687: 'Depth Anything V2',\n",
       "  688: 'Transferring disentangled representations: bridging the gap between synthetic and real images',\n",
       "  689: 'Absorb & Escape: Overcoming Single Model Limitations in Generating Heterogeneous Genomic Sequences',\n",
       "  690: 'Can We Leave Deepfake Data Behind in Training Deepfake Detector?',\n",
       "  691: 'RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation',\n",
       "  692: 'UNION: Unsupervised 3D Object Detection using Object Appearance-based Pseudo-Classes',\n",
       "  693: 'Adaptive Variance Reduction for Stochastic Optimization under Weaker Assumptions',\n",
       "  694: 'Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement Learning',\n",
       "  695: 'SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey',\n",
       "  696: 'Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning',\n",
       "  697: 'Quadratic Quantum Variational Monte Carlo',\n",
       "  698: 'Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics',\n",
       "  699: 'Rethinking the Power of Timestamps for Robust Time Series Forecasting: A Global-Local Fusion Perspective',\n",
       "  700: 'Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure',\n",
       "  701: 'Advancing Fine-Grained Classification by Structure and Subject Preserving Augmentation',\n",
       "  702: 'RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling',\n",
       "  703: 'TFG: Unified Training-Free Guidance for Diffusion Models',\n",
       "  704: 'Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models',\n",
       "  705: 'Is Your HD Map Constructor Reliable under Sensor Corruptions?',\n",
       "  706: 'Efficient multi-prompt evaluation of LLMs',\n",
       "  707: 'FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations',\n",
       "  708: 'Learning the Expected Core of Strictly Convex Stochastic Cooperative Games',\n",
       "  709: 'ParallelEdits: Efficient Multi-Aspect Text-Driven Image Editing with Attention Grouping',\n",
       "  710: 'Looks Too Good To Be True: An Information-Theoretic Analysis of Hallucinations in Generative Restoration Models',\n",
       "  711: 'Implicit Optimization Bias of Next-token Prediction in Linear Models',\n",
       "  712: 'Not so griddy: Internal representations of RNNs path integrating more than one agent',\n",
       "  713: 'Out-Of-Distribution Detection with Diversification (Provably)',\n",
       "  714: 'Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using Discrete State Space Diffusion Model',\n",
       "  715: 'QVAE-Mole: The Quantum VAE with Spherical Latent Variable Learning for 3-D Molecule Generation',\n",
       "  716: 'Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement Learning',\n",
       "  717: 'Decision Mamba: A Multi-Grained State Space Model with Self-Evolution Regularization for Offline RL',\n",
       "  718: 'Distribution Learning with Valid Outputs Beyond the Worst-Case',\n",
       "  719: 'FairQueue: Rethinking Prompt Learning for Fair Text-to-Image Generation',\n",
       "  720: 'ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs',\n",
       "  721: 'SnapKV: LLM Knows What You are Looking for Before Generation',\n",
       "  722: 'Boosting the Potential of Large Language Models with an Intelligent Information Assistant',\n",
       "  723: 'Parallel Backpropagation for Shared-Feature Visualization',\n",
       "  724: 'A two-scale Complexity Measure for Deep Learning Models',\n",
       "  725: 'Communication-Efficient Federated Group Distributionally Robust Optimization',\n",
       "  726: 'On Differentially Private U Statistics',\n",
       "  727: 'Sketched Lanczos uncertainty score: a low-memory summary of the Fisher information',\n",
       "  728: 'Unified Generative and Discriminative Training for Multi-modal Large Language Models',\n",
       "  729: 'Why Do We Need Weight Decay in Modern Deep Learning?',\n",
       "  730: 'Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization',\n",
       "  731: 'On conditional diffusion models for PDE simulations',\n",
       "  732: 'Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers',\n",
       "  733: 'Generalizing Weather Forecast to Fine-grained Temporal Scales via Physics-AI Hybrid Modeling',\n",
       "  734: 'Cross-Modality Perturbation Synergy Attack for Person Re-identification',\n",
       "  735: 'NeuroBOLT: Resting-state EEG-to-fMRI Synthesis with Multi-dimensional Feature Mapping',\n",
       "  736: 'Fast Last-Iterate Convergence of Learning in Games Requires Forgetful Algorithms',\n",
       "  737: 'Coherent 3D Scene Diffusion From a Single RGB Image',\n",
       "  738: 'DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs',\n",
       "  739: 'Neuro-Symbolic Data Generation for Math Reasoning',\n",
       "  740: 'Consistency Diffusion Bridge Models',\n",
       "  741: 'Dense Associative Memory Through the Lens of Random Features',\n",
       "  742: 'A Simple yet Universal Framework for Depth Completion',\n",
       "  743: 'Identifying Equivalent Training Dynamics',\n",
       "  744: 'DRACO: A Denoising-Reconstruction Autoencoder for Cryo-EM',\n",
       "  745: 'Human-Object Interaction Detection Collaborated with Large Relation-driven Diffusion Models',\n",
       "  746: 'Ensemble sampling for linear bandits: small ensembles suffice',\n",
       "  747: 'A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer Problems',\n",
       "  748: 'Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias',\n",
       "  749: 'If You Want to Be Robust, Be Wary of Initialization',\n",
       "  750: 'Is Score Matching Suitable for Estimating Point Processes?',\n",
       "  751: 'Is Knowledge Power? On the (Im)possibility of Learning from Strategic Interactions',\n",
       "  752: 'Data Free Backdoor Attacks',\n",
       "  753: 'Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning',\n",
       "  754: 'Linear Causal Bandits: Unknown Graph and Soft Interventions',\n",
       "  755: 'The Implicit Bias of Adam on Separable Data',\n",
       "  756: \"Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs\",\n",
       "  757: 'SIRIUS : Contexual Sparisty with Correction for Efficient LLMs',\n",
       "  758: 'Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion',\n",
       "  759: 'MiSO: Optimizing brain stimulation to create neural activity states',\n",
       "  760: 'Ferrari: Federated Feature Unlearning via Optimizing Feature Sensitivity',\n",
       "  761: 'Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in LLMs',\n",
       "  762: 'Causal Discovery from Event Sequences by Local Cause-Effect Attribution',\n",
       "  763: 'RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content',\n",
       "  764: 'Corruption-Robust Linear Bandits: Minimax Optimality and Gap-Dependent Misspecification',\n",
       "  765: 'Structured Matrix Basis for Multivariate Time Series Forecasting with Interpretable Dynamics',\n",
       "  766: 'PAC-Bayes-Chernoff bounds for unbounded losses',\n",
       "  767: 'Transcoders find interpretable LLM feature circuits',\n",
       "  768: 'SymILO: A Symmetry-Aware Learning Framework for Integer Linear Optimization',\n",
       "  769: 'Bias in Motion: Theoretical Insights into the Dynamics of Bias in SGD Training',\n",
       "  770: 'Long-range Brain Graph Transformer',\n",
       "  771: 'One-to-Multiple: A Progressive Style Transfer Unsupervised Domain-Adaptive Framework for Kidney Tumor Segmentation',\n",
       "  772: 'CLIP in Mirror: Disentangling text from visual images through reflection',\n",
       "  773: 'Grammar-Aligned Decoding',\n",
       "  774: 'Controlled maximal variability along with reliable performance in recurrent neural networks',\n",
       "  775: 'Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models',\n",
       "  776: 'Global Rewards in Restless Multi-Armed Bandits',\n",
       "  777: 'A Comprehensive Analysis on the Learning Curve in Kernel Ridge Regression',\n",
       "  778: 'Statistical Efficiency of Distributional Temporal Difference Learning',\n",
       "  779: 'Faster Repeated Evasion Attacks in Tree Ensembles',\n",
       "  780: 'DiPEx: Dispersing Prompt Expansion for Class-Agnostic Object Detection',\n",
       "  781: 'ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization',\n",
       "  782: 'Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks',\n",
       "  783: 'Pedestrian-Centric 3D Pre-collision Pose and Shape Estimation from Dashcam Perspective',\n",
       "  784: 'Aligning Diffusion Models by Optimizing Human Utility',\n",
       "  785: 'BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models',\n",
       "  786: \"Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks using the Marginal Likelihood\",\n",
       "  787: 'First-Order Minimax Bilevel Optimization',\n",
       "  788: 'Sample Complexity of Algorithm Selection Using Neural Networks and Its Applications to Branch-and-Cut',\n",
       "  789: 'LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models',\n",
       "  790: 'A Simple Image Segmentation Framework via In-Context Examples',\n",
       "  791: 'Pre-trained Large Language Models Use Fourier Features to Compute Addition',\n",
       "  792: 'Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators',\n",
       "  793: 'ReMAP: Neural Model Reprogramming with Network Inversion and Retrieval-Augmented Mapping for Adaptive Motion Forecasting',\n",
       "  794: 'Cross-video Identity Correlating for Person Re-identification Pre-training',\n",
       "  795: 'Cloud Object Detector Adaptation by Integrating Different Source Knowledge',\n",
       "  796: 'DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning',\n",
       "  797: 'Mirror and Preconditioned Gradient Descent in Wasserstein Space',\n",
       "  798: 'Retrieval-Retro: Retrieval-based Inorganic Retrosynthesis with Expert Knowledge',\n",
       "  799: 'Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm',\n",
       "  800: 'MathPile: A Billion-Token-Scale Pretraining Corpus for Math',\n",
       "  801: 'Generative Fractional Diffusion Models',\n",
       "  802: 'Least Squares Regression Can Exhibit Under-Parameterized Double Descent',\n",
       "  803: 'AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation',\n",
       "  804: 'Decentralized Noncooperative Games with Coupled Decision-Dependent Distributions',\n",
       "  805: 'Geometric Trajectory Diffusion Models',\n",
       "  806: 'GoMatching: A Simple Baseline for Video Text Spotting via Long and Short Term Matching',\n",
       "  807: 'Multi-Scale VMamba: Hierarchy in Hierarchy Visual State Space Model',\n",
       "  808: 'AdanCA: Neural Cellular Automata As Adaptors For More Robust Vision Transformer',\n",
       "  809: 'PhyRecon: Physically Plausible Neural Scene Reconstruction',\n",
       "  810: 'Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling',\n",
       "  811: 'A Combinatorial Algorithm for the Semi-Discrete Optimal Transport Problem',\n",
       "  812: 'Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep Learning Era',\n",
       "  813: 'Learning 1D Causal Visual Representation with De-focus Attention Networks',\n",
       "  814: 'Nonlinear dynamics of localization in neural receptive fields',\n",
       "  815: 'Learning-Augmented Approximation Algorithms for Maximum Cut and Related Problems',\n",
       "  816: 'AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning',\n",
       "  817: 'Score Distillation via Reparametrized DDIM',\n",
       "  818: 'UltraMedical: Building Specialized Generalists in Biomedicine',\n",
       "  819: 'Smoke and Mirrors in Causal Downstream Tasks',\n",
       "  820: 'Newton Losses: Using Curvature Information for Learning with Differentiable Algorithms',\n",
       "  821: 'Capturing the denoising effect of PCA via compression ratio',\n",
       "  822: 'Automated Multi-level Preference for MLLMs',\n",
       "  823: 'What Matters in Graph Class Incremental Learning? An Information Preservation Perspective',\n",
       "  824: 'LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages',\n",
       "  825: 'Are High-Degree Representations Really Unnecessary in Equivariant Graph Neural Networks?',\n",
       "  826: 'Improving Decision Sparsity',\n",
       "  827: 'Cluster-Learngene: Inheriting Adaptive Clusters for Vision Transformers',\n",
       "  828: 'Multi-Instance Partial-Label Learning with Margin Adjustment',\n",
       "  829: 'Geometric Exploitation for Indoor Panoramic Semantic Segmentation',\n",
       "  830: 'Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement',\n",
       "  831: 'ReactZyme: A Benchmark for Enzyme-Reaction Prediction',\n",
       "  832: 'Policy Mirror Descent with Lookahead',\n",
       "  833: 'Generalization of Hamiltonian algorithms',\n",
       "  834: 'Stochastic Optimization Algorithms for Instrumental Variable Regression with Streaming Data',\n",
       "  835: 'Constrained Diffusion Models via Dual Training',\n",
       "  836: 'Better by default: Strong pre-tuned MLPs and boosted trees on tabular data',\n",
       "  837: 'DAPE: Data-Adaptive Positional Encoding for Length Extrapolation',\n",
       "  838: 'LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential Recommendation',\n",
       "  839: 'Achieving Tractable Minimax Optimal Regret in Average Reward MDPs',\n",
       "  840: 'Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction',\n",
       "  841: 'When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search',\n",
       "  842: 'Diffusion-based  Layer-wise Semantic Reconstruction for Unsupervised Out-of-Distribution Detection',\n",
       "  843: 'Fast samplers for Inverse Problems in Iterative Refinement models',\n",
       "  844: 'Advancing Spiking Neural Networks for Sequential Modeling with Central Pattern Generators',\n",
       "  845: 'Evaluating the World Model Implicit in a Generative Model',\n",
       "  846: 'Contextual Linear Optimization with Bandit Feedback',\n",
       "  847: 'A Surprisingly Simple Approach to Generalized Few-Shot Semantic Segmentation',\n",
       "  848: 'Towards Open Respiratory Acoustic Foundation Models: Pretraining and Benchmarking',\n",
       "  849: 'Are We on the Right Way for Evaluating Large Vision-Language Models?',\n",
       "  850: 'On the Benefits of Public Representations for Private Transfer Learning under Distribution Shift',\n",
       "  851: 'Reparameterized Multi-Resolution Convolutions for Long Sequence Modelling',\n",
       "  852: 'Streaming Bayes GFlowNets',\n",
       "  853: 'Inverse Factorized Soft Q-Learning for Cooperative Multi-agent Imitation Learning',\n",
       "  854: 'Learning from Pattern Completion: Self-supervised Controllable Generation',\n",
       "  855: '4DBInfer:  A 4D Benchmarking Toolbox for Graph-Centric Predictive Modeling on RDBs',\n",
       "  856: 'Diffeomorphic interpolation for efficient persistence-based topological optimization',\n",
       "  857: 'A theoretical case-study of Scalable Oversight in Hierarchical Reinforcement Learning',\n",
       "  858: 'Exploring Low-Dimensional Subspace in Diffusion Models for Controllable Image Editing',\n",
       "  859: 'Efficiency for Free: Ideal Data Are Transportable Representations',\n",
       "  860: 'Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction',\n",
       "  861: 'MambaLLIE: Implicit Retinex-Aware Low Light Enhancement with Global-then-Local State Space',\n",
       "  862: 'On Softmax Direct Preference Optimization for Recommendation',\n",
       "  863: 'First-Explore, then Exploit: Meta-Learning to Solve Hard Exploration-Exploitation Trade-Offs',\n",
       "  864: 'GenRL: Multimodal-foundation world models for generalization in embodied agents',\n",
       "  865: 'Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing',\n",
       "  866: 'Learning Truncated Causal History Model for Video Restoration',\n",
       "  867: 'UDPM: Upsampling Diffusion Probabilistic Models',\n",
       "  868: 'Rethinking Inverse Reinforcement Learning: from Data Alignment to Task Alignment',\n",
       "  869: 'AlphaMath Almost Zero: Process Supervision without Process',\n",
       "  870: 'Optimal deep learning of holomorphic operators between Banach spaces',\n",
       "  871: 'Multi-Agent Imitation Learning: Value is Easy, Regret is Hard',\n",
       "  872: 'Constrained Adaptive Attack: Effective Adversarial Attack Against Deep Neural Networks for Tabular Data',\n",
       "  873: 'How Far Can Transformers Reason? The Globality Barrier and Inductive Scratchpad',\n",
       "  874: 'FineCLIP: Self-distilled Region-based CLIP for Better Fine-grained Understanding',\n",
       "  875: 'Generative Forests',\n",
       "  876: 'Scalable DBSCAN with Random Projections',\n",
       "  877: 'Near-Optimal Distributed Minimax Optimization under the Second-Order Similarity',\n",
       "  878: 'M$^3$GPT: An Advanced Multimodal, Multitask Framework  for Motion Comprehension and Generation',\n",
       "  879: 'From Transparent to Opaque: Rethinking Neural Implicit Surfaces with $\\\\alpha$-NeuS',\n",
       "  880: 'Test Where Decisions Matter: Importance-driven Testing for Deep Reinforcement Learning',\n",
       "  881: \"Don't Look Twice: Faster Video Transformers with Run-Length Tokenization\",\n",
       "  882: 'Value-Based Deep Multi-Agent Reinforcement Learning with Dynamic Sparse Training',\n",
       "  883: 'Rethinking Memory and Communication Costs for Efficient Data Parallel Training of Large Language Models',\n",
       "  884: 'GTBench: Uncovering the Strategic Reasoning Capabilities of LLMs via Game-Theoretic Evaluations',\n",
       "  885: 'DMNet: Self-comparison Driven Model for Subject-independent Seizure Detection',\n",
       "  886: 'OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation',\n",
       "  887: 'Perplexity-aware Correction for Robust Alignment with Noisy Preferences',\n",
       "  888: 'The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof',\n",
       "  889: 'Probabilistic Graph Rewiring via Virtual Nodes',\n",
       "  890: 'FuseFL: One-Shot Federated Learning through the Lens of Causality with Progressive Model Fusion',\n",
       "  891: 'RL-GPT: Integrating Reinforcement Learning and Code-as-policy',\n",
       "  892: 'A Layer-Wise Natural Gradient Optimizer for Training Deep Neural Networks',\n",
       "  893: 'Multi-Chain Graphs of Graphs: A New Approach to Analyzing Blockchain Datasets',\n",
       "  894: 'Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning',\n",
       "  895: 'Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm',\n",
       "  896: 'MoEUT: Mixture-of-Experts Universal Transformers',\n",
       "  897: 'Visual Perception by Large Language Model’s Weights',\n",
       "  898: 'Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective',\n",
       "  899: 'ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos',\n",
       "  900: '$C^2M^3$: Cycle-Consistent Multi-Model Merging',\n",
       "  901: 'NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking',\n",
       "  902: 'LaKD: Length-agnostic Knowledge Distillation for Trajectory Prediction with Any Length Observations',\n",
       "  903: 'Stochastic Optimal Control for Diffusion Bridges in Function Spaces',\n",
       "  904: 'Enhancing LLM Reasoning via Vision-Augmented Prompting',\n",
       "  905: 'MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs',\n",
       "  906: 'LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding',\n",
       "  907: 'MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning',\n",
       "  908: 'QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization',\n",
       "  909: 'AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis',\n",
       "  910: 'Multi-LLM Debate: Framework, Principals, and Interventions',\n",
       "  911: 'MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts',\n",
       "  912: 'Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation',\n",
       "  913: 'Not All Tokens Are What You Need for Pretraining',\n",
       "  914: 'A2PO: Towards Effective Offline Reinforcement Learning from an Advantage-aware Perspective',\n",
       "  915: 'LAVIB: A Large-scale Video Interpolation Benchmark',\n",
       "  916: 'Chain of Thoughtlessness? An Analysis of CoT in Planning',\n",
       "  917: 'Face2QR: A Unified Framework for Aesthetic, Face-Preserving, and Scannable QR Code Generation',\n",
       "  918: 'A Critical Evaluation of AI Feedback for Aligning Large Language Models',\n",
       "  919: 'MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution',\n",
       "  920: 'Global Convergence in Training Large-Scale Transformers',\n",
       "  921: 'Constrained Sampling with Primal-Dual Langevin Monte Carlo',\n",
       "  922: 'Scalable Neural Network Verification with Branch-and-bound Inferred Cutting Planes',\n",
       "  923: 'MoLE: Enhancing Human-centric Text-to-image Diffusion via Mixture of  Low-rank Experts',\n",
       "  924: 'DF40: Toward Next-Generation Deepfake Detection',\n",
       "  925: 'Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models',\n",
       "  926: 'C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory',\n",
       "  927: 'VideoTetris: Towards Compositional Text-to-Video Generation',\n",
       "  928: 'Kermut: Composite kernel regression for protein variant effects',\n",
       "  929: 'Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech Foundation Models',\n",
       "  930: 'HOPE: Shape Matching Via Aligning Different K-hop Neighbourhoods',\n",
       "  931: 'SubgDiff: A Subgraph Diffusion Model to Improve Molecular Representation Learning',\n",
       "  932: 'Non-parametric classification via expand-and-sparsify representation',\n",
       "  933: 'Adversarially Robust Decision Transformer',\n",
       "  934: 'Protecting Your LLMs with Information Bottleneck',\n",
       "  935: 'ActAnywhere: Subject-Aware Video Background Generation',\n",
       "  936: 'Time-Reversal Provides Unsupervised Feedback to LLMs',\n",
       "  937: 'UQE: A Query Engine for Unstructured Databases',\n",
       "  938: 'Tactile DreamFusion: Exploiting Tactile Sensing for 3D Generation',\n",
       "  939: 'Hints-In-Browser: Benchmarking Language Models for Programming Feedback Generation',\n",
       "  940: 'Model Fusion through Bayesian Optimization in Language Model Fine-Tuning',\n",
       "  941: 'Mind the Graph When Balancing Data for Fairness or Robustness',\n",
       "  942: 'RAGraph: A General Retrieval-Augmented Graph Learning Framework',\n",
       "  943: \"On Giant's Shoulders: Effortless Weak to Strong by Dynamic Logits Fusion\",\n",
       "  944: 'Measuring Dejavu Memorization Efficiently',\n",
       "  945: 'Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML',\n",
       "  946: 'Re-assembling the past: The RePAIR dataset and benchmark for real world 2D and 3D puzzle solving',\n",
       "  947: 'Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models',\n",
       "  948: 'Using Time-Aware Graph Neural Networks to Predict Temporal Centralities in Dynamic Graphs',\n",
       "  949: 'Fast T2T: Optimization Consistency Speeds Up Diffusion-Based Training-to-Testing Solving for Combinatorial Optimization',\n",
       "  950: 'Combining Statistical Depth and Fermat Distance for Uncertainty Quantification',\n",
       "  951: 'Learning Human-like Representations to Enable Learning Human Values',\n",
       "  952: 'Implicit Regularization Paths of Weighted Neural Representations',\n",
       "  953: 'Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model',\n",
       "  954: 'MV2Cyl: Reconstructing 3D Extrusion Cylinders from Multi-View Images',\n",
       "  955: 'Hierarchical and Density-based Causal Clustering',\n",
       "  956: 'How to Use Diffusion Priors under Sparse Views?',\n",
       "  957: 'Exponential Quantum Communication Advantage in Distributed Inference and Learning',\n",
       "  958: 'Towards Efficient and Optimal Covariance-Adaptive Algorithms for Combinatorial Semi-Bandits',\n",
       "  959: 'Zero-to-Hero: Enhancing Zero-Shot Novel View Synthesis via Attention Map Filtering',\n",
       "  960: 'PrefPaint: Aligning Image Inpainting Diffusion Model with Human Preference',\n",
       "  961: 'Federated Learning from Vision-Language Foundation Models: Theoretical Analysis and Method',\n",
       "  962: 'SciCode: A Research Coding Benchmark Curated by Scientists',\n",
       "  963: 'Binarized Diffusion Model for Image Super-Resolution',\n",
       "  964: 'Micro-Bench: A Microscopy Benchmark for Vision-Language Understanding',\n",
       "  965: 'Delving into the Reversal Curse: How Far Can Large Language Models Generalize?',\n",
       "  966: 'Language Without Borders: A Dataset and Benchmark for Code-Switching Lip Reading',\n",
       "  967: 'On the Parameter Identifiability of Partially Observed Linear Causal Models',\n",
       "  968: 'Accelerating Transformers with Spectrum-Preserving Token Merging',\n",
       "  969: 'The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale',\n",
       "  970: 'Should We Really Edit Language Models? On the Evaluation of Edited Language Models',\n",
       "  971: 'Model-free Low-Rank Reinforcement Learning via Leveraged Entry-wise Matrix Estimation',\n",
       "  972: 'Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment',\n",
       "  973: 'Solving Minimum-Cost Reach Avoid using Reinforcement Learning',\n",
       "  974: 'Toward a Well-Calibrated Discrimination via Survival Outcome-Aware Contrastive Learning',\n",
       "  975: 'Image Understanding Makes for A Good Tokenizer for Image Generation',\n",
       "  976: 'Inverse M-Kernels for Linear Universal Approximators of Non-Negative Functions',\n",
       "  977: 'Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models',\n",
       "  978: 'Towards Combating Frequency Simplicity-biased Learning for Domain Generalization',\n",
       "  979: 'Generated and Pseudo Content guided Prototype Refinement for Few-shot Point Cloud Segmentation',\n",
       "  980: 'Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training',\n",
       "  981: 'Estimating the Hallucination Rate of Generative AI',\n",
       "  982: 'Do causal predictors generalize better to new domains?',\n",
       "  983: 'Computation-Aware Gaussian Processes: Model Selection And Linear-Time Inference',\n",
       "  984: 'Frequency Adaptive Normalization For Non-stationary Time Series Forecasting',\n",
       "  985: 'Towards Human-AI Complementarity with Prediction Sets',\n",
       "  986: 'Evidence of Learned Look-Ahead in a Chess-Playing Neural Network',\n",
       "  987: 'Universal Online Convex Optimization with $1$ Projection per Round',\n",
       "  988: 'Towards Croppable Implicit Neural Representations',\n",
       "  989: 'EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular Data Classification via Large Language Models',\n",
       "  990: 'Fit for our purpose, not yours: Benchmark for a low-resource, Indigenous language',\n",
       "  991: 'On Statistical Rates and  Provably Efficient Criteria of Latent Diffusion Transformers (DiTs)',\n",
       "  992: 'Beyond Redundancy: Information-aware Unsupervised Multiplex Graph Structure Learning',\n",
       "  993: 'Active learning of neural population dynamics using two-photon holographic optogenetics',\n",
       "  994: 'DARNet: Dual Attention Refinement Network with Spatiotemporal Construction for Auditory Attention Detection',\n",
       "  995: 'High-dimensional (Group) Adversarial Training in Linear Regression',\n",
       "  996: 'Enhancing Robustness of Graph Neural Networks on Social Media with Explainable Inverse Reinforcement Learning',\n",
       "  997: 'Interpretable Mesomorphic Networks for Tabular Data',\n",
       "  998: 'Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models',\n",
       "  999: 'GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps',\n",
       "  ...},\n",
       " 'authors': {0: 'Ionut-Vlad Modoranu, Mher Safaryan, Grigory Malinovsky, Eldar Kurtic, Thomas Robert, Peter Richtárik, Dan Alistarh',\n",
       "  1: 'Yanbin Wei, Shuai Fu, Weisen Jiang, Zejian Zhang, Zhixiong Zeng, Qi Wu, James T. Kwok, Yu Zhang',\n",
       "  2: 'Changhoon Song, Yesom Park, Myungjoo Kang',\n",
       "  3: 'Zikai Xiong, Niccolò Dalmasso, Shubham Sharma, Freddy Lecue, Daniele Magazzeni, Vamsi K. Potluru, Tucker Balch, Manuela Veloso',\n",
       "  4: 'Yuanyu Wan, Chang Yao, Mingli Song, Lijun Zhang',\n",
       "  5: 'Tomas Rigaux, Hisashi Kashima',\n",
       "  6: 'Ying Cheng, Yang Li, Junjie He, Rui Feng',\n",
       "  7: 'Markus Pettersen, Frederik Rogge, Mikkel Elle Lepperød',\n",
       "  8: 'Chih-Hung Liu, Gleb Novikov',\n",
       "  9: 'Hancheng Ye, Jiakang Yuan, Renqiu Xia, Xiangchao Yan, Tao Chen, Junchi Yan, Botian Shi, Bo Zhang',\n",
       "  10: 'Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, Min Lin',\n",
       "  11: 'Shangzi Xue, Zhenya Huang, Jiayu Liu, Xin lin, Yuting Ning, Binbin Jin, Xin Li, Qi Liu',\n",
       "  12: 'Jiesong Liu, Feng Zhang, Jiawei Guan, Xipeng Shen',\n",
       "  13: 'Audrey Huang, Nan Jiang',\n",
       "  14: 'Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Guo Qin, Haoran Zhang, Yong Liu, Yunzhong Qiu, Jianmin Wang, Mingsheng Long',\n",
       "  15: 'Mohamed Elrefaie, Florin Morar, Angela Dai, Faez Ahmed',\n",
       "  16: 'Yuxuan Duan, Yan Hong, Bo Zhang, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang, Li Niu, Liqing Zhang',\n",
       "  17: 'Weibo Gao, Qi Liu, Linan Yue, Fangzhou Yao, Hao Wang, Yin Gu, Zheng Zhang',\n",
       "  18: 'Minghao Chen, Yihang Li, Yanting Yang, Shiyu Yu, Binbin Lin, Xiaofei He',\n",
       "  19: 'Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu',\n",
       "  20: 'Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, Baining Guo',\n",
       "  21: 'Andrew Davison, S. Carlyle Morgan, Owen G. Ward',\n",
       "  22: 'Zijie Huang, Wanjia Zhao, Jingdong Gao, Ziniu Hu, Xiao Luo, Yadi Cao, Yuanzhou Chen, Yizhou Sun, Wei Wang',\n",
       "  23: 'Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, János Kramár, Rohin Shah, Neel Nanda',\n",
       "  24: 'Xiaosong Jia, Zhenjie Yang, Qifeng Li, Zhiyuan Zhang, Junchi Yan',\n",
       "  25: 'Gautham Vasan, Mohamed Elsayed, Alireza Azimi, Jiamin He, Fahim Shariar, Colin Bellinger, Martha White, A. Rupam Mahmood',\n",
       "  26: 'Ruifeng Ren, Yong Liu',\n",
       "  27: 'Nicholas Babaev, Kirill Tamogashev, Azat Saginbaev, Ivan Shchekotov, Hanbin Bae, Hosang Sung, WonJun Lee, Hoon-Young Cho, Pavel Andreev',\n",
       "  28: 'Can Jin, Tong Che, Hongwu Peng, Yiyuan Li, Dimitris N. Metaxas, Marco Pavone',\n",
       "  29: 'Wuxuan Shi, Mang Ye',\n",
       "  30: 'Matthew V Macfarlane, Edan Toledo, Donal Byrne, Paul Duckworth, Alexandre Laterre',\n",
       "  31: 'Jin Woo Lee, Jaehyun Park, Min Jun Choi, Kyogu Lee',\n",
       "  32: 'Jiaqi Wang, Xiaochen Wang, Lingjuan Lyu, Jinghui Chen, Fenglong Ma',\n",
       "  33: 'Yu Zhang, Changhao Pan, Wenxiang Guo, Ruiqi Li, Zhiyuan Zhu, Jialei Wang, Wenhao Xu, Jingyu Lu, Zhiqing Hong, Chuxin Wang, LiChao Zhang, Jinzheng He, Ziyue Jiang, Yuxin Chen, Chen Yang, Jiecheng Zhou, Xinyu Cheng, Zhou Zhao',\n",
       "  34: 'Yifan Li, Yikai Wang, Yanwei Fu, Dongyu Ru, Zheng Zhang, Tong He',\n",
       "  35: 'Andrea Amaduzzi, Pierluigi Zama Ramirez, Giuseppe Lisanti, Samuele Salti, Luigi Di Stefano',\n",
       "  36: 'Zhihang Yuan, Hanling Zhang, Pu Lu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, Yu Wang',\n",
       "  37: 'Vincent Hanke, Tom Blanchard, Franziska Boenisch, Iyiola E. Olatunji, Michael Backes, Adam Dziedzic',\n",
       "  38: 'Kseniya Cherenkova, Elona Dupont, Anis Kacem, Gleb Gusev, Djamila Aouada',\n",
       "  39: 'Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami',\n",
       "  40: 'Weicai Ye, Chenhao Ji, Zheng Chen, Junyao Gao, Xiaoshui Huang, Song-Hai Zhang, Wanli Ouyang, Tong He, Cairong Zhao, Guofeng Zhang',\n",
       "  41: 'Mouad El Bouchattaoui, Myriam Tami, Benoit Lepetit, Paul-Henry Cournède',\n",
       "  42: 'Weizhi Gao, Zhichao Hou, Han Xu, Xiaorui Liu',\n",
       "  43: 'Sergio Hernan Garrido Mejia, Patrick Blöbaum, Bernhard Schölkopf, Dominik Janzing',\n",
       "  44: 'Yanzhi Li, Keqiu Li, Guohui Li, Zumin Wang, Changqing Ji, Lubo Wang, Die Zuo, Qing Guo, Feng Zhang, Manyu Wang, Di Lin',\n",
       "  45: 'Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang',\n",
       "  46: 'Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, Oleksii Kuchaiev',\n",
       "  47: 'Sophie Xhonneux, Alessandro Sordoni, Stephan Günnemann, Gauthier Gidel, Leo Schwinn',\n",
       "  48: 'Zhu Yu, Runmin Zhang, Jiacheng Ying, Junchen Yu, Xiaohai Hu, Lun Luo, Si-Yuan Cao, Hui-Liang Shen',\n",
       "  49: 'Quentin Leboutet, Nina Wiedemann, Zhipeng Cai, Michael Paulitsch, Kai Yuan',\n",
       "  50: 'Samuel Teuber, Stefan Mitsch, André Platzer',\n",
       "  51: 'Jason Hu, Bowen Song, Xiaojian Xu, Liyue Shen, Jeffrey A. Fessler',\n",
       "  52: 'Yangru Huang, Peixi Peng, Yifan Zhao, Guangyao Chen, Yonghong Tian',\n",
       "  53: 'Julian Rodemann, Christoph Jansen, Georg Schollmeyer',\n",
       "  54: 'Yikun Jiang, Huanyu Wang, Lei Xie, Hanbin Zhao, Chao Zhang, Hui Qian, John C.S. Lui',\n",
       "  55: 'Pierre Clavier, Laixi Shi, Erwan Le Pennec, Eric Mazumdar, Adam Wierman, Matthieu Geist',\n",
       "  56: 'Jun Xia, Shaorong Chen, Jingbo Zhou, Xiaojun Shan, Wenjie Du, Zhangyang Gao, Cheng Tan, Bozhen Hu, Jiangbin Zheng, Stan Z. Li',\n",
       "  57: 'Zhenhui Ye, Tianyun Zhong, Yi Ren, Ziyue Jiang, Jiawei Huang, Rongjie Huang, Jinglin liu, Jinzheng He, Chen Zhang, Zehan Wang, Xize Chen, Xiang Yin, Zhou Zhao',\n",
       "  58: 'Kun Zhou, Beichen Zhang, Jiapeng Wang, Zhipeng Chen, Wayne Xin Zhao, Jing Sha, Zhichao Sheng, Shijin Wang, Ji-Rong Wen',\n",
       "  59: 'Kun Yan, Zeyu Wang, Lei Ji, Yuntao Wang, Nan Duan, Shuai Ma',\n",
       "  60: 'Linus Ericsson, Miguel Espinosa, Chenhongyi Yang, Antreas Antoniou, Amos Storkey, Shay B. Cohen, Steven McDonagh, Elliot J. Crowley',\n",
       "  61: 'Julia C. Costacurta, Shaunak Bhandarkar, David Zoltowski, Scott W. Linderman',\n",
       "  62: 'Felipe Garrido-Lucero, Benjamin Heymann, Maxime Vono, Patrick Loiseau, Vianney Perchet',\n",
       "  63: 'Xingyu Zhu, Beier Zhu, Yi Tan, Shuo Wang, Yanbin Hao, Hanwang Zhang',\n",
       "  64: 'Xiaoyuan Zhang, Liang Zhao, Yingying Yu, Xi Lin, Yifan Chen, Han Zhao, Qingfu Zhang',\n",
       "  65: 'Shaurya Dewan, Rushikesh Zawar, Prakanshul Saxena, Yingshan Chang, Andrew Luo, Yonatan Bisk',\n",
       "  66: 'Bing Cao, Yinan Xia, Yi Ding, Changqing Zhang, Qinghua Hu',\n",
       "  67: 'Huilong Jin, Yingxue Zhang, Lei Shi, Shuang Zhang, Feifei Kou, Jiapeng Yang, Chuangying Zhu, Jia Luo',\n",
       "  68: 'Haoye Dong, Aviral Chharia, Wenbo Gou, Francisco Vicente Carrasco, Fernando De la Torre',\n",
       "  69: 'Zhengyi Luo, Jinkun Cao, Sammy Christen, Alexander Winkler, Kris Kitani, Weipeng Xu',\n",
       "  70: 'Zifan Song, Yudong Wang, Wenwei Zhang, Kuikun Liu, Chengqi Lyu, Demin Song, Qipeng Guo, Hang Yan, Dahua Lin, Kai Chen, Cairong Zhao',\n",
       "  71: 'Xiaoyuan Zhang, Genghui Li, Xi Lin, Yichi Zhang, Yifan Chen, Qingfu Zhang',\n",
       "  72: 'Manuel Meier, Berken Utku Demirel, Christian Holz',\n",
       "  73: 'Wanhua Li, Zibin Meng, Jiawei Zhou, Donglai Wei, Chuang Gan, Hanspeter Pfister',\n",
       "  74: 'Recep Yusuf Bekci',\n",
       "  75: 'Jeremias Traub, Till J. Bungert, Carsten T. Lüth, Michael Baumgartner, Klaus H. Maier-Hein, Lena Maier-Hein, Paul F. Jäger',\n",
       "  76: 'Shentong Mo, Shengbang Tong',\n",
       "  77: 'Anqi Mao, Mehryar Mohri, Yutao Zhong',\n",
       "  78: 'Xinyi Xu, Shuaiqi Wang, Chuan-Sheng Foo, Bryan Kian Hsiang Low, Giulia Fanti',\n",
       "  79: 'Zander W. Blasingame, Chen Liu',\n",
       "  80: 'Connor Brennan, Andrew Robert Williams, Omar G. Younis, Vedant Vyas, Daria Yasafova, Irina Rish',\n",
       "  81: 'Daniel Omer, Or Sheffet',\n",
       "  82: 'Haozhe Tian, Homayoun Hamedmoghadam, Robert Shorten, Pietro Ferraro',\n",
       "  83: 'David Samuel',\n",
       "  84: 'Jiahe Bai, Baojian Zhou, Deqing Yang, Yanghua Xiao',\n",
       "  85: 'Dailing Zhang, Shiyu Hu, Xiaokun Feng, Xuchen Li, Meiqi Wu, Jing Zhang, Kaiqi Huang',\n",
       "  86: 'Roi Livni, Shay Moran, Kobbi Nissim, Chirag Pabbaraju',\n",
       "  87: 'Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, Jitao Sang',\n",
       "  88: 'Mohammad Mahmudul Alam, Alexander Oberle, Edward Raff, Stella Biderman, Tim Oates, James Holt',\n",
       "  89: 'Wei Liu, Chenxi Wang, Yifei Wang, Zihao Xie, Rennai Qiu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Chen Qian',\n",
       "  90: 'Jingwei Liu, Ling Yang, Hongyan Li, Shenda Hong',\n",
       "  91: 'Hadi Hosseini, Debmalya Mandal, Amrit Puhan',\n",
       "  92: 'Qijian Zhang, Junhui Hou, Wenping Wang, Ying He',\n",
       "  93: 'Lin Gui, Cristina Gârbacea, Victor Veitch',\n",
       "  94: 'Hyunseok Lee, Jihoon Tack, Jinwoo Shin',\n",
       "  95: 'Atli Kosson, Bettina Messmer, Martin Jaggi',\n",
       "  96: 'Niki Maria Foteinopoulou, Enjie Ghorbel, Djamila Aouada',\n",
       "  97: 'Siyuan Xu, Minghui Zhu',\n",
       "  98: 'Butakov I. D., Tolmachev A. D., Malanchuk S. V., Neopryatnaya A. M., Frolov A. A.',\n",
       "  99: 'Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, Baobao Chang',\n",
       "  100: 'Jason Yang, Ariane Mora, Shengchao Liu, Bruce J. Wittmann, Anima Anandkumar, Frances H. Arnold, Yisong Yue',\n",
       "  101: 'Yiwei Zhou, Xiaobo Xia, Zhiwei Lin, Bo Han, Tongliang Liu',\n",
       "  102: 'Fangjinhua Wang, Marie-Julie Rakotosaona, Michael Niemeyer, Richard Szeliski, Marc Pollefeys, Federico Tombari',\n",
       "  103: 'Huzi Cheng, Joshua W. Brown',\n",
       "  104: 'Adrian Bulat, Yassine Ouali, Georgios Tzimiropoulos',\n",
       "  105: 'Xiang Li, Jian Ding, Mohamed Elhoseiny',\n",
       "  106: 'Naoki Hiratani',\n",
       "  107: 'Asaf Cassel, Aviv Rosenberg',\n",
       "  108: 'Tianyi Zhang, Jonah Yi, Zhaozhuo Xu, Anshumali Shrivastava',\n",
       "  109: 'Qiannan Zhang, Weishen Pan, Zilong Bai, Chang Su, Fei Wang',\n",
       "  110: 'Lasse Vuursteen',\n",
       "  111: 'Chengzhengxu Li, Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Chen Liu, Yu Lan, Chao Shen',\n",
       "  112: 'Chao Chen, Chenghua Guo, Rufeng Chen, Guixiang Ma, Ming Zeng, Xiangwen Liao, Xi Zhang, Sihong Xie',\n",
       "  113: 'Bernardo Esteves, Miguel Vasco, Francisco S. Melo',\n",
       "  114: 'Ruohan Li, Yiqun Xie, Xiaowei Jia, Dongdong Wang, Yanhua Li, Yingxue Zhang, Zhihao Wang, Zhili Li',\n",
       "  115: 'Junyu Liu, Xiangjun Peng',\n",
       "  116: 'Yixiao Xu, Binxing Fang, Mohan Li, Keke Tang, Zhihong Tian',\n",
       "  117: 'Yang Yang, Wendi Ren, Shuang Li',\n",
       "  118: 'Zhongzhen Huang, Yankai Jiang, Rongzhao Zhang, Shaoting Zhang, Xiaofan Zhang',\n",
       "  119: 'Jamie Lohoff, Emre Neftci',\n",
       "  120: 'Krishna Sri Ipsit Mantri, Xinzhi (Aurora) Wang, Carola-Bibiane Schönlieb, Bruno Ribeiro, Beatrice Bevilacqua, Moshe Eliasof',\n",
       "  121: 'Eric Balkanski, Will Ma, Andreas Maggiori',\n",
       "  122: 'Alexander Tyurin, Marta Pozzi, Ivan Ilin, Peter Richtárik',\n",
       "  123: 'Lucas Slot, Stefan Tiegel, Manuel Wiedmer',\n",
       "  124: 'Lakshmi Narasimhan Govindarajan, Abhiram Iyer, Valmiki Kothare, Ila Fiete',\n",
       "  125: 'Andres Potapczynski, Shikai Qiu, Marc Finzi, Christopher Ferri, Zixi Chen, Micah Goldblum, C. Bayan Bruss, Christopher De Sa, Andrew Gordon Wilson',\n",
       "  126: 'Jiamian Wang, Pichao Wang, Dongfang Liu, Qiang Guan, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao',\n",
       "  127: 'Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, Romann M. Weber',\n",
       "  128: 'Huayang Huang, Yu Wu, Qian Wang',\n",
       "  129: 'Nikhil Behari, Edwin Zhang, Yunfan Zhao, Aparna Taneja, Dheeraj Nagaraj, Milind Tambe',\n",
       "  130: 'Jitao Zhao, Di Jin, Meng Ge, Lianze Shan, Xin Wang, Dongxiao He, Zhiyong Feng',\n",
       "  131: 'Yiqun Mei, Jiacong Xu, Vishal M. Patel',\n",
       "  132: 'Atharva Mete, Haotian Xue, Albert Wilcox, Yongxin Chen, Animesh Garg',\n",
       "  133: 'Long Wei, Peiyan Hu, Ruiqi Feng, Haodong Feng, Yixuan Du, Tao Zhang, Rui Wang, Yue Wang, Zhi-Ming Ma, Tailin Wu',\n",
       "  134: 'Wenliang Zhao, Minglei Shi, Xumin Yu, Jie Zhou, Jiwen Lu',\n",
       "  135: 'Zhengbang Zhu, Minghuan Liu, Liyuan Mao, Bingyi Kang, Minkai Xu, Yong Yu, Stefano Ermon, Weinan Zhang',\n",
       "  136: 'Gabriele Farina, Charilaos Pipis',\n",
       "  137: 'Sheng-Yu Wang, Aaron Hertzmann, Alexei A. Efros, Jun-Yan Zhu, Richard Zhang',\n",
       "  138: 'Xi Yu, Shinjae Yoo, Yuewei Lin',\n",
       "  139: 'Linus Jeary, Tom Kuipers, Mehran Hosseini, Nicola Paoletti',\n",
       "  140: 'Zhuopeng Xu, Yujie Li, Cheng Liu, Ning Gui',\n",
       "  141: 'Longfei Ma, Yiyou Sun, Kaize Ding, Zemin Liu, Fei Wu',\n",
       "  142: 'Ian Covert, Chanwoo Kim, Su-In Lee, James Zou, Tatsunori Hashimoto',\n",
       "  143: 'Haocheng Luo, Tuan Truong, Tung Pham, Mehrtash Harandi, Dinh Phung, Trung Le',\n",
       "  144: 'Ruikai Cui, Xibin Song, Weixuan Sun, Senbo Wang, Weizhe Liu, Shenzhou Chen, Taizhang Shang, Yang Li, Nick Barnes, Hongdong Li, Pan Ji',\n",
       "  145: 'Seijin Kobayashi, Yassir Akram, Johannes von Oswald',\n",
       "  146: 'Lingxiang Jia, Yuchen Ying, Zunlei Feng, Zipeng Zhong, Shaolun Yao, Jiacong Hu, Mingjiang Duan, Xingen Wang, Jie Song, Mingli Song',\n",
       "  147: 'Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, Yueting Zhuang',\n",
       "  148: 'Hang Zhou, Yehui Tang, Haochen Qin, Yujie Yang, Renren Jin, Deyi Xiong, Kai Han, Yunhe Wang',\n",
       "  149: 'Boyao Li, Alexander J. Thomson, Houssam Nassif, Matthew M. Engelhard, David Page',\n",
       "  150: 'Benedikt Böck, Sadaf Syed, Wolfgang Utschick',\n",
       "  151: 'Hezhe Qiao, Qingsong Wen, Xiaoli Li, Ee-Peng Lim, Guansong Pang',\n",
       "  152: 'Xiuying Wei, Skander Moalla, Razvan Pascanu, Caglar Gulcehre',\n",
       "  153: 'Shihong Ding, Long Yang, Luo Luo, Cong Fang',\n",
       "  154: 'Rayna Andreeva, Benjamin Dupuis, Rik Sarkar, Tolga Birdal, Umut Şimşekli',\n",
       "  155: 'Zhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, Yu Qiao',\n",
       "  156: 'Binghui Xie, Yixuan Wang, Yongqiang Chen, Kaiwen Zhou, Yu Li, Wei Meng, James Cheng',\n",
       "  157: 'Shivam Grover, Amin Jalali, Ali Etemad',\n",
       "  158: 'Youcheng Zhang, Liwen Zhang, Zijun Hu, Pengcheng Pi, Teng Li, Yuanpei Chen, Shi Peng, Zhe Ma',\n",
       "  159: 'Jun-Kun Chen, Yu-Xiong Wang',\n",
       "  160: \"Laurent Mertens, Elahe' Yargholi, Hans Op de Beeck, Jan Van den Stock, Joost Vennekens\",\n",
       "  161: 'Dan Shi, Renren Jin, Tianhao Shen, Weilong Dong, Xinwei Wu, Deyi Xiong',\n",
       "  162: 'Zhuoyan Li, Ming Yin',\n",
       "  163: 'Yu Zeng, Yang Zhang, Jiachen Liu, Linlin Shen, Kaijun Deng, Weizhao He, Jinbao Wang',\n",
       "  164: 'Vladimir Malinovskii, Denis Mazur, Ivan Ilin, Denis Kuznedelev, Konstantin Burlachenko, Kai Yi, Dan Alistarh, Peter Richtarik',\n",
       "  165: 'Klara Kaleb, Barbara Feulner, Juan A. Gallego, Claudia Clopath',\n",
       "  166: 'Abhinav Joshi, Areeb Ahmad, Ashutosh Modi',\n",
       "  167: 'Jianyi Zhang, Da-Cheng Juan, Cyrus Rashtchian, Chun-Sung Ferng, Heinrich Jiang, Yiran Chen',\n",
       "  168: 'Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Junjie Hu, Yixuan Li, Patrick McDaniel, Muhao Chen, Bo Li, Chaowei Xiao',\n",
       "  169: 'Peiyuan Feng, Yichen He, Guanhua Huang, Yuan Lin, Hanchong Zhang, Yuchen Zhang, Hang Li',\n",
       "  170: 'Hang Yin, Xiuwei Xu, Zhenyu Wu, Jie Zhou, Jiwen Lu',\n",
       "  171: 'Reuben Adams, John Shawe-Taylor, Benjamin Guedj',\n",
       "  172: 'Xixi Wu, Yifei Shen, Caihua Shan, Kaitao Song, Siwei Wang, Bohang Zhang, Jiarui Feng, Hong Cheng, Wei Chen, Yun Xiong, Dongsheng Li',\n",
       "  173: 'Junoh Lee, Changyeon Won, Hyunjun Jung, Inhwan Bae, Hae-Gon Jeon',\n",
       "  174: 'Timon Barlag, Vivian Holzapfel, Laura Strieker, Jonni Virtema, Heribert Vollmer',\n",
       "  175: 'Nikil Roashan Selvam, Amil Merchant, Stefano Ermon',\n",
       "  176: 'Alexander D. Goldie, Chris Lu, Matthew T. Jackson, Shimon Whiteson, Jakob N. Foerster',\n",
       "  177: 'Jiatao Gu, Ying Shen, Shuangfei Zhai, Yizhe Zhang, Navdeep Jaitly, Josh Susskind',\n",
       "  178: 'Qingqi Zhang, Ruize Xu, Risi Kondor',\n",
       "  179: 'Runjia Zeng, Cheng Han, Qifan Wang, Chunshu Wu, Tong Geng, Lifu Huang, Ying Nian Wu, Dongfang Liu',\n",
       "  180: 'Benjamin Rozonoyer, Michael Boratko, Dhruvesh Patel, Wenlong Zhao, Shib Dasgupta, Hung Le, Andrew McCallum',\n",
       "  181: 'Chester Holtz, Pengwen Chen, Zhengchao Wan, Chung-Kuan Cheng, Gal Mishne',\n",
       "  182: 'Hezhen Hu, Zhiwen Fan, Tianhao Wu, Yihan Xi, Seoyoung Lee, Georgios Pavlakos, Zhangyang Wang',\n",
       "  183: 'Bobak T. Kiani, Jason Wang, Melanie Weber',\n",
       "  184: 'Wei Li, Hehe Fan, Yongkang Wong, Mohan Kankanhalli, Yi Yang',\n",
       "  185: 'Yi-Bo Wang, Jun-Yi Hang, Min-Ling Zhang',\n",
       "  186: 'Keying Kuang, Frances Dean, Jack B. Jedlicki, David Ouyang, Anthony Philippakis, David Sontag, Ahmed Alaa',\n",
       "  187: 'Feng Lu, Xinyao Zhang, Canming Ye, Shuting Dong, Lijun Zhang, Xiangyuan Lan, Chun Yuan',\n",
       "  188: 'Xavier Gonzalez, Andrew Warrington, Jimmy T.H. Smith, Scott W. Linderman',\n",
       "  189: 'Zhifan Ye, Chenxi Wan, Chaojian Li, Jihoon Hong, Sixu Li, Leshu Li, Yongan Zhang, Yingyan (Celine) Lin',\n",
       "  190: 'Alexander Soen, Ke Sun',\n",
       "  191: 'Yue Liu, Shihao Zhu, Jun Xia, Yingwei Ma, Jian Ma, Xinwang Liu, Shengju Yu, Kejun Zhang, Wenliang Zhong',\n",
       "  192: 'Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu',\n",
       "  193: 'Yooju Shin, Jaehyun Park, Hwanjun Song, Susik Yoon, Byung Suk Lee, Jae-Gil Lee',\n",
       "  194: 'Léo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault Le Sellier De Chezelles, Quentin Cappart, Nicolas Chapados, Alexandre Lacoste, Alexandre Drouin',\n",
       "  195: 'Qinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, Xiao Huang',\n",
       "  196: 'Zhi Zheng, Changliang Zhou, Xialiang Tong, Mingxuan Yuan, Zhenkun Wang',\n",
       "  197: 'Chong Ma, Hanqi Jiang, Wenting Chen, Yiwei Li, Zihao Wu, Xiaowei Yu, Zhengliang Liu, Lei Guo, Dajiang Zhu, Tuo Zhang, Dinggang Shen, Tianming Liu, Xiang Li',\n",
       "  198: 'Desik Rengarajan, Nitin Ragothaman, Dileep Kalathil, Srinivas Shakkottai',\n",
       "  199: 'Li Ji-An, Corey Y Zhou, Marcus K. Benna, Marcelo G. Mattar',\n",
       "  200: 'Wuyang Chen, Jialin Song, Pu Ren, Shashank Subramanian, Dmitriy Morozov, Michael W. Mahoney',\n",
       "  201: 'Fei Shen, Jinhui Tang',\n",
       "  202: 'Albert Gong, Kyuseong Choi, Raaz Dwivedi',\n",
       "  203: 'Xinyu Yuan, Zhihao Zhan, Zuobai Zhang, Manqi Zhou, Jianan Zhao, Boyu Han, Yue Li, Jian Tang',\n",
       "  204: 'Mixue Xie, Shuang Li, Binhui Xie, Chi Harold Liu, Jian Liang, Zixun Sun, Ke Feng, Chengwei Zhu',\n",
       "  205: 'Tam Thuc Do, Parham Eftekhar, Seyed Alireza Hosseini, Gene Cheung, Philip A. Chou',\n",
       "  206: 'Mayank Shrivastava, Berivan Isik, Qiaobo Li, Sanmi Koyejo, Arindam Banerjee',\n",
       "  207: 'Yonghan Jung, Jin Tian, Elias Bareinboim',\n",
       "  208: 'Elizabeth Collins-Woodfin, Inbar Seroussi, Begoña García Malaxechebarría, Andrew W. Mackenzie, Elliot Paquette, Courtney Paquette',\n",
       "  209: 'Biqing Qi, Yiang Luo, Junqi Gao, Pengfei Li, Kai Tian, Zhiyuan Ma, Bowen Zhou',\n",
       "  210: 'Joseph Ortiz, Antoine Dedieu, Wolfgang Lehrach, J. Swaroop Guntupalli, Carter Wendelken, Ahmad Humayun, Guangyao Zhou, Sivaramakrishnan Swaminathan, Miguel Lázaro-Gredilla, Kevin Murphy',\n",
       "  211: 'Haoyu Dong, Huiqiao Fu, Wentao Xu, Zhehao Zhou, Chunlin Chen',\n",
       "  212: 'Yang Zhou, Tan Li Hui Faith, Yanyu Xu, Sicong Leng, Xinxing Xu, Yong Liu, Rick Siow Mong Goh',\n",
       "  213: 'Awni Altabaa, Zhuoran Yang',\n",
       "  214: 'Ziqiao Wang, Yongyi Mao',\n",
       "  215: 'Woo Kyung Kim, Youngseok Lee, Jooyoung Kim, Honguk Woo',\n",
       "  216: 'Zhen Chen, Yi Zhang, Fu Wang, Xingyu Zhao, Xiaowei Huang, Wenjie Ruan',\n",
       "  217: 'Antonio Terpin, Nicolas Lanzetti, Martín Gadea, Florian Dörfler',\n",
       "  218: 'Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, Mike Zheng Shou',\n",
       "  219: 'Wei Yu, Bowen Yang, Qinglin Liu, Jianing Li, Shengping Zhang, Xiangyang Ji',\n",
       "  220: 'Boqiang Zhang, Zuan Gao, Yadong Qu, Hongtao Xie',\n",
       "  221: 'Jesus Zarzar, Bernard Ghanem',\n",
       "  222: 'Kai Sandbrink, Jan P. Bauer, Alexandra M. Proca, Andrew M. Saxe, Christopher Summerfield, Ali Hummos',\n",
       "  223: 'Deepak Sridhar, Abhishek Peri, Rohith Rachala, Nuno Vasconcelos',\n",
       "  224: 'Weihao Lu, Haobo Zhang, Yicheng Li, Qian Lin',\n",
       "  225: 'Tianjing Zhang, Yuhui Quan, Hui Ji',\n",
       "  226: 'Yuanning Cui, Zequn Sun, Wei Hu',\n",
       "  227: 'Ali Younis, Erik B. Sudderth',\n",
       "  228: 'Lingxiao Zhao, Xueying Ding, Leman Akoglu',\n",
       "  229: 'Weichao Zhao, Hao Feng, Qi Liu, Jingqun Tang, Shu Wei, Binghong Wu, Lei Liao, Yongjie Ye, Hao Liu, Wengang Zhou, Houqiang Li, Can Huang',\n",
       "  230: 'Chujie Gao, Siyuan Wu, Yue Huang, Dongping Chen, Qihui Zhang, Zhengyan Fu, Yao Wan, Lichao Sun, Xiangliang Zhang',\n",
       "  231: 'Tianle Gu, Zeyang Zhou, Kexin Huang, Dandan Liang, Yixu Wang, Haiquan Zhao, Yuanqi Yao, Xingge Qiao, Keqing Wang, Yujiu Yang, Yan Teng, Yu Qiao, Yingchun Wang',\n",
       "  232: 'Yiheng Wang, Tianyu Wang, Yuying Zhang, Hongji Zhang, Haoyu Zheng, Guanjie Zheng, Linghe Kong',\n",
       "  233: 'Thomas W. Mitchel, Michael Taylor, Vincent Sitzmann',\n",
       "  234: 'Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, Furu Wei',\n",
       "  235: 'Xingyu Zhou, Wei Zhang',\n",
       "  236: 'Samyadeep Basu, Martin Grayson, Cecily Morrison, Besmira Nushi, Soheil Feizi, Daniela Massiceti',\n",
       "  237: 'Bin Ren, Yawei Li, Jingyun Liang, Rakesh Ranjan, Mengyuan Liu, Rita Cucchiara, Luc Van Gool, Ming-Hsuan Yang, Nicu Sebe',\n",
       "  238: 'Zhikai Chen, Haitao Mao, Jingzhe Liu, Yu Song, Bingheng Li, Wei Jin, Bahare Fatemi, Anton Tsitsulin, Bryan Perozzi, Hui Liu, Jiliang Tang',\n",
       "  239: 'Zhe Zhao, Haibin Wen, Zikang Wang, Pengkun Wang, Fanfu Wang, Song Lai, Qingfu Zhang, Yang Wang',\n",
       "  240: 'Haipeng Luo, Spandan Senapati, Vatsal Sharan',\n",
       "  241: 'Mohammad Sadil Khan, Sankalp Sinha, Talha Uddin Sheikh, Didier Stricker, Sk Aziz Ali, Muhammad Zeshan Afzal',\n",
       "  242: 'Xuanyu Yi, Zike Wu, Qiuhong Shen, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, Shuicheng Yan, Xinchao Wang, Hanwang Zhang',\n",
       "  243: 'Ang Bian, Wei Li, Hangjie Yuan, Chengrong Yu, Mang Wang, Zixiang Zhao, Aojun Lu, Pengliang Ji, Tao Feng',\n",
       "  244: 'Jonathan Thomm, Giacomo Camposampiero, Aleksandar Terzic, Michael Hersche, Bernhard Schölkopf, Abbas Rahimi',\n",
       "  245: 'Xiaoge Deng, Tao Sun, Shengwei Li, Dongsheng Li, Xicheng Lu',\n",
       "  246: 'Yubo Ye, Maryam Toloubidokhti, Sumeet Vadhavkar, Xiajun Jiang, Huafeng Liu, Linwei Wang',\n",
       "  247: 'Gongfan Fang, Hongxu Yin, Saurav Muralidharan, Greg Heinrich, Jeff Pool, Jan Kautz, Pavlo Molchanov, Xinchao Wang',\n",
       "  248: 'Divyansh Pareek, Simon S. Du, Sewoong Oh',\n",
       "  249: 'Dongfang Li, Zhenyu Liu, Xinshuo Hu, Zetian Sun, Baotian Hu, Min Zhang',\n",
       "  250: 'Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, Junxian He',\n",
       "  251: 'Ori Press, Andreas Hochlehnert, Ameya Prabhu, Vishaal Udandarao, Ofir Press, Matthias Bethge',\n",
       "  252: 'Yue Lu, Shizhou Zhang, De Cheng, Yinghui Xing, Nannan Wang, Peng Wang, Yanning Zhang',\n",
       "  253: 'Scott Geng, Cheng-Yu Hsieh, Vivek Ramanujan, Matthew Wallingford, Chun-Liang Li, Pang Wei Koh, Ranjay Krishna',\n",
       "  254: 'Polina Turishcheva, Max F. Burg, Fabian H. Sinz, Alexander S. Ecker',\n",
       "  255: 'Rohan Baskar Prabhakar, Hengrui Zhang, David Wentzlaff',\n",
       "  256: 'Hongyu Shen, Yici Yan, Zhizhen Zhao',\n",
       "  257: 'Manh Cuong Dao, Phi Le Nguyen, Thao Nguyen Truong, Trong Nghia Hoang',\n",
       "  258: 'Hui Wei, Zhixiang Wang, Kewei Zhang, Jiaqi Hou, Yuanwei Liu, Hao Tang, Zheng Wang',\n",
       "  259: 'Gang Liu, Jiaxin Xu, Tengfei Luo, Meng Jiang',\n",
       "  260: 'Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, Nouha Dziri',\n",
       "  261: 'Hrittik Roy, Marco Miani, Carl Henrik Ek, Philipp Hennig, Marvin Pförtner, Lukas Tatzel, Søren Hauberg',\n",
       "  262: 'Matteo Zecchin, Osvaldo Simeone',\n",
       "  263: 'Arjun Subramonian, Jian Kang, Yizhou Sun',\n",
       "  264: 'Michael Crawshaw, Mingrui Liu',\n",
       "  265: 'Julius Hense, Mina Jamshidi Idaji, Oliver Eberle, Thomas Schnake, Jonas Dippel, Laure Ciernik, Oliver Buchstab, Andreas Mock, Frederick Klauschen, Klaus-Robert Müller',\n",
       "  266: 'Ayush Sawarni, Nirjhar Das, Siddharth Barman, Gaurav Sinha',\n",
       "  267: 'Gefen Dawidowicz, Elad Hirsch, Ayellet Tal',\n",
       "  268: 'Jiapu Wang, Kai Sun, Linhao Luo, Wei Wei, Yongli Hu, Alan Wee-Chung Liew, Shirui Pan, Baocai Yin',\n",
       "  269: 'Joowon Lee, Jared D. Huling, Guanhua Chen',\n",
       "  270: 'Taira Tsuchiya, Shinji Ito',\n",
       "  271: 'Runze Yang, Longbing Cao, Jianxun Li, Jie Yang',\n",
       "  272: 'Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Federico Fusco',\n",
       "  273: 'Guhao Feng, Han Zhong',\n",
       "  274: 'Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li',\n",
       "  275: 'Xuandong Zhao, Kexun Zhang, Zihao Su, Saastha Vasan, Ilya Grishchenko, Christopher Kruegel, Giovanni Vigna, Yu-Xiang Wang, Lei Li',\n",
       "  276: 'Qiang Li, Hoi-To Wai',\n",
       "  277: 'Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, Jiaqi Wang',\n",
       "  278: 'Jinda Jia, Cong Xie, Hanlin Lu, Daoce Wang, Hao Feng, Chengming Zhang, Baixi Sun, Haibin Lin, Zhi Zhang, Xin Liu, Dingwen Tao',\n",
       "  279: 'Clark Mingxuan Ju, William Shiao, Zhichun Guo, Yanfang Ye, Yozen Liu, Neil Shah, Tong Zhao',\n",
       "  280: 'Jung-hun Kim, Milan Vojnović, Se-Young Yun',\n",
       "  281: 'Aniket Das, Dheeraj Nagaraj, Soumyabrata Pal, Arun Sai Suggala, Prateek Varshney',\n",
       "  282: 'Alexander Nikitin, Jannik Kossen, Yarin Gal, Pekka Marttinen',\n",
       "  283: 'Ruochen Liu, Hao Chen, Yuanchen Bei, Qijie Shen, Fangwei Zhong, Senzhang Wang, Jianxin Wang',\n",
       "  284: 'Jonathan Hayase, Alisa Liu, Yejin Choi, Sewoong Oh, Noah A. Smith',\n",
       "  285: 'Jörg K.H. Franke, Michael Hefenbrock, Gregor Koehler, Frank Hutter',\n",
       "  286: 'Luke Eilers, Raoul-Martin Memmesheimer, Sven Goedeke',\n",
       "  287: 'Leo Schwinn, David Dobre, Sophie Xhonneux, Gauthier Gidel, Stephan Günnemann',\n",
       "  288: 'Haiquan Lu, Yefan Zhou, Shiwei Liu, Zhangyang Wang, Michael W. Mahoney, Yaoqing Yang',\n",
       "  289: 'Heeseong Shin, Chaehyun Kim, Sunghwan Hong, Seokju Cho, Anurag Arnab, Paul Hongsuck Seo, Seungryong Kim',\n",
       "  290: 'Zihan Zhou, Muhammad Qasim Elahi, Murat Kocaoglu',\n",
       "  291: 'Shangkun Sun, Jiaming Liu, Huaxia Li, Guoqing Liu, Thomas H Li, Wei Gao',\n",
       "  292: 'Sanae Lotfi, Yilun Kuang, Brandon Amos, Micah Goldblum, Marc Finzi, Andrew Gordon Wilson',\n",
       "  293: 'Han Huang, Haitian Zhong, Tao Yu, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan',\n",
       "  294: 'Yue Wang, Zhongchang Sun, Shaofeng Zou',\n",
       "  295: 'Alex Elenter, Spyros Angelopoulos, Christoph Dürr, Yanni Lefki',\n",
       "  296: 'Shirley Wu, Kaidi Cao, Bruno Ribeiro, James Zou, Jure Leskovec',\n",
       "  297: 'Yuhang Wen, Mengyuan Liu, Songtao Wu, Beichen Ding',\n",
       "  298: 'Joel Daniel Andersson, Monika Henzinger, Rasmus Pagh, Teresa Anna Steiner, Jalaj Upadhyay',\n",
       "  299: 'Ke Liang, Yue Liu, Hao Li, Lingyuan Meng, Suyuan Liu, Siwei Wang, Sihang Zhou, Xinwang Liu',\n",
       "  300: 'Fangcong Yin, Xi Ye, Greg Durrett',\n",
       "  301: 'Jie Ma, Min Hu, Pinghui Wang, Wangchun Sun, Lingyun Song, Hongbin Pei, Jun Liu, Youtian Du',\n",
       "  302: 'Yawar Siddiqui, Tom Monnier, Filippos Kokkinos, Mahendra Kariya, Yanir Kleiman, Emilien Garreau, Oran Gafni, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, David Novotny',\n",
       "  303: 'Chunlin Tian, Zhan Shi, Zhijiang Guo, Li Li, Chengzhong Xu',\n",
       "  304: 'Yijing Liu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, Wei Chen',\n",
       "  305: 'Biao Gong, Shuai Tan, Yutong Feng, Xiaoying Xie, Yuyuan Li, Chaochao Chen, Kecheng Zheng, Yujun Shen, Deli Zhao',\n",
       "  306: 'Haixiang Sun, Ye Shi',\n",
       "  307: 'Derui Zhu, Dingfan Chen, Xiongfei Wu, Jiahui Geng, Zhuo Li, Jens Grossklags, Lei Ma',\n",
       "  308: 'Puze Liu, Jonas Günster, Niklas Funk, Simon Gröger, Dong Chen, Haitham Bou-Ammar, Julius Jankowski, Ante Marić, Sylvain Calinon, Andrej Orsula, Miguel Olivares-Mendez, Hongyi Zhou, Rudolf Lioutikov, Gerhard Neumann, Amarildo Likmeta, Amirhossein Zhalehmehrabi, Thomas Bonenfant, Marcello Restelli, Davide Tateo, Ziyuan Liu, Jan Peters',\n",
       "  309: 'Xiang Fu, Andrew Rosen, Kyle Bystrom, Rui Wang, Albert Musaelian, Boris Kozinsky, Tess Smidt, Tommi Jaakkola',\n",
       "  310: 'Yongcheng Jing, Seok-Hee Hong, Dacheng Tao',\n",
       "  311: 'Yingzhe Peng, Chenduo Hao, Xinting Hu, Jiawei Peng, Xin Geng, Xu Yang',\n",
       "  312: 'Beomseok Kang, Priyabrata Saha, Sudarshan Sharma, Biswadeep Chakraborty, Saibal Mukhopadhyay',\n",
       "  313: 'Hefei Li, Chao Peng, Chenyang Xu, Zhengfeng Yang',\n",
       "  314: 'Shahar Yadin, Noam Elata, Tomer Michaeli',\n",
       "  315: 'Yearang Lee, Ho-Joong Kim, Seong-Whan Lee',\n",
       "  316: 'Jianyi Yang, Pengfei Li, Adam Wierman, Shaolei Ren',\n",
       "  317: 'Daniel de Vassimon Manela, Laura Battaglia, Robin J. Evans',\n",
       "  318: 'Liqiang Lin, Wenpeng Wu, Chi-Wing Fu, Hao Zhang, Hui Huang',\n",
       "  319: 'Aaron Defazio, Xingyu (Alice) Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, Ashok Cutkosky',\n",
       "  320: 'Mingyi Li, Xiao Zhang, Qi Wang, Tengfei Liu, Ruofan Wu, Weiqiang Wang, Fuzhen Zhuang, Hui Xiong, Dongxiao Yu',\n",
       "  321: 'Zhaokun Zhou, Yijie Lu, Yanhao Jia, Kaiwei Che, Jun Niu, Liwei Huang, Xinyu Shi, Yuesheng Zhu, Guoqi Li, Zhaofei Yu, Li Yuan',\n",
       "  322: 'Eric Zhao, Pranjal Awasthi, Zhengdao Chen, Sreenivas Gollapudi, Daniel Delling',\n",
       "  323: 'Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark',\n",
       "  324: 'Zifan Liu, Amin Karbasi, Theodoros Rekatsinas',\n",
       "  325: 'Ashok Cutkosky, Zakaria Mhammedi',\n",
       "  326: 'Andy Yang, David Chiang, Dana Angluin',\n",
       "  327: 'Sifei Liu, Shalini De Mello, Jan Kautz',\n",
       "  328: 'Hyun-Young Park, Shahab Asoodeh, Si-Hyeon Lee',\n",
       "  329: 'Zican Dong, Junyi Li, Xin Men, Wayne Xin Zhao, Bingning Wang, Zhen Tian, Weipeng Chen, Ji-Rong Wen',\n",
       "  330: 'Adarsh Jamadandi, Celia Rubio-Madrigal, Rebekka Burkholz',\n",
       "  331: 'Shubham Chowdhary, Giulia De Pasquale, Nicolas Lanzetti, Ana-Andreea Stoica, Florian Dörfler',\n",
       "  332: 'Yuezhu Xu, S. Sivaranjani',\n",
       "  333: 'Mariia Vladimirova, Eustache Diemert, Federico Pavone',\n",
       "  334: 'Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, Xin Luna Dong',\n",
       "  335: 'Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, Jie Fu',\n",
       "  336: 'David Perera, Victor Letzelter, Théo Mariotte, Adrien Cortés, Mickael Chen, Slim Essid, Gaël Richard',\n",
       "  337: 'Jaeseok Jang, Hyuk-Yoon Kwon',\n",
       "  338: 'Minseon Gwak, Seongrok Moon, Joohwan Ko, PooGyeon Park',\n",
       "  339: 'Matthieu Kirchmeyer, Pedro O. Pinheiro, Saeed Saremi',\n",
       "  340: 'Jiatong Li, Renjun Hu, Kunzhe Huang, Yan Zhuang, Qi Liu, Mengxiao Zhu, Xing Shi, Wei Lin',\n",
       "  341: 'Yongchun Li, Santanu S. Dey, Weijun Xie',\n",
       "  342: 'Jiangyuan Li, Jiayi Wang, Raymond K. W. Wong, Kwun Chuen Gary Chan',\n",
       "  343: 'Weihang Xu, Maryam Fazel, Simon S. Du',\n",
       "  344: 'Elizabeth Louise Baker, Gefan Yang, Michael L. Severinsen, Christy Anna Hipsley, Stefan Sommer',\n",
       "  345: 'Yusu Hong, Junhong Lin',\n",
       "  346: 'Po-Wei Huang, Patrick Rebentrost',\n",
       "  347: 'David P. Woodruff, Samson Zhou',\n",
       "  348: 'Roi Cohen, Konstantin Dobler, Eden Biran, Gerard de Melo',\n",
       "  349: 'Gennaro Gala, Cassio de Campos, Antonio Vergari, Erik Quaeghebeur',\n",
       "  350: 'Tuan Anh Pham, Vikas Garg',\n",
       "  351: 'Takeshi Koshizuka, Masahiro Fujisawa, Yusuke Tanaka, Issei Sato',\n",
       "  352: 'Xuechen Zhang, Xiangyu Chang, Mingchen Li, Amit Roy-Chowdhury, Jiasi Chen, Samet Oymak',\n",
       "  353: 'Jeremiah Birrell, Reza Ebrahimi, Rouzbeh Behnia, Jason Pacheco',\n",
       "  354: 'Yu-Hu Yan, Peng Zhao, Zhi-Hua Zhou',\n",
       "  355: 'Xuexun Liu, Xiaoxu Xu, Jinlong Li, Qiudan Zhang, Xu Wang, Nicu Sebe, Lin Ma',\n",
       "  356: 'Maryam Aliakbarpour, Piotr Indyk, Ronitt Rubinfeld, Sandeep Silwal',\n",
       "  357: 'Luca Barsellotti, Roberto Bigazzi, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara',\n",
       "  358: 'Hongduan Tian, Feng Liu, Zhanke Zhou, Tongliang Liu, Chengqi Zhang, Bo Han',\n",
       "  359: 'Ziyi Chen, Yan Wen, Zhengmian Hu, Heng Huang',\n",
       "  360: 'Ziyad Benomar, Evgenii Chzhen, Nicolas Schreuder, Vianney Perchet',\n",
       "  361: 'Xueying Jiang, Sheng Jin, Xiaoqin Zhang, Ling Shao, Shijian Lu',\n",
       "  362: 'Matt MacDermott, James Fox, Francesco Belardinelli, Tom Everitt',\n",
       "  363: 'Liwei Huang, Zhengyu Ma, Liutao Yu, Huihui Zhou, Yonghong Tian',\n",
       "  364: 'Yijun Liu, Jiequan Cui, Zhuotao Tian, Senqiao Yang, Qingdong He, Xiaoling Wang, Jingyong Su',\n",
       "  365: \"David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Christian Salamea, Dan John Velasco, David Ifeoluwa Adelani, David Le Meur, Emilio Villa-Cueva, Fajri Koto, Fauzan Farooqui, Frederico Belcavello, Ganzorig Batnasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta, Haiyue Song, Henok Biadglign Ademtew, Hernán Maina, Holy Lovenia, Israel Abebe Azime, Jan Christian Blaise Cruz, Jay Gala, Jiahui Geng, Jesus-German Ortiz-Barajas, Jinheon Baek, Jocelyn Dunstan, Laura Alonso Alemany, Kumaranage Ravindu Yasas Nagasinghe, Luciana Benotti, Luis Fernando D'Haro, Marcelo Viridiano, Marcos Estecha-Garitagoitia, Maria Camila Buitrago Cabrera, Mario Rodríguez-Cantelar, Mélanie Jouitteau, Mihail Mihaylov, Naome Etori, Mohamed Fazli Mohamed Imam, Muhammad Farid Adilazuarda, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Olivier Niyomugisha, Paula Mónica Silva, Pranjal Chitale, Raj Dabre, Rendi Chevi, Ruochen Zhang, Ryandito Diandaru, Samuel Cahyawijaya, Santiago Góngora, Soyeong Jeong, Sukannya Purkayastha, Tatsuki Kuribayashi, Teresa Clifford, Thanmay Jayakumar, Tiago Timponi Torrent, Toqeer Ehsan, Vladimir Araujo, Yova Kementchedjhieva, Zara Burzo, Zheng Wei Lim, Zheng Xin Yong, Oana Ignat, Joan Nwatu, Rada Mihalcea, Thamar Solorio, Alham Fikri Aji\",\n",
       "  366: 'Dong Huang, Yuhao Qing, Weiyi Shang, Heming Cui, Jie M. Zhang',\n",
       "  367: 'George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, Swarat Chaudhuri',\n",
       "  368: 'Dongsu Lee, Minhae Kwon',\n",
       "  369: 'Jindong Jiang, Fei Deng, Gautam Singh, Minseung Lee, Sungjin Ahn',\n",
       "  370: 'Yongwei Nie, Mingxian Fan, Chengjiang Long, Qing Zhang, Jian Zhu, Xuemiao Xu',\n",
       "  371: 'Yu Xiang, Jie Qiao, Zhefeng Liang, Zihuai Zeng, Ruichu Cai, Zhifeng Hao',\n",
       "  372: \"Chu'nan Liu, Lilian Denzler, Yihong Chen, Andrew Martin, Brooks Paige\",\n",
       "  373: 'Floor Eijkelboom, Grigory Bartosh, Christian A. Naesseth, Max Welling, Jan-Willem van de Meent',\n",
       "  374: 'Jaehee Kim, Yukyung Lee, Pilsung Kang',\n",
       "  375: 'Haicang Zhou, Weiming Huang, Yile Chen, Tiantian He, Gao Cong, Yew-Soon Ong',\n",
       "  376: 'Haizhou Du, Yijian Chen, Ryan Yang, Yuchen Li, Linghe Kong',\n",
       "  377: 'Gwanghyun Kim, Alonso Martinez, Yu-Chuan Su, Brendan Jou, José Lezama, Agrim Gupta, Lijun Yu, Lu Jiang, Aren Jansen, Jacob Walker, Krishna Somandepalli',\n",
       "  378: 'Haoran Lu, Ruihai Wu, Yitong Li, Sijie Li, Ziyu Zhu, Chuanruo Ning, Yan Shen, Longzan Luo, Yuanpei Chen, Hao Dong',\n",
       "  379: 'Hayden McTavish, Jon Donnelly, Margo Seltzer, Cynthia Rudin',\n",
       "  380: 'Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Duyu Tang, Kai Han, Yunhe Wang',\n",
       "  381: 'Yiwen Qiu, Yujia Zheng, Kun Zhang',\n",
       "  382: 'Barna Pásztor, Parnian Kassraie, Andreas Krause',\n",
       "  383: 'Sanghyun Son, Matheus Gadelha, Yang Zhou, Zexiang Xu, Ming C. Lin, Yi Zhou',\n",
       "  384: 'Jake Silberg, Kyle Swanson, Elana Simon, Angela Zhang, Zaniar Ghazizadeh, Scott Ogden, Hisham Hamadeh, James Zou',\n",
       "  385: 'Xingkui Zhu, Yiran Guan, Dingkang Liang, Yuchao Chen, Yuliang Liu, Xiang Bai',\n",
       "  386: 'Rongzhe Wei, Eli Chien, Pan Li',\n",
       "  387: 'Jialin Luo, Yuanzhi Wang, Ziqi Gu, Yide Qiu, Shuaizhen Yao, Fuyun Wang, Chunyan Xu, Wenhua Zhang, Dan Wang, Zhen Cui',\n",
       "  388: 'Mingjia Li, Shuang Li, Tongrui Su, Longhui Yuan, Jian Liang, Wei Li',\n",
       "  389: 'Wang Lin, Jingyuan Chen, Jiaxin Shi, Zirun Guo, Yichen Zhu, Zehan Wang, Tao Jin, Zhou Zhao, Fei Wu, Shuicheng YAN, Hanwang Zhang',\n",
       "  390: 'Xin Cai, Zhiyuan You, Hailong Zhang, Wentao Liu, Jinwei Gu, Tianfan Xue',\n",
       "  391: 'Yuda Song, Gokul Swamy, Aarti Singh, J. Andrew Bagnell, Wen Sun',\n",
       "  392: 'Delin Qu, Qizhi Chen, Pingrui Zhang, Xianqiang Gao, Bin Zhao, Zhigang Wang, Dong Wang, Xuelong Li',\n",
       "  393: 'Kairan Zhao, Meghdad Kurmanji, George-Octavian Barbulescu, Eleni Triantafillou, Peter Triantafillou',\n",
       "  394: 'Zakaria Mhammedi, Dylan J. Foster, Alexander Rakhlin',\n",
       "  395: 'Sergey Samsonov, Eric Moulines, Qi-Man Shao, Zhuo-Song Zhang, Alexey Naumov',\n",
       "  396: 'Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, Aviral Kumar',\n",
       "  397: 'Nikita Starodubcev, Mikhail Khoroshikh, Artem Babenko, Dmitry Baranchuk',\n",
       "  398: 'Miao Lu, Han Zhong, Tong Zhang, Jose Blanchet',\n",
       "  399: 'Jiabao Ji, Yujian Liu, Yang Zhang, Gaowen Liu, Ramana Rao Kompella, Sijia Liu, Shiyu Chang',\n",
       "  400: 'Lei Zhu, Fangyun Wei, Yanye Lu, Dong Chen',\n",
       "  401: 'Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, David Atanasov, Robie Gonzales, Subhabrata Majumdar, Carsten Maple, Hassan Sajjad, Frank Rudzicz',\n",
       "  402: 'Changdae Oh, Hyesu Lim, Mijoo Kim, Dongyoon Han, Sangdoo Yun, Jaegul Choo, Alexander Hauptmann, Zhi-Qi Cheng, Kyungwoo Song',\n",
       "  403: 'Peiran Dong, Bingjie Wang, Song Guo, Junxiao Wang, Jie Zhang, Zicong Hong',\n",
       "  404: 'Hongyi Zhou, Denis Blessing, Ge Li, Onur Celik, Xiaogang Jia, Gerhard Neumann, Rudolf Lioutikov',\n",
       "  405: 'Peng Tan, Hai-Tian Liu, Zhi-Hao Tan, Zhi-Hua Zhou',\n",
       "  406: 'Woosung Kim, Hayeong Lee, Jongmin Lee, Byung-Jun Lee',\n",
       "  407: 'Shinsaku Sakaue, Taihei Oki',\n",
       "  408: 'Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, Ying Shan',\n",
       "  409: 'Jingyi Zhang, Jiaxing Huang, Xiaoqin Zhang, Ling Shao, Shijian Lu',\n",
       "  410: 'Sobihan Surendran, Adeline Fermanian, Antoine Godichon-Baggioni, Sylvain Le Corff',\n",
       "  411: 'Keegan Harris, Zhiwei Steven Wu, Maria-Florina Balcan',\n",
       "  412: 'Haiyang Huang, Yingfan Wang, Cynthia Rudin',\n",
       "  413: 'Yuqi Wang, Ke Cheng, Jiawei He, Qitai Wang, Hengchen Dai, Yuntao Chen, Fei Xia, Zhaoxiang Zhang',\n",
       "  414: 'Qiang Wu, Gechang Yao, Zhixi Feng, Shuyuan Yang',\n",
       "  415: 'Chenlin Zhou, Han Zhang, Zhaokun Zhou, Liutao Yu, Liwei Huang, Xiaopeng Fan, Li Yuan, Zhengyu Ma, Huihui Zhou, Yonghong Tian',\n",
       "  416: 'Yongzhe Jia, Xuyun Zhang, Hongsheng Hu, Kim-Kwang Raymond Choo, Lianyong Qi, Xiaolong Xu, Amin Beheshti, Wanchun Dou',\n",
       "  417: 'Miles Hutson, Isaac Kauvar, Nick Haber',\n",
       "  418: 'Pham Duy Khanh, Hoang-Chau Luong, Boris S. Mordukhovich, Dat Ba Tran',\n",
       "  419: 'Ariel D. Procaccia, Benjamin Schiffer, Shirley Zhang',\n",
       "  420: 'Boris Repasky, Anthony Dick, Ehsan Abbasnejad',\n",
       "  421: 'Tianyu He, Darshil Doshi, Aritra Das, Andrey Gromov',\n",
       "  422: 'Hanyue Lou, Jinxiu Liang, Minggui Teng, Bin Fan, Yong Xu, Boxin Shi',\n",
       "  423: 'Junyu Lu, Bo Xu, Xiaokun Zhang, Hongbo Wang, Haohao Zhu, Dongyu Zhang, Liang Yang, Hongfei Lin',\n",
       "  424: 'Tariq Berrada, Pietro Astolfi, Melissa Hall, Reyhane Askari-Hemmat, Yohann Benchetrit, Marton Havasi, Matthew Muckley, Karteek Alahari, Adriana Romero-Soriano, Jakob Verbeek, Michal Drozdzal',\n",
       "  425: 'Xinyu Zhao, Guoheng Sun, Ruisi Cai, Yukun Zhou, Pingzhi Li, Peihao Wang, Bowen Tan, Yexiao He, Li Chen, Yi Liang, Beidi Chen, Binhang Yuan, Hongyi Wang, Ang Li, Zhangyang Wang, Tianlong Chen',\n",
       "  426: 'Peng Wang, Songshuo Lu, Yaohua Tang, Sijie Yan, Wei Xia, Yuanjun Xiong',\n",
       "  427: 'Takeshi Noda, Chao Chen, Weiqi Zhang, Xinhai Liu, Yu-Shen Liu, Zhizhong Han',\n",
       "  428: 'Hongzhi Ruan, Haibao Yu, Wenxian Yang, Siqi Fan, Zaiqing Nie',\n",
       "  429: 'Matthew Dowling, Yuan Zhao, Il Memming Park',\n",
       "  430: 'Louis Serrano, Thomas X Wang, Etienne Le Naour, Jean-Noël Vittaut, Patrick Gallinari',\n",
       "  431: 'Hang Guo, Tao Dai, Yuanchao Bai, Bin Chen, Xudong Ren, Zexuan Zhu, Shu-Tao Xia',\n",
       "  432: 'Paul Couairon, Mustafa Shukor, Jean-Emmanuel Haugeard, Matthieu Cord, Nicolas Thome',\n",
       "  433: 'James Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, Tianle Cai',\n",
       "  434: 'Zhe Liu, Jinghua Hou, Xinyu Wang, Xiaoqing Ye, Jingdong Wang, Hengshuang Zhao, Xiang Bai',\n",
       "  435: 'Yinshuang Xu, Dian Chen, Katherine Liu, Sergey Zakharov, Rares Ambrus, Kostas Daniilidis, Vitor Guizilini',\n",
       "  436: 'Zheng Zhan, Yushu Wu, Yifan Gong, Zichong Meng, Zhenglun Kong, Changdi Yang, Geng Yuan, Pu Zhao, Wei Niu, Yanzhi Wang',\n",
       "  437: 'Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Viswanath Ganapathy, Shreya Kadambi, Rafael Esteves, Shubhankar Borse, Paul Whatmough, Risheek Garrepalli, Mart Van Baalen, Harris Teague, Markus Nagel',\n",
       "  438: 'Nithish Kannen, Arif Ahmad, Marco Andreetto, Vinodkumar Prabhakaran, Utsav Prabhu, Adji Bousso Dieng, Pushpak Bhattacharyya, Shachi Dave',\n",
       "  439: 'Jeonghwan Cheon, Sang Wan Lee, Se-Bum Paik',\n",
       "  440: 'Quanling Meng, Qinglin Liu, Zonglin Li, Xiangyuan Lan, Shengping Zhang, Liqiang Nie',\n",
       "  441: 'Yuhui Quan, Tianxiang Zheng, Hui Ji',\n",
       "  442: 'Marcel Kollovieh, Bertrand Charpentier, Daniel Zügner, Stephan Günnemann',\n",
       "  443: 'Seon-Ho Lee, Jue Wang, Zhikang Zhang, David Fan, Xinyu Li',\n",
       "  444: 'Juhao Liang, Zhenyang Cai, Jianqing Zhu, Huang Huang, Kewei Zong, Bang An, Abdulmohsen Alharthi, Juncai He, Lian Zhang, Haizhou Li, Benyou Wang, Jinchao Xu',\n",
       "  445: 'Tristan Cinquin, Marvin Pförtner, Vincent Fortuin, Philipp Hennig, Robert Bamler',\n",
       "  446: 'Paul Mangold, Sergey Samsonov, Safwan Labbi, Ilya Levin, Reda Alami, Alexey Naumov, Eric Moulines',\n",
       "  447: 'Ziyi Liu, Idan Attias, Daniel M. Roy',\n",
       "  448: 'Ieva Petrulionyte, Julien Mairal, Michael Arbel',\n",
       "  449: 'Zhe Jiao, Martin Keller-Ressel',\n",
       "  450: 'Zhongwang Zhang, Pengxiao Lin, Zhiwei Wang, Yaoyu Zhang, Zhi-Qin John Xu',\n",
       "  451: 'Xuan Huang, Hanhui Li, Wanquan Liu, Xiaodan Liang, Yiqiang Yan, Yuhao Cheng, Chengqiang Gao',\n",
       "  452: 'Arpit Agarwal, Eric Balkanski',\n",
       "  453: 'Xinping Chen, Xiao Ke, Wenzhong Guo',\n",
       "  454: 'Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, Vaishaal Shankar',\n",
       "  455: 'Xiaobin Li, Kai Wu, Yujian Betterrest Li, Xiaoyu Zhang, Handing Wang, Jing Liu',\n",
       "  456: 'Alex Jinpeng Wang, Linjie Li, Yiqi Lin, Min Li, Lijuan Wang, Mike Zheng Shou',\n",
       "  457: 'Ruihao Zheng, Zhenkun Wang',\n",
       "  458: 'Steven Morad, Chris Lu, Ryan Kortvelesy, Stephan Liwicki, Jakob Foerster, Amanda Prorok',\n",
       "  459: 'Wenhao Wang, Yifan Sun, Zhentao Tan, Yi Yang',\n",
       "  460: 'Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, Yaliang Li',\n",
       "  461: 'Egor Gladin, Pavel Dvurechensky, Alexander Mielke, Jia-Jie Zhu',\n",
       "  462: 'Junxi Xiao, Qinliang Su',\n",
       "  463: 'Arthur da Cunha, Mikael Møller Høgsgaard, Kasper Green Larsen',\n",
       "  464: 'Tianyi Qiu, Yang Zhang, Xuchuan Huang, Jasmine Xinze Li, Jiaming Ji, Yaodong Yang',\n",
       "  465: 'Thomas Kwa, Drake Thomas, Adrià Garriga-Alonso',\n",
       "  466: 'Xingchi Li, Guanxun Li, Xianyang Zhang',\n",
       "  467: 'Alaia Solko-Breslin, Seewon Choi, Ziyang Li, Neelay Velingker, Rajeev Alur, Mayur Naik, Eric Wong',\n",
       "  468: 'Yuan He, Zhangdie Yuan, Jiaoyan Chen, Ian Horrocks',\n",
       "  469: 'Shivang Rawat, David J. Heeger, Stefano Martiniani',\n",
       "  470: 'Che Liu, Cheng Ouyang, Sibo Cheng, Anand Shah, Wenjia Bai, Rossella Arcucci',\n",
       "  471: 'Xinyi Wu, Amir Ajorlou, Yifei Wang, Stefanie Jegelka, Ali Jadbabaie',\n",
       "  472: 'Aaron Mishkin, Ahmed Khaled, Yuanhao Wang, Aaron Defazio, Robert M. Gower',\n",
       "  473: 'Haiji Liang, Ruize Han',\n",
       "  474: 'Yang Xu, Yihong Gu, Cong Fang',\n",
       "  475: 'Xiaokun Feng, Xuchen Li, Shiyu Hu, Dailing Zhang, Meiqi Wu, Jing Zhang, Xiaotang Chen, Kaiqi Huang',\n",
       "  476: 'Mingrui Zhang, Chunyang Wang, Stephan Kramer, Joseph G. Wallwork, Siyi Li, Jiancheng Liu, Xiang Chen, Matthew D. Piggott',\n",
       "  477: 'Guang-Yuan Hao, Jiji Zhang, Biwei Huang, Hao Wang, Kun Zhang',\n",
       "  478: 'Changwoo Lee, Soo Min Kwon, Qing Qu, Hun-Seok Kim',\n",
       "  479: 'Yiping Wang, Yifang Chen, Wendan Yan, Alex Fang, Wenjing Zhou, Kevin Jamieson, Simon Shaolei Du',\n",
       "  480: 'Xiaoxing Wang, Xiaohan Qin, Xiaokang Yang, Junchi Yan',\n",
       "  481: 'Shengyuan Chen, Qinggang Zhang, Junnan Dong, Wen Hua, Qing Li, Xiao Huang',\n",
       "  482: 'Qiyao Liang, Ziming Liu, Mitchell Ostrow, Ila Fiete',\n",
       "  483: 'Antoine Scheid, Aymeric Capitaine, Etienne Boursier, Eric Moulines, Michael I. Jordan, Alain Durmus',\n",
       "  484: 'Pedro R. A. S. Bassi, Wenxuan Li, Yucheng Tang, Fabian Isensee, Zifu Wang, Jieneng Chen, Yu-Cheng Chou, Saikat Roy, Yannick Kirchhoff, Maximilian Rokuss, Ziyan Huang, Jin Ye, Junjun He, Tassilo Wald, Constantin Ulrich, Michael Baumgartner, Klaus H. Maier-Hein, Paul Jaeger, Yiwen Ye, Yutong Xie, Jianpeng Zhang, Ziyang Chen, Yong Xia, Zhaohu Xing, Lei Zhu, Yousef Sadegheih, Afshin Bozorgpour, Pratibha Kumari, Reza Azad, Dorit Merhof, Pengcheng Shi, Ting Ma, Yuxin Du, Fan Bai, Tiejun Huang, Bo Zhao, Haonan Wang, Xiaomeng Li, Hanxue Gu, Haoyu Dong, Jichen Yang, Maciej A. Mazurowski, Saumya Gupta, Linshan Wu, Jiaxin Zhuang, Hao Chen, Holger Roth, Daguang Xu, Matthew B. Blaschko, Sergio Decherchi, Andrea Cavalli, Alan L. Yuille, Zongwei Zhou',\n",
       "  485: 'Rongzhen Wang, Chenyu Zheng, Guoqiang Wu, Xu Min, Xiaolu Zhang, Jun Zhou, Chongxuan Li',\n",
       "  486: 'Taejong Joo, Diego Klabjan',\n",
       "  487: 'Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, Yu Qiao',\n",
       "  488: 'Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, Lidong Bing',\n",
       "  489: 'Hongyao Tang, Glen Berseth',\n",
       "  490: 'Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek F. Wong, Emine Yilmaz, Shuming Shi, Zhaopeng Tu',\n",
       "  491: 'Walter Simoncini, Spyros Gidaris, Andrei Bursuc, Yuki M. Asano',\n",
       "  492: 'Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro',\n",
       "  493: 'Jihao Andreas Lin, Shreyas Padhy, Bruno Mlodozeniec, Javier Antorán, José Miguel Hernández-Lobato',\n",
       "  494: 'Hao Ma, Tianyi Hu, Zhiqiang Pu, Boyin Liu, Xiaolin Ai, Yanyan Liang, Min Chen',\n",
       "  495: 'Austin Coursey, Junyi Ji, Marcos Quinones-Grueiro, William Barbour, Yuhang Zhang, Tyler Derr, Gautam Biswas, Daniel B. Work',\n",
       "  496: 'Diba Hashemi, Lie He, Martin Jaggi',\n",
       "  497: 'Xiao Tan, Yiqin Wang, Yangyang Shen, Dian Shen, Meng Wang, Peibo Duan, Beilun Wang',\n",
       "  498: 'Shraddha Barke, Emmanuel Anaya Gonzalez, Saketh Ram Kasibatla, Taylor Berg-Kirkpatrick, Nadia Polikarpova',\n",
       "  499: 'Anshul Gupta, Samy Tafasca, Arya Farkhondeh, Pierre Vuillecard, Jean-Marc Odobez',\n",
       "  500: 'Feiran Jia, Ziyu Ye, Shiyang Lai, Kai Shu, Jindong Gu, Adel Bibi, Ziniu Hu, David Jurgens, James Evans, Philip H.S. Torr, Bernard Ghanem, Guohao Li, Chengxing Xie, Canyu Chen',\n",
       "  501: 'Yuxin Xiao, Chaoqun Wan, Yonggang Zhang, Wenxiao Wang, Binbin Lin, Xiaofei He, Xu Shen, Jieping Ye',\n",
       "  502: 'Pragya Singh, Ritvik Budhiraja, Ankush Gupta, Anshul Goswami, Mohan Kumar, Pushpendra Singh',\n",
       "  503: 'Anoop Cherian, Kuan-Chuan Peng, Suhas Lohit, Joanna Matthiesen, Kevin Smith, Joshua B. Tenenbaum',\n",
       "  504: 'Xiaoyue Xu, Qinyuan Ye, Xiang Ren',\n",
       "  505: 'Yangjun Ruan, Chris J. Maddison, Tatsunori Hashimoto',\n",
       "  506: 'Guy Tennenholtz, Yinlam Chow, Chih-Wei Hsu, Lior Shani, Ethan Liang, Craig Boutilier',\n",
       "  507: 'Richard Nock, Yishay Mansour',\n",
       "  508: 'Harit Vishwakarma, Yi Chen, Sui Jiet Tay, Satya Sai Srinath Namburi, Frederic Sala, Ramya Korlakai Vinayak',\n",
       "  509: 'Zhaoyang Sun, Shengwu Xiong, Yaxiong Chen, Fei Du, Weihua Chen, Fan Wang, Yi Rong',\n",
       "  510: 'Li Ma, Haoyu Han, Juanhui Li, Harry Shomer, Hui Liu, Xiaofeng Gao, Jiliang Tang',\n",
       "  511: 'Alex Rutherford, Michael Beukman, Timon Willi, Bruno Lacerda, Nick Hawes, Jakob Foerster',\n",
       "  512: 'Raj Agrawal, Sam Witty, Andy Zane, Eli Bingham',\n",
       "  513: 'Xiayan Ji, Anton Xue, Eric Wong, Oleg Sokolsky, Insup Lee',\n",
       "  514: 'Kaibo Zhang, Yunjuan Wang, Raman Arora',\n",
       "  515: 'Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Xiuwei Shang, Weiming Zhang, Nenghai Yu',\n",
       "  516: 'Rakshit S Trivedi, Akbir Khan, Jesse Clifton, Lewis Hammond, Edgar A. Duéñez-Guzmán, John P Agapiou, Jayd Matyas, Sasha Vezhnevets, Dipam Chakraborty, Yue Zhao, Marko Tesic, Barna Pásztor, Yunke Ao, Omar G. Younis, Jiawei Huang, Benjamin Swain, Haoyuan Qin, Mian Deng, Ziwei Deng, Utku Erdoğanaras, Natasha Jaques, Jakob Nicolaus Foerster, Vincent Conitzer, Jose Hernandez-Orallo, Dylan Hadfield-Menell, Joel Z Leibo',\n",
       "  517: 'Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, Gordon Wetzstein',\n",
       "  518: 'Hee Jae Kim, Kathakoli Sengupta, Masaki Kuribayashi, Hernisa Kacorri, Eshed Ohn-Bar',\n",
       "  519: 'Min Jae Song',\n",
       "  520: 'Alexander Braun, Sherry Sarkar',\n",
       "  521: 'Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, Max Ryabinin',\n",
       "  522: 'Kevin Christian Wibisono, Yixin Wang',\n",
       "  523: 'Lingao Xiao, Yang He',\n",
       "  524: 'Chenghua Guo, Han Yu, Jiaxin Liu, Chao Chen, Qi Li, Sihong Xie, Xi Zhang',\n",
       "  525: 'Elliot Paquette, Courtney Paquette, Lechao Xiao, Jeffrey Pennington',\n",
       "  526: 'Abhinav Kumar, Kirankumar Shiragur, Caroline Uhler',\n",
       "  527: 'Hongfu Gao, Feipeng Zhang, Wenyu Jiang, Jun Shu, Feng Zheng, Hongxin Wei',\n",
       "  528: 'Mingzhe Du, Luu Anh Tuan, Bin Ji, Qian Liu, See-Kiong Ng',\n",
       "  529: 'Tianwei Xiong, Yuqing Wang, Daquan Zhou, Zhijie Lin, Jiashi Feng, Xihui Liu',\n",
       "  530: 'Tongle Wu, Ying Sun',\n",
       "  531: 'Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, Abhinav Bhatele',\n",
       "  532: 'Yang Li, Shaobo Han, Shihao Ji',\n",
       "  533: 'Hanchen Xia, Weidong Liu, Xiaojun Mao',\n",
       "  534: 'Hoang Phuc Hau Luu, Hanlin Yu, Bernardo Williams, Petrus Mikkola, Marcelo Hartmann, Kai Puolamäki, Arto Klami',\n",
       "  535: 'Qiujiang Jin, Ruichen Jiang, Aryan Mokhtari',\n",
       "  536: 'Shen Li, Yuyang Zhang, Zhaolin Ren, Claire Liang, Na Li, Julie A. Shah',\n",
       "  537: 'Scott Jeen, Tom Bewley, Jonathan M. Cullen',\n",
       "  538: 'Chang-Wei Shi, Yi-Rui Yang, Wu-Jun Li',\n",
       "  539: 'Amir Hossein Kargaran, François Yvon, Hinrich Schütze',\n",
       "  540: 'Esraa Elelimy, Adam White, Michael Bowling, Martha White',\n",
       "  541: 'Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, Deva Ramanan',\n",
       "  542: 'Shreyas Chaudhari, Ameet Deshpande, Bruno Castro da Silva, Philip S. Thomas',\n",
       "  543: 'Jiahao Ying, Yixin Cao, Yushi Bai, Qianru Sun, Bo Wang, Wei Tang, Zhaojun Ding, Yizhe Yang, Xuanjing Huang, Shuicheng Yan',\n",
       "  544: 'Yizun Lin, Zhao-Rong Lai, Cheng Li',\n",
       "  545: 'Juntao Dai, Tianle Chen, Xuyao Wang, Ziran Yang, Taiye Chen, Jiaming Ji, Yaodong Yang',\n",
       "  546: \"Yannis Karmim, Marc Lafon, Raphaël Fournier S'niehotta, Nicolas Thome\",\n",
       "  547: 'Shangding Gu, Laixi Shi, Yuhao Ding, Alois Knoll, Costas Spanos, Adam Wierman, Ming Jin',\n",
       "  548: 'Daehee Lee, Minjong Yoo, Woo Kyung Kim, Wonje Choi, Honguk Woo',\n",
       "  549: 'Apurv Shukla, Debabrota Basu',\n",
       "  550: 'Zhongchang Sun, Sihong He, Fei Miao, Shaofeng Zou',\n",
       "  551: 'Jiaxu Leng, Zhanjie Wu, Mingpi Tan, Yiran Liu, Ji Gan, Haosheng Chen, Xinbo Gao',\n",
       "  552: 'Zhengming Chen, Ruichu Cai, Feng Xie, Jie Qiao, Anpeng Wu, Zijian Li, Zhifeng Hao, Kun Zhang',\n",
       "  553: 'Christopher Blöcker, Chester Tan, Ingo Scholtes',\n",
       "  554: 'Jianan Yang, Chenchao Gao, Zhiqing Xiao, Junbo Zhao, Sai Wu, Gang Chen, Haobo Wang',\n",
       "  555: 'Charbel Sakr, Brucek Khailany',\n",
       "  556: 'Zhanhao Hu, Julien Piet, Geng Zhao, Jiantao Jiao, David Wagner',\n",
       "  557: 'Fei Ni, Jianye Hao, Shiguang Wu, Longxin Kou, Yifu Yuan, Zibin Dong, Jinyi Liu, Mingzhi Li, Yuzheng Zhuang, Yan Zheng',\n",
       "  558: 'Yiğit Ekin, Ahmet Burak Yildirim, Erdem Eren Caglar, Aykut Erdem, Erkut Erdem, Aysegul Dundar',\n",
       "  559: 'Nikita Kalinin, Christoph Lampert',\n",
       "  560: 'Chengyi Cai, Zesheng Ye, Lei Feng, Jianzhong Qi, Feng Liu',\n",
       "  561: 'Haogeng Liu, Quanzeng You, Xiaotian Han, Yongfei Liu, Huaibo Huang, Ran He, Hongxia Yang',\n",
       "  562: 'Max Hamilton, Christian Lange, Elijah Cole, Alexander Shepard, Samuel Heinrich, Oisin Mac Aodha, Grant Van Horn, Subhransu Maji',\n",
       "  563: 'Matthew Wallingford, Anand Bhattad, Aditya Kusupati, Vivek Ramanujan, Matt Deitke, Sham Kakade, Aniruddha Kembhavi, Roozbeh Mottaghi, Wei-Chiu Ma, Ali Farhadi',\n",
       "  564: 'Juexiao Zhang, Gao Zhu, Sihang Li, Xinhao Liu, Haorui Song, Xinran Tang, Chen Feng',\n",
       "  565: 'Kyoungseok Jang, Junpei Komiyama, Kazutoshi Yamazaki',\n",
       "  566: 'Xiaojuan Tang, Jiaqi Li, Yitao Liang, Song-chun Zhu, Muhan Zhang, Zilong Zheng',\n",
       "  567: 'Zhuoping Zhou, Davoud Ataee Tarzanagh, Bojian Hou, Qi Long, Li Shen',\n",
       "  568: 'Jiawei Chen, Chunhui Zhao',\n",
       "  569: 'Yi-Shan Wu, Yijie Zhang, Badr-Eddine Chérief-Abdellatif, Yevgeny Seldin',\n",
       "  570: 'Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry, Evangelos Milios, Sageev Oore, Hassan Sajjad',\n",
       "  571: 'Sangyun Shin, Yuhang He, Madhu Vankadari, Ta-Ying Cheng, Qian Xie, Andrew Markham, Niki Trigoni',\n",
       "  572: 'Dingshuo Chen, Zhixun Li, Yuyan Ni, Guibin Zhang, Ding Wang, Qiang Liu, Shu Wu, Jeffrey Xu Yu, Liang Wang',\n",
       "  573: 'Yilun Jin, Zheng Li, Chenwei Zhang, Tianyu Cao, Yifan Gao, Pratik Jayarao, Mao Li, Xin Liu, Ritesh Sarkhel, Xianfeng Tang, Haodong Wang, Zhengyang Wang, Wenju Xu, Jingfeng Yang, Qingyu Yin, Xian Li, Priyanka Nigam, Yi Xu, Kai Chen, Qiang Yang, Meng Jiang, Bing Yin',\n",
       "  574: 'Minui Hong, Junhyeog Yun, Insu Jeon, Gunhee Kim',\n",
       "  575: 'Diego Doimo, Alessandro Serra, Alessio Ansuini, Alberto Cazzaniga',\n",
       "  576: 'Zhaorui Tan, Xi Yang, Qiufeng Wang, Anh Nguyen, Kaizhu Huang',\n",
       "  577: 'Yeonguk Yu, Minhwan Ko, Sungho Shin, Kangmin Kim, Kyoobin Lee',\n",
       "  578: 'Matthijs Pals, A Erdem Sağtekin, Felix Pei, Manuel Gloeckler, Jakob H Macke',\n",
       "  579: 'Corinna Cortes, Anqi Mao, Christopher Mohri, Mehryar Mohri, Yutao Zhong',\n",
       "  580: 'Ruiqi Zhang, Jingfeng Wu, Peter L. Bartlett',\n",
       "  581: 'Xin Hu, Xiaole Tang, Ruixuan Yu, Jian Sun',\n",
       "  582: 'Bo Liu, Lemeng Wu, Lizhang Chen, Kaizhao Liang, Jiaxu Zhu, Chen Liang, Raghuraman Krishnamoorthi, Qiang Liu',\n",
       "  583: 'MohammadTaghi Hajiaghayi, Shayan Chashm Jahan, Mohammad Sharifi, Suho Shin, Max Springer',\n",
       "  584: 'MohammadTaghi Hajiaghayi, Sébastien Lahaie, Keivan Rezaei, Suho Shin',\n",
       "  585: 'Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, Jian Zhang',\n",
       "  586: 'Adithya Bhaskar, Alexander Wettig, Dan Friedman, Danqi Chen',\n",
       "  587: 'Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, Ling-Yu Duan',\n",
       "  588: 'Declan McNamara, Jackson Loper, Jeffrey Regier',\n",
       "  589: 'Robi Bhattacharjee, Ulrike von Luxburg',\n",
       "  590: 'Xinyu Xu, Yizheng Zhang, Yong-Lu Li, Lei Han, Cewu Lu',\n",
       "  591: 'Samin Yeasar Arnob, Riyasat Ohib, Sergey Plis, Amy Zhang, Alessandro Sordoni, Doina Precup',\n",
       "  592: 'Jonathan Roberts, Kai Han, Neil Houlsby, Samuel Albanie',\n",
       "  593: 'Megan Tjandrasuwita, Jie Xu, Armando Solar-Lezama, Wojciech Matusik',\n",
       "  594: 'Lisha Chen, AFM Saif, Yanning Shen, Tianyi Chen',\n",
       "  595: 'Jingdi Chen, Hanhan Zhou, Yongsheng Mei, Carlee Joe-Wong, Gina Adam, Nathaniel D. Bastian, Tian Lan',\n",
       "  596: 'Eva Giboulot, Teddy Furon',\n",
       "  597: 'Daeho Um, Ji Won Yoon, Seong Jin Ahn, Yunha Yeo',\n",
       "  598: 'Benyuan Meng, Qianqian Xu, Zitai Wang, Zhiyong Yang, Xiaochun Cao, Qingming Huang',\n",
       "  599: 'Xiong-Hui Chen, Ziyan Wang, Yali Du, Shengyi Jiang, Meng Fang, Yang Yu, Jun Wang',\n",
       "  600: 'Ismail R. Alkhouri, Shijun Liang, Evan Bell, Qing Qu, Rongrong Wang, Saiprasad Ravishankar',\n",
       "  601: 'Aleksandros Sobczyk, Marko Mladenović, Mathieu Luisier',\n",
       "  602: 'Chaolong Ying, Xinjian Zhao, Tianshu Yu',\n",
       "  603: 'Yanmin Wu, Jiarui Meng, Haijie Li, Chenming Wu, Yahao Shi, Xinhua Cheng, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Jian Zhang',\n",
       "  604: 'Ioannis Caragiannis, Evi Micha, Nisarg Shah',\n",
       "  605: 'Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, Stefano Ermon',\n",
       "  606: 'Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, Pengfei Liu',\n",
       "  607: 'David Debot, Pietro Barbiero, Francesco Giannini, Gabriele Ciravegna, Michelangelo Diligenti, Giuseppe Marra',\n",
       "  608: 'Yongyuan Liang, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, Huazhe Xu',\n",
       "  609: 'Yuqing Wang, Ye He, Molei Tao',\n",
       "  610: 'Grzegorz Stefański, Paweł Daniluk, Artur Szumaczuk, Jakub Tkaczuk',\n",
       "  611: 'Yunzhe Qi, Yikun Ban, Arindam Banerjee, Jingrui He',\n",
       "  612: 'Weimin Bai, Yifei Wang, Wenzheng Chen, He Sun',\n",
       "  613: 'Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, Jiaqi Wang',\n",
       "  614: 'Hanna Foerster, Robert Mullins, Ilia Shumailov, Jamie Hayes',\n",
       "  615: 'Aditya Bommakanti, Harshith Reddy Vonteri, Konstantinos Skitsas, Sayan Ranu, Davide Mottin, Panagiotis Karras',\n",
       "  616: 'Anish Madan, Neehar Peri, Shu Kong, Deva Ramanan',\n",
       "  617: 'Parnian Kassraie, Aram-Alexandre Pooladian, Michal Klein, James Thornton, Jonathan Niles-Weed, Marco Cuturi',\n",
       "  618: 'Dorian Baudry, Hugo Richard, Maria Cherifa, Clément Calauzènes, Vianney Perchet',\n",
       "  619: 'Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio',\n",
       "  620: 'Anthony Fuller, Daniel G. Kyrollos, Yousef Yassin, James R. Green',\n",
       "  621: 'Yan Huang, Xiang Li, Yipeng Shen, Niao He, Jinming Xu',\n",
       "  622: 'Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Rezende, Yoshua Bengio, Michael Mozer, Sanjeev Arora',\n",
       "  623: 'Yonghan Jung, Min Woo Park, Sanghack Lee',\n",
       "  624: 'Ryoma Yataka, Adriano Cardace, Pu (Perry) Wang, Petros Boufounos, Ryuhei Takahashi',\n",
       "  625: 'Maciej Sypetkowski, Frederik Wenkel, Farimah Poursafaei, Nia Dickson, Karush Suri, Philip Fradkin, Dominique Beaini',\n",
       "  626: 'Yinxuan Huang, Chengmin Gao, Bin Li, Xiangyang Xue',\n",
       "  627: 'Yixuan Even Xu, Hanrui Zhang, Yu Cheng, Vincent Conitzer',\n",
       "  628: 'Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, Ranjay Krishna',\n",
       "  629: 'Kai Chen, Yiyao Ma, Xingyu Lin, Stephen James, Jianshu Zhou, Yun-Hui Liu, Pieter Abbeel, Qi Dou',\n",
       "  630: 'Deepak Ravikumar, Efstathia Soufleri, Kaushik Roy',\n",
       "  631: 'Chen Hang, Zhe Ma, Haoming Chen, Xuwei Fang, Weisheng Xie, Faming Fang, Guixu Zhang, Hongbin Wang',\n",
       "  632: 'Zunnan Xu, Yukang Lin, Haonan Han, Sicheng Yang, Ronghui Li, Yachao Zhang, Xiu Li',\n",
       "  633: 'Yann Bourreau, Marco Bressan, T-H. Hubert Chan, Qipeng Kuang, Mauro Sozio',\n",
       "  634: 'Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, Dahua Lin',\n",
       "  635: 'Eyar Azar, Boaz Nadler',\n",
       "  636: 'Sunghyeon Woo, Baesung Park, Byeongwook Kim, Minjung Jo, Se Jung Kwon, Dongsuk Jeon, Dongsoo Lee',\n",
       "  637: 'Andrew Szot, Bogdan Mazoure, Harsh Agrawal, Devon Hjelm, Zsolt Kira, Alexander Toshev',\n",
       "  638: 'Adela Frances DePavia, Olga Medrano Martín del Campo, Erasmo Tani',\n",
       "  639: 'Ning Ding, Yehui Tang, Haochen Qin, Zhenli Zhou, Chao Xu, Lin Li, Kai Han, Heng Liao, Yunhe Wang',\n",
       "  640: 'Zeng Tao, Tong Yang, Junxiong Lin, Xinji Mai, Haoran Wang, Beining Wang, Enyu Zhou, Yan Wang, Wenqiang Zhang',\n",
       "  641: 'Yunwei Ren, Zixuan Wang, Jason D. Lee',\n",
       "  642: 'Bei Li, Tong Zheng, Rui Wang, Jiahao Liu, Qingyan Guo, Junliang Guo, Xu Tan, Tong Xiao, Jingbo Zhu, Jingang Wang, Xunliang Cai',\n",
       "  643: 'Enayat Ullah, Michael Menart, Raef Bassily, Cristóbal Guzmán, Raman Arora',\n",
       "  644: 'Xixi Jia, Fangchen Feng, Deyu Meng, Defeng Sun',\n",
       "  645: 'Zhenxiong Tan, Kaixin Wang, Xinchao Wang',\n",
       "  646: 'Daiqing Qi, Handong Zhao, Sheng Li',\n",
       "  647: 'Guanyu Nie, Vaneet Aggarwal, Christopher John Quinn',\n",
       "  648: 'Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, Xizhou Zhu, Ping Luo, Yu Qiao, Jifeng Dai, Wenqi Shao, Wenhai Wang',\n",
       "  649: 'Artur Szałata, Andrew Benz, Robrecht Cannoodt, Mauricio Cortes, Jason Fong, Sunil Kuppasani, Richard Lieberman, Tianyu Liu, Javier A. Mas-Rosario, Rico Meinl, Jalil Nourisa, Jared Tumiel, Tin M. Tunjic, Mengbo Wang, Noah Weber, Hongyu Zhao, Benedict Anchang, Fabian J. Theis, Malte D. Luecken, Daniel B. Burkhardt',\n",
       "  650: 'Zhou Fang, Yong-Lu Li, Lixin Yang, Cewu Lu',\n",
       "  651: 'Noah Golowich, Ankur Moitra',\n",
       "  652: 'Kanan Gupta, Jonathan W. Siegel, Stephan Wojtowytsch',\n",
       "  653: 'Chandramouli S. Sastry, Sri Harsha Dumpala, Sageev Oore',\n",
       "  654: 'Jiaxi Hu, Yuehong Hu, Wei Chen, Ming Jin, Shirui Pan, Qingsong Wen, Yuxuan Liang',\n",
       "  655: 'Nicolás Astorga, Tennison Liu, Nabeel Seedat, Mihaela van der Schaar',\n",
       "  656: 'Minghui Chen, Meirui Jiang, Xin Zhang, Qi Dou, Zehua Wang, Xiaoxiao Li',\n",
       "  657: 'Di Ming, Peng Ren, Yunlong Wang, Xin Feng',\n",
       "  658: 'Zhenwei Tang, Difan Jiao, Reid McIlroy-Young, Jon Kleinberg, Siddhartha Sen, Ashton Anderson',\n",
       "  659: 'Yuan Qiu, Nolan Bridges, Peng Chen',\n",
       "  660: 'Shogo Iwazaki, Shinya Suzumura',\n",
       "  661: 'Hoin Jung, Taeuk Jang, Xiaoqian Wang',\n",
       "  662: 'Eleonora Gualdoni, Mycal Tucker, Roger P. Levy, Noga Zaslavsky',\n",
       "  663: 'Tom Sander, Pierre Fernandez, Alain Durmus, Matthijs Douze, Teddy Furon',\n",
       "  664: 'Natalie Maus, Kyurae Kim, Geoff Pleiss, David Eriksson, John P. Cunningham, Jacob R. Gardner',\n",
       "  665: 'Bonwoo Lee, Jeongyoun Ahn, Cheolwoo Park',\n",
       "  666: 'Siwei Tu, Weidong Yang, Ben Fei',\n",
       "  667: 'Taewon Park, Hyun-Chul Kim, Minho Lee',\n",
       "  668: 'Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, Li Yuan',\n",
       "  669: 'Jonas Kulhanek, Songyou Peng, Zuzana Kukelova, Marc Pollefeys, Torsten Sattler',\n",
       "  670: 'Boxuan Zhang, Jianing Zhu, Zengmao Wang, Tongliang Liu, Bo Du, Bo Han',\n",
       "  671: 'Joshua Robinson, Rishabh Ranjan, Weihua Hu, Kexin Huang, Jiaqi Han, Alejandro Dobles, Matthias Fey, Jan E. Lenssen, Yiwen Yuan, Zecheng Zhang, Xinwei He, Jure Leskovec',\n",
       "  672: 'Yikun Ban, Jiaru Zou, Zihao Li, Yunzhe Qi, Dongqi Fu, Jian Kang, Hanghang Tong, Jingrui He',\n",
       "  673: 'Zhiqi Li, Yiming Chen, Peidong Liu',\n",
       "  674: 'Bin Fan, Jiaoyang Yin, Yuchao Dai, Chao Xu, Tiejun Huang, Boxin Shi',\n",
       "  675: 'Aleksandr Lobanov, Alexander Gasnikov, Andrei Krasnov',\n",
       "  676: 'Andreas Schlaginhaufen, Maryam Kamgarpour',\n",
       "  677: 'Jiahao Wang, Caixia Yan, Haonan Lin, Weizhan Zhang, Mengmeng Wang, Tieliang Gong, Guang Dai, Hao Sun',\n",
       "  678: 'Alkis Kalavasis, Amin Karbasi, Argyris Oikonomou, Katerina Sotiraki, Grigoris Velegkas, Manolis Zampetakis',\n",
       "  679: 'Zhengyi Li, Kang Yang, Jin Tan, Wen-jie Lu, Haoqi Wu, Xiao Wang, Yu Yu, Derun Zhao, Yancheng Zheng, Minyi Guo, Jingwen Leng',\n",
       "  680: 'Mikhail Khodak, Lester Mackey, Alexandra Chouldechova, Miroslav Dudík',\n",
       "  681: 'Rebecca Saul, Chang Liu, Noah Fleischmann, Richard Zak, Kristopher Micinski, Edward Raff, James Holt',\n",
       "  682: 'Levi E. Lingsch, Dana Grund, Siddhartha Mishra, Georgios Kissas',\n",
       "  683: 'Dongyan Lucy Huo, Yixuan Zhang, Yudong Chen, Qiaomin Xie',\n",
       "  684: 'Anka Reuel, Amelia Hardy, Chandler Smith, Max Lamparth, Malcolm Hardy, Mykel J. Kochenderfer',\n",
       "  685: 'Joel Dyer, Nicholas Bishop, Yorgos Felekis, Fabio Massimo Zennaro, Anisoara Calinescu, Theodoros Damoulas, Michael Wooldridge',\n",
       "  686: 'Yutao Dou, Huimin Yu, Wei Li, Jingyang Li, Fei Xia, Jian Xiao',\n",
       "  687: 'Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao',\n",
       "  688: 'Jacopo Dapueto, Nicoletta Noceti, Francesca Odone',\n",
       "  689: 'Zehui Li, Yuhao Ni, Guoxuan Xia, William Beardall, Akashaditya Das, Guy-Bart Stan, Yiren Zhao',\n",
       "  690: 'Jikang Cheng, Zhiyuan Yan, Ying Zhang, Yuhao Luo, Zhongyuan Wang, Chen Li',\n",
       "  691: 'Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Cheng Jiayang, Cunxiang Wang, Shichao Sun, Huanyu Li, Zizhao Zhang, Binjie Wang, Jiarong Jiang, Tong He, Zhiguo Wang, Pengfei Liu, Yue Zhang, Zheng Zhang',\n",
       "  692: 'Ted Lentsch, Holger Caesar, Dariu M. Gavrila',\n",
       "  693: 'Wei Jiang, Sifan Yang, Yibo Wang, Lijun Zhang',\n",
       "  694: 'Xinran Li, Ling Pan, Jun Zhang',\n",
       "  695: 'Kien X. Nguyen, Fengchun Qiao, Arthur Trembanis, Xi Peng',\n",
       "  696: 'Brandon Huang, Chancharik Mitra, Assaf Arbelle, Leonid Karlinsky, Trevor Darrell, Roei Herzig',\n",
       "  697: 'Baiyu Su, Qiang Liu',\n",
       "  698: 'Jonas Spinner, Victor Bresó, Pim de Haan, Tilman Plehn, Jesse Thaler, Johann Brehmer',\n",
       "  699: 'Chengsen Wang, Qi Qi, Jingyu Wang, Haifeng Sun, Zirui Zhuang, Jinming Wu, Jianxin Liao',\n",
       "  700: 'Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, Chulhee Yun',\n",
       "  701: 'Eyal Michaeli, Ohad Fried',\n",
       "  702: 'Tianhang Wang, Fan Lu, Zehan Zheng, Guang Chen, Changjun Jiang',\n",
       "  703: 'Haotian Ye, Haowei Lin, Jiaqi Han, Minkai Xu, Sheng Liu, Yitao Liang, Jianzhu Ma, James Zou, Stefano Ermon',\n",
       "  704: 'Junjiao Tian, Chengyue Huang, Zsolt Kira',\n",
       "  705: 'Xiaoshuai Hao, Mengchuan Wei, Yifan Yang, Haimei Zhao, Hui Zhang, Yi Zhou, Qiang Wang, Weiming Li, Lingdong Kong, Jing Zhang',\n",
       "  706: 'Felipe Maia Polo, Ronald Xu, Lucas Weber, Mírian Silva, Onkar Bhardwaj, Leshem Choshen, Allysson Flavio Melo de Oliveira, Yuekai Sun, Mikhail Yurochkin',\n",
       "  707: 'Ziyao Wang, Zheyu Shen, Yexiao He, Guoheng Sun, Hongyi Wang, Lingjuan Lyu, Ang Li',\n",
       "  708: 'Nam Phuong Tran, The Anh Ta, Shuqing Shi, Debmalya Mandal, Yali Du, Long Tran-Thanh',\n",
       "  709: 'Mingzhen Huang, Jialing Cai, Shan Jia, Vishnu Suresh Lokhande, Siwei Lyu',\n",
       "  710: 'Regev Cohen, Idan Kligvasser, Ehud Rivlin, Daniel Freedman',\n",
       "  711: 'Christos Thrampoulidis',\n",
       "  712: 'William T. Redman, Francisco Acosta, Santiago Acosta-Mendoza, Nina Miolane',\n",
       "  713: 'Haiyun Yao, Zongbo Han, Huazhu Fu, Xi Peng, Qinghua Hu, Changqing Zhang',\n",
       "  714: 'Wenjia Xie, Hao Wang, Luankang Zhang, Rui Zhou, Defu Lian, Enhong Chen',\n",
       "  715: 'Huanjin Wu, Xinyu Ye, Junchi Yan',\n",
       "  716: 'Adhyyan Narang, Andrew Wagenmaker, Lillian J. Ratliff, Kevin Jamieson',\n",
       "  717: 'Qi Lv, Xiang Deng, Gongwei Chen, Michael Yu Wang, Liqiang Nie',\n",
       "  718: 'Nick Rittler, Kamalika Chaudhuri',\n",
       "  719: 'Christopher T. H. Teo, Milad Abdollahzadeh, Xinda Ma, Ngai-Man Cheung',\n",
       "  720: 'Irene Huang, Wei Lin, M. Jehanzeb Mirza, Jacob A. Hansen, Sivan Doveh, Victor Ion Butoi, Roei Herzig, Assaf Arbelle, Hilde Kuehne, Trevor Darrell, Chuang Gan, Aude Oliva, Rogerio Feris, Leonid Karlinsky',\n",
       "  721: 'Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, Deming Chen',\n",
       "  722: 'Yujia Zhou, Zheng Liu, Zhicheng Dou',\n",
       "  723: 'Alexander Lappe, Anna Bognár, Ghazaleh Ghamkhari Nejad, Albert Mukovskiy, Lucas Martini, Martin A. Giese, Rufin Vogels',\n",
       "  724: 'Massimiliano Datres, Gian Paolo Leonardi, Alessio Figalli, David Sutter',\n",
       "  725: 'Zhishuai Guo, Tianbao Yang',\n",
       "  726: 'Kamalika Chaudhuri, Po-Ling Loh, Shourya Pandey, Purnamrita Sarkar',\n",
       "  727: 'Marco Miani, Lorenzo Beretta, Søren Hauberg',\n",
       "  728: 'Wei Chow, Juncheng Li, Qifan Yu, Kaihang Pan, Hao Fei, Zhiqi Ge, Shuai Yang, Siliang Tang, Hanwang Zhang, Qianru Sun',\n",
       "  729: \"Francesco D'Angelo, Maksym Andriushchenko, Aditya Varre, Nicolas Flammarion\",\n",
       "  730: 'Kai Hu, Weichen Yu, Yining Li, Tianjun Yao, Xiang Li, Wenhe Liu, Lijun Yu, Zhiqiang Shen, Kai Chen, Matt Fredrikson',\n",
       "  731: 'Aliaksandra Shysheya, Cristiana Diaconu, Federico Bergamin, Paris Perdikaris, José Miguel Hernández-Lobato, Richard E. Turner, Emile Mathieu',\n",
       "  732: 'Jonas Ngnawé, Sabyasachi Sahoo, Yann Pequignot, Frédéric Precioso, Christian Gagné',\n",
       "  733: 'Wanghan Xu, Fenghua Ling, Wenlong Zhang, Tao Han, Hao Chen, Wanli Ouyang, Lei Bai',\n",
       "  734: 'Yunpeng Gong, Zhun Zhong, Yansong Qu, Zhiming Luo, Rongrong Ji, Min Jiang',\n",
       "  735: 'Yamin Li, Ange Lou, Ziyuan Xu, Shengchao Zhang, Shiyu Wang, Dario J. Englot, Soheil Kolouri, Daniel Moyer, Roza G. Bayrak, Catie Chang',\n",
       "  736: 'Yang Cai, Gabriele Farina, Julien Grand-Clément, Christian Kroer, Chung-Wei Lee, Haipeng Luo, Weiqiang Zheng',\n",
       "  737: 'Manuel Dahnert, Angela Dai, Norman Müller, Matthias Nießner',\n",
       "  738: 'Lingchen Meng, Jianwei Yang, Rui Tian, Xiyang Dai, Zuxuan Wu, Jianfeng Gao, Yu-Gang Jiang',\n",
       "  739: 'Zenan Li, Zhi Zhou, Yuan Yao, Yu-Feng Li, Chun Cao, Fan Yang, Xian Zhang, Xiaoxing Ma',\n",
       "  740: 'Guande He, Kaiwen Zheng, Jianfei Chen, Fan Bao, Jun Zhu',\n",
       "  741: 'Benjamin Hoover, Duen Horng Chau, Hendrik Strobelt, Parikshit Ram, Dmitry Krotov',\n",
       "  742: 'Jin-Hwi Park, Hae-Gon Jeon',\n",
       "  743: 'William T. Redman, Juan Bello-Rivas, Maria Fonoberova, Ryan Mohr, Yannis G. Kevrekidis, Igor Mezić',\n",
       "  744: 'Yingjun Shen, Haizhao Dai, Qihe Chen, Yan Zeng, Jiakai Zhang, Yuan Pei, Jingyi Yu',\n",
       "  745: 'Liulei Li, Wenguan Wang, Yi Yang',\n",
       "  746: 'David Janz, Alexander E. Litvak, Csaba Szepesvári',\n",
       "  747: 'Mohammad-Amin Charusaie, Samira Samadi',\n",
       "  748: 'Shan Chen, Jack Gallifant, Mingye Gao, Pedro Moreira, Nikolaj Munch, Ajay Muthukkumar, Arvind Rajan, Jaya Kolluri, Amelia Fiske, Janna Hastings, Hugo Aerts, Brian Anthony, Leo Anthony Celi, William G. La Cava, Danielle S. Bitterman',\n",
       "  749: 'Sofiane Ennadir, Johannes F. Lutzeyer, Michalis Vazirgiannis, El Houcine Bergou',\n",
       "  750: 'Haoqun Cao, Zizhuo Meng, Tianjun Ke, Feng Zhou',\n",
       "  751: 'Nivasini Ananthakrishnan, Nika Haghtalab, Chara Podimata, Kunhe Yang',\n",
       "  752: 'Bochuan Cao, Jinyuan Jia, Chuxuan Hu, Wenbo Guo, Zhen Xiang, Jinghui Chen, Bo Li, Dawn Song',\n",
       "  753: 'Chenyu Yang, Xizhou Zhu, Jinguo Zhu, Weijie Su, Junjie Wang, Xuan Dong, Wenhai Wang, Lewei Lu, Bin Li, Jie Zhou, Yu Qiao, Jifeng Dai',\n",
       "  754: 'Zirui Yan, Ali Tajer',\n",
       "  755: 'Chenyang Zhang, Difan Zou, Yuan Cao',\n",
       "  756: 'Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, Tom Goldstein',\n",
       "  757: 'Yang Zhou, Zhuoming Chen, Zhaozhuo Xu, Xi Victoria Lin, Beidi Chen',\n",
       "  758: 'Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, Vincent Sitzmann',\n",
       "  759: 'Yuki Minai, Joana Soldado-Magraner, Matthew A. Smith, Byron M. Yu',\n",
       "  760: 'Hanlin Gu, Win Kent Ong, Chee Seng Chan, Lixin Fan',\n",
       "  761: 'Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, Bryan Hooi',\n",
       "  762: 'Joscha Cüppers, Sascha Xu, Ahmed Musa, Jilles Vreeken',\n",
       "  763: 'João Monteiro, Pierre-André Noël, Étienne Marcotte, Sai Rajeswar, Valentina Zantedeschi, David Vázquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian',\n",
       "  764: 'Haolin Liu, Artin Tajdini, Andrew Wagenmaker, Chen-Yu Wei',\n",
       "  765: 'Xiaodan Chen, Xiucheng Li, Xinyang Chen, Zhijun Li',\n",
       "  766: 'Ioar Casado, Luis A. Ortega, Aritz Pérez, Andrés R. Masegosa',\n",
       "  767: 'Jacob Dunefsky, Philippe Chlenski, Neel Nanda',\n",
       "  768: 'Qian Chen, Tianjian Zhang, Linxin Yang, Qingyu Han, Akang Wang, Ruoyu Sun, Xiaodong Luo, Tsung-Hui Chang',\n",
       "  769: 'Anchit Jain, Rozhin Nobahari, Aristide Baratin, Stefano Sarao Mannelli',\n",
       "  770: 'Shuo Yu, Shan Jin, Ming Li, Tabinda Sarwar, Feng Xia',\n",
       "  771: 'Kai Hu, Jinhao Li, Yuan Zhang, Xiongjun Ye, Xieping Gao',\n",
       "  772: 'Tiancheng Wang, Yuguang Yang, Linlin Yang, Shaohui Lin, Juan Zhang, Guodong Guo, Baochang Zhang',\n",
       "  773: \"Kanghee Park, Jiayu Wang, Taylor Berg-Kirkpatrick, Nadia Polikarpova, Loris D'Antoni\",\n",
       "  774: 'Chiara Mastrogiuseppe, Rubén Moreno-Bote',\n",
       "  775: 'Sangwoong Yoon, Himchan Hwang, Dohyun Kwon, Yung-Kyun Noh, Frank C. Park',\n",
       "  776: 'Naveen Raman, Zheyuan Ryan Shi, Fei Fang',\n",
       "  777: 'Tin Sum Cheng, Aurelien Lucchi, Anastasis Kratsios, David Belius',\n",
       "  778: 'Yang Peng, Liangyu Zhang, Zhihua Zhang',\n",
       "  779: 'Lorenzo Cascioli, Laurens Devos, Ondrej Kuzelka, Jesse Davis',\n",
       "  780: 'Jia Syuen Lim, Zhuoxiao Chen, Mahsa Baktashmotlagh, Zhi Chen, Xin Yu, Zi Huang, Yadan Luo',\n",
       "  781: 'Haoran You, Yipin Guo, Yichao Fu, Wei Zhou, Huihong Shi, Xiaofan Zhang, Souvik Kundu, Amir Yazdanbakhsh, Yingyan (Celine) Lin',\n",
       "  782: 'Yufei Guo, Yuanpei Chen, Zecheng Hao, Weihang Peng, Zhou Jie, Yuhan Zhang, Xiaode Liu, Zhe Ma',\n",
       "  783: 'Meijun Wang, Yu Meng, Zhongwei Qiu, Chao Zheng, Yan Xu, Xiaorui Peng, Jian Gao',\n",
       "  784: 'Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, Kazuki Kozuka',\n",
       "  785: 'Qijun Luo, Hengxu Yu, Xiao Li',\n",
       "  786: 'Rayen Dhahri, Alexander Immer, Betrand Charpentier, Stephan Günnemann, Vincent Fortuin',\n",
       "  787: 'Yifan Yang, Zhaofeng Si, Siwei Lyu, Kaiyi Ji',\n",
       "  788: 'Hongyu Cheng, Sammy Khalife, Barbara Fiedorowicz, Amitabh Basu',\n",
       "  789: 'Haitao Li, You Chen, Qingyao Ai, Yueyue Wu, Ruizhe Zhang, Yiqun Liu',\n",
       "  790: 'Yang Liu, Chenchen Jing, Hengtao Li, Muzhi Zhu, Hao Chen, Xinlong Wang, Chunhua Shen',\n",
       "  791: 'Tianyi Zhou, Deqing Fu, Vatsal Sharan, Robin Jia',\n",
       "  792: 'Benedikt Alkin, Andreas Fürst, Simon Schmid, Lukas Gruber, Markus Holzleitner, Johannes Brandstetter',\n",
       "  793: 'Sharmita Dey, Sarath Ravindran Nair',\n",
       "  794: 'Jialong Zuo, Ying Nie, Hanyu Zhou, Huaxin Zhang, Haoyu Wang, Tianyu Guo, Nong Sang, Changxin Gao',\n",
       "  795: 'Shuaifeng Li, Mao Ye, Lihua Zhou, Nianxin Li, Siying Xiao, Song Tang, Xiatian Zhu',\n",
       "  796: 'Zijian Zhou, Xiaoqiang Lin, Xinyi Xu, Alok Prakash, Daniela Rus, Bryan Kian Hsiang Low',\n",
       "  797: 'Clément Bonet, Théo Uscidda, Adam David, Pierre-Cyril Aubin-Frankowski, Anna Korba',\n",
       "  798: 'Heewoong Noh, Namkyeong Lee, Gyoung S. Na, Chanyoung Park',\n",
       "  799: 'Sattar Vakili, Julia Olkhovskaya',\n",
       "  800: 'Zengzhi Wang, Xuefeng Li, Rui Xia, Pengfei Liu',\n",
       "  801: 'Gabriel Nobis, Maximilian Springenberg, Marco Aversa, Michael Detzel, Rembert Daems, Roderick Murray-Smith, Shinichi Nakajima, Sebastian Lapuschkin, Stefano Ermon, Tolga Birdal, Manfred Opper, Christoph Knochenhauer, Luis Oala, Wojciech Samek',\n",
       "  802: 'Xinyue Li, Rishi Sonthalia',\n",
       "  803: 'Yuhan Zhu, Yuyang Ji, Zhiyu Zhao, Gangshan Wu, Limin Wang',\n",
       "  804: 'Wenjing Yan, Xuanyu Cao',\n",
       "  805: 'Jiaqi Han, Minkai Xu, Aaron Lou, Haotian Ye, Stefano Ermon',\n",
       "  806: 'Haibin He, Maoyuan Ye, Jing Zhang, Juhua Liu, Bo Du, Dacheng Tao',\n",
       "  807: 'Yuheng Shi, Minjing Dong, Chang Xu',\n",
       "  808: 'Yitao Xu, Tong Zhang, Sabine Süsstrunk',\n",
       "  809: 'Junfeng Ni, Yixin Chen, Bohan Jing, Nan Jiang, Bin Wang, Bo Dai, Puhao Li, Yixin Zhu, Song-Chun Zhu, Siyuan Huang',\n",
       "  810: 'Mingze Wang, Weinan E',\n",
       "  811: 'Pankaj K. Agarwal, Sharath Raghvendra, Pouyan Shirzadian, Keegan Yao',\n",
       "  812: 'Yohann Perron, Vladyslav Sydorov, Adam P. Wijker, Damian Evans, Christophe Pottier, Loic Landrieu',\n",
       "  813: 'Chenxin Tao, Xizhou Zhu, Shiqian Su, Lewei Lu, Changyao Tian, Xuan Luo, Gao Huang, Hongsheng Li, Yu Qiao, Jie Zhou, Jifeng Dai',\n",
       "  814: 'Leon Lufkin, Andrew Saxe, Erin Grant',\n",
       "  815: \"Vincent Cohen-Addad, Tommaso d'Orsi, Anupam Gupta, Euiwoong Lee, Debmalya Panigrahi\",\n",
       "  816: 'Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis N. Ioannidis, Karthik Subbian, Jure Leskovec, James Zou',\n",
       "  817: 'Artem Lukoianov, Haitz Sáez de Ocáriz Borde, Kristjan Greenewald, Vitor Campagnolo Guizilini, Timur Bagautdinov, Vincent Sitzmann, Justin Solomon',\n",
       "  818: 'Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, Xingtai Lv, Jin-Fang Hu, Zhiyuan Liu, Bowen Zhou',\n",
       "  819: 'Riccardo Cadei, Lukas Lindorfer, Sylvia Cremer, Cordelia Schmid, Francesco Locatello',\n",
       "  820: 'Felix Petersen, Christian Borgelt, Tobias Sutter, Hilde Kuehne, Oliver Deussen, Stefano Ermon',\n",
       "  821: 'Chandra Sekhar Mukherjee, Nikhil Deorkar, Jiapeng Zhang',\n",
       "  822: 'Mengxi Zhang, Wenhao Wu, Yu Lu, Yuxin Song, Kang Rong, Huanjin Yao, Jianbo Zhao, Fanglong Liu, Haocheng Feng, Jingdong Wang, Yifan Sun',\n",
       "  823: 'Jialu Li, Yu Wang, Pengfei Zhu, Wanyu Lin, Qinghua Hu',\n",
       "  824: 'Andrew Bean, Simi Hellsten, Harry Mayne, Jabez Magomere, Ethan A. Chi, Ryan Chi, Scott A. Hale, Hannah Rose Kirk',\n",
       "  825: 'Jiacheng Cen, Anyi Li, Ning Lin, Yuxiang Ren, Zihe Wang, Wenbing Huang',\n",
       "  826: 'Yiyang Sun, Tong Wang, Cynthia Rudin',\n",
       "  827: 'Qiufeng Wang, Xu Yang, Fu Feng, Jing Wang, Xin Geng',\n",
       "  828: 'Wei Tang, Yin-Fang Yang, Zhaofei Wang, Weijia Zhang, Min-Ling Zhang',\n",
       "  829: 'Duc Cao Dinh, Seok Joon Kim, Kyusung Cho',\n",
       "  830: 'Zhehao Huang, Xinwen Cheng, JingHao Zheng, Haoran Wang, Zhengbao He, Tao Li, Xiaolin Huang',\n",
       "  831: 'Chenqing Hua, Bozitao Zhong, Sitao Luan, Liang Hong, Guy Wolf, Doina Precup, Shuangjia Zheng',\n",
       "  832: 'Kimon Protopapas, Anas Barakat',\n",
       "  833: 'Andreas Maurer',\n",
       "  834: 'Xuxing Chen, Abhishek Roy, Yifan Hu, Krishnakumar Balasubramanian',\n",
       "  835: 'Shervin Khalafi, Dongsheng Ding, Alejandro Ribeiro',\n",
       "  836: 'David Holzmüller, Léo Grinsztajn, Ingo Steinwart',\n",
       "  837: 'Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li',\n",
       "  838: 'Qidong Liu, Xian Wu, Yejing Wang, Zijian Zhang, Feng Tian, Yefeng Zheng, Xiangyu Zhao',\n",
       "  839: 'Victor Boone, Zihan Zhang',\n",
       "  840: 'Anton Rodomanov, Xiaowen Jiang, Sebastian Stich',\n",
       "  841: 'Xuan Chen, Yuzhou Nie, Wenbo Guo, Xiangyu Zhang',\n",
       "  842: 'Ying Yang, De Cheng, Chaowei Fang, Yubiao Wang, Changzhe Jiao, Lechao Cheng, Nannan Wang, Xinbo Gao',\n",
       "  843: 'Kushagra Pandey, Ruihan Yang, Stephan Mandt',\n",
       "  844: 'Changze Lv, Dongqi Han, Yansen Wang, Xiaoqing Zheng, Xuanjing Huang, Dongsheng Li',\n",
       "  845: 'Keyon Vafa, Justin Y. Chen, Ashesh Rambachan, Jon Kleinberg, Sendhil Mullainathan',\n",
       "  846: 'Yichun Hu, Nathan Kallus, Xiaojie Mao, Yanchen Wu',\n",
       "  847: 'Tomoya Sakai, Haoxiang Qiu, Takayuki Katsuki, Daiki Kimura, Takayuki Osogami, Tadanobu Inoue',\n",
       "  848: 'Yuwei Zhang, Tong Xia, Jing Han, Yu Yvonne Wu, Georgios Rizos, Yang Liu, Mohammed Mosuily, Jagmohan Chauhan, Cecilia Mascolo',\n",
       "  849: 'Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, Feng Zhao',\n",
       "  850: 'Pratiksha Thaker, Amrith Setlur, Zhiwei Steven Wu, Virginia Smith',\n",
       "  851: 'Harry Jake Cunningham, Giorgio Giannone, Mingtian Zhang, Marc Peter Deisenroth',\n",
       "  852: 'Tiago da Silva, Daniel Augusto de Souza, Diego Mesquita',\n",
       "  853: 'The Viet Bui, Tien Mai, Thanh Hong Nguyen',\n",
       "  854: 'Zhiqiang Chen, Guofan Fan, Jinying Gao, Lei Ma, Bo Lei, Tiejun Huang, Shan Yu',\n",
       "  855: 'Minjie Wang, Quan Gan, David Wipf, Zhenkun Cai, Ning Li, Jianheng Tang, Yanlin Zhang, Zizhao Zhang, Zunyao Mao, Yakun Song, Yanbo Wang, Jiahang Li, Han Zhang, Guang Yang, Xiao Qin, Chuan Lei, Muhan Zhang, Weinan Zhang, Christos Faloutsos, Zheng Zhang',\n",
       "  856: 'Mathieu Carrière, Marc Theveneau, Théo Lacombe',\n",
       "  857: 'Tom Yan, Zachary Lipton',\n",
       "  858: 'Siyi Chen, Huijie Zhang, Minzhe Guo, Yifu Lu, Peng Wang, Qing Qu',\n",
       "  859: 'Peng Sun, Yi Jiang, Tao Lin',\n",
       "  860: 'Haoran Luo, Haihong E, Yuhao Yang, Tianyu Yao, Yikai Guo, Zichen Tang, Wentai Zhang, Shiyao Peng, Kaiyang Wan, Meina Song, Wei Lin, Yifan Zhu, Luu Anh Tuan',\n",
       "  861: 'Jiangwei Weng, Zhiqiang Yan, Ying Tai, Jianjun Qian, Jian Yang, Jun Li',\n",
       "  862: 'Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, Tat-Seng Chua',\n",
       "  863: 'Ben Norman, Jeff Clune',\n",
       "  864: 'Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron Courville, Sai Rajeswar',\n",
       "  865: 'Letian Peng, Jingbo Shang',\n",
       "  866: 'Amirhosein Ghasemabadi, Muhammad Kamran Janjua, Mohammad Salameh, Di Niu',\n",
       "  867: 'Shady Abu-Hussein, Raja Giryes',\n",
       "  868: 'Weichao Zhou, Wenchao Li',\n",
       "  869: 'Guoxin Chen, Minpeng Liao, Chengxi Li, Kai Fan',\n",
       "  870: 'Ben Adcock, Nick Dexter, Sebastian Moraga',\n",
       "  871: 'Jingwu Tang, Gokul Swamy, Fei Fang, Zhiwei Steven Wu',\n",
       "  872: 'Thibault Simonetto, Salah Ghamizi, Maxime Cordy',\n",
       "  873: 'Emmanuel Abbe, Samy Bengio, Aryo Lotfi, Colin Sandon, Omid Saremi',\n",
       "  874: 'Dong Jing, Xiaolong He, Yutian Luo, Nanyi Fei, Guoxing Yang, Wei Wei, Huiwen Zhao, Zhiwu Lu',\n",
       "  875: 'Richard Nock, Mathieu Guillame-Bert',\n",
       "  876: 'HaoChuan Xu, Ninh Pham',\n",
       "  877: 'Qihao Zhou, Haishan Ye, Luo Luo',\n",
       "  878: 'Mingshuang Luo, Ruibing Hou, Zhuo Li, Hong Chang, Zimo Liu, Yaowei Wang, Shiguang Shan',\n",
       "  879: 'Haoran Zhang, Junkai Deng, Xuhui Chen, Fei Hou, Wencheng Wang, Hong Qin, Chen Qian, Ying He',\n",
       "  880: 'Stefan Pranger, Hana Chockler, Martin Tappler, Bettina Könighofer',\n",
       "  881: 'Rohan Choudhury, Guanglei Zhu, Sihan Liu, Koichiro Niinuma, Kris M. Kitani, László A. Jeni',\n",
       "  882: 'Pihe Hu, Shaolong Li, Zhuoran Li, Ling Pan, Longbo Huang',\n",
       "  883: 'Hanxiao Zhang, Lin Ju, Chan Wu, Jinjing Huang, Youshao Xiao, Zhenglei Zhou, Zhiming Fan, Zhaoxin Huan, Siyuan Li, Fanzhuang Meng, Lei Liang, Xiaolu Zhang, Jun Zhou',\n",
       "  884: 'Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, Kaidi Xu',\n",
       "  885: 'Shihao Tu, Linfeng Cao, Daoze Zhang, Junru Chen, Lvbin Ma, Yin Zhang, Yang Yang',\n",
       "  886: 'Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, Yu-Gang Jiang',\n",
       "  887: 'Keyi Kong, Xilie Xu, Di Wang, Jingfeng Zhang, Mohan Kankanhalli',\n",
       "  888: 'Derek Lim, Theo (Moe) Putterman, Robin Walters, Haggai Maron, Stefanie Jegelka',\n",
       "  889: 'Chendi Qian, Andrei Manolache, Christopher Morris, Mathias Niepert',\n",
       "  890: 'Zhenheng Tang, Yonggang Zhang, Peijie Dong, Yiu-ming Cheung, Amelie Chi Zhou, Bo Han, Xiaowen Chu',\n",
       "  891: 'Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, Jiaya Jia',\n",
       "  892: 'Xiaolei Liu, Shaoshuai Li, Kaixin Gao, Binfeng Wang',\n",
       "  893: 'Bingqiao Luo, Zhen Zhang, Qian Wang, Bingsheng He',\n",
       "  894: 'Mingcheng Li, Dingkang Yang, Yang Liu, Shunli Wang, Jiawei Chen, Shuaibing Wang, Jinjie Wei, Yue Jiang, Qingyao Xu, Xiaolu Hou, Mingyang Sun, Ziyun Qian, Dongliang Kou, Lihua Zhang',\n",
       "  895: 'Leo Zhou, Joao Basso, Song Mei',\n",
       "  896: 'Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber, Christopher Potts, Christopher D. Manning',\n",
       "  897: 'Feipeng Ma, Hongwei Xue, Yizhou Zhou, Guangting Wang, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siying Wu, Mike Zheng Shou, Xiaoyan Sun',\n",
       "  898: 'Yongxin Zhu, Bocheng Li, Hang Zhang, Xin Li, Linli Xu, Lidong Bing',\n",
       "  899: 'Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, Yen-Chun Chen, Yu-Chiang Frank Wang',\n",
       "  900: 'Donato Crisostomi, Marco Fumero, Daniele Baieri, Florian Bernard, Emanuele Rodolà',\n",
       "  901: 'Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, Kashyap Chitta',\n",
       "  902: 'Yuhang Li, Changsheng Li, Ruilin Lv, Rongqing Li, Ye Yuan, Guoren Wang',\n",
       "  903: 'Byoungwoo Park, Jungwon Choi, Sungbin Lim, Juho Lee',\n",
       "  904: 'Ziyang Xiao, Dongxiang Zhang, Xiongwei Han, Xiaojin Fu, Wing Yin Yu, Tao Zhong, Sai Wu, Yuan Wang, Jianwei Yin, Gang Chen',\n",
       "  905: 'Jihyung Kil, Zheda Mai, Justin Lee, Arpita Chowdhury, Zihe Wang, Kerrie Cheng, Lemeng Wang, Ye Liu, Wei-Lun Chao',\n",
       "  906: 'Haoning Wu, Dongxu Li, Bei Chen, Junnan Li',\n",
       "  907: 'Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan S. Ilgen, Emma Pierson, Pang Wei Koh, Yulia Tsvetkov',\n",
       "  908: 'Qi Song, Tianxiang Gong, Shiqi Gao, Haoyi Zhou, Jianxin Li',\n",
       "  909: 'Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, Jiankang Deng, Xiatian Zhu',\n",
       "  910: 'Andrew Estornell, Yang Liu',\n",
       "  911: 'Rachel S.Y. Teo, Tan M. Nguyen',\n",
       "  912: 'Wenfang Yao, Chen Liu, Kejing Yin, William K. Cheung, Jing Qin',\n",
       "  913: 'Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen',\n",
       "  914: 'Yunpeng Qing, Shunyu Liu, Jingyuan Cong, Kaixuan Chen, Yihe Zhou, Mingli Song',\n",
       "  915: 'Alexandros Stergiou',\n",
       "  916: 'Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati',\n",
       "  917: 'Xuehao Cui, Guangyang Wu, Zhenghao Gan, Guangtao Zhai, Xiaohong Liu',\n",
       "  918: 'Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn, Kushal Arora, Thomas Kollar',\n",
       "  919: 'Wenzhuo Liu, Fei Zhu, Shijie Ma, Cheng-Lin Liu',\n",
       "  920: 'Cheng Gao, Yuan Cao, Zihao Li, Yihan He, Mengdi Wang, Han Liu, Jason M. Klusowski, Jianqing Fan',\n",
       "  921: 'Luiz F. O. Chamon, Mohammad Reza Karimi, Anna Korba',\n",
       "  922: 'Duo Zhou, Christopher Brix, Grani A Hanasusanto, Huan Zhang',\n",
       "  923: 'Jie Zhu, Yixiong Chen, Mingyu Ding, Ping Luo, Leye Wang, Jingdong Wang',\n",
       "  924: 'Zhiyuan Yan, Taiping Yao, Shen Chen, Yandan Zhao, Xinghe Fu, Junwei Zhu, Donghao Luo, Chengjie Wang, Shouhong Ding, Yunsheng Wu, Li Yuan',\n",
       "  925: 'Jinlin Lai, Justin Domke, Daniel Sheldon',\n",
       "  926: 'Tianjiao Luo, Tim Pearce, Huayu Chen, Jianfei Chen, Jun Zhu',\n",
       "  927: 'Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di Zhang, Bin Cui',\n",
       "  928: 'Peter Mørch Groth, Mads Herbert Kerrn, Lars Olsen, Jesper Salomon, Wouter Boomsma',\n",
       "  929: 'Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Chengwei Qin, Pin-Yu Chen, Eng Siong Chng, Chao Zhang',\n",
       "  930: 'Barakeel Fanseu Kamhoua, Huamin Qu',\n",
       "  931: 'Jiying Zhang, Zijing Liu, Yu Wang, Bin Feng, Yu Li',\n",
       "  932: 'Kaushik Sinha',\n",
       "  933: 'Xiaohang Tang, Afonso Marques, Parameswaran Kamalaruban, Ilija Bogunovic',\n",
       "  934: 'Zichuan Liu, Zefan Wang, Linjie Xu, Jinyu Wang, Lei Song, Tianchun Wang, Chunlin Chen, Wei Cheng, Jiang Bian',\n",
       "  935: 'Boxiao Pan, Zhan Xu, Chun-Hao Paul Huang, Krishna Kumar Singh, Yang Zhou, Leonidas J. Guibas, Jimei Yang',\n",
       "  936: 'Varun Yerram, Rahul Madhavan, Sravanti Addepalli, Arun Suggala, Karthikeyan Shanmugam, Prateek Jain',\n",
       "  937: 'Hanjun Dai, Bethany Yixin Wang, Xingchen Wan, Bo Dai, Sherry Yang, Azade Nova, Pengcheng Yin, Phitchaya Mangpo Phothilimthana, Charles Sutton, Dale Schuurmans',\n",
       "  938: 'Ruihan Gao, Kangle Deng, Gengshan Yang, Wenzhen Yuan, Jun-Yan Zhu',\n",
       "  939: 'Nachiket Kotalwar, Alkis Gotovos, Adish Singla',\n",
       "  940: 'Chaeyun Jang, Hyungi Lee, Jungtaek Kim, Juho Lee',\n",
       "  941: \"Jessica Schrouff, Alexis Bellot, Amal Rannen-Triki, Alan Malek, Isabela Albuquerque, Arthur Gretton, Alexander D'Amour, Silvia Chiappa\",\n",
       "  942: 'Xinke Jiang, Rihong Qiu, Yongxin Xu, Wentao Zhang, Yichen Zhu, Ruizhe Zhang, Yuchen Fang, Xu Chu, Junfeng Zhao, Yasha Wang',\n",
       "  943: 'Chenghao Fan, Zhenyi Lu, Wei Wei, Jie Tian, Xiaoye Qu, Dangyang Chen, Yu Cheng',\n",
       "  944: 'Narine Kokhlikyan, Bargav Jayaraman, Florian Bordes, Chuan Guo, Kamalika Chaudhuri',\n",
       "  945: 'Tehila Dahan, Kfir Y. Levy',\n",
       "  946: 'Theodore Tsesmelis, Luca Palmieri, Marina Khoroshiltseva, Adeela Islam, Gur Elkin, Ofir Itzhak Shahar, Gianluca Scarpellini, Stefano Fiorini, Yaniv Ohayon, Nadav Alali, Sinem Aslan, Pietro Morerio, Sebastiano Vascon, Elena Gravina, Maria Cristina Napolitano, Giuseppe Scarpati, Gabriel Zuchtriegel, Alexandra Spühler, Michel E. Fuchs, Stuart James, Ohad Ben-Shahar, Marcello Pelillo, Alessio Del Bue',\n",
       "  947: 'Frederik Kunstner, Alan Milligan, Robin Yadav, Mark Schmidt, Alberto Bietti',\n",
       "  948: 'Franziska Heeg, Ingo Scholtes',\n",
       "  949: 'Yang Li, Jinpei Guo, Runzhong Wang, Hongyuan Zha, Junchi Yan',\n",
       "  950: 'Hai-Vy Nguyen, Fabrice Gamboa, Reda Chhaibi, Sixin Zhang, Serge Gratton, Thierry Giaccone',\n",
       "  951: 'Andrea H. Wynn, Ilia Sucholutsky, Thomas L. Griffiths',\n",
       "  952: 'Jin-Hong Du, Pratik Patil',\n",
       "  953: 'Min Zhao, Hongzhou Zhu, Chendong Xiang, Kaiwen Zheng, Chongxuan Li, Jun Zhu',\n",
       "  954: 'Eunji Hong, Minh Hieu Nguyen, Mikaela Angelina Uy, Minhyuk Sung',\n",
       "  955: 'Kwangho Kim, Jisu Kim, Larry A. Wasserman, Edward H. Kennedy',\n",
       "  956: 'Qisen Wang, Yifan Zhao, Jiawei Ma, Jia Li',\n",
       "  957: 'Dar Gilboa, Hagay Michaeli, Daniel Soudry, Jarrod R. McClean',\n",
       "  958: 'Julien Zhou, Pierre Gaillard, Thibaud Rahier, Houssam Zenati, Julyan Arbel',\n",
       "  959: 'Ido Sobol, Chenfeng Xu, Or Litany',\n",
       "  960: 'Kendong Liu, Zhiyu Zhu, Chuanhao Li, Hui Liu, Huanqiang Zeng, Junhui Hou',\n",
       "  961: 'Bikang Pan, Wei Huang, Ye Shi',\n",
       "  962: 'Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, Hao Peng',\n",
       "  963: 'Zheng Chen, Haotong Qin, Yong Guo, Xiongfei Su, Xin Yuan, Linghe Kong, Yulun Zhang',\n",
       "  964: 'Alejandro Lozano, Jeffrey Nirschl, James Burgess, Sanket Rajan Gupte, Yuhui Zhang, Alyssa Unell, Serena Yeung-Levy',\n",
       "  965: 'Zhengkai Lin, Zhihang Fu, Kai Liu, Liang Xie, Binbin Lin, Wenxiao Wang, Deng Cai, Yue Wu, Jieping Ye',\n",
       "  966: 'Xueyi Zhang, Chengwei Zhang, Mingrui Lao, Peng Zhao, Jun Tang, Yanming Guo, Siqi Cai, Xianghu Yue, Haizhou Li',\n",
       "  967: 'Xinshuai Dong, Ignavier Ng, Biwei Huang, Yuewen Sun, Songyao Jin, Roberto Legaspi, Peter Spirtes, Kun Zhang',\n",
       "  968: 'Hoai-Chau Tran, Duy M. H. Nguyen, Duy M. Nguyen, TrungTin Nguyen, Ngan Le, Pengtao Xie, Daniel Sonntag, James Zou, Binh T. Nguyen, Mathias Niepert',\n",
       "  969: 'Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf',\n",
       "  970: 'Qi Li, Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Xinglin Pan, Xiaowen Chu',\n",
       "  971: 'Stefan Stojanovic, Yassir Jedra, Alexandre Proutiere',\n",
       "  972: 'Xin Xiao, Bohong Wu, Jiacong Wang, Chunyuan Li, Xun Zhou, Haoyuan Guo',\n",
       "  973: 'Oswin So, Cheng Ge, Chuchu Fan',\n",
       "  974: 'Dongjoon Lee, Hyeryn Park, Changhee Lee',\n",
       "  975: 'Luting Wang, Yang Zhao, Zijian Zhang, Jiashi Feng, Si Liu, Bingyi Kang',\n",
       "  976: 'Hideaki Kim',\n",
       "  977: 'Bowen Ping, Shuo Wang, Hanqing Wang, Xu Han, Yuzhuang Xu, Yukun Yan, Yun Chen, Baobao Chang, Zhiyuan Liu, Maosong Sun',\n",
       "  978: 'Xilin He, Jingyu Hu, Qinliang Lin, Cheng Luo, Weicheng Xie, Siyang Song, Muhammad Haris Khan, Linlin Shen',\n",
       "  979: 'Lili Wei, Congyan Lang, Ziyi Chen, Tao Wang, Yidong Li, Jun Liu',\n",
       "  980: 'Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li',\n",
       "  981: 'Andrew Jesson, Nicolas Beltran-Velez, Quentin Chu, Sweta Karlekar, Jannik Kossen, Yarin Gal, John P. Cunningham, David Blei',\n",
       "  982: 'Vivian Y. Nastl, Moritz Hardt',\n",
       "  983: 'Jonathan Wenger, Kaiwen Wu, Philipp Hennig, Jacob R. Gardner, Geoff Pleiss, John P. Cunningham',\n",
       "  984: 'Weiwei Ye, Songgaojun Deng, Qiaosha Zou, Ning Gui',\n",
       "  985: 'Giovanni De Toni, Nastaran Okati, Suhas Thejaswi, Eleni Straitouri, Manuel Gomez-Rodriguez',\n",
       "  986: 'Erik Jenner, Shreyas Kapur, Vasil Georgiev, Cameron Allen, Scott Emmons, Stuart Russell',\n",
       "  987: 'Wenhao Yang, Yibo Wang, Peng Zhao, Lijun Zhang',\n",
       "  988: 'Maor Ashkenazi, Eran Treister',\n",
       "  989: 'Jinhee Kim, Taesung Kim, Jaegul Choo',\n",
       "  990: 'Suzanne Duncan, Gianna Leoni, Lee Steven, Keoni Mahelona, Peter-Lucas Jones',\n",
       "  991: 'Jerry Yao-Chieh Hu, Weimin Wu, Zhuoru Li, Sophia Pi, Zhao Song, Han Liu',\n",
       "  992: 'Zhixiang Shen, Shuo Wang, Zhao Kang',\n",
       "  993: 'Andrew Wagenmaker, Lu Mi, Marton Rozsa, Matthew S. Bull, Karel Svoboda, Kayvon Daie, Matthew D. Golub, Kevin Jamieson',\n",
       "  994: 'Sheng Yan, Cunhang Fan, Hongyu Zhang, Xiaoke Yang, Jianhua Tao, Zhao Lv',\n",
       "  995: 'Yiling Xie, Xiaoming Huo',\n",
       "  996: 'Yuefei Lyu, Chaozhuo Li, Sihong Xie, Xi Zhang',\n",
       "  997: 'Arlind Kadra, Sebastian Pineda Arango, Josif Grabocka',\n",
       "  998: 'Aviv Bick, Kevin Y. Li, Eric P. Xing, J. Zico Kolter, Albert Gu',\n",
       "  999: 'Muhammad Umair Nasir, Steven James, Julian Togelius',\n",
       "  ...},\n",
       " 'abstract': {0: 'We propose a new variant of the Adam optimizer called MicroAdam that specifically minimizes memory overheads, while maintaining theoretical convergence guarantees. We achieve this by compressing the gradient information before it is fed into the optimizer state, thereby reducing its memory footprint significantly. We control the resulting compression  error via a novel instance of the classical error feedback mechanism from distributed optimization in which the error correction information is itself compressed to allow for practical memory gains. We prove that the resulting approach maintains theoretical convergence guarantees competitive to those of AMSGrad, while providing good practical performance. Specifically, we show that MicroAdam can be implemented efficiently on GPUs: on both million-scale (BERT) and billion-scale (LLaMA) models, MicroAdam provides practical convergence competitive to that of the uncompressed Adam baseline, with  lower memory usage and similar running time. Our code is available at https://github.com/IST-DASLab/MicroAdam.',\n",
       "  1: 'Large Language Models (LLMs) are increasingly used for various tasks with graph structures. Though LLMs can process graph information in a textual format, they overlook the rich vision modality, which is an intuitive way for humans to comprehend structural information and conduct general graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., $\\\\textit{visual graph}$) are still unexplored. To fill the gap, we innovatively propose an end-to-end framework, called $\\\\textbf{G}$raph to v$\\\\textbf{I}$sual and $\\\\textbf{T}$extual Integr$\\\\textbf{A}$tion (GITA), which firstly incorporates visual graphs into general graph reasoning. Besides, we establish  $\\\\textbf{G}$raph-based $\\\\textbf{V}$ision-$\\\\textbf{L}$anguage $\\\\textbf{Q}$uestion $\\\\textbf{A}$nswering (GVLQA) dataset from existing graph data, which is the first vision-language dataset for general graph reasoning purposes. Extensive experiments on the GVLQA dataset and five real-world datasets show that GITA outperforms mainstream LLMs in terms of general graph reasoning capabilities. Moreover, We highlight the effectiveness of the layout augmentation on visual graphs and pretraining on the GVLQA dataset.',\n",
       "  2: 'This paper analyzes the inverse relationship between the order of partial differential equations (PDEs) and the convergence of gradient descent in physics-informed neural networks (PINNs) with the power of ReLU activation. The integration of the PDE into a loss function endows PINNs with a distinctive feature to require computing derivatives of model up to the PDE order. Although it has been empirically observed that PINNs encounter difficulties in convergence when dealing with high-order or high-dimensional PDEs, a comprehensive theoretical understanding of this issue remains elusive. This paper offers theoretical support for this pathological behavior by demonstrating that the gradient flow converges in a lower probability when the PDE order is higher. In addition, we show that PINNs struggle to address high-dimensional problems because the influence of dimensionality on convergence is exacerbated with increasing PDE order. To address the pathology, we use the insights garnered to consider variable splitting that decomposes the high-order PDE into a system of lower-order PDEs. We prove that by reducing the differential order, the gradient flow of variable splitting is more likely to converge to the global optimum. Furthermore, we present numerical experiments in support of our theoretical claims.',\n",
       "  3: \"Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored.  In this work, we present fair Wasserstein coresets ($\\\\texttt{FWC}$), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. $\\\\texttt{FWC}$ uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of $\\\\texttt{FWC}$ is equivalent to Lloyd's algorithm for k-medians and k-means clustering. Experiments conducted on both synthetic and real datasets show that $\\\\texttt{FWC}$:  (i) achieves a competitive fairness-performance tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (GPT-3.5 and GPT-4).\",\n",
       "  4: 'We investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay. Let $n,T,\\\\bar{d}$ denote the dimensionality, time horizon, and average delay, respectively. Previous studies have achieved an $O(\\\\sqrt{n}T^{3/4}+(n\\\\bar{d})^{1/3}T^{2/3})$ regret bound for this problem, whose delay-independent part matches the regret of the classical non-delayed bandit gradient descent algorithm. However, there is a large gap between its delay-dependent part, i.e., $O((n\\\\bar{d})^{1/3}T^{2/3})$, and an existing $\\\\Omega(\\\\sqrt{\\\\bar{d}T})$ lower bound. In this paper, we illustrate that this gap can be filled in the worst case, where $\\\\bar{d}$ is very close to the maximum delay $d$. Specifically, we first develop a novel algorithm, and prove that it enjoys a regret bound of $O(\\\\sqrt{n}T^{3/4}+\\\\sqrt{dT})$ in general. Compared with the previous result, our regret bound is better for $d=O((n\\\\bar{d})^{2/3}T^{1/3})$, and the delay-dependent part is tight in the worst case. The primary idea is to decouple the joint effect of the delays and the bandit feedback on the regret by carefully incorporating the delayed bandit feedback with a blocking update mechanism. Furthermore, we show that the proposed algorithm can improve the regret bound to $O((nT)^{2/3}\\\\log^{1/3}T+d\\\\log T)$ for strongly convex functions. Finally, if the action sets are unconstrained, we demonstrate that it can be simply extended to achieve an $O(n\\\\sqrt{T\\\\log T}+d\\\\log T)$ regret bound for strongly convex and smooth functions.',\n",
       "  5: 'Mastering games is a hard task, as games can be extremely complex, and still fundamentally different in structure from one another. While the AlphaZero algorithm has demonstrated an impressive ability to learn the rules and strategy of a large variety of games, ranging from Go and Chess, to Atari games, its reliance on extensive computational resources and rigid Convolutional Neural Network (CNN) architecture limits its adaptability and scalability. A model trained to play on a $19\\\\times 19$ Go board cannot be used to play on a smaller $13\\\\times 13$ board, despite the similarity between the two Go variants.In this paper, we focus on Chess, and explore using a more generic Graph-based Representation of a game state, rather than a grid-based one, to introduce a more general architecture based on Graph Neural Networks (GNN). We also expand the classical Graph Attention Network (GAT) layer to incorporate edge-features, to naturally provide a generic policy output format.Our experiments, performed on smaller networks than the initial AlphaZero paper, show that this new architecture outperforms previous architectures with a similar number of parameters, being able to increase playing strength an order of magnitude faster. We also show that the model, when trained on a smaller $5\\\\times 5$ variant of chess, is able to be quickly fine-tuned to play on regular $8\\\\times 8$ chess, suggesting that this approach yields promising generalization abilities.Our code is available at https://github.com/akulen/AlphaGateau.',\n",
       "  6: 'With the rapid development of multimedia technology, audio-visual learning has emerged as a promising research topic within the field of multimodal analysis. In this paper, we explore parameter-efficient transfer learning for audio-visual learning and propose the Audio-Visual Mixture of Experts (\\\\ourmethodname) to inject adapters into pre-trained models flexibly. Specifically, we introduce unimodal and cross-modal adapters as multiple experts to specialize in intra-modal and inter-modal information, respectively, and employ a lightweight router to dynamically allocate the weights of each expert according to the specific demands of each task. Extensive experiments demonstrate that our proposed approach \\\\ourmethodname achieves superior performance across multiple audio-visual tasks, including AVE, AVVP, AVS, and AVQA. Furthermore, visual-only experimental results also indicate that our approach can tackle challenging scenes where modality information is missing.The source code is available at \\\\url{https://github.com/yingchengy/AVMOE}.',\n",
       "  7: \"Hippocampal place cells are known for their spatially selective firing patterns, which has led to the suggestion that they encode an animal's location. However, place cells also respond to contextual cues, such as smell. Furthermore, they have the ability to remap, wherein the firing fields and rates of cells change in response to changes in the environment. How place cell responses emerge, and how these representations remap is not fully understood. In this work, we propose a similarity-based objective function that translates proximity in space, to proximity in representation. We show that a neural network trained to minimize the proposed objective learns place-like representations. We also show that the proposed objective is easily extended to include other sources of information, such as context information, in the same way. When trained to encode multiple contexts, networks learn distinct representations, exhibiting remapping behaviors between contexts. The proposed objective is invariant to orthogonal transformations. Such transformations of the original trained representation (e.g. rotations), therefore yield new representations distinct from the original, without explicit relearning, akin to remapping. Our findings shed new light on the formation and encoding properties of place cells, and also demonstrate an interesting case of representational reuse.\",\n",
       "  8: 'We develop a technique to design efficiently computable estimators for sparse linear regression in the simultaneous presence of two adversaries: oblivious and adaptive.Consider the model $y^*=X^*\\\\beta^*+ \\\\eta$ where $X^*$ is an $n\\\\times d$ random design matrix, $\\\\beta^*\\\\in \\\\mathbb{R}^d$ is a $k$-sparse vector, and the noise $\\\\eta$ is independent of $X^*$ and chosen by the \\\\emph{oblivious adversary}. Apart from the independence of $X^*$, we only require a small fraction entries of $\\\\eta$ to have magnitude at most $1$. The \\\\emph{adaptive adversary} is allowed to arbitrarily corrupt an $\\\\varepsilon$-fraction of the samples $(X_1^*, y_1^*),\\\\ldots, (X_n^*, y_n^*)$.Given the $\\\\varepsilon$-corrupted samples $(X_1, y_1),\\\\ldots, (X_n, y_n)$, the goal is to estimate $\\\\beta^*$.  We assume that the rows of $X^*$ are iid samples from some $d$-dimensional distribution $\\\\mathcal{D}$  with zero mean and (unknown) covariance matrix $\\\\Sigma$ with bounded condition number.We design several robust algorithms that outperform the state of the art even in the special case of Gaussian noise $\\\\eta \\\\sim N(0,1)^n$. In particular, we provide a polynomial-time algorithm that with high probability recovers $\\\\beta^*$ up to error $O(\\\\sqrt{\\\\varepsilon})$  as long as  $n \\\\ge \\\\tilde{O}(k^2/\\\\varepsilon)$, only assuming some bounds on the third and the fourth moments of $\\\\mathcal{D}$. In addition, prior to this work, even in the special case of Gaussian design $\\\\mathcal{D} = N(0,\\\\Sigma)$ and noise $\\\\eta \\\\sim N(0,1)$, no polynomial time algorithm was known to achieve error $o(\\\\sqrt{\\\\varepsilon})$ in the sparse setting $n < d^2$.  We show that under some assumptions on the fourth and the eighth moments of $\\\\mathcal{D}$, there is a polynomial-time algorithm that achieves error $o(\\\\sqrt{\\\\varepsilon})$ as long as $n \\\\ge \\\\tilde{O}(k^4 / \\\\varepsilon^3)$. For Gaussian distribution $\\\\mathcal{D} = N(0,\\\\Sigma)$, this algorithm achieves error $O(\\\\varepsilon^{3/4})$. Moreover, our algorithm achieves error $o(\\\\sqrt{\\\\varepsilon})$ for all log-concave distributions if $\\\\varepsilon \\\\le 1/\\\\text{polylog(d)}$. Our algorithms are based on the filtering  of the covariates that uses sum-of-squares relaxations, and weighted Huber loss minimization with $\\\\ell_1$ regularizer.  We provide a novel analysis of weighted penalized Huber loss that is suitable for heavy-tailed designs in the presence of two adversaries. Furthermore, we complement our algorithmic results with Statistical Query lower bounds, providing evidence that our estimators are likely to have nearly optimal sample complexity.',\n",
       "  9: 'Diffusion models have recently achieved great success in the synthesis of high-quality images and videos. However, the existing denoising techniques in diffusion models are commonly based on step-by-step noise predictions, which suffers from high computation cost, resulting in a prohibitive latency for interactive applications. In this paper, we propose AdaptiveDiffusion to relieve this bottleneck by adaptively reducing the noise prediction steps during the denoising process. Our method considers the potential of skipping as many noise prediction steps as possible while keeping the final denoised results identical to the original full-step ones. Specifically, the skipping strategy is guided by the third-order latent difference that indicates the stability between timesteps during the denoising process, which benefits the reusing of previous noise prediction results. Extensive experiments on image and video diffusion models demonstrate that our method can significantly speed up the denoising process while generating identical results to the original process, achieving up to an average 2-5x speedup without quality degradation. The code is available at https://github.com/UniModal4Reasoning/AdaptiveDiffusion',\n",
       "  10: 'The recent development of chain-of-thought (CoT) decoding has enabled large language models (LLMs) to generate explicit logical reasoning paths for complex problem-solving. However, research indicates that these paths are not always deliberate and optimal. The tree-of-thought (ToT) method employs tree-searching to extensively explore the reasoning space and find better reasoning paths that CoT decoding might overlook. This deliberation, however, comes at the cost of significantly increased inference complexity. In this work, we demonstrate that fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to achieve similar or better performance, thereby avoiding the substantial inference burden. This is achieved through \\\\emph{Chain of Preference Optimization} (CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process. Extensive experimental results show that CPO significantly improves LLM performance in solving a variety of complex problems, including question answering, fact verification, and arithmetic reasoning, demonstrating its effectiveness. Our code is available at https://github.com/sail-sg/CPO.',\n",
       "  11: 'In this paper, we introduce DeAR (Decompose-Analyze-Rethink), a framework that iteratively builds a reasoning tree to tackle intricate problems within a single large language model (LLM). Unlike approaches that extend or search for rationales, DeAR is featured by 1) adopting a tree-based question decomposition manner to plan the organization of rationales, which mimics the logical planning inherentin human cognition; 2) globally updating the rationales at each reasoning step through natural language feedback. Specifically, the Decompose stage decomposes the question into simpler sub-questions, storing them as new nodes; the Analyze stage generates and self-checks rationales for sub-questions at each node evel; and the Rethink stage updates parent-node rationales based on feedback from their child nodes. By generating and updating the reasoning process from a more global perspective, DeAR constructs more adaptive and accurate logical structures for complex problems, facilitating timely error correction compared to rationale-extension and search-based approaches such as Tree-of-Thoughts (ToT) and Graph-of-Thoughts (GoT). We conduct extensive experiments on three reasoning benchmarks, including ScienceQA, StrategyQA, and GSM8K, which cover a variety of reasoning tasks, demonstrating that our approach significantly reduces logical errors and enhances performance across various LLMs. Furthermore, we validate that DeAR is an efficient method that achieves a superior trade-off between accuracy and reasoning time compared to ToT and GoT.',\n",
       "  12: 'Hyperparameter Optimization (HPO) plays a pivotal role in unleashing the potential of iterative machine learning models. This paper addresses a crucial aspect that has largely been overlooked in HPO: the impact of uncertainty in ML model training. The paper introduces the concept of uncertainty-aware HPO and presents a novel approach called the UQ-guided scheme for quantifying uncertainty. This scheme offers a principled and versatile method to empower HPO techniques in handling model uncertainty during their exploration of the candidate space.By constructing a probabilistic model and implementing probability-driven candidate selection and budget allocation, this approach enhances the quality of the resulting model hyperparameters. It achieves a notable performance improvement of over 50\\\\% in terms of accuracy regret and exploration time.',\n",
       "  13: 'Occupancy functions play an instrumental role in reinforcement learning (RL) for guiding exploration, handling distribution shift, and optimizing general objectives beyond the expected return. Yet, computationally efficient policy optimization methods that use (only) occupancy functions are virtually non-existent. In this paper, we establish the theoretical foundations of model-free policy gradient (PG) methods that compute the gradient through the occupancy for both online and offline RL, without modeling value functions. Our algorithms reduce gradient estimation to squared-loss regression and are computationally oracle-efficient. We characterize the sample complexities of both local and global convergence, accounting for both finite-sample estimation error and the roles of exploration (online) and data coverage (offline). Occupancy-based PG naturally handles arbitrary offline data distributions, and, with one-line algorithmic changes, can be adapted to optimize any differentiable objective functional.',\n",
       "  14: 'Deep models have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous variables can provide valuable external information for endogenous variables. Thus, unlike well-established multivariate or univariate forecasting paradigms that either treat all the variables equally or ignore exogenous information, this paper focuses on a more practical setting: time series forecasting with exogenous variables. We propose a novel approach, TimeXer, to ingest external information to enhance the forecasting of endogenous variables. With deftly designed embedding layers, TimeXer empowers the canonical Transformer with the ability to reconcile endogenous and exogenous information, where patch-wise self-attention and variate-wise cross-attention are used simultaneously. Moreover, global endogenous tokens are learned to effectively bridge the causal information underlying exogenous series into endogenous temporal patches. Experimentally, TimeXer achieves consistent state-of-the-art performance on twelve real-world forecasting benchmarks and exhibits notable generality and scalability. Code is available at this repository: https://github.com/thuml/TimeXer.',\n",
       "  15: 'We present DrivAerNet++, the largest and most comprehensive multimodal dataset for aerodynamic car design. DrivAerNet++ comprises 8,000 diverse car designs modeled with high-fidelity computational fluid dynamics (CFD) simulations. The dataset includes diverse car configurations such as fastback, notchback, and estateback, with different underbody and wheel designs to represent both internal combustion engines and electric vehicles. Each entry in the dataset features detailed 3D meshes, parametric models, aerodynamic coefficients, and extensive flow and surface field data, along with segmented parts for car classification and point cloud data. This dataset supports a wide array of machine learning applications including data-driven design optimization, generative modeling, surrogate model training, CFD simulation acceleration, and geometric classification. With more than 39 TB of publicly available engineering data, DrivAerNet++ fills a significant gap in available resources, providing high-quality, diverse data to enhance model training, promote generalization, and accelerate automotive design processes. Along with rigorous dataset validation, we also provide ML benchmarking results on the task of aerodynamic drag prediction, showcasing the breadth of applications supported by our dataset. This dataset is set to significantly impact automotive design and broader engineering disciplines by fostering innovation and improving the fidelity of aerodynamic evaluations. Dataset and code available at: https://github.com/Mohamedelrefaie/DrivAerNet',\n",
       "  16: 'The recent progress in text-to-image models pretrained on large-scale datasets has enabled us to generate various images as long as we provide a text prompt describing what we want. Nevertheless, the availability of these models is still limited when we expect to generate images that fall into a specific domain either hard to describe or just unseen to the models. In this work, we propose DomainGallery, a few-shot domain-driven image generation method which aims at finetuning pretrained Stable Diffusion on few-shot target datasets in an attribute-centric manner. Specifically, DomainGallery features prior attribute erasure, attribute disentanglement, regularization and enhancement. These techniques are tailored to few-shot domain-driven generation in order to solve key issues that previous works have failed to settle. Extensive experiments are given to validate the superior performance of DomainGallery on a variety of domain-driven generation scenarios.',\n",
       "  17: \"Learners sharing similar implicit cognitive states often display comparable observable problem-solving performances. Leveraging collaborative connections among such similar learners proves valuable in comprehending human learning. Motivated by the success of collaborative modeling in various domains, such as recommender systems, we aim to investigate how collaborative signals among learners contribute to the diagnosis of human cognitive states (i.e., knowledge proficiency) in the context of intelligent education.The primary challenges lie in identifying implicit collaborative connections and disentangling the entangled cognitive factors of learners for improved explainability and controllability in learner Cognitive Diagnosis (CD). However, there has been no work on CD capable of simultaneously modeling collaborative and disentangled cognitive states. To address this gap, we present Coral, a $\\\\underline{Co}$llabo$\\\\underline{ra}$tive cognitive diagnosis model with disentang$\\\\underline{l}$ed representation learning. Specifically, Coral first introduces a disentangled state encoder to achieve the initial disentanglement of learners' states.Subsequently, a meticulously designed collaborative representation learning procedure captures collaborative signals. It dynamically constructs a collaborative graph of learners by iteratively searching for optimal neighbors in a context-aware manner. Using the constructed graph, collaborative information is extracted through node representation learning. Finally, a decoding process aligns the initial cognitive states and collaborative states, achieving co-disentanglement with practice performance reconstructions.Extensive experiments demonstrate the superior performance of Coral, showcasing significant improvements over state-of-the-art methods across several real-world datasets.Our code is available at https://github.com/bigdata-ustc/Coral.\",\n",
       "  18: 'Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce a case-conditioned prompting strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\\\\% with GPT-4-turbo and 86.2\\\\% with GPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at https://github.com/minghchen/automanual.',\n",
       "  19: 'Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding. A pivotal challenge is the development of an effective method to encapsulate video content into a set of representative tokens to align with LLMs. In this work, we introduce Slot-VLM, a new framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference. Particularly, we design an Object-Event Slots module, i.e., OE-Slots, that adaptively aggregates the dense video tokens from the vision encoder to a set of representative slots. In order to take into account both the spatial object details and the varied temporal dynamics, we build OE-Slots with two branches: the Object-Slots branch and the Event-Slots branch. The Object-Slots branch focuses on extracting object-centric slots from features of high spatial resolution but low frame sample rate, emphasizing detailed object information. The Event-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features. These complementary slots are combined to form the vision context, serving as the input to the LLM for effective video reasoning. Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering.',\n",
       "  20: 'We introduce VASA, a framework for generating lifelike talking faces with appealing visual affective skills (VAS) given a single static image and a speech audio clip. Our premiere model, VASA-1, is capable of not only generating lip movements that are exquisitely synchronized with the audio, but also producing a large spectrum of facial nuances and natural head motions that contribute to the perception of authenticity and liveliness. The core innovations include a diffusion-based holistic facial dynamics and head movement generation model that works in a face latent space, and the development of such an expressive and disentangled face latent space using videos.Through extensive experiments including evaluation on a set of new metrics, we show that our method significantly outperforms previous methods along various dimensions comprehensively. Our method delivers high video quality with realistic facial and head dynamics and also supports the online generation of 512$\\\\times$512 videos at up to 40 FPS with negligible starting latency.It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors.',\n",
       "  21: 'Embedding the nodes of a large network into an Euclidean space is a common objective in modernmachine learning, with a variety of tools available. These embeddings can then be used as features fortasks such as community detection/node clustering or link prediction, where they achieve state of the artperformance. With the exception of spectral clustering methods, there is little theoretical understandingfor commonly used approaches to learning embeddings. In this work we examine the theoreticalproperties of the embeddings learned by node2vec. Our main result shows that the use of k-meansclustering on the embedding vectors produced by node2vec gives weakly consistent community recoveryfor the nodes in (degree corrected) stochastic block models. We also discuss the use of these embeddingsfor node and link prediction tasks. We demonstrate this result empirically for bothreal and simulated networks, and examine how this relatesto other embedding tools for network data.',\n",
       "  22: 'Learning complex physical dynamics purely from data is challenging due to the intrinsic properties of systems to be satisfied. Incorporating physics-informed priors, such as in Hamiltonian Neural Networks (HNNs), achieves high-precision modeling for energy-conservative systems. However, real-world systems often deviate from strict energy conservation and follow different physical priors. To address this, we present a framework that achieves high-precision modeling for a wide range of dynamical systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a novel regularization term. It helps preserve energies for conservative systems while serving as a strong inductive bias for non-conservative, reversible systems. While TRS is a domain-specific physical prior, we present the first theoretical proof that TRS loss can universally improve modeling accuracy by minimizing higher-order Taylor terms in ODE integration, which is numerically beneficial to various systems regardless of their properties, even for irreversible systems. By integrating the TRS loss within neural ordinary differential equation models, the proposed model TREAT demonstrates superior performance on diverse physical systems. It achieves a significant 11.5% MSE improvement in a challenging chaotic triple-pendulum scenario, underscoring TREAT’s broad applicability and effectiveness.',\n",
       "  23: \"Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of those activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.\",\n",
       "  24: \"In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the data-driven manner. However, existing E2E-AD methods are mostly evaluated under the open-loop log-replay manner with L2 errors and collision rate as metrics (e.g., in nuScenes), which could not fully reflect the driving performance of algorithms as recently acknowledged in the community. For those E2E-AD methods evaluated under the closed-loop protocol, they are tested in fixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as metrics, which is known for high variance due to the unsmoothed metric function and large randomness in the long route. Besides, these methods usually collect their own data for training, which makes algorithm-level fair comparison infeasible.    To fulfill the paramount need of comprehensive, realistic, and fair testing environments for Full Self-Driving (FSD), we present Bench2Drive, the first benchmark for evaluating E2E-AD systems' multiple abilities in a closed-loop manner. Bench2Drive's official training data consists of 2 million fully annotated frames, collected from 10000 short clips uniformly distributed under 44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny, foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2. Its evaluation protocol requires E2E-AD models to pass 44 interactive scenarios under different locations and weathers which sums up to 220 routes and thus provides a comprehensive and disentangled assessment about their driving capability under different situations. We implement state-of-the-art E2E-AD models and evaluate them in Bench2Drive, providing insights regarding current status and future directions.\",\n",
       "  25: 'Modern deep policy gradient methods achieve effective performance on simulated robotic tasks, but they all require large replay buffers or expensive batch updates, or both, making them incompatible for real systems with resource-limited computers. We show that these methods fail catastrophically when limited to small replay buffers or during incremental learning, where updates only use the most recent sample without batch updates or a replay buffer. We propose a novel incremental deep policy gradient method --- Action Value Gradient (AVG) and a set of normalization and scaling techniques to address the challenges of instability in incremental learning. On robotic simulation benchmarks, we show that AVG is the only incremental method that learns effectively, often achieving final performance comparable to batch policy gradient methods. This advancement enabled us to show for the first time effective deep reinforcement learning with real robots using only incremental updates, employing a robotic manipulator and a mobile robot.',\n",
       "  26: \"Pre-trained large language models based on Transformers have demonstrated remarkable in-context learning (ICL) abilities. With just a few demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we attempt to explore the ICL process in Transformers through a lens of representation learning. Initially, leveraging kernel methods, we figure out a dual model for one softmax attention layer. The ICL inference process of the attention layer aligns with the training procedure of its  dual model, generating token representation predictions that are equivalent to the dual model's test outputs. We delve into the training process of this  dual model from a representation learning standpoint and further derive a generalization error bound related to the quantity of demonstration tokens. Subsequently, we extend our theoretical conclusions to more complicated scenarios, including one Transformer layer and multiple attention layers. Furthermore, drawing inspiration from existing representation learning methods especially contrastive learning, we propose potential modifications for the attention layer. Finally, experiments are designed to support our findings.\",\n",
       "  27: \"In this paper, we address the challenge of speech enhancement in real-world recordings, which often contain various forms of distortion, such as background noise, reverberation, and microphone artifacts.We revisit the use of Generative Adversarial Networks (GANs) for speech enhancement and theoretically show that GANs are naturally inclined to seek the point of maximum density within the conditional clean speech distribution, which, as we argue, is essential for speech enhancement task.We study various feature extractors for perceptual loss to facilitate the stability of adversarial training, developing a methodology for probing the structure of the feature space.This leads us to integrate WavLM-based perceptual loss into MS-STFT adversarial training pipeline, creating an effective and stable training procedure for the speech enhancement model.The resulting speech enhancement model, which we refer to as FINALLY, builds upon the HiFi++ architecture, augmented with a WavLM encoder and a novel training pipeline.Empirical results on various datasets confirm our model's ability to produce clear, high-quality speech at 48 kHz, achieving state-of-the-art performance in the field of speech enhancement. Demo page: https://samsunglabs.github.io/FINALLY-page/\",\n",
       "  28: 'Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to imitate. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and, in turn, provide feedback to help the main model capture more generalizable and imitable correlations. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and methodologies like Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to training models on the original dataset. The results suggest the effectiveness and efficiency of LoT in identifying generalizable information at the right scales while discarding spurious data correlations, thus making LoT a valuable addition to current machine learning. Code is available at https://github.com/jincan333/LoT.',\n",
       "  29: 'Non-exemplar class-incremental learning (NECIL) is a challenging task that requires recognizing both old and new classes without retaining any old class samples. Current works mainly deal with the conflicts between old and new classes retrospectively as a new task comes in. However, the lack of old task data makes balancing old and new classes difficult. Instead, we propose a Prospective Representation Learning (PRL) approach to prepare the model for handling conflicts in advance. In the base phase, we squeeze the embedding distribution of the current classes to reserve space for forward compatibility with future classes. In the incremental phase, we make the new class features away from the saved prototypes of old classes in a latent space while aligning the current embedding space with the latent space when updating the model. Thereby, the new class features are clustered in the reserved space to minimize the shock of the new classes on the former classes. Our approach can help existing NECIL baselines to balance old and new classes in a plug-and-play manner. Extensive experiments on several benchmarks demonstrate that our approach outperforms the state-of-the-art methods.',\n",
       "  30: 'Leveraging planning during learning and decision-making is central to the long-term development of intelligent agents. Recent works have successfully combined tree-based search methods and self-play learning mechanisms to this end. However, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they often result in a negative impact on performance. In this paper, we introduce SPO: Sequential Monte Carlo Policy Optimisation, a model-based reinforcement learning algorithm grounded within the Expectation Maximisation (EM) framework. We show that SPO provides robust policy improvement and efficient scaling properties. The sample-based search makes it directly applicable to both discrete and continuous action spaces without modifications. We demonstrate statistically significant improvements in performance relative to model-free and model-based baselines across both continuous and discrete environments. Furthermore, the parallel nature of SPO’s search enables effective utilisation of hardware accelerators, yielding favourable scaling laws.',\n",
       "  31: 'While significant advancements have been made in music generation and differentiable sound synthesis within machine learning and computer audition, the simulation of instrument vibration guided by physical laws has been underexplored. To address this gap, we introduce a novel model for simulating the spatio-temporal motion of nonlinear strings, integrating modal synthesis and spectral modeling within a neural network framework. Our model leverages mechanical properties and fundamental frequencies as inputs, outputting string states across time and space that solve the partial differential equation characterizing the nonlinear string. Empirical evaluations demonstrate that the proposed architecture achieves superior accuracy in string motion simulation compared to existing baseline architectures. The code and demo are available online.',\n",
       "  32: 'This study introduces the Federated Medical Knowledge Injection (FedMEKI) platform, a new benchmark designed to address the unique challenges of integrating medical knowledge into foundation models under privacy constraints. By leveraging a cross-silo federated learning approach, FedMEKI circumvents the issues associated with centralized data collection, which is often prohibited under health regulations like the Health Insurance Portability and Accountability Act (HIPAA) in the USA. The platform is meticulously designed to handle multi-site, multi-modal, and multi-task medical data, which includes 7 medical modalities, including images, signals, texts, laboratory test results, vital signs, input variables, and output variables. The curated dataset to validate FedMEKI covers 8 medical tasks, including 6 classification tasks (lung opacity detection, COVID-19 detection, electrocardiogram (ECG) abnormal detection, mortality prediction, sepsis protection, and enlarged cardiomediastinum detection) and 2 generation tasks (medical visual question answering (MedVQA) and ECG noise clarification). This comprehensive dataset is partitioned across several clients to facilitate the decentralized training process under 16 benchmark approaches. FedMEKI not only preserves data privacy but also enhances the capability of medical foundation models by allowing them to learn from a broader spectrum of medical knowledge without direct data exposure, thereby setting a new benchmark in the application of foundation models within the healthcare sector.',\n",
       "  33: 'The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability.To tackle these problems, we present GTSinger, a large Global, multi-Technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks.Particularly,(1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset;(2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles;(3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control;(4) GTSinger offers realistic music scores, assisting real-world musical composition;(5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks.Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion.',\n",
       "  34: \"Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's groundbreaking work. Although CLIP performs well, the typical direct latent feature alignment lacks clarity in its representation and similarity scores. On the other hand, lexical representation, a vector whose element represents the similarity between the sample and a word from the vocabulary, is a natural sparse representation and interpretable, providing exact matches for individual words.However, lexical representations are difficult to learn due to no ground-truth supervision and false-discovery issues, and thus requires complex design to train effectively.In this paper, we introduce LexVLA, a more interpretable VLA framework by learning a unified lexical representation for both modalities without complex design. We use DINOv2 as our visual model for its local-inclined features and Llama 2, a generative language model, to leverage its in-context lexical prediction ability.To avoid the false discovery, we propose an overuse penalty to refrain the lexical representation from falsely frequently activating meaningless words.We demonstrate that these two pre-trained uni-modal models can be well-aligned by fine-tuning on the modest multi-modal dataset and avoid intricate training configurations. On cross-modal retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset, outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those trained from scratch on even bigger datasets (e.g., 1.1B data, including CC-12M).We conduct extensive experiments to analyze LexVLA. Codes are available at https://github.com/Clementine24/LexVLA.\",\n",
       "  35: 'Multimodal Large Language Models (MLLMs) have demonstrated an excellent understanding of images and 3D data. However, both modalities have shortcomings in holistically capturing the appearance and geometry of objects. Meanwhile, Neural Radiance Fields (NeRFs), which encode information within the weights of a simple Multi-Layer Perceptron (MLP), have emerged as an increasingly widespread modality that simultaneously encodes the geometry and photorealistic appearance of objects. This paper investigates the feasibility and effectiveness of ingesting NeRF into MLLM. We create LLaNA, the first general-purpose NeRF-languageassistant capable of performing new tasks such as NeRF captioning and Q&A. Notably, our method directly processes the weights of the NeRF’s MLP to extract information about the represented objects without the need to render images or materialize 3D data structures. Moreover, we build a dataset of NeRFs with text annotations for various NeRF-language tasks with no human intervention.Based on this dataset, we develop a benchmark to evaluate the NeRF understanding capability of our method. Results show that processing NeRF weights performs favourably against extracting 2D or 3D representations from NeRFs.',\n",
       "  36: 'Diffusion Transformers (DiT) excel at image and video generation but face computational challenges due to the quadratic complexity of self-attention operators. We propose DiTFastAttn, a post-training compression method to alleviate the computational bottleneck of DiT.We identify three key redundancies in the attention computation during DiT inference: (1) spatial redundancy, where many attention heads focus on local information; (2) temporal redundancy, with high similarity between the attention outputs of neighboring steps; (3) conditional redundancy, where conditional and unconditional inferences exhibit significant similarity. We propose three techniques to reduce these redundancies: (1) $\\\\textit{Window Attention with Residual Sharing}$ to reduce spatial redundancy; (2) $\\\\textit{Attention Sharing across Timesteps}$ to exploit the similarity between steps; (3) $\\\\textit{Attention Sharing across CFG}$ to skip redundant computations during conditional generation.',\n",
       "  37: 'While open Large Language Models (LLMs) have made significant progress, they still fall short of matching the performance of their closed, proprietary counterparts, making the latter attractive even for the use on highly private data. Recently, various new methods have been proposed to adapt closed LLMs to private data without leaking private information to third parties and/or the LLM provider. In this work, we analyze the privacy protection and performance of the four most recent methods for private adaptation of closed LLMs. By examining their threat models and thoroughly comparing their performance under different privacy levels according to differential privacy (DP), various LLM architectures, and multiple datasets for classification and generation tasks, we find that: (1) all the methods leak query data, i.e., the (potentially sensitive) user data that is queried at inference time, to the LLM provider, (2) three out of four methods also leak large fractions of private training data to the LLM provider while the method that protects private data requires a local open LLM, (3) all the methods exhibit lower performance compared to three private gradient-based adaptation methods for local open LLMs, and (4) the private adaptation methods for closed LLMs incur higher monetary training and query costs than running the alternative methods on local open LLMs.This yields the conclusion that, to achieve truly privacy-preserving LLM adaptations that yield high performance and more privacy at lower costs, taking into account current methods and models, one should use open LLMs.',\n",
       "  38: 'Within the realm of Computer-Aided Design (CAD), Boundary-Representation (B-Rep) is the standard option for modeling shapes. We present SpelsNet, a neural architecture for the segmentation of 3D point clouds into surface primitive elements under topological supervision of its B-Rep graph structure. We also propose a point-to-BRep adjacency representation that allows for adapting conventional Linear Algebraic Representation of B-Rep graph structure to the point cloud domain. Thanks to this representation, SpelsNet learns from both spatial and topological domains to enable accurate and topologically consistent surface primitive element segmentation. In particular, SpelsNet is composed of two main components; (1) a supervised 3D spatial segmentation head that outputs B-Rep element types and memberships; (2) a graph-based head that leverages the proposed topological supervision. To enable the learning of SpelsNet with the proposed point-to-BRep adjacency supervision, we extend two existing CAD datasets with the required annotations, and conduct a thorough experimental validation on them. The obtained results showcase the efficacy of SpelsNet and its topological supervision compared to a set of baselines and state-of-the-art approaches.',\n",
       "  39: 'LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facilitates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve < 0.1 perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model.',\n",
       "  40: 'Diffusion-based methods have achieved remarkable achievements in 2D image or 3D object generation, however, the generation of 3D scenes and even $360^{\\\\circ}$ images remains constrained, due to the limited number of scene datasets, the complexity of 3D scenes themselves, and the difficulty of generating consistent multi-view images. To address these issues, we first establish a large-scale panoramic video-text dataset containing millions of consecutive panoramic keyframes with corresponding panoramic depths, camera poses, and text descriptions. Then, we propose a novel text-driven panoramic generation framework, termed DiffPano, to achieve scalable, consistent, and diverse panoramic scene generation. Specifically, benefiting from the powerful generative capabilities of stable diffusion, we fine-tune a single-view text-to-panorama diffusion model with LoRA on the established panoramic video-text dataset. We further design a spherical epipolar-aware multi-view diffusion model to ensure the multi-view consistency of the generated panoramic images. Extensive experiments demonstrate that DiffPano can generate scalable, consistent, and diverse panoramic images with given unseen text descriptions and camera poses.',\n",
       "  41: 'Estimating treatment effects over time holds significance in various domains, including precision medicine, epidemiology, economy, and marketing. This paper introduces a unique approach to counterfactual regression over time, emphasizing long-term predictions. Distinguishing itself from existing models like Causal Transformer, our approach highlights the efficacy of employing RNNs for long-term forecasting, complemented by Contrastive Predictive Coding (CPC) and Information Maximization (InfoMax). Emphasizing efficiency, we avoid the need for computationally expensive transformers. Leveraging CPC, our method captures long-term dependencies within time-varying confounders. Notably, recent models have disregarded the importance of invertible representation, compromising identification assumptions. To remedy this, we employ the InfoMax principle, maximizing a lower bound of mutual information between sequence data and its representation. Our method achieves state-of-the-art counterfactual estimation results using both synthetic and real-world data, marking the pioneering incorporation of Contrastive Predictive Encoding in causal inference.',\n",
       "  42: 'Implicit models such as Deep Equilibrium Models (DEQs) have emerged as promising alternative approaches for building deep neural networks. Their certified robustness has gained increasing research attention due to security concerns. Existing certified defenses for DEQs employing interval bound propagation and Lipschitz-bounds not only offer conservative certification bounds but also are restricted to specific forms of DEQs. In this paper, we provide the first randomized smoothing certified defense for DEQs to solve these limitations. Our study reveals that simply applying randomized smoothing to certify DEQs provides certified robustness generalized to large-scale datasets but incurs extremely expensive computation costs. To reduce computational redundancy, we propose a novel Serialized Randomized Smoothing (SRS) approach that leverages historical information. Additionally, we derive a new certified radius estimation for SRS to theoretically ensure the correctness of our algorithm. Extensive experiments and ablation studies on image recognition demonstrate that our algorithm can significantly accelerate the certification of DEQs by up to 7x almost without sacrificing the certified accuracy. The implementation will be publicly available upon the acceptance of this work. Our code is available at https://github.com/WeizhiGao/Serialized-Randomized-Smoothing.',\n",
       "  43: 'We study the differences arising from merging predictors in the causal and anticausal directions using the same data.In particular we study the asymmetries that arise in a simple model where we merge the predictors using one binary variable as target and two continuous variables as predictors.We use Causal Maximum Entropy (CMAXENT) as inductive bias to merge the predictors, however, we expect similar differences to hold also when we use other merging methods that take into account asymmetries between cause and effect.We show that if we observe all bivariate distributions, the CMAXENT solution reduces to a logistic regression in the causal direction and Linear Discriminant Analysis (LDA) in the anticausal direction.Furthermore, we study how the decision boundaries of these two solutions differ whenever we observe only some of the bivariate distributions implications for Out-Of-Variable (OOV) generalisation.',\n",
       "  44: 'The latest research on wildfire forecast and backtracking has adopted AI models, which require a large amount of data from wildfire scenarios to capture fire spread patterns. This paper explores using cost-effective simulated wildfire scenarios to train AI models and apply them to the analysis of real-world wildfire. This solution requires AI models to minimize the Sim2Real gap, a brand-new topic in the fire spread analysis research community. To investigate the possibility of minimizing the Sim2Real gap, we collect the Sim2Real-Fire dataset that contains 1M simulated scenarios with multi-modal environmental information for training AI models. We prepare 1K real-world wildfire scenarios for testing the AI models. We also propose a deep transformer, S2R-FireTr, which excels in considering the multi-modal environmental information for forecasting and backtracking the wildfire. S2R-FireTr surpasses state-of-the-art methods in real-world wildfire scenarios.',\n",
       "  45: \"Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciInstruct, a suite of scientific instructions for training scientific language models capable of college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We analyze the curated SciInstruct from multiple interesting perspectives (e.g., domain, scale, source, question type, answer length, etc.). To verify the effectiveness of SciInstruct, we fine-tuned different language models with SciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMath, enhancing their scientific and mathematical reasoning capabilities, without sacrificing the language understanding capabilities of the base model. We release all codes and SciInstruct at https://github.com/THUDM/SciGLM.\",\n",
       "  46: \"High-quality preference datasets are essential for training reward models that can effectively guide large language models (LLMs) in generating high-quality responses aligned with human preferences.As LLMs become stronger and better aligned, permissively licensed preference datasets, such as Open Assistant, HH-RLHF, and HelpSteer need to be updated to remain effective for reward modeling.Methods that distil preference data from proprietary LLMs such as GPT-4 have restrictions on commercial usage imposed by model providers.To improve upon both generated responses and attribute labeling quality, we release HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0).  Using a powerful Nemotron-4-340B base model trained on HelpSteer2, we are able to achieve the SOTA score (92.0%) on Reward-Bench's primary dataset, outperforming currently listed open and proprietary models, as of June 12th, 2024.Notably, HelpSteer2 consists of only ten thousand response pairs, an order of magnitude fewer than existing preference datasets (e.g., HH-RLHF), which makes it highly efficient for training reward models. Our extensive experiments demonstrate that reward models trained with HelpSteer2 are effective in aligning LLMs. Additionally, we propose SteerLM 2.0, a model alignment approach that can effectively make use of the rich multi-attribute score predicted by our reward models. HelpSteer2 is available at https://huggingface.co/datasets/nvidia/HelpSteer2 and code is available at https://github.com/NVIDIA/NeMo-Aligner\",\n",
       "  47: 'Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.',\n",
       "  48: 'Vision-based Semantic Scene Completion (SSC) has gained much attention due to its widespread applications in various 3D perception tasks. Existing sparse-to-dense approaches typically employ shared context-independent queries across various input images, which fails to capture distinctions among them as the focal regions of different inputs vary and may result in undirected feature aggregation of cross-attention. Additionally, the absence of depth information may lead to points projected onto the image plane sharing the same 2D position or similar sampling points in the feature map, resulting in depth ambiguity. In this paper, we present a novel context and geometry aware voxel transformer. It utilizes a context aware query generator to initialize context-dependent queries tailored to individual input images, effectively capturing their unique characteristics and aggregating information within the region of interest. Furthermore, it extend deformable cross-attention from 2D to 3D pixel space, enabling the differentiation of points with similar image coordinates based on their depth coordinates. Building upon this module, we introduce a neural network named CGFormer to achieve semantic scene completion. Simultaneously, CGFormer leverages multiple 3D representations (i.e., voxel and TPV) to boost the semantic and geometric representation abilities of the transformed 3D volume from both local and global perspectives. Experimental results demonstrate that CGFormer achieves state-of-the-art performance on the SemanticKITTI and SSCBench-KITTI-360 benchmarks, attaining a mIoU of 16.87 and 20.05, as well as an IoU of 45.99 and 48.07, respectively. Remarkably, CGFormer even outperforms approaches employing temporal images as inputs or much larger image backbone networks.',\n",
       "  49: \"Providing functionality through articulation and interaction with objects is a key objective in 3D generation. We introduce MIDGArD (Modular Interpretable Diffusion over Graphs for Articulated Designs), a novel diffusion-based framework for articulated 3D asset generation. MIDGArD improves over foundational work in the field by enhancing quality, consistency, and controllability in the generation process. This is achieved through MIDGArD's modular approach that separates the problem into two primary components: structure generation and shape generation. The structure generation module of MIDGArD aims at producing coherent articulation features from noisy or incomplete inputs. It acts on the object's structural and kinematic attributes, represented as features of a graph that are being progressively denoised to issue coherent and interpretable articulation solutions. This denoised graph then serves as an advanced conditioning mechanism for the shape generation module, a 3D generative model that populates each link of the articulated structure with consistent 3D meshes. Experiments show the superiority of MIDGArD on the quality, consistency, and interpretability of the generated assets. Importantly, the generated models are fully simulatable, i.e., can be seamlessly integrated into standard physics engines such as MuJoCo, broadening MIDGArD's applicability to fields such as digital content creation, meta realities, and robotics.\",\n",
       "  50: 'While neural networks (NNs) have a large potential as autonomous controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs— especially when safety is needed for unbounded time horizons. One reason for this is the intractability of analyzing NNs, ODEs and hybrid systems. To this end, we introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The first general approach that allows reusing control theory literature for NNCS verification. By joining forces, we can exploit the efficiency of NN verification tools while retaining the rigor of differential dynamic logic (dL). Based on a provably safe control envelope in dL, we derive a specification for the NN which is proven with NN verification tools. We show that a proof of the NN’s adherence to the specification is then mirrored by a dL proof on the infinite-time safety of the NNCS.The NN verification properties resulting from hybrid systems typically contain nonlinear arithmetic over formulas with arbitrary logical structure while efficient NN verification tools merely support linear constraints. To overcome this divide, we present Mosaic: An efficient, sound and complete verification approach for polynomial real arithmetic properties on piece-wise linear NNs. Mosaic partitions complex NN verification queries into simple queries and lifts off-the-shelf linear constraint tools to the nonlinear setting in a completeness-preserving manner by combining approximation with exact reasoning for counterexample regions. In our evaluation we demonstrate the versatility of VerSAILLE and Mosaic: We prove infinite-time safety on the classical Vertical Airborne Collision Avoidance NNCS verification benchmark for some scenarios while (exhaustively) enumerating counterexample regions in unsafe scenarios. We also show that our approach significantly outperforms the State-of-the-Art tools in closed-loop NNV',\n",
       "  51: 'Diffusion models can learn strong image priors from underlying data distribution and use them to solve inverse problems,but the training process is computationally expensive and requires lots of data.Such bottlenecks prevent most existing works from being feasible for high-dimensional and high-resolution data such as 3D images.This paper proposes a method to learn an efficient data prior for the entire image by training diffusion models only on patches of images.Specifically, we propose a patch-based position-aware diffusion inverse solver, called PaDIS, where we obtain the score function of the whole image through scores of patches and their positional encoding and utilize this as the prior for solving inverse problems.First of all, we show that this diffusion model achieves an improved memory efficiency and data efficiencywhile still maintaining the  capability to generate entire images via positional encoding.Additionally, the proposed PaDIS model is highly flexible and can be plugged in with different diffusion inverse solvers (DIS).We demonstrate that the proposed PaDIS approach enables solving various inverse problems in both natural and medical image domains, including CT reconstruction, deblurring, and superresolution, given only patch-based priors.Notably, PaDIS outperforms previous DIS methods trained on entire image priors in the case of limited training data, demonstrating the data efficiency of our proposed approach by learning patch-based prior.',\n",
       "  52: 'Accurate environment dynamics modeling is crucial for obtaining effective state representations in visual reinforcement learning (RL) applications. However, when facing multiple input modalities, existing dynamics modeling methods (e.g., DeepMDP) usually stumble in addressing the complex and volatile relationship between different modalities. In this paper, we study the problem of efficient dynamics modeling for multi-modal visual RL. We find that under the existence of modality heterogeneity, modality-correlated and distinct features are equally important but play different roles in reflecting the evolution of environmental dynamics. Motivated by this fact, we propose Dissected Dynamics Modeling (DDM), a novel multi-modal dynamics modeling method for visual RL. Unlike existing methods, DDM explicitly distinguishes consistent and inconsistent information across modalities and treats them separately with a divide-and-conquer strategy. This is done by dispatching the features carrying different information into distinct dynamics modeling pathways, which naturally form a series of implicit regularizations along the learning trajectories. In addition, a reward predictive function is further introduced to filter task-irrelevant information in both modality-consistent and inconsistent features, ensuring information integrity while avoiding potential distractions. Extensive experiments show that DDM consistently achieves competitive performance in challenging multi-modal visual environments.',\n",
       "  53: 'We demonstrate that numerous machine learning algorithms are specific instances of one single paradigm: reciprocal learning. These instances range from active learning over multi-armed bandits to self-training. We show that all these algorithms not only learn parameters from data but also vice versa: They iteratively alter training data in a way that depends on the current model fit. We introduce reciprocal learning as a generalization of these algorithms using the language of decision theory. This allows us to study under what conditions they converge. The key is to guarantee that reciprocal learning contracts such that the Banach fixed-point theorem applies. In this way, we find that reciprocal learning converges at linear rates to an approximately optimal model under some assumptions on the loss function, if their predictions are probabilistic and the sample adaption is both non-greedy and either randomized or regularized. We interpret these findings and provide corollaries that relate them to active learning, self-training, and bandits.',\n",
       "  54: 'Large language models have shown an impressive societal impact owing to their excellent understanding and logical reasoning skills. However, such strong ability relies on a huge amount of computing resources, which makes it difficult to deploy LLMs on computing resource-constrained platforms. Currently, LLMs process each token equivalently, but we argue that not every word is equally important. Some words should not be allocated excessive computing resources, particularly for dispensable terms in simple questions. In this paper, we propose a novel dynamic inference paradigm for LLMs, namely D-LLMs, which adaptively allocate computing resources in token processing. We design a dynamic decision module for each transformer layer that decides whether a network unit should be executed or skipped. Moreover, we tackle the issue of adapting D-LLMs to real-world applications, specifically concerning the missing KV-cache when layers are skipped. To overcome this, we propose a simple yet effective eviction policy to exclude the skipped layers from subsequent attention calculations. The eviction policy not only enables D-LLMs to be compatible with prevalent applications but also reduces considerable storage resources. Experimentally, D-LLMs show superior performance, in terms of computational cost and KV storage utilization. It can reduce up to 45\\\\% computational cost and KV storage on Q\\\\&A, summarization, and math solving tasks, 50\\\\% on commonsense reasoning tasks.',\n",
       "  55: \"To address the challenges of sim-to-real gap and sample efficiency in reinforcement learning (RL), this work studies distributionally robust Markov decision processes (RMDPs) --- optimize the worst-case performance when the deployed environment is within an uncertainty set around some nominal MDP. Despite recent efforts, the sample complexity of RMDPs has remained largely undetermined. While the statistical implications of distributional robustness in RL have been explored in some specific cases, the generalizability of the existing findings remains unclear, especially in comparison to standard RL.  Assuming access to a generative model that samples from the nominal MDP, we examine the sample complexity of RMDPs using a class of generalized $L_p$ norms as the 'distance' function for the uncertainty set, under two commonly adopted $sa$-rectangular and $s$-rectangular conditions. Our results imply that RMDPs can be more sample-efficient to solve than standard MDPs using generalized $L_p$ norms in both $sa$- and $s$-rectangular cases, potentially inspiring more empirical research. We provide a near-optimal upper bound and a matching minimax lower bound for the $sa$-rectangular scenarios. For $s$-rectangular cases, we improve the state-of-the-art upper bound and also derive a lower bound using $L_\\\\infty$ norm that verifies the tightness.\",\n",
       "  56: 'Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the high-throughput analysis of protein composition in biological tissues. Despite the development of several deep learning methods for predicting amino acid sequences (peptides) responsible for generating the observed mass spectra, training data biases hinder further advancements of \\\\emph{de novo} peptide sequencing. Firstly, prior methods struggle to identify amino acids with Post-Translational Modifications (PTMs) due to their lower frequency in training data compared to canonical amino acids, further resulting in unsatisfactory peptide sequencing performance. Secondly, various noise and missing peaks in mass spectra reduce the reliability of training data (Peptide-Spectrum Matches, PSMs). To address these challenges, we propose AdaNovo, a novel and domain knowledge-inspired framework that calculates Conditional Mutual Information (CMI) between the mass spectra and amino acids or peptides, using CMI for robust training against above biases. Extensive experiments indicate that AdaNovo outperforms previous competitors on the widely-used 9-species benchmark, meanwhile yielding 3.6\\\\% - 9.4\\\\% improvements in PTMs identification. The supplements contain the code.',\n",
       "  57: \"Talking face generation (TFG) aims to animate a target identity's face to create realistic talking videos. Personalized TFG is a variant that emphasizes the perceptual identity similarity of the synthesized result (from the perspective of appearance and talking style). While previous works typically solve this problem by learning an individual neural radiance field (NeRF) for each identity to implicitly store its static and dynamic information, we find it inefficient and non-generalized due to the per-identity-per-training framework and the limited training data. To this end, we propose MimicTalk, the first attempt that exploits the rich knowledge from a NeRF-based person-agnostic generic model for improving the efficiency and robustness of personalized TFG. To be specific, (1) we first come up with a person-agnostic 3D TFG model as the base model and propose to adapt it into a specific identity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help the model learn the personalized static appearance and facial dynamic features; (3) To generate the facial motion of the personalized talking style, we propose an in-context stylized audio-to-motion model that mimics the implicit talking style provided in the reference video without information loss by an explicit style representation. The adaptation process to an unseen identity can be performed in 15 minutes, which is 47 times faster than previous person-dependent methods. Experiments show that our MimicTalk surpasses previous baselines regarding video quality, efficiency, and expressiveness. Video samples are available at https://mimictalk.github.io .\",\n",
       "  58: 'Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications.To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis.To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data.To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM.Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels.Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts.The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM.We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model. The whole process only needs to invoke GPT-4 API 9.3k times and use 4.6B data for training.Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings.Our code and data will be publicly released in \\\\url{https://github.com/RUCAIBox/JiuZhang3.0}.',\n",
       "  59: 'In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by ubiquitous wearable devices such as MR glasses, as a proxy for human attention to guide VLMs. We propose a novel approach, Voila-A, for gaze alignment to enhance the effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we introduce a new model VOILA-A that integrate gaze information into VLMs while maintain pretrained knowledge from webscale dataset. We evaluate Voila-A using a hold-out validation set and a newly collected VOILA-GAZE testset, which features real-life scenarios captured with a gaze-tracking device. Our experimental results demonstrate that Voila-A significantly outperforms several baseline models. By aligning model attention with human gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and fosters engaging human-AI interaction across a wide range of applications.',\n",
       "  60: 'Neural architecture search (NAS) finds high performing networks for a given task. Yet the results of NAS are fairly prosaic; they did not e.g. create a shift from convolutional structures to transformers. This is not least because the search spaces in NAS often aren’t diverse enough to include such transformations a priori. Instead, for NAS to provide greater potential for fundamental design shifts, we need a novel expressive search space design which is built from more fundamental operations. To this end, we introduce einspace, a search space based on a parameterised probabilistic context-free grammar. Our space is versatile, supporting architectures of various sizes and complexities, while also containing diverse network operations which allow it to model convolutions, attention components and more. It contains many existing competitive architectures, and provides flexibility for discovering new ones. Using this search space, we perform experiments to find novel architectures as well as improvements on existing ones on the diverse Unseen NAS datasets. We show that competitive architectures can be obtained by searching from scratch, and we consistently find large improvements when initialising the search with strong baselines. We believe that this work is an important advancement towards a transformative NAS paradigm where search space expressivity and strategic search initialisation play key roles.',\n",
       "  61: 'A core aim in theoretical and systems neuroscience is to develop models which help us better understand biological intelligence. Such models range broadly in both complexity and biological plausibility. One widely-adopted example is task-optimized recurrent neural networks (RNNs), which have been used to generate hypotheses about how the brain’s neural dynamics may organize to accomplish tasks. However, task-optimized RNNs typically have a fixed weight matrix representing the synaptic connectivity between neurons. From decades of neuroscience research, we know that synaptic weights are constantly changing, controlled in part by chemicals such as neuromodulators. In this work we explore the computational implications of synaptic gain scaling, a form of neuromodulation, using task-optimized low-rank RNNs.In our neuromodulated RNN (NM-RNN) model, a neuromodulatory subnetwork outputs a low-dimensional neuromodulatory signal that dynamically scales the low-rank recurrent weights of an output-generating RNN. In empirical experiments, we find that the structured flexibility in the NM-RNN allows it to both train and generalize with a higher degree of accuracy than low-rank RNNs on a set of canonical tasks.Additionally, via theoretical analyses we show how neuromodulatory gain scaling endows networks with gating mechanisms commonly found in artificial RNNs. We end by analyzing the low-rank dynamics of trained NM-RNNs, to show how task computations are distributed.',\n",
       "  62: 'We consider the dataset valuation problem, that is the problem of quantifying the incremental gain, to some relevant pre-defined utility of a machine learning task, of aggregating an individual dataset to others.The Shapley value is a natural tool to perform dataset valuation due to its formal axiomatic justification, which can be combined with Monte Carlo integration to overcome the computational tractability challenges. Such generic approximation methods, however, remain expensive in some cases. In this paper, we exploit the knowledge about the structure of the dataset valuation problem to devise more efficient Shapley value estimators. We propose a novel approximation, referred to as discrete uniform Shapley, which is expressed as an expectation under a discrete uniform distribution with support of reasonable size. We justify the relevancy of the proposed framework via asymptotic and non-asymptotic theoretical guarantees and illustrate its benefits via an extensive set of numerical experiments.',\n",
       "  63: 'Vision-language models, such as CLIP, have shown impressive generalization capacities when using appropriate text descriptions. While optimizing prompts on downstream labeled data has proven effective in improving performance, these methods entail labor costs for annotations and are limited by their quality. Additionally, since CLIP is pre-trained on highly imbalanced Web-scale data, it suffers from inherent label bias that leads to suboptimal performance.  To tackle the above challenges, we propose a label-**F**ree p**ro**mpt distribution **l**earning and b**i**as **c**orrection framework, dubbed as **Frolic**, which boosts zero-shot performance without the need for labeled data. Specifically, our Frolic learns distributions over prompt prototypes to capture diverse visual representations and adaptively fuses these with the original CLIP through confidence matching.This fused model is further enhanced by correcting label bias via a label-free logit adjustment. Notably, our method is not only training-free but also circumvents the necessity for hyper-parameter tuning. Extensive experimental results across 16 datasets demonstrate the efficacy of our approach, particularly outperforming the state-of-the-art by an average of $2.6\\\\%$ on 10 datasets with CLIP ViT-B/16 and achieving an average margin of $1.5\\\\%$ on ImageNet and its five distribution shifts with CLIP ViT-B/16. Codes are available in [https://github.com/zhuhsingyuu/Frolic](https://github.com/zhuhsingyuu/Frolic).',\n",
       "  64: 'Multiobjective optimization problems (MOPs) are prevalent in machine learning, with applications in multi-task learning, learning under fairness or robustness constraints, etc. Instead of reducing multiple objective functions into a scalar objective, MOPs aim to optimize for the so-called Pareto optimality or Pareto set learning, which involves optimizing more than one objective function simultaneously, over models with thousands to millions of parameters. Existing benchmark libraries for MOPs mainly focus on evolutionary algorithms, most of which are zeroth-order or meta-heuristic methods that do not effectively utilize higher-order information from objectives and cannot scale to large-scale models with millions of parameters. In light of the above challenges, this paper introduces \\\\algoname, the first multiobjective optimization library that supports state-of-the-art gradient-based methods, provides a fair and comprehensive benchmark, and is open-sourced for the community.',\n",
       "  65: 'Text-to-image diffusion models have made significant progress in generating naturalistic images from textual inputs, and demonstrate the capacity to learn and represent complex visual-semantic relationships. While these diffusion models have achieved remarkable success, the underlying mechanisms driving their performance are not yet fully accounted for, with many unanswered questions surrounding what they learn, how they represent visual-semantic relationships, and why they sometimes fail to generalize. Our work presents Diffusion Partial Information Decomposition (DiffusionPID), a novel technique that applies information-theoretic principles to decompose the input text prompt into its elementary components, enabling a detailed examination of how individual tokens and their interactions shape the generated image. We introduce a formal approach to analyze the uniqueness, redundancy, and synergy terms by applying PID to the denoising model at both the image and pixel level. This approach enables us to characterize how individual tokens and their interactions affect the model output. We first present a fine-grained analysis of characteristics utilized by the model to uniquely localize specific concepts, we then apply our approach in bias analysis and show it can recover gender and ethnicity biases. Finally, we use our method to visually characterize word ambiguity and similarity from the model’s perspective and illustrate the efficacy of our method for prompt intervention. Our results show that PID is a potent tool for evaluating and diagnosing text-to-image diffusion models. Link to project page: https://rbz-99.github.io/Diffusion-PID/.',\n",
       "  66: 'The inherent challenge of image fusion lies in capturing the correlation of multi-source images and comprehensively integrating effective information from different sources. Most existing techniques fail to perform dynamic image fusion while notably lacking theoretical guarantees, leading to potential deployment risks in this field. Is it possible to conduct dynamic image fusion with a clear theoretical justification? In this paper, we give our solution from a generalization perspective. We proceed to reveal the generalized form of image fusion and derive a new test-time dynamic image fusion paradigm. It provably reduces the upper bound of generalization error. Specifically, we decompose the fused image into multiple components corresponding to its source data. The decomposed components represent the effective information from the source data, thus the gap between them reflects the \\\\textit{Relative Dominability} (RD) of the uni-source data in constructing the fusion image. Theoretically, we prove that the key to reducing generalization error hinges on the negative correlation between the RD-based fusion weight and the uni-source reconstruction loss. Intuitively, RD dynamically highlights the dominant regions of each source and can be naturally converted to the corresponding fusion weight, achieving robust results. Extensive experiments and discussions with in-depth analysis on multiple benchmarks confirm our findings and superiority. Our code is available at https://github.com/Yinan-Xia/TTD.',\n",
       "  67: 'Due to its low storage cost and fast search speed, cross-modal retrieval based on hashing has attracted widespread attention and is widely used in real-world applications of social media search. However, most existing hashing methods are often limited by uncomprehensive feature representations and semantic associations, which greatly restricts their performance and applicability in practical applications. To deal with this challenge, in this paper, we propose an end-to-end graph attention network hashing (EGATH) for cross-modal retrieval, which can not only capture direct semantic associations between images and texts but also match semantic content between different modalities. We adopt the contrastive language image pretraining (CLIP) combined with the Transformer to improve understanding and generalization ability in semantic consistency across different data modalities. The classifier based on graph attention network is applied to obtain predicted labels to enhance cross-modal feature representation. We construct hash codes using an optimization strategy and loss function to preserve the semantic information and compactness of the hash code. Comprehensive experiments on the NUS-WIDE, MIRFlickr25K, and MS-COCO benchmark datasets show that our EGATH significantly outperforms against several state-of-the-art methods.',\n",
       "  68: \"3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, yet they do not fully achieve robust and accurate performance, primarily due to inefficiently modeling spatial relations between joints. To address this problem, we propose a novel graph-guided Mamba framework, named Hamba, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba's scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to efficiently learn the spatial relationships between joints for improving reconstruction performance. Specifically, we design a Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5\\\\% fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space features and jointly considers global and local features to improve performance. Experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND. At the time of this paper's acceptance, Hamba holds the top position, Rank 1, in two competition leaderboards on 3D hand reconstruction.\",\n",
       "  69: \"We present a method for controlling a simulated humanoid to grasp an object and move it to follow an object's trajectory. Due to the challenges in controlling a humanoid with dexterous hands, prior methods often use a disembodied hand and only consider vertical lifts or short trajectories. This limited scope hampers their applicability for object manipulation required for animation and simulation. To close this gap, we learn a controller that can pick up a large number (>1200) of objects and carry them to follow randomly generated trajectories. Our key insight is to leverage a humanoid motion representation that provides human-like motor skills and significantly speeds up training. Using only simplistic reward, state, and object representations, our method shows favorable scalability on diverse objects and trajectories. For training, we do not need a dataset of paired full-body motion and object trajectories. At test time, we only require the object mesh and desired trajectories for grasping and transporting. To demonstrate the capabilities of our method, we show state-of-the-art success rates in following object trajectories and generalizing to unseen objects. Code and models will be released.\",\n",
       "  70: 'Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance. However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained Code LLMs. In this paper, we present AlchemistCoder, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed AlchemistPrompts, to harmonize different data sources and instruction-response pairs. Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review. Extensive experiments demonstrate that AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence. Source code and models are available at https://github.com/InternLM/AlchemistCoder.',\n",
       "  71: \"Multiobjective optimization (MOO) plays a critical role in various real-world domains. A major challenge therein is generating $K$ uniform Pareto-optimal solutions to represent the entire Pareto front. To address this issue, this paper firstly introduces \\\\emph{fill distance} to evaluate the $K$ design points, which provides a quantitative metric for the representativeness of the design. However, directly specifying the optimal design that minimizes the fill distance is nearly intractable due to the nested $\\\\min-\\\\max-\\\\min$ optimization problem. To address this, we propose a surrogate ``max-packing'' design for the fill distance design, which is easier to optimize and leads to a rate-optimal design with a fill distance at most $4\\\\times$ the minimum value.    Extensive experiments on synthetic and real-world benchmarks demonstrate that our proposed paradigm efficiently produces high-quality, representative solutions and outperforms baseline methods.\",\n",
       "  72: 'Reflective photoplethysmography (PPG) has become the default sensing technique in wearable devices to monitor cardiac activity via a person’s heart rate (HR). However, PPG-based HR estimates can be substantially impacted by factors such as the wearer’s activities, sensor placement and resulting motion artifacts, as well as environmental characteristics such as temperature and ambient light. These and other factors can significantly impact and decrease HR prediction reliability. In this paper, we show that state-of-the-art HR estimation methods struggle when processing representative data from everyday activities in outdoor environments, likely because they rely on existing datasets that captured controlled conditions. We introduce a novel multimodal dataset and benchmark results for continuous PPG recordings during outdoor activities from 16 participants over 13.5 hours, captured from four wearable sensors, each worn at a different location on the body, totaling 216 hours. Our recordings include accelerometer, temperature, and altitude data, as well as a synchronized Lead I-based electrocardiogram for ground-truth HR references. Participants completed a round trip from Zurich to Jungfraujoch, a tall mountain in Switzerland over the course of one day. The trip included outdoor and indoor activities such as walking, hiking, stair climbing, eating, drinking, and resting at various temperatures and altitudes (up to 3,571 m above sea level) as well as using cars, trains, cable cars, and lifts for transport—all of which impacted participants’ physiological dynamics. We also present a novel method that estimates HR values more robustly in such real-world scenarios than existing baselines.Dataset & code for HR estimation: https://siplab.org/projects/WildPPG',\n",
       "  73: 'Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images. While current methods adopt the paradigm of training a dedicated network end-to-end using labeled image data, they are limited in terms of generalizability and interpretability. To address these issues, we first present a simple yet well-crafted framework named SocialGPT, which combines the perception capability of Vision Foundation Models (VFMs) and the reasoning capability of Large Language Models (LLMs) within a modular framework, providing a strong baseline for social relation recognition. Specifically, we instruct VFMs to translate image content into a textual social story, and then utilize LLMs for text-based reasoning. SocialGPT introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps. Without additional model training, it achieves competitive zero-shot results on two databases while offering interpretable answers, as LLMs can generate language-based explanations for the decisions. The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired. As we essentially convert a visual classification task into a generative task of LLMs, automatic prompt optimization encounters a unique long prompt optimization issue. To address this issue, we further propose the Greedy Segment Prompt Optimization (GSPO), which performs a greedy search by utilizing gradient information at the segment level. Experimental results show that GSPO significantly improves performance, and our method also generalizes to different image styles. The code is available at https://github.com/Mengzibin/SocialGPT.',\n",
       "  74: 'Choice models are essential for understanding decision-making processes in domains like online advertising, product recommendations, and assortment optimization. The Multinomial Logit (MNL) model is particularly versatile in selecting products or advertisements for display. However, challenges arise with unknown MNL parameters and delayed feedback, requiring sellers to learn customers’ choice behavior and make dynamic decisions with biased knowledge due to delays. We address these challenges by developing an algorithm that handles delayed feedback, balancing exploration and exploitation using confidence bounds and optimism. We first consider a censored setting where a threshold for considering feedback is imposed by business requirements. Our algorithm demonstrates a $\\\\tilde{O}(\\\\sqrt{NT})$ regret, with a matching lower bound up to a logarithmic term. Furthermore, we extend our analysis to environments with non-thresholded delays, achieving a $\\\\tilde{O}(\\\\sqrt{NT})$ regret. To validate our approach, we conduct experiments that confirm the effectiveness of our algorithm.',\n",
       "  75: 'Selective Classification, wherein models can reject low-confidence predictions, promises reliable translation of machine-learning based classification systems to real-world scenarios such as clinical diagnostics. While current evaluation of these systems typically assumes fixed working points based on pre-defined rejection thresholds, methodological progress requires benchmarking the general performance of systems akin to the $\\\\mathrm{AUROC}$ in standard classification. In this work, we define 5 requirements for multi-threshold metrics in selective classification regarding task alignment, interpretability, and flexibility, and show how current approaches fail to meet them. We propose the Area under the Generalized Risk Coverage curve ($\\\\mathrm{AUGRC}$), which meets all requirements and can be directly interpreted as the average risk of undetected failures. We empirically demonstrate the relevance of $\\\\mathrm{AUGRC}$ on a comprehensive benchmark spanning 6 data sets and 13 confidence scoring functions. We find that the proposed metric substantially changes metric rankings on 5 out of the 6 data sets.',\n",
       "  76: 'In recent advancements in unsupervised visual representation learning, the Joint-Embedding Predictive Architecture (JEPA) has emerged as a significant method for extracting visual features from unlabeled imagery through an innovative masking strategy. Despite its success, two primary limitations have been identified: the inefficacy of Exponential Moving Average (EMA) from I-JEPA in preventing entire collapse and the inadequacy of I-JEPA prediction in accurately learning the mean of patch representations. Addressing these challenges, this study introduces a novel framework, namely C-JEPA (Contrastive-JEPA), which integrates the Image-based Joint-Embedding Predictive Architecture with the Variance-Invariance-Covariance Regularization (VICReg) strategy. This integration is designed to effectively learn the variance/covariance for preventing entire collapse and ensuring invariance in the mean of augmented views, thereby overcoming the identified limitations. Through empirical and theoretical evaluations, our work demonstrates that C-JEPA significantly enhances the stability and quality of visual representation learning. When pre-trained on the ImageNet-1K dataset, C-JEPA exhibits rapid and improved convergence in both linear probing and fine-tuning performance metrics.',\n",
       "  77: 'We present a detailed study of surrogate losses and algorithms for multi-label learning, supported by $H$-consistency bounds. We first show that, for the simplest form of multi-label loss (the popular Hamming loss), the well-known consistent binary relevance surrogate suffers from a sub-optimal dependency on the number of labels in terms of $H$-consistency bounds, when using smooth losses such as logistic losses. Furthermore, this loss function fails to account for label correlations. To address these drawbacks, we introduce a novel surrogate loss, *multi-label logistic loss*,  that accounts for label correlations and benefits from label-independent $H$-consistency bounds. We then broaden our analysis to cover a more extensive family of multi-label losses, including all common ones and a new extension defined based on linear-fractional functions with respect to the confusion matrix. We also extend our multi-label logistic losses to more comprehensive multi-label comp-sum losses, adapting comp-sum losses from standard classification to the multi-label learning. We prove that this family of surrogate losses benefits from $H$-consistency bounds, and thus Bayes-consistency, across any general multi-label loss. Our work thus proposes a unified surrogate loss framework benefiting from strong consistency guarantees for any multi-label loss, significantly expanding upon previous work which only established Bayes-consistency and for specific loss functions. Additionally, we adapt constrained losses from standard classification to multi-label constrained losses in a similar way, which also benefit from $H$-consistency bounds and thus Bayes-consistency for any multi-label loss. We further describe efficient gradient computation algorithms for minimizing the multi-label logistic loss.',\n",
       "  78: \"Data valuation is a class of techniques for quantitatively assessing the value of data for applications like pricing in data marketplaces. Existing data valuation methods define a value for a discrete dataset. However, in many use cases, users are interested in not only the value of the dataset, but that of the distribution from which the dataset was sampled. For example, consider a buyer trying to evaluate whether to purchase data from different vendors. The buyer may observe (and compare) only a small preview sample from each vendor, to decide which vendor's data distribution is most useful to the buyer and purchase. The core question is how should we compare the values of data distributions from their samples? Under a Huber characterization of the data heterogeneity across vendors, we propose a maximum mean discrepancy (MMD)-based valuation method which enables theoretically principled and actionable policies for comparing data distributions from samples. We empirically demonstrate that our method is sample-efficient and effective in identifying valuable data distributions against several existing baselines, on multiple real-world datasets (e.g., network intrusion detection, credit card fraud detection) and downstream applications (classification, regression).\",\n",
       "  79: 'The optimization of the latents and parameters of diffusion models with respect to some differentiable metric defined on the output of the model is a challenging and complex problem. The sampling for diffusion models is done by solving either the probability flow ODE or diffusion SDE wherein a neural network approximates the score function allowing a numerical ODE/SDE solver to be used. However, naive backpropagation techniques are memory intensive, requiring the storage of all intermediate states, and face additional complexity in handling the injected noise from the diffusion term of the diffusion SDE. We propose a novel family of bespoke ODE solvers to the continuous adjoint equations for diffusion models, which we call AdjointDEIS. We exploit the unique construction of diffusion SDEs to further simplify the formulation of the continuous adjoint equations using exponential integrators. Moreover, we provide convergence order guarantees for our bespoke solvers. Significantly, we show that continuous adjoint equations for diffusion SDEs actually simplify to a simple ODE. Lastly, we demonstrate the effectiveness of AdjointDEIS for guided generation with an adversarial attack in the form of the face morphing problem. Our code will be released on our project page https://zblasingame.github.io/AdjointDEIS/',\n",
       "  80: 'Leveraging the depth and flexibility of XLand as well as the rapid prototyping features of the Unity engine, we present the United Unity Universe — an open-source toolkit designed to accelerate the creation of innovative reinforcement learning environments. This toolkit includes a robust implementation of XLand 2.0 complemented by a user-friendly interface which allows users to modify the details of procedurally generated terrains and task rules with ease. Additionally, we provide a curated selection of terrains and rule sets, accompanied by implementations of reinforcement learning baselines to facilitate quick experimentation with novel architectural designs for adaptive agents. Furthermore, we illustrate how the United Unity Universe serves as a high-level language that enables researchers to develop diverse and endlessly variable 3D environments within a unified framework. This functionality establishes the United Unity Universe (U3) as an essential tool for advancing the field of reinforcement learning, especially in the development of adaptive and generalizable learning systems.',\n",
       "  81: \"We present the first algorithm for testing equivalence  between two continuous distributions using differential privacy (DP). Our algorithm is a private version of the algorithm of Diakonikolas et al. The algorithm of Diakonikolas et al uses the data itself to repeatedly discretize the real line so that --- when the two distributions are far apart in ${\\\\cal A}_k$-norm --- one of the discretized distributions exhibits large $L_2$-norm difference; and upon repeated sampling such large gap would be detected. Designing its private analogue poses two difficulties. First, our DP algorithm can not resample new datapoints as a change to a single datapoint may lead to a very large change in the descretization of the real line. In contrast, the (sorted) index of the discretization point changes only by $1$ between neighboring instances, and so we use a novel algorithm that set the discretization points using random Bernoulli noise, resulting in only a few buckets being affected under the right coupling. Second, our algorithm, which doesn't resample data, requires we also revisit the utility analysis of the original algorithm and prove its correctness w.r.t. the original sorted data; a problem we tackle using sampling a subset of Poisson-drawn size from each discretized bin. Lastly, since any distribution can be reduced to a continuous distribution, our algorithm is successfully carried to multiple other families of distributions and thus has numerous applications.\",\n",
       "  82: 'Reinforcement Learning (RL) is a powerful method for controlling dynamic systems, but its learning mechanism can lead to unpredictable actions that undermine the safety of critical systems. Here, we propose RL with Adaptive Regularization (RL-AR), an algorithm that enables safe RL exploration by combining the RL policy with a policy regularizer that hard-codes the safety constraints. RL-AR performs policy combination via a \"focus module,\" which determines the appropriate combination depending on the state—relying more on the safe policy regularizer for less-exploited states while allowing unbiased convergence for well-exploited states. In a series of critical control applications, we demonstrate that RL-AR not only ensures safety during training but also achieves a return competitive with the standards of model-free RL that disregards safety.',\n",
       "  83: \"While in-context learning is commonly associated with causal language models, such as GPT, we demonstrate that this capability also 'emerges' in masked language models. Through an embarrassingly simple inference technique, we enable an existing masked model, DeBERTa, to perform generative tasks without additional training or architectural changes. Our evaluation reveals that the masked and causal language models behave very differently, as they clearly outperform each other on different categories of tasks. These complementary strengths suggest that the field's focus on causal models for in-context learning may be limiting – both architectures can develop these capabilities, but with distinct advantages; pointing toward promising hybrid approaches that combine the strengths of both objectives.\",\n",
       "  84: 'Efficient computation of graph diffusion equations (GDEs), such as Personalized PageRank, Katz centrality, and the Heat kernel, is crucial for clustering, training neural networks, and many other graph-related problems. Standard iterative methods require accessing the whole graph per iteration, making them time-consuming for large-scale graphs. While existing local solvers approximate diffusion vectors through heuristic local updates, they often operate sequentially and are typically designed for specific diffusion types, limiting their applicability. Given that diffusion vectors are highly localizable, as measured by the participation ratio, this paper introduces a novel framework for approximately solving GDEs using a local diffusion process. This framework reveals the suboptimality of existing local solvers. Furthermore, our approach effectively localizes standard iterative solvers by designing simple and provably sublinear time algorithms. These new local solvers are highly parallelizable, making them well-suited for implementation on GPUs. We demonstrate the effectiveness of our framework in quickly obtaining approximate diffusion vectors, achieving up to a hundred-fold speed improvement, and its applicability to large-scale dynamic graphs. Our framework could also facilitate more efficient local message-passing mechanisms for GNNs.',\n",
       "  85: 'Human visual search ability enables efficient and accurate tracking of an arbitrary moving target, which is a significant research interest in cognitive neuroscience. The recently proposed Central-Peripheral Dichotomy (CPD) theory sheds light on how humans effectively process visual information and track moving targets in complex environments. However, existing visual object tracking algorithms still fall short of matching human performance in maintaining tracking over time, particularly in complex scenarios requiring robust visual search skills. These scenarios often involve Spatio-Temporal Discontinuities (i.e., STDChallenge), prevalent in long-term tracking and global instance tracking. To address this issue, we conduct research from a human-like modeling perspective: (1) Inspired by the CPD, we pro- pose a new tracker named CPDTrack to achieve human-like visual search ability. The central vision of CPDTrack leverages the spatio-temporal continuity of videos to introduce priors and enhance localization precision, while the peripheral vision improves global awareness and detects object movements. (2) To further evaluate and analyze STDChallenge, we create the STDChallenge Benchmark. Besides, by incorporating human subjects, we establish a human baseline, creating a high- quality environment specifically designed to assess trackers’ visual search abilities in videos across STDChallenge. (3) Our extensive experiments demonstrate that the proposed CPDTrack not only achieves state-of-the-art (SOTA) performance in this challenge but also narrows the behavioral differences with humans. Additionally, CPDTrack exhibits strong generalizability across various challenging benchmarks. In summary, our research underscores the importance of human-like modeling and offers strategic insights for advancing intelligent visual target tracking. Code and models are available at https://github.com/ZhangDailing8/CPDTrack.',\n",
       "  86: \"Credit attribution is crucial across various fields. In academic research, proper citation acknowledges prior work and establishes original contributions. Similarly, in generative models, such as those trained on existing artworks or music, it is important to ensure that any generated content influenced by these works appropriately credits the original creators.We study credit attribution by machine learning algorithms. We propose new definitions--relaxations of Differential Privacy--that weaken the stability guarantees for a designated subset of $k$ datapoints. These $k$ datapoints can be used non-stably with permission from their owners, potentially in exchange for compensation. Meanwhile, the remaining datapoints are guaranteed to have no significant influence on the algorithm's output.Our framework extends well-studied notions of stability, including Differential Privacy ($k = 0$), differentially private learning with public data (where the $k$ public datapoints are fixed in advance),and stable sample compression (where the $k$ datapoints are selected adaptively by the algorithm).We examine the expressive power of these stability notions within the PAC learning framework, provide a comprehensive characterization of learnability for algorithms adhering to these principles, and propose directions and questions for future research.\",\n",
       "  87: 'Mobile device operation tasks are increasingly becoming a popular multi-modal AI application scenario. Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants. Instead, MLLM-based agents, which enhance capabilities through tool invocation, are gradually being applied to this scenario. However, the two major navigation challenges in mobile device operation tasks — task progress navigation and focus content navigation — are difficult to effectively solve under the single-agent architecture of existing work. This is due to the overly long token sequences and the interleaved text-image data format, which limit performance. To address these navigation challenges effectively, we propose Mobile-Agent-v2, a multi-agent architecture for mobile device operation assistance. The architecture comprises three agents: planning agent, decision agent, and reflection agent. The planning agent condenses lengthy, interleaved image-text history operations and screens summaries into a pure-text task progress, which is then passed on to the decision agent. This reduction in context length makes it easier for decision agent to navigate the task progress. To retain focus content, we design a memory unit that updates with task progress by decision agent. Additionally, to correct erroneous operations, the reflection agent observes the outcomes of each operation and handles any mistake accordingly. Experimental results indicate that Mobile-Agent-v2 achieves over a 30% improvement in task completion compared to the single-agent architecture of Mobile-Agent. The code is open-sourced at https://github.com/X-PLUG/MobileAgent.',\n",
       "  88: \"Vector Symbolic Architectures (VSAs) are one approach to developing Neuro-symbolic AI, where two vectors in $\\\\mathbb{R}^d$ are 'bound' together to produce a new vector in the same space. VSAs support the commutativity and associativity of this binding operation, along with an inverse operation, allowing one to construct symbolic-style manipulations over real-valued vectors. Most VSAs were developed before deep learning and automatic differentiation became popular and instead focused on efficacy in hand-designed systems. In this work, we introduce the Hadamard-derived linear Binding (HLB), which is designed to have favorable computational efficiency, and efficacy in classic VSA tasks, and perform well in differentiable systems.\",\n",
       "  89: \"Large Language Model Multi-Agent Systems (LLM-MAS) have greatly progressed in solving complex tasks. It communicates among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents' collaborations are leveraged to perform multi-person tasks, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems. In iAgents, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents' communication towards effective information exchange. Together with InfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry. Experimental results show that iAgents can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes.\",\n",
       "  90: 'While time series diffusion models have received considerable focus from many recent works, the performance of existing models remains highly unstable. Factors limiting time series diffusion models include insufficient time series datasets and the absence of guidance. To address these limitations, we propose a Retrieval-Augmented Time series Diffusion model (RATD). The framework of RATD consists of two parts: an embedding-based retrieval process and a reference-guided diffusion model. In the first part, RATD retrieves the time series that are most relevant to historical time series from the database as references. The references are utilized to guide the denoising process in the second part. Our approach allows leveraging meaningful samples within the database to aid in sampling, thus maximizing the utilization of datasets. Meanwhile, this reference-guided mechanism also compensates for the deficiencies of existing time series diffusion models in terms of guidance. Experiments and visualizations on multiple datasets demonstrate the effectiveness of our approach, particularly in complicated prediction tasks. Our code is available at https://github.com/stanliu96/RATD',\n",
       "  91: \"We consider the problem of recovering the ground truth ordering (ranking, top-$k$, or others) over a large number of alternatives. The wisdom of crowd is a heuristic approach based on Condorcet's Jury theorem to address this problem through collective opinions.This approach fails to recover the ground truth when the majority of the crowd is misinformed. The \\\\emph{surprisingly popular} (SP) algorithm~\\\\citep{prelec2017solution} is an alternative approach that is able to recover the ground truth even when experts are in minority. The SP algorithm requires the voters to predict other voters' report in the form of a full probability distribution over all rankings of alternatives. However, when the number of alternatives, $m$, is large, eliciting the prediction report or even the vote over $m$ alternatives might be too costly. In this paper, we design a scalable alternative of the SP algorithm which only requires eliciting partial preferences from the voters, and propose new variants of the SP algorithm. In particular, we propose two versions---\\\\emph{Aggregated-SP} and \\\\emph{Partial-SP}---that ask voters to report vote and prediction on a subset of size $k$ ($\\\\ll m$) in terms of top alternative, partial rank, or an approval set. Through a large-scale crowdsourcing experiment on MTurk, we show that both of our approaches outperform conventional preference aggregation algorithms for the recovery of ground truth rankings, when measured in terms of Kendall-Tau distance and Spearman's $\\\\rho$. We further analyze the collected data and demonstrate that voters' behavior in the experiment, including the minority of the experts, and the SP phenomenon, can be correctly simulated by a  concentric mixtures of Mallows model. Finally, we provide theoretical bounds on the sample complexity of SP algorithms with partial rankings to demonstrate the theoretical guarantees of the proposed methods.\",\n",
       "  92: 'Surface parameterization plays an essential role in numerous computer graphics and geometry processing applications. Traditional parameterization approaches are designed for high-quality meshes laboriously created by specialized 3D modelers, thus unable to meet the processing demand for the current explosion of ordinary 3D data. Moreover, their working mechanisms are typically restricted to certain simple topologies, thus relying on cumbersome manual efforts (e.g., surface cutting, part segmentation) for pre-processing. In this paper, we introduce the Flatten Anything Model (FAM), an unsupervised neural architecture to achieve global free-boundary surface parameterization via learning point-wise mappings between 3D points on the target geometric surface and adaptively-deformed UV coordinates within the 2D parameter domain. To mimic the actual physical procedures, we ingeniously construct geometrically-interpretable sub-networks with specific functionalities of surface cutting, UV deforming, unwrapping, and wrapping, which are assembled into a bi-directional cycle mapping framework. Compared with previous methods, our FAM directly operates on discrete surface points without utilizing connectivity information, thus significantly reducing the strict requirements for mesh quality and even applicable to unstructured point cloud data. More importantly, our FAM is fully-automated without the need for pre-cutting and can deal with highly-complex topologies, since its learning process adaptively finds reasonable cutting seams and UV boundaries. Extensive experiments demonstrate the universality, superiority, and inspiring potential of our proposed neural surface parameterization paradigm. Our code is available at https://github.com/keeganhk/FlattenAnything.',\n",
       "  93: 'This paper concerns the problem of aligning samples from large language models to human preferences using *best-of-$n$* sampling, where we draw $n$ samples, rank them, and return the best one. We consider two fundamental problems. First: what is the relationship between best-of-$n$ and other (RLHF-type) approaches to aligning LLMs? In particular, when should one be preferred to the other? We show that the best-of-$n$ sampling distribution is essentially equivalent to the policy learned by RLHF if we apply a particular monotone transformation to the reward function. Moreover, we show that this transformation yields the best possible trade-off between win-rate against the base model vs KL distance from the base model. Then, best-of-$n$ is a Pareto-optimal win-rate vs KL solution.The second problem we consider is how to fine-tune a model to mimic the best-of-$n$ sampling distribution, to avoid drawing $n$ samples for each inference. We derive *BonBon Alignment* as a method for achieving this. Experiments show that BonBon alignment yields a model that achieves high win rates while minimally affecting off-target aspects of the generations.',\n",
       "  94: 'The remarkable capabilities and easy accessibility of large language models (LLMs) have significantly increased societal risks (e.g., fake news generation), necessitating the development of LLM-generated text (LGT) detection methods for safe usage. However, detecting LGTs is challenging due to the vast number of LLMs, making it impractical to account for each LLM individually; hence, it is crucial to identify the common characteristics shared by these models. In this paper, we draw attention to a common feature of recent powerful LLMs, namely the alignment training, i.e., training LLMs to generate human-preferable texts. Our key finding is that as these aligned LLMs are trained to maximize the human preferences, they generate texts with higher estimated preferences even than human-written texts; thus, such texts are easily detected by using the reward model (i.e., an LLM trained to model human preference distribution). Based on this finding, we propose two training schemes to further improve the detection ability of the reward model, namely (i) continual preference fine-tuning to make reward model prefer aligned LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a rephrased texts from human-written texts using aligned LLMs), which serves as a median preference text corpus between LGTs and human-written texts to learn the decision boundary better. We provide an extensive evaluation by considering six text domains across twelve aligned LLMs, where our method demonstrates state-of-the-art results.',\n",
       "  95: 'Learning Rate Warmup is a popular heuristic for training neural networks, especially at larger batch sizes, despite limited understanding of its benefits. Warmup decreases the update size $\\\\Delta \\\\mathbf{w}_t = \\\\eta_t \\\\mathbf{u}_t$ early in training by using lower values for the learning rate $\\\\eta_t$. In this work we argue that warmup benefits training by keeping the overall size of $\\\\Delta \\\\mathbf{w}_t$ limited, counteracting large initial values of $\\\\mathbf{u}_t$. Focusing on small-scale GPT training with AdamW/Lion, we explore the following question: *Why and by which criteria are early updates $\\\\mathbf{u}_t$ too large?* We analyze different metrics for the update size including the $\\\\ell_2$-norm, resulting directional change, and impact on the representations of the network, providing a new perspective on warmup. In particular, we find that warmup helps counteract large angular updates as well as a limited critical batch size early in training. Finally, we show that the need for warmup can be significantly reduced or eliminated by modifying the optimizer to explicitly normalize $\\\\mathbf{u}_t$ based on the aforementioned metrics.',\n",
       "  96: \"Explainability in artificial intelligence is crucial for restoring trust, particularly in areas like face forgery detection, where viewers often struggle to distinguish between real and fabricated content. Vision and Large Language Models (VLLM) bridge computer vision and natural language, offering numerous applications driven by strong common-sense reasoning. Despite their success in various tasks, the potential of vision and language remains underexplored in face forgery detection, where they hold promise for enhancing explainability by leveraging the intrinsic reasoning capabilities of language to analyse fine-grained manipulation areas.    For that reason, few works have recently started to frame the problem of deepfake detection as a Visual Question Answering (VQA) task, nevertheless omitting the realistic and informative open-ended multi-label setting. With the rapid advances in the field of VLLM, an exponential rise of investigations in that direction is expected.    As such, there is a need for a clear experimental methodology that converts face forgery detection to a Visual Question Answering (VQA) task to systematically and fairly evaluate different VLLM architectures. Previous evaluation studies in deepfake detection have mostly focused on the simpler binary task, overlooking evaluation protocols for multi-label fine-grained detection and text-generative models. We propose a multi-staged approach that diverges from the traditional binary evaluation protocol and conducts a comprehensive evaluation study to compare the capabilities of several VLLMs in this context.    In the first stage, we assess the models' performance on the binary task and their sensitivity to given instructions using several prompts. In the second stage, we delve deeper into fine-grained detection by identifying areas of manipulation in a multiple-choice VQA setting. In the third stage, we convert the fine-grained detection to an open-ended question and compare several matching strategies for the multi-label classification task. Finally, we qualitatively evaluate the fine-grained responses of the VLLMs included in the benchmark.    We apply our benchmark to several popular models, providing a detailed comparison of binary, multiple-choice, and open-ended VQA evaluation across seven datasets. \\\\url{https://nickyfot.github.io/hitchhickersguide.github.io/}\",\n",
       "  97: \"Meta-reinforcement learning (Meta-RL) has attracted attention due to its capability to enhance reinforcement learning (RL) algorithms, in terms of data efficiency and generalizability. In this paper, we develop a bilevel optimization framework for meta-RL (BO-MRL) to learn the meta-prior for task-specific policy adaptation, which implements multiple-step policy optimization on one-time data collection. Beyond existing meta-RL analyses, we provide upper bounds of the expected optimality gap over the task distribution. This metric measures the distance of the policy adaptation from the learned meta-prior to the task-specific optimum, and quantifies the model's generalizability to the task distribution. We empirically validate the correctness of the derived upper bounds and demonstrate the superior effectiveness of the proposed algorithm over benchmarks.\",\n",
       "  98: 'We propose a novel approach to the problem of mutual information (MI) estimation via introducing a family of estimators based on normalizing flows. The estimator maps original data to the target distribution, for which MI is easier to estimate. We additionally explore the target distributions with known closed-form expressions for MI. Theoretical guarantees are provided to demonstrate that our approach yields MI estimates for the original data. Experiments with high-dimensional data are conducted to highlight the practical advantages of the proposed method.',\n",
       "  99: 'This paper presents UltraEdit, a large-scale (~ 4M editing samples), automatically generated dataset for instruction-based image editing. Our key idea is to address the drawbacks in existing image editing datasets like InstructPix2Pix and MagicBrush, and provide a systematic approach to producing massive and high-quality image editing samples: 1) UltraEdit includes more diverse editing instructions by combining LLM creativity and in-context editing examples by human raters; 2) UltraEdit is anchored on real images (photographs or artworks), which offers more diversity and less biases than those purely synthesized by text-to-image models; 3) UltraEdit supports region-based editing with high-quality, automatically produced region annotations. Our experiments show that canonical diffusion-based editing baselines trained on UltraEdit set new records on challenging MagicBrush and Emu-Edit benchmarks, respectively. Our analysis further confirms the crucial role of real image anchors and region-based editing data. The dataset, code, and models will be made public.',\n",
       "  100: 'Enzymes are important proteins that catalyze chemical reactions. In recent years, machine learning methods have emerged to predict enzyme function from sequence; however, there are no  standardized benchmarks to evaluate these methods. We introduce CARE, a benchmark and dataset suite for the Classification And Retrieval of Enzymes (CARE). CARE centers on two tasks: (1) classification of a protein sequence by its enzyme commission (EC) number and (2) retrieval of an EC number given a chemical reaction. For each task, we design train-test splits to evaluate different kinds of out-of-distribution generalization that are relevant to real use cases. For the classification task, we provide baselines for state-of-the-art methods. Because the retrieval task has not been previously formalized, we propose a method called Contrastive Reaction-EnzymE Pretraining (CREEP) as one of the first baselines for this task and compare it to the recent method, CLIPZyme. CARE is available at https://github.com/jsunn-y/CARE/.',\n",
       "  101: 'The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differentiated uni-modal features between natural and adversarial examples. The proposed framework gives access to learn adversarial text supervision, which provides superior cross-modal adversarial alignment and matches state-of-the-art zero-shot adversarial robustness with only 1\\\\% training data. Code is available at: https://github.com/lionel-w2/FAP.',\n",
       "  102: 'Neural 3D scene representations have shown great potential for 3D reconstruction from 2D images. However, reconstructing real-world captures of complex scenes still remains a challenge. Existing generic 3D reconstruction methods often struggle to represent fine geometric details and do not adequately model reflective surfaces of large-scale scenes. Techniques that explicitly focus on reflective surfaces can model complex and detailed reflections by exploiting better reflection parameterizations. However, we observe that these methods are often not robust in real scenarios where non-reflective as well as reflective components are present. In this work, we propose UniSDF, a general purpose 3D reconstruction method that can reconstruct large complex scenes with reflections. We investigate both camera view as well as reflected view-based color parameterization techniques and find that explicitly blending these representations in 3D space enables reconstruction of surfaces that are more geometrically accurate, especially for reflective surfaces. We further combine this representation with a multi-resolution grid backbone that is trained in a coarse-to-fine manner, enabling faster reconstructions than prior methods. Extensive experiments on object-level datasets DTU, Shiny Blender as well as unbounded datasets Mip-NeRF 360 and Ref-NeRF real demonstrate that our method is able to robustly reconstruct complex large-scale scenes with fine details and reflective surfaces, leading to the best overall performance. Project page: https://fangjinhuawang.github.io/UniSDF.',\n",
       "  103: \"Goal-directed planning presents a challenge for classical RL algorithms due to the vastness of the combinatorial state and goal spaces, while humans and animals adapt to complex environments, especially with diverse, non-stationary objectives, often employing intermediate goals for long-horizon tasks.Here, we propose a goal reduction mechanism for effectively deriving subgoals from arbitrary and distant original goals, using a novel loop-removal technique.The product of the method, called goal-reducer, distills high-quality subgoals from a replay buffer, all without the need for prior global environmental knowledge.Simulations show that the goal-reducer can be integrated into RL frameworks like Deep Q-learning and Soft Actor-Critic.It accelerates performance in both discrete and continuous action space tasks, such as grid world navigation and robotic arm manipulation, relative to the corresponding standard RL models.Moreover, the goal-reducer, when combined with a local policy, without iterative training, outperforms its integrated deep RL counterparts in solving a navigation task.This goal reduction mechanism also models human problem-solving.Comparing the model's performance and activation with human behavior and fMRI data in a treasure hunting task, we found matching representational patterns between an goal-reducer agent's components and corresponding human brain areas, particularly the vmPFC and basal ganglia. The results suggest that humans may use a similar computational framework for goal-directed behaviors.\",\n",
       "  104: 'Current post-training quantization methods for LLMs compress the weights down to 4-bits, with moderate to low degradation in accuracy. However, further reducing the number of bits or accelerating the network while avoiding large accuracy drops, especially for smaller, sub 7B models, remains an actively researched and open problem. To address this, in this work, we introduce Quantization with Binary Bases (QBB), a new approach for low-bit quantization that effectively removes (nearly) all multiplications, reducing the implementation to summations. Our novel approach works by decomposing the original weights into a set of binary (1-bit) matrices using an iterative process. For a given layer, starting from a weight matrix, we first construct an initial approximation using an analytical solution, where each new binary matrix, paired with a scaling vector, approximates the residual error of the previous estimation. Secondly, using gradient descent and a progressive learning curriculum, we find the optimal set of binary matrices and scaling vectors that minimize the $\\\\ell_2$ distance between the produced approximation and original weights. Thirdly, as previous steps are input agnostic, we holistically optimize the scaling vectors alone, calibrating them in student-teacher fashion, with the teacher providing both the data,  by autoregressive generation starting from a random token, and the target logits. When evaluated across multiple LLM families, our approach matches and outperforms all prior works, setting a new state-of-the-art result using a summation-only based approach.',\n",
       "  105: 'We introduce a new benchmark designed to advance the development of general-purpose, large-scale vision-language models for remote sensing images. Although several vision-language datasets in remote sensing have been proposed to pursue this goal, existing datasets are typically tailored to single tasks, lack detailed object information, or suffer from inadequate quality control. Exploring these improvement opportunities, we present a Versatile vision-language Benchmark for Remote Sensing image understanding, termed VRSBench. This benchmark comprises 29,614 images, with 29,614 human-verified detailed captions, 52,472 object references, and 123,221 question-answer pairs. It facilitates the training and evaluation of vision-language models across a broad spectrum of remote sensing image understanding tasks. We further evaluated state-of-the-art models on this benchmark for three vision-language tasks: image captioning, visual grounding, and visual question answering. Our work aims to significantly contribute to the development of advanced vision-language models in the field of remote sensing. The data and code can be accessed at https://vrsbench.github.io.',\n",
       "  106: 'Continual learning of partially similar tasks poses a challenge for artificial neural networks, as task similarity presents both an opportunity for knowledge transfer and a risk of interference and catastrophic forgetting.However, it remains unclear how task similarity in input features and readout patterns influences knowledge transfer and forgetting, as well as how they interact with common algorithms for continual learning.Here, we develop a linear teacher-student model with latent structure and show analytically that high input feature similarity coupled with low readout similarity is catastrophic for both knowledge transfer and retention. Conversely, the opposite scenario is relatively benign. Our analysis further reveals that task-dependent activity gating improves knowledge retention at the expense of transfer, while task-dependent plasticity gating does not affect either retention or transfer performance at the over-parameterized limit. In contrast, weight regularization based on the Fisher information metric significantly improves retention, regardless of task similarity, without compromising transfer performance. Nevertheless, its diagonal approximation and regularization in the Euclidean space are much less robust against task similarity. We demonstrate consistent results in a permuted MNIST task with latent variables. Overall, this work provides insights into when continual learning is difficult and how to mitigate it.',\n",
       "  107: 'Policy Optimization (PO) methods are among the most popular Reinforcement Learning (RL) algorithms in practice. Recently, Sherman et al. [2023a] proposed a PO-based algorithm with rate-optimal regret guarantees under the linear Markov Decision Process (MDP) model. However, their algorithm relies on a costly pure exploration warm-up phase that is hard to implement in practice. This paper eliminates this undesired warm-up phase, replacing it with a simple and efficient contraction mechanism. Our PO algorithm achieves rate-optimal regret with improved dependence on the other parameters of the problem (horizon and function approximation dimension) in two fundamental settings: adversarial losses with full-information feedback and stochastic losses with bandit feedback.',\n",
       "  108: 'Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As batch size, context length, or model size increases, the size of key and value (KV) cache quickly becomes the main contributor to GPU memory usage and the bottleneck of inference latency and throughput. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. Currently, KV cache quantization is performed per-channel or per-token independently. Our analysis shows that distinct channels of a key/value activation embedding are highly interdependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropy, which implies that per-channel independent quantization is sub-optimal. To mitigate this sub-optimality, we propose Coupled Quantization (CQ), which couples multiple key/value channels together for quantization to exploit their interdependence and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ compares favorably with existing baselines in preserving model quality, and improves inference throughput by 1.4–3.5$\\\\times$ relative to the uncompressed baseline. Furthermore, we demonstrate that CQ can preserve model quality reasonably with KV cache quantized down to 1 bit.',\n",
       "  109: \"Phenotype imputation plays a crucial role in improving comprehensive and accurate medical evaluation, which in turn can optimize patient treatment and bolster the reliability of clinical research. Despite the adoption of various techniques, multi-modal biological data, which can provide crucial insights into a patient's overall health, is often overlooked. With multi-modal biological data, patient characterization can be enriched from two distinct views: the biological view and the phenotype view. However, the heterogeneity and imprecise nature of the multimodal data still pose challenges in developing an effective method to model from two views. In this paper, we propose a novel framework to incorporate multi-modal biological data via view decoupling. Specifically, we segregate the modeling of biological data from phenotype data in a graph-based learning framework. From the biological view, the latent factors in biological data are discovered to model patient correlation. From the phenotype view, phenotype co-occurrence can be modeled to reveal patterns across patients. Then patients are encoded from these two distinct views. To mitigate the influence of noise and irrelevant information in biological data, we devise a cross-view contrastive knowledge distillation aimed at distilling insights from the biological view to enhance phenotype imputation. We show that phenotype imputation with the proposed model significantly outperforms the state-of-the-art models on the real-world biomedical database.\",\n",
       "  110: 'We study distributed goodness-of-fit testing for discrete distribution under bandwidth and differential privacy constraints. Information constraint distributed goodness-of-fit testing is a problem that has received considerable attention recently. The important case of discrete distributions is theoretically well understood in the classical case where all data is available in one \"central\" location. In a federated setting, however, data is distributed across multiple \"locations\" (e.g. servers) and cannot readily be shared due to e.g. bandwidth or privacy constraints that each server needs to satisfy. We show how recently derived results for goodness-of-fit testing for the mean of a multivariate Gaussian model extend to the discrete distributions, by leveraging Le Cam\\'s theory of statistical equivalence. In doing so, we derive matching minimax upper- and lower-bounds for the goodness-of-fit testing for discrete distributions under bandwidth or privacy constraints in the regime where number of samples held locally are large.',\n",
       "  111: \"Recent advances in prompt optimization have notably enhanced the performance of pre-trained language models (PLMs) on downstream tasks. However, the potential of optimized prompts on domain generalization has been under-explored. To explore the nature of prompt generalization on unknown domains, we conduct pilot experiments and find that (i) Prompts gaining more attention weight from PLMs’ deep layers are more generalizable and (ii) Prompts with more stable attention distributions in PLMs’ deep layers are more generalizable. Thus, we offer a fresh objective towards domain-generalizable prompts optimization named ''Concentration'', which represents the ''lookback'' attention from the current decoding token to the prompt tokens, to increase the attention strength on prompts and reduce the fluctuation of attention distribution.We adapt this new objective to popular soft prompt and hard prompt optimization methods, respectively. Extensive experiments demonstrate that our idea improves comparison prompt optimization methods by 1.42% for soft prompt generalization and 2.16% for hard prompt generalization in accuracy on the multi-source domain generalization setting, while maintaining satisfying in-domain performance. The promising results validate the effectiveness of our proposed prompt optimization objective and provide key insights into domain-generalizable prompts.\",\n",
       "  112: \"To foster trust in machine learning models, explanations must be faithful and stable for consistent insights. Existing relevant works rely on the $\\\\ell_p$ distance for stability assessment, which diverges from human perception. Besides, existing adversarial training (AT) associated with intensive computations may lead to an arms race. To address these challenges, we introduce a novel metric to assess the stability of top-$k$ salient features. We introduce R2ET which trains for stable explanation by efficient and effective regularizer,and analyze R2ET by multi-objective optimization to prove numerical and statistical stability of explanations. Moreover, theoretical connections between R2ET and certified robustness justify R2ET's stability in all attacks. Extensive experiments across various data modalities and model architectures show that R2ET achieves superior stability against stealthy attacks, and generalizes effectively across different explanation methods. The code can be found at https://github.com/ccha005/R2ET.\",\n",
       "  113: 'We contribute NeuralSolver, a novel recurrent solver that can efficiently and consistently extrapolate, i.e., learn algorithms from smaller problems (in terms of observation size) and execute those algorithms in large problems. Contrary to previous recurrent solvers, NeuralSolver can be naturally applied in both same-size problems, where the input and output sizes are the same, and in different-size problems, where the size of the input and output differ. To allow for this versatility, we design NeuralSolver with three main components: a recurrent module, that iteratively processes input information at different scales, a processing module, responsible for aggregating the previously processed information, and a curriculum-based training scheme, that improves the extrapolation performance of the method.To evaluate our method we introduce a set of novel different-size tasks and we show that NeuralSolver consistently outperforms the prior state-of-the-art recurrent solvers in extrapolating to larger problems, considering smaller training problems and requiring less parameters than other approaches.',\n",
       "  114: 'Solar power is a critical source of renewable energy, offering significant potential to lower greenhouse gas emissions and mitigate climate change. However, the cloud induced-variability of solar radiation reaching the earth’s surface presents a challenge for integrating solar power into the grid (e.g., storage and backup management). The new generation of geostationary satellites such as GOES-16 has become an important data source for large-scale and high temporal frequency solar radiation forecasting. However, no machine-learning-ready dataset has integrated geostationary satellite data with fine-grained solar radiation information to support forecasting model development and benchmarking with consistent metrics. We present SolarCube, a new ML-ready benchmark dataset for solar radiation forecasting. SolarCube covers 19 study areas distributed over multiple continents: North America, South America, Asia, and Oceania. The dataset supports short (i.e., 30 minutes to 6 hours) and long-term (i.e., day-ahead or longer) solar radiation forecasting at both point-level (i.e., specific locations of monitoring stations) and area-level, by processing and integrating data from multiple sources, including geostationary satellite images, physics-derived solar radiation, and ground station observations from different monitoring networks over the globe. We also evaluated a set of forecasting models for point- and image-based time-series data to develop performance benchmarks under different testing scenarios. The dataset is available at https://doi.org/10.5281/zenodo.11498739. A Python library is available to conveniently generate different variations of the dataset based on user needs, along with baseline models at https://github.com/Ruohan-Li/SolarCube.',\n",
       "  115: 'Feint behaviors refer to a set of deceptive behaviors in a nuanced manner, which enable players to obtain temporal and spatial advantages over opponents in competitive games. Such behaviors are crucial tactics in most competitive multi-player games (e.g., boxing, fencing, basketball, motor racing, etc.). However, existing literature does not provide a comprehensive (and/or concrete) formalization for Feint behaviors, and their implications on game strategies. In this work, we introduce the first comprehensive formalization of Feint behaviors at both action-level and strategy-level, and provide concrete implementation and quantitative evaluation of them in multi-player games. The key idea of our work is to (1) allow automatic generation of Feint behaviors via Palindrome-directed templates, combine them into meaningful behavior sequences via a Dual-Behavior Model; (2) concertize the implications from our formalization of Feint on game strategies, in terms of temporal, spatial, and their collective impacts respectively; and (3) provide a unified implementation scheme of Feint behaviors in existing MARL frameworks. The experimental results show that our design of Feint behaviors can (1) greatly improve the game reward gains; (2) significantly improve the diversity of Multi-Player Games; and (3) only incur negligible overheads in terms of time consumption.',\n",
       "  116: 'Language models have shown vulnerability against backdoor attacks, threatening the security of services based on them. To mitigate the threat, existing solutions attempted to search for backdoor triggers, which can be time-consuming when handling a large search space. Looking into the attack process, we observe that poisoned data will create a long-tailed effect in the victim model, causing the decision boundary to shift towards the attack targets. Inspired by this observation, we introduce LT-Defense, the first searching-free backdoor defense via exploiting the long-tailed effect. Specifically, LT-Defense employs a small set of clean examples and two metrics to distinguish backdoor-related features in the target model. Upon detecting a backdoor model, LT-Defense additionally provides test-time backdoor freezing and attack target prediction. Extensive experiments demonstrate the effectiveness of LT-Defense in both detection accuracy and efficiency, e.g., in task-agnostic scenarios, LT-Defense achieves 98% accuracy across 1440 models with less than 1% of the time cost of state-of-the-art solutions.',\n",
       "  117: 'Exploring the integration of if-then logic rules within neural network architectures  presents an intriguing area. This integration seamlessly transforms the rule learning task into neural network training using backpropagation and stochastic gradient descent. From a well-trained sparse and shallow neural network, one can interpret each layer and neuron through the language of logic rules, and a global explanatory rule set can be directly extracted. However, ensuring interpretability may impose constraints on the flexibility, depth, and width of neural networks. In this paper, we propose HyperLogic: a novel framework leveraging hypernetworks to generate weights of the main network. HyperLogic can unveil multiple diverse rule sets, each capable of capturing heterogeneous patterns in data. This provides a simple yet effective method to increase model flexibility and preserve interpretability. We theoretically analyzed the benefits of the HyperLogic by examining the approximation error and generalization capabilities under two types of regularization terms: sparsity and diversity regularizations. Experiments on real data demonstrate that our method can learn more diverse, accurate, and concise rules.',\n",
       "  118: 'Existing promptable segmentation methods in the medical imaging field primarily consider either textual or visual prompts to segment relevant objects, yet they often fall short when addressing anomalies in medical images, like tumors, which may vary greatly in shape, size, and appearance. Recognizing the complexity of medical scenarios and the limitations of textual or visual prompts, we propose a novel dual-prompt schema that leverages the complementary strengths of visual and textual prompts for segmenting various organs and tumors. Specifically, we introduce $\\\\textbf{\\\\textit{CAT}}$, an innovative model that $\\\\textbf{C}$oordinates $\\\\textbf{A}$natomical prompts derived from 3D cropped images with $\\\\textbf{T}$extual prompts enriched by medical domain knowledge. The model architecture adopts a general query-based design, where prompt queries facilitate segmentation queries for mask prediction. To synergize two types of prompts within a unified framework, we implement a ShareRefiner, which refines both segmentation and prompt queries while disentangling the two types of prompts. Trained on a consortium of 10 public CT datasets, $\\\\textbf{\\\\textit{CAT}}$ demonstrates superior performance in multiple segmentation tasks. Further validation on a specialized in-house dataset reveals the remarkable capacity of segmenting tumors across multiple cancer stages. This approach confirms that coordinating multimodal prompts is a promising avenue for addressing complex scenarios in the medical domain.',\n",
       "  119: 'Computing Jacobians with automatic differentiation is ubiquitous in many scientific domains such as machine learning, computational fluid dynamics, robotics and finance. Even small savings in the number of computations or memory usage in Jacobian computations can already incur massive savings in energy consumption and runtime. While there exist many methods that allow for such savings, they generally trade computational efficiency for approximations of the exact Jacobian.In this paper, we present a novel method to optimize the number of necessary multiplications for Jacobian computation by leveraging deep reinforcement learning (RL) and a concept called cross-country elimination while still computing the exact Jacobian. Cross-country elimination is a framework for automatic differentiation that phrases Jacobian accumulation as ordered elimination of all vertices on the computational graph where every elimination incurs a certain computational cost.Finding the optimal elimination order that minimizes the number of necessary multiplications can be seen as a single player game which in our case is played by an RL agent.We demonstrate that this method achieves up to 33% improvements over state-of-the-art methods on several relevant tasks taken from relevant domains.Furthermore, we show that these theoretical gains translate into actual runtime improvements by providing a cross-country elimination interpreter in JAX that can execute the obtained elimination orders.',\n",
       "  120: 'In this paper, we propose a novel activation function tailored specifically for graph data in Graph Neural Networks (GNNs). Motivated by the need for graph-adaptive and flexible activation functions, we introduce DiGRAF, leveraging Continuous Piecewise-Affine Based (CPAB) transformations, which we augment with an additional GNN to learn a graph-adaptive diffeomorphic activation function in an end-to-end manner. In addition to its graph-adaptivity and flexibility, DiGRAF also possesses properties that are widely recognized as desirable for activation functions, such as differentiability, boundness within the domain, and computational efficiency. We conduct an extensive set of experiments across diverse datasets and tasks, demonstrating a consistent and superior performance of DiGRAF compared to traditional and graph-specific activation functions, highlighting its effectiveness as an activation function for GNNs. Our code is available at https://github.com/ipsitmantri/DiGRAF.',\n",
       "  121: \"Algorithms with predictions is a recent framework for decision-making under uncertainty that leverages the power of machine-learned predictions without making any assumption about their quality. The goal in this framework is for algorithms to achieve an improved performance when the predictions are accurate while maintaining acceptable guarantees when the predictions are erroneous. A serious concern with algorithms that use predictions is that  these predictions can be biased and, as a result, cause the algorithm to make decisions that are deemed unfair. We show that this concern manifests itself in the classical secretary problem in the learning-augmented setting---the state-of-the-art algorithm can have zero probability of accepting the best candidate, which we deem unfair, despite promising to accept a candidate whose expected value is at least $\\\\max\\\\{\\\\Omega (1) , 1 - O(\\\\varepsilon)\\\\}$ times the optimal value, where $\\\\varepsilon$ is the prediction error.We show how to preserve this promise while also guaranteeing to accept the best candidate with probability $\\\\Omega(1)$.  Our algorithm and analysis are based on a new ``pegging'' idea that diverges from existing works and simplifies/unifies some of their results. Finally, we extend to the $k$-secretary problem and complement our theoretical analysis with experiments.\",\n",
       "  122: 'We consider nonconvex stochastic optimization problems in the asynchronous centralized distributed setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers. Using an unbiassed compression technique, we develop a new method—Shadowheart SGD—that provably improves the time complexities of all previous centralized methods. Moreover, we show that the time complexity of Shadowheart SGD is optimal in the family of centralized methods with compressed communication. We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method.',\n",
       "  123: 'Rubinfeld \\\\& Vasilyan recently introduced the framework of *testable learning* as an extension of the classical agnostic model. It relaxes distributional assumptions which are difficult to verify by conditions that can be checked efficiently by a *tester*. The tester has to accept whenever the data truly satisfies the original assumptions, and the learner has to succeed whenever the tester accepts. We focus on the setting where the tester has to accept standard Gaussian data. There, it is known that basic concept classes such as halfspaces can be learned testably with the same time complexity as in the (distribution-specific) agnostic model. In this work, we ask whether there is a price to pay for testably learning more complex concept classes. In particular, we consider polynomial threshold functions (PTFs), which naturally generalize halfspaces. We show that PTFs of arbitrary constant degree can be testably learned up to excess error $\\\\varepsilon > 0$ in time $n^{\\\\mathrm{poly}(1/\\\\varepsilon)}$. This qualitatively matches the best known guarantees in the agnostic model. Our results build on a connection between testable learning and *fooling*. In particular, we show that distributions that approximately match at least $\\\\mathrm{poly}(1/\\\\varepsilon)$ moments of the standard Gaussian fool constant-degree PTFs (up to error $\\\\varepsilon$). As a secondary result, we prove that a direct approach to show testable learning (without fooling), which was successfully used for halfspaces, cannot work for PTFs.',\n",
       "  124: \"Visual representations become progressively more abstract along the cortical hierarchy. These abstract representations define notions like objects and shapes, but at the cost of spatial specificity. By contrast, low-level regions represent spatially local but simple input features. How do spatially non-specific representations of abstract concepts in high-level areas flexibly modulate the low-level sensory representations in appropriate ways to guide context-driven and goal-directed behaviors across a range of tasks? We build a biologically motivated and trainable neural network model of dynamics in the visual pathway, incorporating local, lateral, and feedforward synaptic connections, excitatory and inhibitory neurons, and long-range top-down inputs conceptualized as low-rank modulations of the input-driven sensory responses by high-level areas. We study this ${\\\\bf D}$ynamical ${\\\\bf C}$ortical ${\\\\bf net}$work ($DCnet$) in a visual cue-delay-search task and show that the model uses its own cue representations to adaptively modulate its perceptual responses to solve the task, outperforming state-of-the-art DNN vision and LLM models. The model's population states over time shed light on the nature of contextual modulatory dynamics, generating predictions for experiments. We fine-tune the same model on classic psychophysics attention tasks, and find that the model closely replicates known reaction time results. This work represents a promising new foundation for understanding and making predictions about perturbations to visual processing in the brain.\",\n",
       "  125: 'Dense linear layers are the dominant computational bottleneck in large neural networks, presenting a critical need for more efficient alternatives. Previous efforts to develop alternatives have focused on a small number of hand-crafted structured matrices, and have neglected to investigate whether these structures can surpass dense layers in terms of compute-optimal scaling laws when both the model size and training examples are optimally allocated. In this work, we present a unifying framework that enables searching among all linear operators expressible via an Einstein summation. This framework encompasses many previously proposed structures, such as low-rank, Kronecker, Tensor-Train, and Monarch, along with many novel structures. We develop a taxonomy of all such operators based on their computational and algebraic properties, which provides insights into their scaling laws. Combining these insights with empirical evaluation, we identify a subset of structures that achieve equal or better performance than dense layers as a function of training compute. To further improve their compute efficiency, we develop a natural extension of these performant structures that convert them into a sparse Mixture-of-Experts layer. The resulting layer significantly outperforms dense layers in compute-optimal training efficiency for GPT-2 language models.',\n",
       "  126: 'Prevalent text-to-video retrieval methods represent multimodal text-video data in a joint embedding space, aiming at bridging the relevant text-video pairs and pulling away irrelevant ones. One main challenge in state-of-the-art retrieval methods lies in the modality gap, which stems from the substantial disparities between text and video and can persist in the joint space. In this work, we leverage the potential of Diffusion models to address the text-video modality gap by progressively aligning text and video embeddings in a unified space. However, we identify two key limitations of existing Diffusion models in retrieval tasks: The L2 loss does not fit the ranking problem inherent in text-video retrieval, and the generation quality heavily depends on the varied initial point drawn from the isotropic Gaussian, causing inaccurate retrieval. To this end, we introduce a new Diffusion-Inspired Truncated Sampler (DITS) that jointly performs progressive alignment and modality gap modeling in the joint embedding space. The key innovation of DITS is to leverage the inherent proximity of text and video embeddings, defining a truncated diffusion flow from the fixed text embedding to the video embedding, enhancing controllability compared to adopting the isotropic Gaussian. Moreover, DITS adopts the contrastive loss to jointly consider the relevant and irrelevant pairs, not only facilitating alignment but also yielding a discriminatively structured embedding. Experiments on five benchmark datasets suggest the state-of-the-art performance of DITS. We empirically find that DITS can also improve the structure of the CLIP embedding space. Code is available at https://github.com/Jiamian- Wang/DITS-text-video-retrieval',\n",
       "  127: 'Advances in latent diffusion models (LDMs) have revolutionized high-resolution image generation, but the design space of the autoencoder that is central to these systems remains underexplored. In this paper, we introduce LiteVAE, a new autoencoder design for LDMs, which leverages the 2D discrete wavelet transform to enhance scalability and computational efficiency over standard variational autoencoders (VAEs) with no sacrifice in output quality. We investigate the training methodologies and the decoder architecture of LiteVAE and propose several enhancements that improve the training dynamics and reconstruction quality. Our base LiteVAE model matches the quality of the established VAEs in current LDMs with a six-fold reduction in encoder parameters, leading to faster training and lower GPU memory requirements, while our larger model outperforms VAEs of comparable complexity across all evaluated metrics (rFID, LPIPS, PSNR, and SSIM).',\n",
       "  128: 'Watermarking generative content serves as a vital tool for authentication, ownership protection, and mitigation of potential misuse. Existing watermarking methods face the challenge of balancing robustness and concealment. They empirically inject a watermark that is both invisible and robust and passively achieve concealment by limiting the strength of the watermark, thus reducing the robustness. In this paper, we propose to explicitly introduce a watermark hiding process to actively achieve concealment, thus allowing the embedding of stronger watermarks. To be specific, we implant a robust watermark in an intermediate diffusion state and then guide the model to hide the watermark in the final generated image. We employ an adversarial optimization algorithm to produce the optimal hiding prompt guiding signal for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength. The watermark can be verified by reversing the generation process. Experiments on various diffusion models demonstrate the watermark remains verifiable even under significant image tampering and shows superior invisibility compared to other state-of-the-art robust watermarking methods.',\n",
       "  129: 'Restless multi-armed bandits (RMAB) have demonstrated success in optimizing resource allocation for large beneficiary populations in public health settings. Unfortunately, RMAB models lack flexibility to adapt to evolving public health policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept automated planners across domains of robotic control and navigation. In this paper, we propose a Decision Language Model (DLM) for RMABs, enabling dynamic fine-tuning of RMAB policies in public health settings using human-language commands. We propose using LLMs as automated planners to (1) interpret human policy preference prompts, (2) propose reward functions as code for a multi-agent RMAB environment, and (3) iterate on the generated reward functions using feedback from grounded RMAB simulations. We illustrate the application of DLM in collaboration with ARMMAN, an India-based non-profit promoting preventative care for pregnant mothers, that currently relies on RMAB policies to optimally allocate health worker calls to low-resource populations.  We conduct a technology demonstration in simulation using the Gemini Pro model, showing DLM can dynamically shape policy outcomes using only human prompts as input.',\n",
       "  130: \"Graph Neural Networks (GNNs), known for their effective graph encoding, are extensively used across various fields. Graph self-supervised pre-training, which trains GNN encoders without manual labels to generate high-quality graph representations, has garnered widespread attention. However, due to the inherent complex characteristics in graphs, GNNs encoders pre-trained on one dataset struggle to directly adapt to others that have different node feature shapes. This typically necessitates either model rebuilding or data alignment. The former results in non-transferability as each dataset need to rebuild a new model, while the latter brings serious knowledge loss since it forces features into a uniform shape by preprocessing such as Principal Component Analysis (PCA). To address this challenge, we propose a new Feature-Universal Graph contrastive pre-training strategy (FUG) that naturally avoids the need for model rebuilding and data reshaping. Specifically, inspired by discussions in existing work on the relationship between contrastive Learning and PCA, we conducted a theoretical analysis and discovered that PCA's optimization objective is a special case of that in contrastive Learning. We designed an encoder with contrastive constraints to emulate PCA's generation of basis transformation matrix, which is utilized to losslessly adapt features in different datasets. Furthermore, we introduced a global uniformity constraint to replace negative sampling, reducing the time complexity from $O(n^2)$ to $O(n)$, and by explicitly defining positive samples, FUG avoids the substantial memory requirements of data augmentation. In cross domain experiments, FUG has a performance close to the re-trained new models. The source code is available at: https://github.com/hedongxiao-tju/FUG.\",\n",
       "  131: 'Referenced-based scene stylization that edits the appearance based on a content-aligned reference image is an emerging research area. Starting with a pretrained neural radiance field (NeRF), existing methods typically learn a novel appearance that matches the given style. Despite their effectiveness, they inherently suffer from time-consuming volume rendering, and thus are impractical for many real-time applications. In this work, we propose ReGS, which adapts 3D Gaussian Splatting (3DGS) for reference-based stylization to enable real-time stylized view synthesis. Editing the appearance of a pretrained 3DGS is challenging as it uses discrete Gaussians as 3D representation, which tightly bind appearance with geometry. Simply optimizing the appearance as prior methods do is often insufficient for modeling continuous textures in the given reference image. To address this challenge, we propose a novel texture-guided control mechanism that adaptively adjusts local responsible Gaussians to a new geometric arrangement, serving for desired texture details. The proposed process is guided by texture clues for effective appearance editing, and regularized by scene depth for preserving original geometric structure. With these novel designs, we show ReGs can produce state-of-the-art stylization results that respect the reference texture while embracing real-time rendering speed for free-view navigation.',\n",
       "  132: 'Generalization capabilities, or rather a lack thereof, is one of the most important unsolved problems in the field of robot learning, and while several large scale efforts have set out to tackle this problem, unsolved it remains. In this paper, we hypothesize that learning temporal action abstractions using latent variable models (LVMs), which learn to map data to a compressed latent space and back, is apromising direction towards low-level skills that can readily be used for new tasks. Although several works have attempted to show this, they have generally been limited by architectures that do not faithfully capture sharable representations. To address this we present Quantized Skill Transformer (QueST), which learns a larger and more flexible latent encoding that is more capable of modeling the breadth of low-level skills necessary for a variety of tasks. To make use of this extra flexibility, QueST imparts causal inductive bias from the action sequence data into the latent space, leading to more semantically useful and transferable representations. We compare to state-of-the-art imitation learning and LVM baselines and see that QueST’s architecture leads to strong performance on several multitask and few-shot learning benchmarks. Further results and videos are available at https://quest-model.github.io.',\n",
       "  133: \"Controlling the evolution of complex physical systems is a fundamental task across science and engineering. Classical techniques suffer from limited applicability or huge computational costs. On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics. In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence. Thus, it can explore globally and plan near-optimal control sequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the discovery of control sequences that significantly deviate from the training distribution. We test our method on three tasks: 1D Burgers' equation, 2D jellyfish movement control, and 2D high-dimensional smoke control, where our generated jellyfish dataset is released as a benchmark for complex physical system control research. Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern observed in the jellyfish, aligning with established findings in the field of fluid dynamics. The project website, jellyfish dataset, and code can be found at https://github.com/AI4Science-WestlakeU/diffphycon.\",\n",
       "  134: \"Building on the success of diffusion models in visual generation, flow-based models reemerge as another prominent family of generative models that have achieved competitive or better performance in terms of both visual quality and inference speed. By learning the velocity field through flow-matching, flow-based models tend to produce a straighter sampling trajectory, which is advantageous during the sampling process. However, unlike diffusion models for which fast samplers are well-developed, efficient sampling of flow-based generative models has been rarely explored. In this paper, we propose a framework called FlowTurbo to accelerate the sampling of flow-based models while still enhancing the sampling quality. Our primary observation is that the velocity predictor's outputs in the flow-based models will become stable during the sampling, enabling the estimation of velocity via a lightweight velocity refiner. Additionally, we introduce several techniques including a pseudo corrector and sample-aware compilation to further reduce inference time. Since FlowTurbo does not change the multi-step sampling paradigm, it can be effectively applied for various tasks such as image editing, inpainting, etc. By integrating FlowTurbo into different flow-based models, we obtain an acceleration ratio of 53.1\\\\%$\\\\sim$58.3\\\\% on class-conditional generation and 29.8\\\\%$\\\\sim$38.5\\\\% on text-to-image generation. Notably, FlowTurbo reaches an FID of 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img), achieving the real-time image generation and establishing the new state-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo.\",\n",
       "  135: 'Offline reinforcement learning (RL) aims to learn policies from pre-existing datasets without further interactions, making it a challenging task. Q-learning algorithms struggle with extrapolation errors in offline settings, while supervised learning methods are constrained by model expressiveness. Recently, diffusion models (DMs) have shown promise in overcoming these limitations in single-agent learning, but their application in multi-agent scenarios remains unclear. Generating trajectories for each agent with independent DMs may impede coordination, while concatenating all agents’ information can lead to low sample efficiency. Accordingly, we propose MADiff, which is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple agents. To our knowledge, MADiff is the first diffusion-based multi-agent learning framework, functioning as both a decentralized policy and a centralized controller. During decentralized executions, MADiff simultaneously performs teammate modeling, and the centralized controller can also be applied in multi-agent trajectory predictions. Our experiments demonstrate that MADiff outperforms baseline algorithms across various multi-agent learning tasks, highlighting its effectiveness in modeling complex multi-agent interactions.',\n",
       "  136: \"It is a well-known fact that correlated equilibria can be computed in polynomial time in a large class of concisely represented games using the celebrated Ellipsoid Against Hope algorithm \\\\citep{Papadimitriou2008:Computing, Jiang2015:Polynomial}. However, the landscape of efficiently computable equilibria in sequential (extensive-form) games remains unknown. The Ellipsoid Against Hope does not apply directly to these games, because they do not have the required ``polynomial type'' property. Despite this barrier, \\\\citet{Huang2008:Computing} altered the algorithm to compute exact extensive-form correlated equilibria.In this paper, we generalize the Ellipsoid Against Hope and develop a simple algorithmic framework for efficiently computing saddle-points in bilinear zero-sum games, even when one of the dimensions is exponentially large. Moreover, the framework only requires a ``good-enough-response'' oracle, which is a weakened notion of a best-response oracle.Using this machinery, we develop a general algorithmic framework for computing exact linear $\\\\Phi$-equilibria in any polyhedral game (under mild assumptions), including correlated equilibria in normal-form games, and extensive-form correlated equilibria in extensive-form games. This enables us to give the first polynomial-time algorithm for computing exact linear-deviation correlated equilibria in extensive-form games, thus resolving an open question by \\\\citet{Farina2023:Polynomial}. Furthermore, even for the cases for which a polynomial time algorithm for exact equilibria was already known, our framework provides a conceptually simpler solution.\",\n",
       "  137: 'The goal of data attribution for text-to-image models is to identify the training images that most influence the generation of a new image. Influence is defined such that, for a given output, if a model is retrained from scratch without the most influential images, the model would fail to reproduce the same output. Unfortunately, directly searching for these influential images is computationally infeasible, since it would require repeatedly retraining models from scratch. In our work, we propose an efficient data attribution method by simulating unlearning the synthesized image. We achieve this by increasing the training loss on the output image, without catastrophic forgetting of other, unrelated concepts. We then identify training images with significant loss deviations after the unlearning process and label these as influential. We evaluate our method with a computationally intensive but \"gold-standard\" retraining from scratch and demonstrate our method\\'s advantages over previous methods.',\n",
       "  138: 'Domain generalization (DG) is a fundamental yet challenging topic in machine learning. Recently, the remarkable zero-shot capabilities of the large pre-trained vision-language model (e.g., CLIP) have made it popular for various downstream tasks. However, the effectiveness of this capacity often degrades when there are shifts in data distribution during testing compared to the training data. In this paper, we propose a novel method, known as CLIPCEIL, a model that utilizes Channel rEfinement and Image-text aLignment to facilitate the CLIP to the inaccessible $\\\\textit{out-of-distribution}$ test datasets that exhibit domain shifts. Specifically, we refine the feature channels in the visual domain to ensure they contain domain-invariant and class-relevant features by using a lightweight adapter. This is achieved by minimizing the inter-domain variance while maximizing the inter-class variance. In the meantime, we ensure the image-text alignment by aligning text embeddings of the class descriptions and their corresponding image embedding while further removing the domain-specific features. Moreover, our model integrates multi-scale CLIP features by utilizing a self-attention fusion module, technically implemented through one Transformer layer. Extensive experiments on five widely used benchmark datasets demonstrate that CLIPCEIL outperforms the existing state-of-the-art methods. The source code is available at \\\\url{https://github.com/yuxi120407/CLIPCEIL}.',\n",
       "  139: \"Conformal Prediction (CP) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable. In such a case, CP's prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability. Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage. Recently, several approaches have been put forward to recover CP guarantees in this setting. These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations. They are, however, limited in that they only support $\\\\ell_2$-bounded perturbations and classification tasks. This paper introduces VRCP (Verifiably Robust Conformal Prediction), a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks. Our VRCP method is the first to support perturbations bounded by arbitrary norms including $\\\\ell_1$, $\\\\ell_2$, and $\\\\ell_\\\\infty$, as well as regression tasks. We evaluate and compare our approach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet) and regression tasks for deep reinforcement learning environments. In every case, VRCP achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the SotA.\",\n",
       "  140: 'Identifying causal relations from purely observational data typically requires additional assumptions on relations and/or noise. Most current methods restrict their analysis to datasets that are assumed to have pure linear or nonlinear relations, which is often not reflective of real-world datasets that contain a combination of both. This paper presents CaPS, an ordering-based causal discovery algorithm that effectively handles linear and nonlinear relations. CaPS introduces a novel identification criterion for topological ordering and incorporates the concept of \"parent score\" during the post-processing optimization stage. These scores quantify the strength of the average causal effect, helping to accelerate the pruning process and correct inaccurate predictions in the pruning step. Experimental results demonstrate that our proposed solutions outperform state-of-the-art baselines on synthetic data with varying ratios of linear and nonlinear relations. The results obtained from real-world data also support the competitiveness of CaPS. Code and datasets are available at https://github.com/E2real/CaPS.',\n",
       "  141: 'The field of graph learning has been substantially advanced by the development of deep learning models, in particular graph neural networks. However, one salient yet largely under-explored challenge is detecting Out-of-Distribution (OOD) nodes on graphs. Prevailing OOD detection techniques developed in other domains like computer vision, do not cater to the interconnected nature of graphs. This work aims to fill this gap by exploring the potential of a simple yet effective method -- OOD score propagation, which propagates OOD scores among neighboring nodes along the graph structure. This post hoc solution can be easily integrated with existing OOD scoring functions, showcasing its excellent flexibility and effectiveness in most scenarios. However, the conditions under which score propagation proves beneficial remain not fully elucidated. Our study meticulously derives these conditions and, inspired by this discovery, introduces an innovative edge augmentation strategy with theoretical guarantee. Empirical evaluations affirm the superiority of our proposed method, outperforming strong OOD detection baselines in various scenarios and settings.',\n",
       "  142: 'Many tasks in explainable machine learning, such as data valuation and feature attribution, perform expensive computation for each data point and are intractable for large datasets. These methods require efficient approximations, and although amortizing the process by learning a network to directly predict the desired output is a promising solution, training such models with exact labels is often infeasible. We therefore explore training amortized models with noisy labels, and we find that this is inexpensive and surprisingly effective. Through theoretical analysis of the label noise and experiments with various models and datasets, we show that this approach tolerates high noise levels and significantly accelerates several feature attribution and data valuation methods, often yielding an order of magnitude speedup over existing approaches.',\n",
       "  143: \"Sharpness-Aware Minimization (SAM) has attracted significant attention for its effectiveness in improving generalization across various tasks. However, its underlying principles remain poorly understood. In this work, we analyze SAM’s training dynamics using the maximum eigenvalue of the Hessian as a measure of sharpness and propose a third-order stochastic differential equation (SDE), which reveals that the dynamics are driven by a complex mixture of second- and third-order terms. We show that alignment between the perturbation vector and the top eigenvector is crucial for SAM’s effectiveness in regularizing sharpness, but find that this alignment is often inadequate in practice, which limits SAM's efficiency. Building on these insights, we introduce Eigen-SAM, an algorithm that explicitly aims to regularize the top Hessian eigenvalue by aligning the perturbation vector with the leading eigenvector. We validate the effectiveness of our theory and the practical advantages of our proposed approach through comprehensive experiments. Code is available at https://github.com/RitianLuo/EigenSAM.\",\n",
       "  144: 'Large Reconstruction Models have made significant strides in the realm of automated 3D content generation from single or multiple input images. Despite their success, these models often produce 3D meshes with geometric inaccuracies, stemming from the inherent challenges of deducing 3D shapes solely from image data. In this work, we introduce a novel framework, the Large Image and Point Cloud Alignment Model (LAM3D), which utilizes 3D point cloud data to enhance the fidelity of generated 3D meshes. Our methodology begins with the development of a point-cloud-based network that effectively generates precise and meaningful latent tri-planes, laying the groundwork for accurate 3D mesh reconstruction. Building upon this, our Image-Point-Cloud Feature Alignment technique processes a single input image, aligning to the latent tri-planes to imbue image features with robust 3D information. This process not only enriches the image features but also facilitates the production of high-fidelity 3D meshes without the need for multi-view input, significantly reducing geometric distortions. Our approach achieves state-of-the-art high-fidelity 3D mesh reconstruction from a single image in just 6 seconds, and experiments on various datasets demonstrate its effectiveness.',\n",
       "  145: 'The effect of regularizers such as weight decay when training deep neural networks is not well understood. We study the influence of weight decay as well as $L2$-regularization when training neural network models in which parameter matrices interact multiplicatively. This combination is of particular interest as this parametrization is common in attention layers, the workhorse of transformers. Here, key-query, as well as value-projection parameter matrices, are multiplied directly with each other: $W_K^TW_Q$ and $PW_V$. We extend previous results and show on one hand that any local minimum of a $L2$-regularized loss of the form $L(AB^\\\\top) + \\\\lambda (\\\\|A\\\\|^2 + \\\\|B\\\\|^2)$ coincides with a minimum of the nuclear norm-regularized loss $L(AB^\\\\top) + \\\\lambda\\\\|AB^\\\\top\\\\|_*$, and on the other hand that the 2 losses become identical exponentially quickly during training. We thus complement existing works linking $L2$-regularization with low-rank regularization, and in particular, explain why such regularization on the matrix product affects early stages of training.Based on these theoretical insights, we verify empirically that the key-query and value-projection matrix products $W_K^TW_Q, PW_V$ within attention layers, when optimized with weight decay, as usually done in vision tasks and language modelling, indeed induce a significant reduction in the rank of $W_K^TW_Q$ and $PW_V$, even in fully online training.We find that, in accordance with existing work, inducing low rank in attention matrix products can damage language model performance, and observe advantages when decoupling weight decay in attention layers from the rest of the parameters.',\n",
       "  146: 'Deep learning-based methods significantly advance the exploration of associations among triple-wise biological entities (e.g., drug-target protein-adverse reaction), thereby facilitating drug discovery and safeguarding human health. However, existing researches only focus on entity-centric information mapping and aggregation, neglecting the crucial role of potential association patterns among different entities. To address the above limitation, we propose a novel association pattern-aware fusion method for biological entity relationship prediction, which effectively integrates the related association pattern information into entity representation learning. Additionally, to enhance the missing information of the low-order message passing, we devise a bind-relation module that considers the strong bind of low-order entity associations. Extensive experiments conducted on three biological datasets quantitatively demonstrate that the proposed method achieves about 4%-23% hit@1 improvements compared with state-of-the-art baselines. Furthermore, the interpretability of association patterns is elucidated in detail, thus revealing the intrinsic biological mechanisms and promoting it to be deployed in real-world scenarios. Our data and code are available at https://github.com/hry98kki/PatternBERP.',\n",
       "  147: 'In recent years, the remarkable progress of large language models (LLMs) has sparked interest in task automation, which involves decomposing complex tasks described by user instructions into sub-tasks and invoking external tools to execute them, playing a central role in autonomous agents. However, there is a lack of systematic and standardized benchmarks to promote the development of LLMs in task automation. To address this, we introduce TaskBench, a comprehensive framework to evaluate the capability of LLMs in task automation. Specifically, task automation can be divided into three critical stages: task decomposition, tool selection, and parameter prediction. To tackle the complexities inherent in these stages, we introduce the concept of Tool Graph to represent decomposed tasks and adopt a back-instruct method to generate high-quality user instructions. We propose TaskEval, a multi-faceted evaluation methodology that assesses LLM performance across these three stages. Our approach combines automated construction with rigorous human verification, ensuring high consistency with human evaluation. Experimental results demonstrate that TaskBench effectively reflects the capabilities of various LLMs in task automation. It provides insights into model performance across different task complexities and domains, pushing the boundaries of what current models can achieve. TaskBench offers a scalable, adaptable, and reliable benchmark for advancing LLM-based autonomous agents.',\n",
       "  148: 'The efficacy of large language models (LLMs) on downstream tasks usually hinges on instruction tuning, which relies critically on the quality of training data. Unfortunately, collecting high-quality and diverse data is both expensive and time-consuming. To mitigate this issue, we propose  a novel Star-Agents framework, which automates the enhancement of data quality across datasets through multi-agent collaboration and assessment. The framework adopts a three-pronged strategy. It  initially generates diverse instruction data with multiple LLM agents through a bespoke sampling method. Subsequently, the generated data undergo a rigorous evaluation using a dual-model method that assesses both difficulty and quality. Finaly, the above process evolves in a dynamic refinement phase, where more effective LLMs are prioritized, enhancing the overall data quality. Our empirical studies, including instruction tuning experiments with models such as Pythia and LLaMA, demonstrate the effectiveness of the proposed framework. Optimized datasets have achieved substantial improvements, with an average increase of 12\\\\% and notable gains in specific metrics, such as a 40\\\\% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset. Codes will be released soon.',\n",
       "  149: 'Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we propose an innovative solution by constructing infinite tree-structured PGMs that correspond exactly to neural networks. Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure. Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs. Potential benefits include improved pedagogy and interpretation of DNNs, and algorithms that can merge the strengths of PGMs and DNNs.',\n",
       "  150: 'This work addresses the fundamental linear inverse problem in compressive sensing (CS) by introducing a new type of regularizing generative prior. Our proposed method utilizes ideas from classical dictionary-based CS and, in particular, sparse Bayesian learning (SBL), to integrate a strong regularization towards sparse solutions. At the same time, by leveraging the notion of conditional Gaussianity, it also incorporates the adaptability from generative models to training data. However, unlike most state-of-the-art generative models, it is able to learn from a few compressed and noisy data samples and requires no optimization algorithm for solving the inverse problem. Additionally, similar to Dirichlet prior networks, our model parameterizes a conjugate prior enabling its application for uncertainty quantification. We support our approach theoretically through the concept of variational inference and validate it empirically using different types of compressible signals.',\n",
       "  151: \"This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the extensively explored unsupervised setting with a fully unlabeled graph. We reveal that having access to the normal nodes, even just a small percentage of normal nodes, helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (namely GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate pseudo anomaly nodes, referred to as 'outlier nodes', for providing effective negative node samples in training a discriminative one-class classifier. The main challenge here lies in the lack of ground truth information about real anomaly nodes. To address this challenge, GGAD is designed to leverage two important priors about the anomaly nodes -- asymmetric local affinity and egocentric closeness -- to generate reliable outlier nodes that assimilate anomaly nodes in both graph structure and feature representations. Comprehensive experiments on six real-world GAD datasets are performed to establish a benchmark for semi-supervised GAD and show that GGAD substantially outperforms state-of-the-art unsupervised and semi-supervised GAD methods with varying numbers of training normal nodes.\",\n",
       "  152: 'State-of-the-art results in large language models (LLMs) often rely on scale, whichbecomes computationally expensive. This has sparked a research agenda to reducethese models’ parameter counts and computational costs without significantlyimpacting their performance. Our study focuses on transformer-based LLMs,specifically targeting the computationally intensive feedforward networks (FFNs),which are less studied than attention blocks. We consider three structured linearparameterizations of the FFN using efficient low-rank and block-diagonal matrices.In contrast to many previous works that examined these approximations, our studyi) explores these structures from a training-from-scratch perspective, ii) scales upto 1.3B parameters, and iii) is conducted within recent Transformer-based LLMsrather than convolutional architectures. We demonstrate that these structures canlead to actual computational gains in various scenarios, including online decodingwhen using a pre-merge technique. Additionally, we propose a novel trainingregime, called self-guided training, aimed at improving the poor training dynamicsthat these approximations exhibit when used from initialization. Interestingly,the scaling performance of structured matrices is explored, revealing steepercurves in scaling training FLOPs, along with a favorable scaling trend in theovertraining regime. Specifically, we show that wide and structured networkscan utilize training FLOPs more efficiently, with fewer parameters and lowerloss than dense models at their optimal trade-off. Our code is available athttps://github.com/CLAIRE-Labo/StructuredFFN/tree/main.',\n",
       "  153: \"We study a typical optimization model where the optimization variable is composed of multiple probability distributions. Though the model appears frequently in practice, such as for policy problems, it lacks specific analysis in the general setting. For this optimization problem,  we propose a new structural condition/landscape description named  generalized quasar-convexity (GQC) beyond the realms of convexity. In contrast to original quasar-convexity \\\\citep{hinder2020near}, GQC allows an individual quasar-convex parameter $\\\\gamma_i$ for each variable block $i$ and the smaller of $\\\\gamma_i$ implies less block-convexity. To minimize the objective function, we consider a generalized oracle termed as the internal function that includes the standard gradient oracle as a special case. We provide optimistic mirror descent (OMD) for multiple distributions and prove that the algorithm can achieve an adaptive $\\\\tilde{\\\\mathcal{O}}((\\\\sum_{i=1}^d1/\\\\gamma_i)\\\\epsilon^{-1})$ iteration complexity to find an $\\\\varepsilon$-suboptimal global solution without pre-known the exact values of $\\\\gamma_i$ when the objective admits ``polynomial-like'' structural. Notably, it achieves iteration complexity that does not explicitly depend on the number of distributions and strictly faster $(\\\\sum_{i=1}^d 1/\\\\gamma_i \\\\text{ v.s. } d\\\\max_{i\\\\in[1:d]} 1/\\\\gamma_i)$ than mirror decent methods. We also extend GQC to the minimax optimization problem proposing the generalized quasar-convexity-concavity (GQCC) condition and a decentralized variant of OMD with regularization. Finally, we show the applications of our algorithmic framework on discounted Markov Decision Processes problem and Markov games, which bring new insights on the landscape analysis of reinforcement learning.\",\n",
       "  154: 'We present a novel set of rigorous and computationally efficient topology-based complexity notions that exhibit a strong correlation with the generalization gap in modern deep neural networks (DNNs). DNNs show remarkable generalization properties, yet the source of these capabilities remains elusive, defying the established statistical learning theory. Recent studies have revealed that properties of training trajectories can be indicative of generalization. Building on this insight, state-of-the-art methods have leveraged the topology of these trajectories, particularly their fractal dimension, to quantify generalization. Most existing works compute this quantity by assuming continuous- or infinite-time training dynamics, complicating the development of practical estimators capable of accurately predicting generalization without access to test data. In this paper, we respect the discrete-time nature of training trajectories and investigate the underlying topological quantities that can be amenable to topological data analysis tools. This leads to a new family of reliable topological complexity measures that provably bound the generalization error, eliminating the need for restrictive geometric assumptions. These measures are computationally friendly, enabling us to propose simple yet effective algorithms for computing generalization indices. Moreover, our flexible framework can be extended to different domains, tasks, and architectures. Our experimental results demonstrate that our new complexity measures exhibit a strong correlation with generalization error in industry-standard architectures such as transformers and deep graph networks. Our approach consistently outperforms existing topological bounds across a wide range of datasets, models, and optimizers, highlighting the practical relevance and effectiveness of our complexity measures.',\n",
       "  155: \"Large language models are usually fine-tuned to align with human preferences. However, fine-tuning a large language model can be challenging. In this work, we introduce $\\\\textit{weak-to-strong search}$, framing the alignment of a large language model as a test-time greedy search to maximize the log-probability difference between small tuned and untuned models while sampling from the frozen large model. This method serves both as (1) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (2) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance.Empirically, we demonstrate the flexibility of weak-to-strong search across different tasks. In controlled-sentiment generation and summarization, we use tuned and untuned $\\\\texttt{gpt2}$s to improve the alignment of large models without additional training. Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0, we show that reusing off-the-shelf small models (e.g., $\\\\texttt{zephyr-7b-beta}$ and its untuned version) can improve the length-controlled win rates of both white-box and black-box large models against $\\\\texttt{gpt-4-turbo}$ (e.g., $34.4\\\\% \\\\rightarrow 37.9\\\\%$ for $\\\\texttt{Llama-3-70B-Instruct}$ and $16.0\\\\% \\\\rightarrow 20.1\\\\%$ for $\\\\texttt{gpt-3.5-turbo-instruct}$), despite the small models' low win rates $\\\\approx 10.0\\\\%$.\",\n",
       "  156: 'Subset selection tasks, such as anomaly detection and compound selection in AI-assisted drug discovery, are crucial for a wide range of applications. Learning subset-valued functions with neural networks has achieved great success by incorporating permutation invariance symmetry into the architecture. However, existing neural set architectures often struggle to either capture comprehensive information from the superset or address complex interactions within the input. Additionally, they often fail to perform in scenarios where superset sizes surpass available memory capacity. To address these challenges, we introduce the novel concept of the Identity Property, which requires models to integrate information from the originating set, resulting in the development of neural networks that excel at performing effective subset selection from large supersets. Moreover, we present the Hierarchical Representation of Neural Subset Selection (HORSE), an attention-based method that learns complex interactions and retains information from both the input set and the optimal subset supervision signal. Specifically, HORSE enables the partitioning of the input ground set into manageable chunks that can be processed independently and then aggregated, ensuring consistent outcomes across different partitions. Through extensive experimentation, we demonstrate that HORSE significantly enhances neural subset selection performance by capturing more complex information and surpasses state-of-the-art methods in handling large-scale inputs by a margin of up to 20%.',\n",
       "  157: 'Existing approaches for learning representations of time-series keep the temporal arrangement of the time-steps intact with the presumption that the original order is the most optimal for learning. However, non-adjacent sections of real-world time-series may have strong dependencies. Accordingly, we raise the question: Is there an alternative arrangement for time-series which could enable more effective representation learning? To address this, we propose a simple plug-and-play neural network layer called Segment, Shuffle, and Stitch (S3) designed to improve representation learning in time-series models. S3 works by creating non-overlapping segments from the original sequence and shuffling them in a learned manner that is optimal for the task at hand. It then re-attaches the shuffled segments back together and performs a learned weighted sum with the original input to capture both the newly shuffled sequence along with the original sequence. S3 is modular and can be stacked to achieve different levels of granularity, and can be added to many forms of neural architectures including CNNs or Transformers with negligible computation overhead. Through extensive experiments on several datasets and state-of-the-art baselines, we show that incorporating S3 results in significant improvements for the tasks of time-series classification, forecasting, and anomaly detection, improving performance on certain datasets by up to 68\\\\%. We also show that S3 makes the learning more stable with a smoother training loss curve and loss landscape compared to the original baseline. The code is available at https://github.com/shivam-grover/S3-TimeSeries.',\n",
       "  158: 'Radar signal interpretation plays a crucial role in remote detection and ranging. With the gradual display of the advantages of neural network technology in signal processing, learning-based radar signal interpretation is becoming a research hot-spot and made great progress. And since radar semantic segmentation (RSS) can provide more fine-grained target information, it has become a more concerned direction in this field. However, the temporal information, which is an important clue for analyzing radar data, has not been exploited sufficiently in present RSS frameworks. In this work, we propose a novel temporal information learning paradigm, i.e., data-driven temporal information aggregation with learned target-history relations. Following this idea, a flexible learning module, called Temporal Relation-Aware Module (TRAM) is carefully designed. TRAM contains two main blocks: i) an encoder for capturing the target-history temporal relations (TH-TRE) and ii) a learnable temporal relation attentive pooling (TRAP) for aggregating temporal information. Based on TRAM, an end-to-end Temporal-Aware RSS Network (TARSS-Net) is presented, which has outstanding performance on publicly available and our collected real-measured datasets. Code and supplementary materials are available at https://github.com/zlw9161/TARSS-Net.',\n",
       "  159: 'This paper proposes ProEdit - a simple yet effective framework for high-quality 3D scene editing guided by diffusion distillation in a novel progressive manner. Inspired by the crucial observation that multi-view inconsistency in scene editing is rooted in the diffusion model’s large feasible output space (FOS), our framework controls the size of FOS and reduces inconsistency by decomposing the overall editing task into several subtasks, which are then executed progressively on the scene. Within this framework, we design a difficulty-aware subtask decomposition scheduler and an adaptive 3D Gaussian splatting (3DGS) training strategy, ensuring high efficiency in performing each subtask. Extensive evaluation shows that our ProEdit achieves state-of-the-art results in various scenes and challenging editing tasks, all through a simple framework without any expensive or sophisticated add-ons like distillation losses, components, or training procedures. Notably, ProEdit also provides a new way to preview, control, and select the aggressivity of editing operation during the editing process.',\n",
       "  160: 'We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition. Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals. Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific. Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code.',\n",
       "  161: 'It is widely acknowledged that large language models (LLMs) encode a vast reservoir of knowledge after being trained on mass data. Recent studies disclose knowledge conflicts in LLM generation, wherein outdated or incorrect parametric knowledge (i.e., encoded knowledge) contradicts new knowledge provided in the context. To mitigate such knowledge conflicts, we propose a novel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to capitalize on neurons that are crucial in processing contextual cues. Specifically, IRCAN first identifies neurons that significantly contribute to context processing, utilizing a context-aware attribution score derived from integrated gradients. Subsequently, the identified context-aware neurons are strengthened via reweighting. In doing so, we steer LLMs to generate context-sensitive outputs with respect to the new knowledge provided in the context. Extensive experiments conducted across a variety of models and tasks demonstrate that IRCAN not only achieves remarkable improvements in handling knowledge conflicts but also offers a scalable, plug-and-play solution that can be integrated seamlessly with existing models. Our codes are released at https://github.com/danshi777/IRCAN.',\n",
       "  162: \"Recent advances in AI models have increased the integration of AI-based decision aids into the human decision making process. To fully unlock the potential of AI-assisted decision making, researchers have computationally modeled how humans incorporate AI recommendations into their final decisions, and utilized these models to improve human-AI team performance. Meanwhile, due to the ``black-box'' nature of AI models, providing AI explanations to human decision makers to help them rely on AI recommendations more appropriately has become a common practice.  In this paper, we explore whether we can quantitatively model how humans integrate both AI recommendations and explanations into their decision process, and whether this quantitative understanding of human behavior from the learned model can be utilized to manipulate AI explanations, thereby nudging individuals towards making targeted decisions. Our extensive human experiments across various tasks demonstrate that  human behavior can be easily influenced by these manipulated explanations towards targeted outcomes, regardless of the intent being adversarial or benign. Furthermore, individuals often fail to detect any anomalies in these  explanations, despite their decisions being affected by them.\",\n",
       "  163: 'Hair editing is a critical image synthesis task that aims to edit hair color and hairstyle using text descriptions or reference images, while preserving irrelevant attributes (e.g., identity, background, cloth). Many existing methods are based on StyleGAN to address this task. However, due to the limited spatial distribution of StyleGAN, it struggles with multiple hair color editing and facial preservation. Considering the advancements in diffusion models, we utilize Latent Diffusion Models (LDMs) for hairstyle editing. Our approach introduces Multi-stage Hairstyle Blend (MHB), effectively separating control of hair color and hairstyle in diffusion latent space. Additionally, we train a warping module to align the hair color with the target region. To further enhance multi-color hairstyle editing, we fine-tuned a CLIP model using a multi-color hairstyle dataset. Our method not only tackles the complexity of multi-color hairstyles but also addresses the challenge of preserving original colors during diffusion editing. Extensive experiments showcase the superiority of our method in editing multi-color hairstyles while preserving facial attributes given textual descriptions and reference images.',\n",
       "  164: 'There has been significant interest in \"extreme\" compression of large language models (LLMs), i.e. to 1-2 bits per parameter, which allows such models to be executed efficiently on resource-constrained devices.  Existing work focused on improved one-shot quantization techniques and weight representations; yet, purely post-training  approaches are reaching diminishing returns in terms of the accuracy-vs-bit-width trade-off. State-of-the-art quantization methods such as QuIP# and AQLM include fine-tuning (part of) the compressed parameters over a limited amount of calibration data; however, such fine-tuning techniques over compressed weights often make exclusive use of straight-through estimators (STE), whose performance is not well-understood in this setting. In this work, we question the use of STE for extreme LLM compression, showing that it can be sub-optimal, and perform a systematic study of quantization-aware fine-tuning strategies for LLMs.We propose PV-Tuning - a representation-agnostic framework that generalizes and improves upon existing fine-tuning strategies, and provides convergence guarantees in restricted cases.On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms prior techniques for highly-performant models such as Llama and Mistral. Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama-2 family models at  2 bits per parameter.',\n",
       "  165: \"How do brain circuits learn to generate behaviour?  While significant strides have been made in understanding learning in artificial neural networks, applying this knowledge to biological networks remains challenging.  For instance, while backpropagation is known to perform accurate credit assignment of error in artificial neural networks, how a similarly powerful process can be realized within the constraints of biological circuits remains largely unclear.  One of the major challenges is that the brain's extensive recurrent connectivity requires the propagation of error through both space and time, a problem that is notoriously difficult to solve in vanilla recurrent neural networks.  Moreover, the extensive feedback connections in the brain are known to influence forward network activity, but the interaction between feedback-driven activity changes and local, synaptic plasticity-based learning is not fully understood.  Building on our previous work modelling motor learning, this work investigates the mechanistic properties of pre-trained networks with feedback control on a standard motor task.  We show that feedback control of the ongoing recurrent network dynamics approximates the optimal first-order gradient with respect to the network activities, allowing for rapid, ongoing movement correction.  Moreover, we show that trial-by-trial adaptation to a persistent perturbation using a local, biologically plausible learning rule that integrates recent activity and error feedback is both more accurate and more efficient with feedback control during learning, due to the decoupling of the recurrent network dynamics and the injection of an adaptive, second-order gradient into the network dynamics.  Thus, our results suggest that feedback control may guide credit assignment in biological recurrent neural networks, enabling both rapid and efficient learning in the brain.\",\n",
       "  166: 'Large Language Models (LLMs) have shown state-of-the-art performance in a variety of tasks, including arithmetic and reasoning; however, to gauge the intellectual capabilities of LLMs, causal reasoning has become a reliable proxy for validating a general understanding of the mechanics and intricacies of the world similar to humans. Previous works in natural language processing (NLP) have either focused on open-ended causal reasoning via causal commonsense reasoning (CCR) or framed a symbolic representation-based question answering for theoretically backed-up analysis via a causal inference engine. The former adds an advantage of real-world grounding but lacks theoretically backed-up analysis/validation, whereas the latter is far from real-world grounding. In this work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed Daily activities) framework, which is built upon human understanding of daily real-world activities to reason about the causal nature of events. We show that the proposed framework facilitates the creation of enormous causal queries (∼ 9 million) and comes close to the mini-turing test, simulating causal reasoning to evaluate the understanding of a daily real-world task. We evaluate multiple LLMs on the created causal queries and find that causal reasoning is challenging even for activities trivial to humans. We further explore (the causal reasoning abilities of LLMs) using the backdoor criterion to determine the causal strength between events.',\n",
       "  167: 'Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and scales (from 2B to 70B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. The results demonstrate that SLED consistently improves factual accuracy by up to 20\\\\% compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.',\n",
       "  168: 'Despite the general capabilities of Large Language Models (LLMs) like GPT-4, these models still request fine-tuning or adaptation with customized data when meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack) under the setting of Language-Model-as-a-Service (LMaaS), where the model\\'s safety has been significantly compromised by fine-tuning on users\\' uploaded examples that contain just a few harmful examples. Though potential defenses have been proposed that the service providers of LMaaS can integrate safety examples into the fine-tuning dataset to reduce safety issues, such approaches require incorporating a substantial amount of data, making it inefficient. To effectively defend against the FJAttack with limited safety examples under LMaaS, we propose the Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In particular, service providers will construct prefixed safety examples with a secret prompt, acting as a \"backdoor trigger\". By integrating prefixed safety examples into the fine-tuning dataset, the subsequent fine-tuning process effectively acts as the \"backdoor attack\", establishing a strong correlation between the secret prompt and safety generations. Consequently, safe responses are ensured once service providers prepend this secret prompt ahead of any user input during inference. Our comprehensive experiments demonstrate that through the Backdoor Enhanced Safety Alignment with adding as few as 11 prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar safety performance as the original aligned models without harming the benign performance. Furthermore, we also present the effectiveness of our method in a more practical setting where the fine-tuning data consists of both FJAttack examples and the fine-tuning task data.',\n",
       "  169: \"We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments)  designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/AGILE.\",\n",
       "  170: 'In this paper, we propose a new framework for zero-shot object navigation.Existing zero-shot object navigation methods prompt LLM with the text of spatially closed objects, which lacks enough scene context for in-depth reasoning.To better preserve the information of environment and fully exploit the reasoning ability of LLM, we propose to represent the observed scene with 3D scene graph. The scene graph encodes the relationships between objects, groups and rooms with a LLM-friendly structure, for which we design a hierarchical chain-of-thought prompt to help LLM reason the goal location according to scene context by traversing the nodes and edges.Moreover, benefit from the scene graph representation, we further design a re-perception mechanism to empower the object navigation framework with the ability to correct perception error.We conduct extensive experiments on MP3D, HM3D and RoboTHOR environments, where SG-Nav surpasses previous state-of-the-art zero-shot methods by more than \\\\textbf{10\\\\%} SR on all benchmarks, while the decision process is explainable. To the best of our knowledge, SG-Nav is the first zero-shot method that achieves even higher performance than supervised object navigation methods on the challenging MP3D benchmark.Code of this project will be released in the final version.',\n",
       "  171: 'Current PAC-Bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis-classifications. We provide the first PAC-Bayes bound capable of providing such rich information by bounding the Kullback-Leibler divergence between the empirical and true probabilities of a set of $M$ error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. We transform our bound into a differentiable training objective. Our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing PAC-Bayes bounds can only bound a particular pre-decided weighting of the error types. In contrast our bound implicitly controls all uncountably many weightings simultaneously.',\n",
       "  172: \"Task planning in language agents is emerging as an important research topic alongside the development of large language models (LLMs). It aims to break down complex user requests in natural language into solvable sub-tasks, thereby fulfilling the original requests. In this context, the sub-tasks can be naturally viewed as a graph, where the nodes represent the sub-tasks, and the edges denote the dependencies among them. Consequently, task planning is a decision-making problem that involves selecting a connected path or subgraph within the corresponding graph and invoking it. In this paper, we explore graph learning-based methods for task planning, a direction that is orthogonal to the prevalent focus on prompt design. Our interest in graph learning stems from a theoretical discovery: the biases of attention and auto-regressive loss impede LLMs' ability to effectively navigate decision-making on graphs, which is adeptly addressed by graph neural networks (GNNs). This theoretical insight led us to integrate GNNs with LLMs to enhance overall performance. Extensive experiments demonstrate that GNN-based methods surpass existing solutions even without training, and minimal training can further enhance their performance. The performance gain increases with a larger task graph size.\",\n",
       "  173: \"3D Gaussian Splatting has shown fast and high-quality rendering results in static scenes by leveraging dense 3D prior and explicit representations. Unfortunately, the benefits of the prior and representation do not involve novel view synthesis for dynamic motions. Ironically, this is because the main barrier is the reliance on them, which requires increasing training and rendering times to account for dynamic motions. In this paper, we design Explicit 4D Gaussian Splatting (Ex4DGS).Our key idea is to firstly separate static and dynamic Gaussians during training, and to explicitly sample positions and rotations of the dynamic Gaussians at sparse timestamps. The sampled positions and rotations are then interpolated to represent both spatially and temporally continuous motions of objects in dynamic scenes as well as reducing computational cost. Additionally, we introduce a progressive training scheme and a point-backtracking technique that improves Ex4DGS's convergence. We initially train Ex4DGS using short timestamps and progressively extend timestamps, which makes it work well with a few point clouds. The point-backtracking is used to quantify the cumulative error of each Gaussian over time, enabling the detection and removal of erroneous Gaussians in dynamic scenes. Comprehensive experiments on various scenes demonstrate the state-of-the-art rendering quality from our method, achieving fast rendering of 62 fps on a single 2080Ti GPU.\",\n",
       "  174: 'We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types. We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions.',\n",
       "  175: 'In diffusion models, samples are generated through an iterative refinement process, requiring hundreds of sequential model evaluations. Several recent methods have introduced approximations (fewer discretization steps or distillation) to trade off speed at the cost of sample quality. In contrast, we introduce Self-Refining Diffusion Samplers (SRDS) that retain sample quality and can improve latency at the cost of additional parallel compute. We take inspiration from the Parareal algorithm, a popular numerical method for parallel-in-time integration of differential equations. In SRDS, a quick but rough estimate of a sample is first created and then iteratively refined in parallel through Parareal iterations. SRDS is not only guaranteed to accurately solve the ODE and converge to the serial solution but also benefits from parallelization across the diffusion trajectory, enabling batched inference and pipelining. As we demonstrate for pre-trained diffusion models, the early convergence of this refinement procedure drastically reduces the number of steps required to produce a sample, speeding up generation for instance by up to 1.7x on a 25-step StableDiffusion-v2 benchmark and up to 4.3x on longer trajectories.',\n",
       "  176: 'While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals traditionally used optimizers. Furthermore, OPEN shows strong generalization characteristics across a range of environments and agent architectures.',\n",
       "  177: 'Diffusion models have emerged as a powerful tool for generating high-quality images from textual descriptions. Despite their successes, these models often exhibit limited diversity in the sampled images, particularly when sampling with a high classifier-free guidance weight. To address this issue, we present Kaleido, a novel approach that enhances the diversity of samples by incorporating autoregressive latent priors. Kaleido integrates an autoregressive language model that encodes the original caption and generates latent variables, serving as abstract and intermediary representations for guiding and facilitating the image generation process.In this paper, we explore a variety of discrete latent representations, including textual descriptions, detection bounding boxes, object blobs, and visual tokens. These representations diversify and enrich the input conditions to the diffusion models, enabling more diverse outputs.Our experimental results demonstrate that Kaleido effectively broadens the diversity of the generated image samples from a given textual description while maintaining high image quality. Furthermore, we show that Kaleido adheres closely to the guidance provided by the generated latent variables, demonstrating its capability to effectively control and direct the image generation process.',\n",
       "  178: 'Recent works have shown that extending the message passing paradigm to subgraphs communicating with other subgraphs, especially via higher order messages, can boost the expressivity of graph neural networks. In such architectures, to faithfully account for local structure such as cycles, the local operations must be equivariant to the automorphism group of the local environment. However, enumerating the automorphism groups of all subgraphs of interest and finding appropriate equivariant operations for each one of them separately is generally not feasible. In this paper we propose a solution to this problem based on spectral graph theory that bypasses having to determine the automorphism group entirely and constructs a basis for equivariant operations directly from the graph Laplacian. We show that this approach can boost the performance of GNNs on some standard benchmarks.',\n",
       "  179: 'With the scale of vision Transformer-based models continuing to grow, finetuning these large-scale pretrained models for new tasks has become increasingly parameter-intensive. Visual prompt tuning is introduced as a parameter-efficient finetuning (PEFT) method to this trend. Despite its successes, a notable research challenge persists within almost all PEFT approaches: significant performance degradation is observed when there is a substantial disparity between the datasets applied in pretraining and finetuning phases. To address this challenge, we draw inspiration from human visual cognition, and propose the Visual Fourier Prompt Tuning (VFPT) method as a general and effective solution for adapting large-scale transformer-based models. Our approach innovatively incorporates the Fast Fourier Transform into prompt embeddings and harmoniously considers both spatial and frequency domain information. Apart from its inherent simplicity and intuitiveness, VFPT exhibits superior performance across all datasets, offering a general solution to dataset challenges, irrespective of data disparities. Empirical results demonstrate that our approach outperforms current state-of-the-art baselines on two benchmarks, with low parameter usage (e.g., 0.57% of model parameters on VTAB-1k) and notable performance enhancements (e.g., 73.20% of mean accuracy on VTAB-1k). Our code is avaliable at https://github.com/runtsang/VFPT.',\n",
       "  180: 'When training node embedding models to represent large directed graphs (digraphs), it is impossible to observe all entries of the adjacency matrix during training. As a consequence most methods employ sampling. For very large digraphs, however, this means many (most) entries may be unobserved during training. In general, observing every entry would be necessary to uniquely identify a graph, however if we know the graph has a certain property some entries can be omitted - for example, only half the entries would be required for a symmetric graph. In this work, we develop a novel framework to identify a subset of entries required to uniquely distinguish a graph among all transitively-closed DAGs. We give an explicit algorithm to compute the provably minimal set of entries, and demonstrate empirically that one can train node embedding models with greater efficiency and performance, provided the energy function has an appropriate inductive bias. We achieve robust performance on synthetic hierarchies and a larger real-world taxonomy, observing improved convergence rates in a resource-constrained setting while reducing the set of training examples by as much as 99%.',\n",
       "  181: 'Laplace learning algorithms for graph-based semi-supervised learning have been shown to produce degenerate predictions at low label rates and in imbalanced class regimes, particularly near class boundaries. We propose CutSSL: a framework for graph-based semi-supervised learning based on continuous nonconvex quadratic programming, which provably obtains \\\\emph{integer} solutions. Our framework is naturally motivated by an \\\\emph{exact} quadratic relaxation of a cardinality-constrained minimum-cut graph partitioning problem. Furthermore, we show our formulation is related to an optimization problem whose approximate solution is the mean-shifted Laplace learning heuristic, thus providing new insight into the performance of this heuristic. We demonstrate that CutSSL significantly surpasses the current state-of-the-art on k-nearest neighbor graphs and large real-world graph benchmarks across a variety of label rates, class imbalance, and label imbalance regimes. Our implementation is available on Colab\\\\footnote{\\\\url{https://colab.research.google.com/drive/1tGU5rxE1N5d0KGcNzlvZ0BgRc7_vob7b?usp=sharing}}.',\n",
       "  182: 'Nuanced expressiveness, especially through detailed hand and facial expressions, is pivotal for enhancing the realism and vitality of digital human representations.In this work, we aim to learn expressive human avatars from a monocular RGB video; a setting that introduces new challenges in capturing and animating fine-grained details.To this end, we introduce EVA, a drivable human model that can recover fine details based on 3D Gaussians and an expressive parametric human model, SMPL-X.Focused on enhancing expressiveness, our work makes three key contributions.First, we highlight the importance of aligning the SMPL-X model with the video frames for effective avatar learning.Recognizing the limitations of current methods for estimating SMPL-X parameters from in-the-wild videos, we introduce a reconstruction module that significantly improves the image-model alignment.Second, we propose a context-aware adaptive density control strategy, which is adaptively adjusting the gradient thresholds to accommodate the varied granularity across body parts.Third, we develop a feedback mechanism that predicts per-pixel confidence to better guide the optimization of 3D Gaussians.Extensive experiments on two benchmarks demonstrate the superiority of our approach both quantitatively and qualitatively, especially on the fine-grained hand and facial details. We make our code available at the project website: https://evahuman.github.io.',\n",
       "  183: 'The manifold hypothesis presumes that high-dimensional data lies on or near a low-dimensional manifold. While the utility of encoding geometric structure has been demonstrated empirically, rigorous analysis of its impact on the learnability of neural networks is largely missing. Several recent results have established hardness results for learning feedforward and equivariant neural networks under i.i.d. Gaussian or uniform Boolean data distributions. In this paper, we investigate the hardness of learning under the manifold hypothesis. We ask, which minimal assumptions on the curvature and regularity of the manifold, if any, render the learning problem efficiently learnable. We prove that learning is hard under input manifolds of bounded curvature by extending proofs of hardness in the SQ and cryptographic settings for boolean data inputs to the geometric setting. On the other hand, we show that additional assumptions on the volume of the data manifold alleviate these fundamental limitations and guarantee learnability via a simple interpolation argument. Notable instances of this regime are manifolds which can be reliably reconstructed via manifold learning. Looking forward, we comment on and empirically explore intermediate regimes of manifolds, which have heterogeneous features commonly found in real world data.',\n",
       "  184: 'Recent advancements in image understanding have benefited from the extensive use of web image-text pairs. However, video understanding remains a challenge despite the availability of substantial web video-text data. This difficulty primarily arises from the inherent complexity of videos and the inefficient language supervision in recent web-collected video-text datasets. In this paper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend large language models (LLMs) for video understanding, without the need for pre-training on real video data. Specifically, we first employ an advanced LLM to automatically generate Textual Videos comprising continuous textual frames, along with corresponding annotations to simulate real video-text data. Then, these annotated textual videos are used to pre-align a language-only LLM with the video modality. To bridge the gap between textual and real videos, we employ the CLIP model as the feature extractor to align image and text modalities. During text-only pre-alignment, the continuous textual frames, encoded as a sequence of CLIP text features, are analogous to continuous CLIP image features, thus aligning the LLM with real video representation. Extensive experiments, including zero-shot evaluation and finetuning on various video understanding tasks, demonstrate that TOPA is an effective and efficient framework for aligning video content with LLMs. In particular, without training on any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0% on the challenging long-form video understanding benchmark, Egoschema. This performance surpasses previous video-text pre-training approaches and proves competitive with recent GPT-3.5 based video agents.',\n",
       "  185: 'In multi-label learning, each training instance is associated with multiple labels simultaneously. Traditional multi-label learning studies primarily focus on closed set scenario, i.e. the class label set of test data is identical to those used in training phase. Nevertheless, in numerous real-world scenarios, the environment is open and dynamic where unknown labels may emerge gradually during testing. In this paper, the problem of multi-label open set recognition (MLOSR) is investigated, which poses significant challenges in classifying and recognizing instances with unknown labels in multi-label setting. To enable open set multi-label prediction, a novel approach named SLAN is proposed by leveraging sub-labeling information enriched by structural information in the feature space. Accordingly, unknown labels are recognized by differentiating the sub-labeling information from holistic supervision. Experimental results on various datasets validate the effectiveness of the proposed approach in dealing with the MLOSR problem.',\n",
       "  186: \"A digital twin is a virtual replica of a real-world physical phenomena that uses mathematical modeling to characterize and simulate its defining features. By constructing digital twins for disease processes, we can perform in-silico simulations that mimic patients' health conditions and counterfactual outcomes under hypothetical interventions in a virtual setting. This eliminates the need for invasive procedures or uncertain treatment decisions. In this paper, we propose a method to identify digital twin model parameters using only noninvasive patient health data. We approach the digital twin modeling as a composite inverse problem, and observe that its structure resembles pretraining and finetuning in self-supervised learning (SSL). Leveraging this, we introduce a physics-informed SSL algorithm that initially pretrains a neural network on the pretext task of learning a differentiable simulator of a physiological process. Subsequently, the model is trained to reconstruct physiological measurements from noninvasive modalities while being constrained by the physical equations learned in pretraining. We apply our method to identify digital twins of cardiac hemodynamics using noninvasive echocardiogram videos, and demonstrate its utility in unsupervised disease detection and in-silico clinical trials.\",\n",
       "  187: 'Visual place recognition (VPR) is an essential task for multiple applications such as augmented reality and robot localization. Over the past decade, mainstream methods in the VPR area have been to use feature representation based on global aggregation, as exemplified by NetVLAD. These features are suitable for large-scale VPR and robust against viewpoint changes. However, the VLAD-based aggregation methods usually learn a large number of (e.g., 64) clusters and their corresponding cluster centers, which directly leads to a high dimension of the yielded global features. More importantly, when there is a domain gap between the data in training and inference, the cluster centers determined on the training set are usually improper for inference, resulting in a performance drop. To this end, we first attempt to improve NetVLAD by removing the cluster center and setting only a small number of (e.g., only 4) clusters. The proposed method not only simplifies NetVLAD but also enhances the generalizability across different domains. We name this method SuperVLAD. In addition, by introducing ghost clusters that will not be retained in the final output, we further propose a very low-dimensional 1-Cluster VLAD descriptor, which has the same dimension as the output of GeM pooling but performs notably better. Experimental results suggest that, when paired with a transformer-based backbone, our SuperVLAD shows better domain generalization performance than NetVLAD with significantly fewer parameters. The proposed method also surpasses state-of-the-art methods with lower feature dimensions on several benchmark datasets. The code is available at https://github.com/lu-feng/SuperVLAD.',\n",
       "  188: \"Transformers and linear state space models can be evaluated in parallel on modern hardware, but evaluating nonlinear RNNs appears to be an inherently sequential problem. Recently, however, Lim et al. '24 developed an approach called DEER, which evaluates nonlinear RNNs in parallel by posing the states as the solution to a fixed-point problem. They derived a parallel form of Newton's method to solve the fixed-point problem and achieved significant speedups over sequential evaluation. However, the computational complexity of DEER is cubic in the state size, and the algorithm can suffer from numerical instability. We address these limitations with two novel contributions. To reduce the computational complexity, we apply quasi-Newton approximations and show they converge comparably to Newton, use less memory, and are faster. To stabilize DEER, we leverage a connection between the Levenberg-Marquardt algorithm and Kalman smoothing, which we call ELK. This connection allows us to stabilize Newton's method while using efficient parallelized Kalman smoothing algorithms to retain performance. Through several experiments, we show that these innovations allow for parallel evaluation of nonlinear RNNs at larger scales and with greater stability.\",\n",
       "  189: '3D Gaussian splatting has recently emerged as a promising technique for novel view synthesis from sparse image sets, yet comes at the cost of requiring millions of 3D Gaussian primitives to reconstruct each 3D scene. This largely limits its application to resource-constrained devices and applications.Despite advances in Gaussian pruning techniques that aim to remove individual 3D Gaussian primitives, the significant reduction in primitives often fails to translate into commensurate increases in rendering speed, impeding efficiency and practical deployment. We identify that this discrepancy arises due to the overlooked impact of fragment count per Gaussian (i.e., the number of pixels each Gaussian is projected onto). To bridge this gap and meet the growing demands for efficient on-device 3D Gaussian rendering, we propose fragment pruning, an orthogonal enhancement to existing pruning methods that can significantly accelerate rendering by selectively pruning fragments within each Gaussian. Our pruning framework dynamically optimizes the pruning threshold for each Gaussian, markedly improving rendering speed and quality. Extensive experiments in both static and dynamic scenes validate the effectiveness of our approach. For instance, by integrating our fragment pruning technique with state-of-the-art Gaussian pruning methods, we achieve up to a 1.71$\\\\times$ speedup on an edge GPU device, the Jetson Orin NX, and enhance rendering quality by an average of 0.16 PSNR on the Tanks\\\\&Temples dataset. Our code is available at https://github.com/GATECH-EIC/Fragment-Pruning.',\n",
       "  190: 'The Fisher information matrix can be used to characterize the local geometry ofthe parameter space of neural networks. It elucidates insightful theories anduseful tools to understand and optimize neural networks. Given its highcomputational cost, practitioners often use random estimators and evaluate onlythe diagonal entries. We examine two popular estimators whose accuracy and samplecomplexity depend on their associated variances. We derive bounds of thevariances and instantiate them in neural networks for regression andclassification. We navigate trade-offs for both estimators based on analyticaland numerical studies. We find that the variance quantities depend on thenon-linearity w.r.t. different parameter groups and should not be neglected whenestimating the Fisher information.',\n",
       "  191: \"Intent learning, which aims to learn users' intents for user understanding and item recommendation, has become a hot research spot in recent years. However, existing methods suffer from complex and cumbersome alternating optimization, limiting performance and scalability. To this end, we propose a novel intent learning method termed \\\\underline{ELCRec}, by unifying behavior representation learning into an \\\\underline{E}nd-to-end \\\\underline{L}earnable \\\\underline{C}lustering framework, for effective and efficient \\\\underline{Rec}ommendation. Concretely, we encode user behavior sequences and initialize the cluster centers (latent intents) as learnable neurons. Then, we design a novel learnable clustering module to separate different cluster centers, thus decoupling users' complex intents. Meanwhile, it guides the network to learn intents from behaviors by forcing behavior embeddings close to cluster centers. This allows simultaneous optimization of recommendation and clustering via mini-batch data. Moreover, we propose intent-assisted contrastive learning by using cluster centers as self-supervision signals, further enhancing mutual promotion. Both experimental results and theoretical analyses demonstrate the superiority of ELCRec from six perspectives. Compared to the runner-up, ELCRec improves NDCG@5 by 8.9\\\\% and reduces computational costs by 22.5\\\\% on the Beauty dataset. Furthermore, due to the scalability and universal applicability, we deploy this method on the industrial recommendation system with 130 million page views and achieve promising results. The codes are available on GitHub\\\\footnote{https://github.com/yueliu1999/ELCRec}. A collection (papers, codes, datasets) of deep group recommendation/intent learning methods is available on GitHub\\\\footnote{https://github.com/yueliu1999/Awesome-Deep-Group-Recommendation}.\",\n",
       "  192: \"Zero-shot graph machine learning, especially with graph neural networks (GNNs), has garnered significant interest due to the challenge of scarce labeled data. While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios. Inspired by the zero-shot capabilities of instruction-fine-tuned large language models (LLMs), we introduce a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and cross-task zero-shot learners for graph machine learning. Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM. We then train a linear projector that transforms the GNN's representations into a fixed number of graph token embeddings without tuning the LLM. A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level). These design choices collectively enhance our method's effectiveness in zero-shot learning, setting it apart from existing methods. Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors. Our code is available at https://github.com/W-rudder/TEA-GLM.\",\n",
       "  193: 'Boundaries are the timestamps at which a class in a time series changes. Recently, representation-based boundary detection has gained popularity, but its emphasis on consecutive distance difference backfires, especially when the changes are gradual. In this paper, we propose a boundary detection method, RECURVE, based on a novel change metric, the curvature of a representation trajectory, to accommodate both gradual and abrupt changes. Here, a sequence of representations in the representation space is interpreted as a trajectory, and a curvature at each timestamp can be computed. Using the theory of random walk, we formally show that the mean curvature is lower near boundaries than at other points. Extensive experiments using diverse real-world time-series datasets confirm the superiority of RECURVE over state-of-the-art methods.',\n",
       "  194: 'The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress towards capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena.',\n",
       "  195: \"Large Language Models (LLMs) have demonstrated remarkable capabilities in many real-world applications. Nonetheless, LLMs are often criticized for their tendency to produce hallucinations, wherein the models fabricate incorrect statements on tasks beyond their knowledge and perception. To alleviate this issue, graph retrieval-augmented generation (GraphRAG) has been extensively explored which leverages the factual knowledge in knowledge graphs (KGs) to ground the LLM's responses in established facts and principles. However, most state-of-the-art LLMs are closed-source, making it challenging to develop a prompting framework that can efficiently and effectively integrate KGs into LLMs with hard prompts only. Generally, existing KG-enhanced LLMs usually suffer from three critical issues, including huge search space, high API costs, and laborious prompt engineering, that impede their widespread application in practice. To this end, we introduce a novel Knowledge Graph based PrompTing framework, namely KnowGPT, to enhance LLMs with domain knowledge. KnowGPT contains a knowledge extraction module to extract the most informative knowledge from KGs, and a context-aware prompt construction module to automatically convert extracted knowledge into effective prompts. Experiments on three benchmarks demonstrate that KnowGPT significantly outperforms all competitors. Notably, KnowGPT achieves a 92.6% accuracy on OpenbookQA leaderboard, comparable to human-level performance.\",\n",
       "  196: 'Single-stage neural combinatorial optimization solvers have achieved near-optimal results on various small-scale combinatorial optimization (CO) problems without requiring expert knowledge. However, these solvers exhibit significant performance degradation when applied to large-scale CO problems. Recently, two-stage neural methods motivated by divide-and-conquer strategies have shown efficiency in addressing large-scale CO problems. Nevertheless, the performance of these methods highly relies on problem-specific heuristics in either the dividing or the conquering procedure, which limits their applicability to general CO problems. Moreover, these methods employ separate training schemes and ignore the interdependencies between the dividing and conquering strategies, often leading to sub-optimal solutions. To tackle these drawbacks, this article develops a unified neural divide-and-conquer framework (i.e., UDC) for solving general large-scale CO problems. UDC offers a Divide-Conquer-Reunion (DCR) training method to eliminate the negative impact of a sub-optimal dividing policy. Employing a high-efficiency Graph Neural Network (GNN) for global instance dividing and a fixed-length sub-path solver for conquering divided sub-problems, the proposed UDC framework demonstrates extensive applicability, achieving superior performance in 10 representative large-scale CO problems. The code is available at https://github.com/CIAM-Group/NCOcode/tree/main/singleobjective/UDC-Large-scale-CO-master',\n",
       "  197: \"In the medical multi-modal frameworks, the alignment of cross-modality features presents a significant challenge. However, existing works have learned features that are implicitly aligned from the data, without considering the explicit relationships in the medical context. This data-reliance may lead to low generalization of the learned alignment relationships. In this work, we propose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for better alignment of medical visual and textual features. We explore the natural auxiliary role of radiologists' eye-gaze data in aligning medical images and text, and introduce a novel approach by using eye-gaze data, collected synchronously by radiologists during diagnostic evaluations. We conduct downstream tasks of image classification and image-text retrieval on four medical datasets, where EGMA achieved state-of-the-art performance and stronger generalization across different datasets. Additionally, we explore the impact of varying amounts of eye-gaze data on model performance, highlighting the feasibility and utility of integrating this auxiliary data into multi-modal alignment framework.\",\n",
       "  198: 'We consider the problem of federated offline reinforcement learning (RL), a scenario under which distributed learning agents must collaboratively learn a high-quality control policy only using small pre-collected datasets generated according to different unknown behavior policies. Na\\\\\"{i}vely combining a standard offline RL approach with a standard federated learning approach to solve this problem can lead to poorly performing policies.  In response, we develop the Federated Ensemble-Directed Offline Reinforcement Learning  Algorithm (FEDORA), which distills the collective wisdom of the clients using an ensemble learning approach.  We develop the FEDORA codebase to utilize distributed compute resources on a federated learning platform. We show that FEDORA significantly outperforms other approaches, including offline RL over the combined data pool, in various complex continuous control environments and real-world datasets. Finally, we demonstrate the performance of FEDORA in the real-world on a mobile robot. We provide our code and a video of our experiments at \\\\url{https://github.com/DesikRengarajan/FEDORA}.',\n",
       "  199: 'Understanding connections between artificial and biological intelligent systems can reveal fundamental principles of general intelligence. While many artificial intelligence models have a neuroscience counterpart, such connections are largely missing in Transformer models and the self-attention mechanism. Here, we examine the relationship between interacting attention heads and human episodic memory. We focus on induction heads, which contribute to in-context learning in Transformer-based large language models (LLMs). We demonstrate that induction heads are behaviorally, functionally, and mechanistically similar to the contextual maintenance and retrieval (CMR) model of human episodic memory. Our analyses of LLMs pre-trained on extensive text data show that CMR-like heads often emerge in the intermediate and late layers, qualitatively mirroring human memory biases. The ablation of CMR-like heads suggests their causal role in in-context learning. Our findings uncover a parallel between the computational mechanisms of LLMs and human memory, offering valuable insights into both research fields.',\n",
       "  200: 'Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insights for solving scientific problems based on partial differential equations (PDEs).  However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining for PDE operator learning. To reduce the need for training data with heavy simulation costs, we mine unlabeled PDE data without simulated solutions,and we pretrain neural operators with physics-inspired reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging a similarity-based method that learns in-context examples, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PDEs demonstrate that our method is highly data-efficient, more generalizable, and even outperforms conventional vision-pretrained models. We provide our code at https://github.com/delta-lab-ai/dataefficientnopt.',\n",
       "  201: 'Diffusion models represent a promising avenue for image generation, having demonstrated competitive performance in pose-guided person image generation. However, existing methods are limited to generating target images from a source image and a target pose, overlooking two critical user scenarios: generating multiple target images with different poses simultaneously and generating target images from multi-view source images.To overcome these limitations, we propose IMAGPose, a unified conditional framework for pose-guided image generation, which incorporates three pivotal modules: a feature-level conditioning (FLC) module, an image-level conditioning (ILC) module, and a cross-view attention (CVA) module. Firstly, the FLC module combines the low-level texture feature from the VAE encoder with the high-level semantic feature from the image encoder, addressing the issue of missing detail information due to the absence of a dedicated person image feature extractor. Then, the ILC module achieves an alignment of images and poses to adapt to flexible and diverse user scenarios by injecting a variable number of source image conditions and introducing a masking strategy.Finally, the CVA module introduces decomposing global and local cross-attention, ensuring local fidelity and global consistency of the person image when multiple source image prompts. The three modules of IMAGPose work together to unify the task of person image generation under various user scenarios.Extensive experiment results demonstrate the consistency and photorealism of our proposed IMAGPose under challenging user scenarios. The code and model will be available at https://github.com/muzishen/IMAGPose.',\n",
       "  202: 'The kernel thinning algorithm of Dwivedi & Mackey (2024) provides a better-than-i.i.d. compression of a generic set of points. By generating high-fidelity coresets of size significantly smaller than the input points, KT is known to speed up unsupervised tasks like Monte Carlo integration, uncertainty quantification, and non-parametric hypothesis testing, with minimal loss in statistical accuracy. In this work, we generalize the KT algorithm to speed up supervised learning problems involving kernel methods. Specifically, we combine two classical algorithms---Nadaraya-Watson (NW) regression or kernel smoothing, and kernel ridge regression (KRR)---with KT to provide a quadratic speed-up in both training and inference times. We show how distribution compression with KT in each setting reduces to constructing an appropriate kernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators. We prove that KT-based regression estimators enjoy significantly superior computational efficiency over the full-data estimators and improved statistical efficiency over i.i.d. subsampling of the training data. En route, we also provide a novel multiplicative error guarantee for compressing with KT.  We validate our design choices with both simulations and real data experiments.',\n",
       "  203: 'Transcriptome foundation models (TFMs) hold great promises of deciphering the transcriptomic language that dictate diverse cell functions by self-supervised learning on large-scale single-cell gene expression data, and ultimately unraveling the complex mechanisms of human diseases. However, current TFMs treat cells as independent samples and ignore the taxonomic relationships between cell types, which are available in cell ontology graphs. We argue that effectively leveraging this ontology information during the TFM pre-training can improve learning biologically meaningful gene co-expression patterns while preserving TFM as a general purpose foundation model for downstream zero-shot and fine-tuning tasks. To this end, we present single cell, Cell-ontology guided TFM (scCello). We introduce cell-type coherence loss and ontology alignment loss, which are minimized along with the masked gene expression prediction loss during the pre-training. The novel loss component guide scCello to learn the cell-type-specific representation and the structural relation between cell types from the cell ontology graph, respectively. We pre-trained scCello on 22 million cells from CellxGene database leveraging their cell-type labels mapped to the cell ontology graph from Open Biological and Biomedical Ontology Foundry. Our TFM demonstrates competitive generalization and transferability performance over the existing TFMs on biologically important tasks including identifying novel cell types of unseen cells, prediction of cell-type-specific marker genes, and cancer drug responses. Source code and modelweights are available at https://github.com/DeepGraphLearning/scCello.',\n",
       "  204: 'Enabling deep models to generalize in non-stationary environments is vital for real-world machine learning, as data distributions are often found to continually change. Recently, evolving domain generalization (EDG) has emerged to tackle the domain generalization in a time-varying system, where the domain gradually evolves over time in an underlying continuous structure. Nevertheless, it typically assumes multiple source domains simultaneously ready. It still remains an open problem to address EDG in the domain-incremental setting, where source domains are non-static and arrive sequentially to mimic the evolution of training domains. To this end, we propose Weight Diffusion (W-Diff), a novel framework that utilizes the conditional diffusion model in the parameter space to learn the evolving pattern of classifiers during the domain-incremental training process. Specifically, the diffusion model is conditioned on the classifier weights of different historical domain (regarded as a reference point) and the prototypes of current domain, to learn the evolution from the reference point to the classifier weights of current domain (regarded as the anchor point). In addition, a domain-shared feature encoder is learned by enforcing prediction consistency among multiple classifiers, so as to mitigate the overfitting problem and restrict the evolving pattern to be reflected in the classifier as much as possible. During inference, we adopt the ensemble manner based on a great number of target domain-customized classifiers, which are cheaply obtained via the conditional diffusion model, for robust prediction. Comprehensive experiments on both synthetic and real-world datasets show the superior generalization performance of W-Diff on unseen domains in the future.',\n",
       "  205: 'We build interpretable and lightweight transformer-like neural networks by unrolling iterative optimization algorithms that minimize graph smoothness priors---the quadratic graph Laplacian regularizer (GLR) and the $\\\\ell_1$-norm graph total variation (GTV)---subject to an interpolation constraint. The crucial insight is that a normalized signal-dependent graph learning module amounts to a variant of the basic self-attention mechanism in conventional transformers. Unlike \"black-box\" transformers that require learning of large key, query and value matrices to compute scaled dot products as affinities and subsequent output embeddings, resulting in huge parameter sets, our unrolled networks employ shallow CNNs to learn low-dimensional features per node to establish pairwise Mahalanobis distances and construct sparse similarity graphs. At each layer, given a learned graph, the target interpolated signal is simply a low-pass filtered output derived from the minimization of an assumed graph smoothness prior, leading to a dramatic reduction in parameter count. Experiments for two image interpolation applications verify the restoration performance, parameter efficiency and robustness to covariate shift of our graph-based unrolled networks compared to conventional transformers.',\n",
       "  206: 'The high communication cost between the server and the clients is a significant bottleneck in scaling distributed learning for overparametrized deep models. One popular approach for reducing this communication overhead is randomized sketching. However, existing theoretical analyses for sketching-based distributed learning (sketch-DL) either incur a prohibitive dependence on the ambient dimension or need additional restrictive assumptions such as heavy-hitters. Nevertheless, despite existing pessimistic analyses, empirical evidence suggests that sketch-DL is competitive with its uncompressed counterpart, thus motivating a sharper analysis. In this work, we introduce a sharper ambient dimension-independent convergence analysis for sketch-DL using the second-order geometry specified by the loss Hessian. Our results imply ambient dimension-independent communication complexity for sketch-DL. We present empirical results both on the loss Hessian and overall accuracy of sketch-DL supporting our theoretical results. Taken together, our results provide theoretical justification for the observed empirical success of sketch-DL.',\n",
       "  207: 'Causal effect identification and estimation are two crucial tasks in causal inference. Although causal effect identification has been theoretically resolved, many existing estimators only address a subset of scenarios, known as the sequential back-door adjustment (SBD) (Pearl and Robins, 1995) or g-formula (Robins, 1986). Recent efforts for developing general-purpose estimators with broader coverage, incorporating the front-door adjustment (FD) (Pearl, 2000) and more, lack scalability due to the high computational cost of summing over high-dimensional variables. In this paper, we introduce a novel approach that achieves broad coverage of causal estimands beyond the SBD, incorporating various sum-product functionals like the FD, while maintaining scalability -- estimated in polynomial time relative to the number of variables and samples. Specifically, we present the class of UCA for which a scalable and doubly robust estimator is developed. In particular, we illustrate the expressiveness of UCA for a wide spectrum of causal estimands (e.g., SBD, FD, and more) in causal inference. We then develop an estimator that exhibits computational efficiency and doubly robustness. The scalability and robustness of the proposed framework are verified through simulations.',\n",
       "  208: 'We develop a framework for analyzing the training and learning rate dynamics on a large class of high-dimensional optimization problems, which we call the high line, trained using one-pass stochastic gradient descent (SGD) with adaptive learning rates. We give exact expressions for the risk and learning rate curves in terms of a deterministic solution to a system of ODEs. We then investigate in detail two adaptive learning rates  -- an idealized exact line search and AdaGrad-Norm -- on the least squares problem. When the data covariance matrix has strictly positive eigenvalues, this idealized exact line search strategy can exhibit arbitrarily slower convergence when compared to the optimal fixed learning rate with SGD. Moreover we exactly characterize the limiting learning rate (as time goes to infinity) for line search in the setting where the data covariance has only two distinct eigenvalues. For noiseless targets, we further demonstrate that the AdaGrad-Norm learning rate converges to a deterministic constant inversely proportional to the average eigenvalue of the data covariance matrix, and identify a phase transition when the covariance density of eigenvalues follows a power law distribution. We provideour code for evaluation at https://github.com/amackenzie1/highline2024.',\n",
       "  209: 'Deep State Space Models (SSMs) have proven effective in numerous task scenarios but face significant security challenges due to Adversarial Perturbations (APs) in real-world deployments. Adversarial Training (AT) is a mainstream approach to enhancing Adversarial Robustness (AR) and has been validated on various traditional DNN architectures. However, its effectiveness in improving the AR of SSMs remains unclear.While many enhancements in SSM components, such as integrating Attention mechanisms and expanding to data-dependent SSM parameterizations, have brought significant gains in Standard Training (ST) settings, their potential benefits in AT remain unexplored. To investigate this, we evaluate existing structural variants of SSMs with AT to assess their AR performance. We observe that pure SSM structures struggle to benefit from AT, whereas incorporating Attention yields a markedly better trade-off between robustness and generalization for SSMs in AT compared to other components. Nonetheless, the integration of Attention also leads to Robust Overfitting (RO) issues.To understand these phenomena, we empirically and theoretically  analyze the output error of SSMs under AP. We find that fixed-parameterized SSMs have output error bounds strictly related to their parameters, limiting their AT benefits, while input-dependent SSMs may face the problem of error explosion. Furthermore, we show that the Attention component effectively scales the output error of SSMs during training, enabling them to benefit more from AT, but at the cost of introducing RO due to its high model complexity.Inspired by this, we propose a simple and effective Adaptive Scaling (AdS) mechanism that brings AT performance close to Attention-integrated SSMs without introducing the issue of RO.',\n",
       "  210: 'Learning from previously collected data via behavioral cloning or offline reinforcement learning (RL) is a powerful recipe for scaling generalist agents by avoiding the need for expensive online learning. Despite strong generalization in some respects, agents are often remarkably brittle to minor visual variations in control-irrelevant factors such as the background or camera viewpoint.  In this paper, we present theDeepMind Control Visual Benchmark (DMC-VB), a dataset collected in the DeepMind Control Suite to evaluate the robustness of offline RL agents for solving continuous control tasks from visual input in the presence of visual distractors. In contrast to prior works, our dataset (a) combines locomotion and navigation tasks of varying difficulties, (b) includes static and dynamic visual variations, (c) considers data generated by policies with different skill levels, (d) systematically returns pairs of state and pixel observation, (e) is an order of magnitude larger, and (f) includes tasks with hidden goals. Accompanying our dataset, we propose three benchmarks to evaluate representation learning methods for pretraining, and carry out experiments on several recently proposed methods. First, we find that pretrained representations do not help policy learning on DMC-VB, and we highlight a large representation gap between policies learned on pixel observations and on states. Second, we demonstrate when expert data is limited, policy learning can benefit from representations pretrained on (a) suboptimal data, and (b) tasks with stochastic hidden goals. Our dataset and benchmark code to train and evaluate agents are available at https://github.com/google-deepmind/dmcvisionbenchmark.',\n",
       "  211: 'Reinforcement Learning (RL) controllers have demonstrated remarkable performance in complex robot control tasks. However, the presence of reality gap often leads to poor performance when deploying policies trained in simulation directly onto real robots. Previous sim-to-real algorithms like Domain Randomization (DR) requires domain-specific expertise and suffers from issues such as reduced control performance and high training costs. In this work, we introduce Evolutionary Adversarial Simulator Identification (EASI), a novel approach that combines Generative Adversarial Network (GAN) and Evolutionary Strategy (ES) to address sim-to-real challenges. Specifically, we consider the problem of sim-to-real as a search problem, where ES acts as a generator in adversarial competition with a neural network discriminator, aiming to find physical parameter distributions that make the state transitions between simulation and reality as similar as possible. The discriminator serves as the fitness function, guiding the evolution of the physical parameter distributions. EASI features simplicity, low cost, and high fidelity, enabling the construction of a more realistic simulator with minimal requirements for real-world data, thus aiding in transferring simulated-trained policies to the real world. We demonstrate the performance of EASI in both sim-to-sim and sim-to-real tasks, showing superior performance compared to existing sim-to-real algorithms.',\n",
       "  212: 'Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples. However, existing MedVLP methods often differ in terms of datasets, preprocessing, and finetuning implementations. This pose great challenges in evaluating how well a MedVLP method generalizes to various clinically-relevant tasks due to the lack of unified, standardized, and comprehensive benchmark. To fill this gap, we propose BenchX, a unified benchmark framework that enables head-to-head comparison and systematical analysis between MedVLP methods using public chest X-ray datasets. Specifically, BenchX is composed of three components: 1) Comprehensive datasets covering nine datasets and four medical tasks; 2) Benchmark suites to standardize data preprocessing, train-test splits, and parameter selection; 3) Unified finetuning protocols that accommodate heterogeneous MedVLP methods for consistent task adaptation in classification, segmentation, and report generation, respectively. Utilizing BenchX, we establish baselines for nine state-of-the-art MedVLP methods and found that the performance of some early MedVLP methods can be enhanced to surpass more recent ones, prompting a revisiting of the developments and conclusions from prior works in MedVLP. Our code are available at https://github.com/yangzhou12/BenchX.',\n",
       "  213: \"In sequential decision-making problems, the information structure describes the causal dependencies between system variables, encompassing the dynamics of the environment and the agents' actions. Classical models of reinforcement learning (e.g., MDPs, POMDPs) assume a restricted and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decision-making problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure. In this paper, we formalize a novel reinforcement learning model which explicitly represents the information structure.We then use this model to carry out an information-structural analysis of the statistical complexity of general sequential decision-making problems, obtaining a characterization via a graph-theoretic quantity of the DAG representation of the information structure. We prove an upper bound on the sample complexity of learning a general sequential decision-making problem in terms of its information structure by exhibiting an algorithm achieving the upper bound. This recovers known tractability results and gives a novel perspective on reinforcement learning in general sequential decision-making problems, providing a systematic way of identifying new tractable classes of problems.\",\n",
       "  214: 'Unsupervised domain adaptation (UDA) plays a crucial role in addressing distribution shifts in machine learning. In this work, we improve the theoretical foundations of UDA proposed in Acuna et al. (2021) by refining their $f$-divergence-based discrepancy and additionally introducing a new measure, $f$-domain discrepancy ($f$-DD). By removing the absolute value function and incorporating a scaling parameter, $f$-DD obtains novel target error and sample complexity bounds, allowing us to recover previous KL-based results and bridging the gap between algorithms and theory presented in Acuna et al. (2021). Using a localization technique, we also develop a fast-rate generalization bound. Empirical results demonstrate the superior performance of $f$-DD-based learning algorithms over previous works in popular UDA benchmarks.',\n",
       "  215: 'Recent advances in data-driven imitation learning and offline reinforcement learning have highlighted the use of expert data for skill acquisition and the development of hierarchical policies based on these skills. However, these approaches have not significantly advanced in adapting these skills to unseen contexts, which may involve changing environmental conditions or different user requirements. In this paper, we present a novel LLM-based policy adaptation framework LDuS which leverages an LLM to guide the generation process of a skill diffusion model upon contexts specified in language, facilitating zero-shot skill-based policy adaptation to different contexts. To implement the skill diffusion model, we adapt the loss-guided diffusion with a sequential in-painting technique, where target trajectories are conditioned by masking them with past state-action sequences, thereby enabling the robust and controlled generation of skill trajectories in test-time. To have a loss function for a given context, we employ the LLM-based code generation with iterative refinement, by which the code and controlled trajectory are validated to align with the context in a closed-loop manner. Through experiments, we demonstrate the zero-shot adaptability of LDuS to various context types including different specification levels, multi-modality, and varied temporal conditions for several robotic manipulation tasks, outperforming other language-conditioned imitation and planning methods.',\n",
       "  216: 'Adversarial robustness and privacy of deep learning (DL) models are two widely studied topics in AI security. Adversarial training (AT) is an effective approach to improve the robustness of DL models against adversarial attacks. However, while models with AT demonstrate enhanced robustness, they become more susceptible to membership inference attacks (MIAs), thus increasing the risk of privacy leakage. This indicates a negative trade-off between adversarial robustness and privacy in general deep learning models. Visual prompting is a novel model reprogramming (MR) technique used for fine-tuning pre-trained models, achieving good performance in vision tasks, especially when combined with the label mapping technique. However, the performance of label-mapping-based visual prompting (LM-VP) under adversarial attacks and MIAs lacks evaluation. In this work, we regard the MR of LM-VP as a unified entity, referred to as the LM-VP model, and take a step toward jointly evaluating the adversarial robustness and privacy of LM-VP models. Experimental results show that the choice of pre-trained models significantly affects the white-box adversarial robustness of LM-VP, and standard AT even substantially degrades its performance. In contrast, transfer AT-trained LM-VP achieves a good trade-off between transferred adversarial robustness and privacy, a finding that has been consistently validated across various pre-trained models.',\n",
       "  217: 'Diffusion regulates numerous natural processes and the dynamics of many successful generative models. Existing models to learn the diffusion terms from observational data rely on complex bilevel optimization problems and model only the drift of the system.We propose a new simple model, JKOnet, which bypasses the complexity of existing architectures while presenting significantly enhanced representational capabilities: JKOnet recovers the potential, interaction, and internal energy components of the underlying diffusion process. JKOnet* minimizes a simple quadratic loss and outperforms other baselines in terms of sample efficiency, computational complexity, and accuracy. Additionally, JKOnet* provides a closed-form optimal solution for linearly parametrized functionals, and, when applied to predict the evolution of cellular processes from real-world data, it achieves state-of-the-art accuracy at a fraction of the computational cost of all existing methods.Our methodology is based on the interpretation of diffusion processes as energy-minimizing trajectories in the probability space via the so-called JKO scheme, which we study via its first-order optimality conditions.',\n",
       "  218: \"We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed  token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.\",\n",
       "  219: 'Existing super-resolution (SR) methods optimize all model weights equally using $\\\\mathcal{L}_1$ or $\\\\mathcal{L}_2$ losses by uniformly sampling image patches without considering dataset imbalances or parameter redundancy, which limits their performance. To address this, we formulate the image SR task as an imbalanced distribution transfer learning problem from a statistical probability perspective, proposing a plug-and-play Weight-Balancing framework (WBSR) to achieve balanced model learning without changing the original model structure and training data. Specifically, we develop a Hierarchical Equalization Sampling (HES) strategy to address data distribution imbalances, enabling better feature representation from texture-rich samples. To tackle model optimization imbalances, we propose a Balanced Diversity Loss (BDLoss) function, focusing on learning texture regions while disregarding redundant computations in smooth regions. After joint training of HES and BDLoss to rectify these imbalances, we present a gradient projection dynamic inference strategy to facilitate accurate and efficient inference. Extensive experiments across various models, datasets, and scale factors demonstrate that our method achieves comparable or superior performance to existing approaches with about 34\\\\% reduction in computational cost.',\n",
       "  220: 'Visual text generation has significantly advanced through diffusion models aimed at producing images with readable and realistic text. Recent works primarily use a ControlNet-based framework, employing standard font text images to control diffusion models. Recognizing the critical role of control information in generating high-quality text, we investigate its influence from three perspectives: input encoding, role at different stages, and output features. Our findings reveal that: 1) Input control information has unique characteristics compared to conventional inputs like Canny edges and depth maps. 2) Control information plays distinct roles at different stages of the denoising process. 3) Output control features significantly differ from the base and skip features of the U-Net decoder in the frequency domain. Based on these insights, we propose TextGen, a novel framework designed to enhance generation quality by optimizing control information. We improve input and output features using Fourier analysis to emphasize relevant information and reduce noise. Additionally, we employ a two-stage generation framework to align the different roles of control information at different stages. Furthermore, we introduce an effective and lightweight dataset for training. Our method achieves state-of-the-art performance in both Chinese and English text generation. The code and dataset are available at https://github.com/CyrilSterling/TextGen.',\n",
       "  221: \"We present a novel approach for digitizing real-world objects by estimating their geometry, material properties, and environmental lighting from a set of posed images with fixed lighting. Our method incorporates into Neural Radiance Field (NeRF) pipelines the split sum approximation used with image-based lighting for real-time physically based rendering. We propose modeling the scene's lighting with a single scene-specific MLP representing pre-integrated image-based lighting at arbitrary resolutions. We accurately model pre-integrated lighting by exploiting a novel regularizer based on efficient Monte Carlo sampling. Additionally, we propose a new method of supervising self-occlusion predictions by exploiting a similar regularizer based on Monte Carlo sampling. Experimental results demonstrate the efficiency and effectiveness of our approach in estimating scene geometry, material properties, and lighting. Our method attains state-of-the-art relighting quality after only ${\\\\sim}1$ hour of training in a single NVIDIA A100 GPU.\",\n",
       "  222: 'Animals survive in dynamic environments changing at arbitrary timescales, but such data distribution shifts are a challenge to neural networks. To adapt to change, neural systems may change a large number of parameters, which is a slow process involving forgetting past information. In contrast, animals leverage distribution changes to segment their stream of experience into tasks and associate them with internal task abstracts. Animals can then respond flexibly by selecting the appropriate task abstraction. However, how such flexible task abstractions may arise in neural systems remains unknown. Here, we analyze a linear gated network where the weights and gates are jointly optimized via gradient descent, but with neuron-like constraints on the gates including a faster timescale, non-negativity, and bounded activity. We observe that the weights self-organize into modules specialized for tasks or sub-tasks encountered, while the gates layer forms unique representations that switch the appropriate weight modules (task abstractions). We analytically reduce the learning dynamics to an effective eigenspace, revealing a virtuous cycle: fast adapting gates drive weight specialization by protecting previous knowledge, while weight specialization in turn increases the update rate of the gating layer. Task switching in the gating layer accelerates as a function of curriculum block size and task training, mirroring key findings in cognitive neuroscience. We show that the discovered task abstractions support generalization through both task and subtask composition, and we extend our findings to a non-linear network switching between two tasks. Overall, our work offers a theory of cognitive flexibility in animals as arising from joint gradient descent on synaptic and neural gating in a neural network architecture.',\n",
       "  223: 'Recent advances in generative modeling with diffusion processes (DPs) enabled breakthroughs in image synthesis. Despite impressive image quality, these models have various prompt compliance problems, including low recall in generating multiple objects, difficulty in generating text in images, and meeting constraints like object locations and pose. For fine-grained editing and manipulation, they also require fine-grained semantic or instance maps that are tedious to produce manually. While prompt compliance can be enhanced by addition of loss functions at inference, this is time consuming and does not scale to complex scenes.   To overcome these limitations, this work introduces a new family of  $\\\\textit{Factor Graph Diffusion Models}$ (FG-DMs) that models the joint distribution of images and conditioning variables, such as semantic, sketch, depth or normal maps via a factor graph decomposition. This joint structure has several advantages, including support for efficient sampling based prompt compliance schemes, which produce images of high object recall, semi-automated fine-grained editing, explainability at intermediate levels, ability to produce labeled datasets for the training of downstream models such as segmentation or depth, training with missing data, and continual learning where new conditioning variables can be added with minimal or no modifications to the existing structure. We propose an implementation of FG-DMs by adapting a pre-trained Stable Diffusion (SD) model to implement all FG-DM factors, using only COCO dataset, and show that it is effective in generating images with 15\\\\% higher recall than SD while retaining its generalization ability. We introduce an attention distillation loss that encourages consistency among the attention maps of all factors, improving the fidelity of the generated conditions and image. We also show that training FG-DMs from scratch on MM-CelebA-HQ, Cityscapes, ADE20K, and COCO produce images of high quality (FID) and diversity (LPIPS).',\n",
       "  224: \"The saturation effects, which originally refer to the fact that kernel ridge regression (KRR) fails to achieve the information-theoretical lower bound when the regression function is over-smooth, have been observed for almost 20 years and were rigorously proved recently for kernel ridge regression and some other spectral algorithms over a fixed dimensional domain. The main focus of this paper is to explore the saturation effects for a large class of spectral algorithms (including the KRR, gradient descent, etc.) in large dimensional settings where $n \\\\asymp d^{\\\\gamma}$. More precisely, we first propose an improved minimax lower bound for the kernel regression problem in large dimensional settings and show that the gradient flow with early stopping strategy will result in an estimator achieving this lower bound (up to a logarithmic factor). Similar to the results in KRR, we can further determine the exact convergence rates (both upper and lower bounds) of a large class of (optimal tuned) spectral algorithms with different qualification $\\\\tau$'s. In particular, we find that these exact rate curves (varying along $\\\\gamma$) exhibit the periodic plateau behavior and the polynomial approximation barrier. Consequently, we can fully depict the saturation effects of the spectral algorithms and reveal a new phenomenon in large dimensional settings (i.e.,  the saturation effect occurs in large dimensional setting as long as the source condition $s>\\\\tau$ while it occurs in fixed dimensional setting as long as $s>2\\\\tau$).\",\n",
       "  225: 'Blind image deblurring (BID) is an important yet challenging image recovery problem. Most existing deep learning methods require supervised training with ground truth (GT) images. This paper introduces a self-supervised method for BID that does not require GT images. The key challenge is to regularize the training to prevent over-fitting due to the absence of GT images. By leveraging an exact relationship among the blurred image, latent image, and blur kernel across consecutive scales, we propose an effective cross-scale consistency loss. This is implemented by representing the image and kernel with implicit neural representations (INRs), whose resolution-free property enables consistent yet efficient computation for network training across multiple scales. Combined with a progressively coarse-to-fine training scheme, the proposed method significantly outperforms existing self-supervised methods  in extensive experiments.',\n",
       "  226: 'Extensive knowledge graphs (KGs) have been constructed to facilitate knowledge-driven tasks across various scenarios. However, existing work usually develops separate reasoning models for different KGs, lacking the ability to generalize and transfer knowledge across diverse KGs and reasoning settings. In this paper, we propose a prompt-based KG foundation model via in-context learning, namely KG-ICL, to achieve a universal reasoning ability.  Specifically, we introduce a prompt graph centered with a query-related example fact as context to understand the query relation. To encode prompt graphs with the generalization ability to unseen entities and relations in queries, we first propose a unified tokenizer that maps entities and relations in prompt graphs to predefined tokens. Then, we propose two message passing neural networks to perform prompt encoding and KG reasoning, respectively. We conduct evaluation on 43 different KGs in both transductive and inductive settings. Results indicate that the proposed KG-ICL outperforms baselines on most datasets, showcasing its outstanding generalization and universal reasoning capabilities. The source code is accessible on GitHub: https://github.com/nju-websoft/KG-ICL.',\n",
       "  227: 'For challenging state estimation problems arising in domains like vision and robotics, particle-based representations attractively enable temporal reasoning about multiple posterior modes.  Particle smoothers offer the potential for more accurate offline data analysis by propagating information both forward and backward in time, but have classically required human-engineered dynamics and observation models.  Extending recent advances in discriminative training of particle filters, we develop a framework for low-variance propagation of gradients across long time sequences when training particle smoothers.  Our \"two-filter\" smoother integrates particle streams that are propagated forward and backward in time, while incorporating stratification and importance weights in the resampling step to provide low-variance gradient estimates for neural network dynamics and observation models.  The resulting mixture density particle smoother is substantially more accurate than state-of-the-art particle filters, as well as search-based baselines, for city-scale global vehicle localization from real-world videos and maps.',\n",
       "  228: 'Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely un-ordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block’s probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN (Maronet al., 2019). Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules.',\n",
       "  229: 'Tables contain factual and quantitative data accompanied by various structures and contents that pose challenges for machine comprehension. Previous methods generally design task-specific architectures and objectives for individual tasks, resulting in modal isolation and intricate workflows. In this paper, we present a novel large vision-language model, TabPedia, equipped with a concept synergy mechanism. In this mechanism, all the involved diverse visual table understanding (VTU) tasks and multi-source visual embeddings are abstracted as concepts. This unified framework allows TabPedia to seamlessly integrate VTU tasks, such as table detection, table structure recognition, table querying, and table question answering, by leveraging the capabilities of large language models (LLMs). Moreover, the concept synergy mechanism enables table perception-related and comprehension-related tasks to work in harmony, as they can effectively leverage the needed clues from the corresponding source perception embeddings. Furthermore, to better evaluate the VTU task in real-world scenarios, we establish a new and comprehensive table VQA benchmark, ComTQA, featuring approximately 9,000 QA pairs. Extensive quantitative and qualitative experiments on both table perception and comprehension tasks, conducted across various public benchmarks, validate the effectiveness of our TabPedia. The superior performance further confirms the feasibility of using LLMs for understanding visual tables when all concepts work in synergy. The benchmark ComTQA has been open-sourced at https://huggingface.co/datasets/ByteDance/ComTQA. The source code and model also have been released at https://github.com/zhaowc-ustc/TabPedia.',\n",
       "  230: 'Large Language Models (LLMs) have achieved remarkable success across various industries and applications, owing to their exceptional generative capabilities. Nevertheless, honesty and helpfulness, which ensure safe and useful real-world deployments, have been considered as the longstanding cornerstones in practice. In this paper, we first established comprehensive principles for honesty LLM and further created the HoneSet with 930 queries across six categories, which is designed to evaluate LLMs’ ability to maintain honesty. Then, we improved the honesty and helpfulness of LLMs in both training-free and fine-tuning settings. Specifically, we propose a training-free method named Curiosity-Driven Prompting, which enables LLMs to express their internal confusion and uncertainty about the given query and then optimize their responses. Moreover, we also propose a two-stage fine-tuning approach, inspired by curriculum learning, to enhance the honesty and helpfulness of LLMs. The method first teaches LLMs to distinguish between honest and dishonest, and then LLMs are trained to learn to respond more helpfully. Experimental results demonstrated that both of the two proposed methods improve the helpfulness of LLMs while making them maintain honesty. Our research has paved the way for more reliable and trustworthy LLMs in real-world applications.',\n",
       "  231: \"Powered by remarkable advancements in Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities in manifold tasks.However, the practical application scenarios of MLLMs are intricate, exposing them to potential malicious instructions and thereby posing safety risks.While current benchmarks do incorporate certain safety considerations, they often lack comprehensive coverage and fail to exhibit the necessary rigor and robustness.For instance, the common practice of employing GPT-4V as both the evaluator and a model to be evaluated lacks credibility, as it tends to exhibit a bias toward its own responses.In this paper, we present MLLMGuard, a multi-dimensional safety evaluation suite for MLLMs, including a bilingual image-text evaluation dataset, inference utilities, and a lightweight evaluator.MLLMGuard's assessment comprehensively covers two languages (English and Chinese) and five important safety dimensions (Privacy, Bias, Toxicity, Truthfulness, and Legality), each with corresponding rich subtasks.Focusing on these dimensions, our evaluation dataset is primarily sourced from platforms such as social media, and it integrates text-based and image-based red teaming techniques with meticulous annotation by human experts.This can prevent inaccurate evaluation caused by data leakage when using open-source datasets and ensures the quality and challenging nature of our benchmark.Additionally, a fully automated lightweight evaluator termed GuardRank is developed, which achieves significantly higher evaluation accuracy than GPT-4.Our evaluation results across 13 advanced models indicate that MLLMs still have a substantial journey ahead before they can be considered safe and responsible.\",\n",
       "  232: 'The rapid progression of urbanization has generated a diverse array of urban data, facilitating significant advancements in urban science and urban computing. Current studies often work on separate problems case by case using diverse data, e.g., air quality prediction, and built-up areas classification. This fragmented approach hinders the urban research field from advancing at the pace observed in Computer Vision and Natural Language Processing, due to two primary reasons. On the one hand, the diverse data processing steps lead to the lack of large-scale benchmarks and therefore decelerate iterative methodology improvement on a single problem. On the other hand, the disparity in multi-modal data formats hinders the combination of the related modal data to stimulate more research findings. To address these challenges, we propose UrbanDataLayer (UDL), a suite of standardized data structures and pipelines for city data engineering, providing a unified data format for researchers. This allows researchers to easily build up large-scale benchmarks and combine multi-modal data, thus expediting the development of multi-modal urban foundation models. To verify the effectiveness of our work, we present four distinct urban problem tasks utilizing the proposed data layer. UrbanDataLayer aims to enhance standardization and operational efficiency within the urban science research community. The examples and source code are available at https://github.com/SJTU-CILAB/udl.',\n",
       "  233: 'Real-world geometry and 3D vision tasks are replete with challenging symmetries that defy tractable analytical expression. In this paper, we introduce Neural Isometries, an autoencoder framework which learns to map the observation space to a general-purpose latent space wherein encodings are related by isometries whenever their corresponding observations are geometrically related in world space. Specifically, we regularize the latent space such that maps between encodings preserve a learned inner product and commute with a learned functional operator, in the same manner as rigid-body transformations commute with the Laplacian. This approach forms an effective backbone for self-supervised representation learning, and we demonstrate that a simple off-the-shelf equivariant network operating in the pre-trained latent space can achieve results on par with meticulously-engineered, handcrafted networks designed to handle complex, nonlinear symmetries. Furthermore, isometric maps capture information about the respective transformations in world space, and we show that this allows us to regress camera poses directly from the coefficients of the maps between encodings of adjacent views of a scene.',\n",
       "  234: 'We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a cross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention. The overall model behaves like a decoder-only Transformer, although YOCO only caches once. The design substantially reduces GPU memory demands, yet retains global attention capability. Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage. Experimental results demonstrate that YOCO achieves favorable performance compared to Transformer in various settings of scaling up model size and number of training tokens. We also extend YOCO to 1M context length with near-perfect needle retrieval accuracy. The profiling results show that YOCO improves inference memory, prefill latency, and throughput by orders of magnitude across context lengths and model sizes.',\n",
       "  235: \"We study the interplay between local differential privacy (LDP) and robustness to Huber corruption and possibly heavy-tailed rewards in the context of multi-armed bandits (MABs). We consider two different practical settings: LDP-then-Corruption (LTC) where each user's locally private response might be further corrupted during the data collection process, and Corruption-then-LDP (CTL) where each user's raw data may be corrupted such that the LDP mechanism will only be applied to the corrupted data. To start with, we present the first tight characterization of the mean estimation error in high probability under both LTC and CTL settings. Leveraging this new result, we then present an almost tight characterization (up to log factor) of the minimax regret in online MABs and sub-optimality in offline MABs under both LTC and CTL settings, respectively. Our theoretical results in both settings are also corroborated by a set of systematic simulations. One key message in this paper is that LTC is a more difficult setting that leads to a worse performance guarantee compared to the CTL setting (in the minimax sense). Our sharp understanding of LTC and CTL also naturally allows us to give the first tight performance bounds for the most practical setting where corruption could happen both before and after the LDP mechanism. As an important by-product, we also give the first correct and tight regret bound for locally private and heavy-tailed online MABs, i.e., without Huber corruption, by identifying a fundamental flaw in the state-of-the-art.\",\n",
       "  236: \"Understanding the mechanisms of information storage and transfer in Transformer-based models is important for driving model understanding progress. Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model's parameters and how information flows to and from these parameters in response to specific prompts. However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs). Given their expanding capabilities and real-world use, we start by studying one aspect of these models -- how MLLMs process information in a factual visual question answering task. We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model's generated answer must satisfy to be correct (e.g. What movie directed by \\\\emph{the director in this photo} has won a \\\\emph{Golden Globe}?). Under this setting, we contribute i) a method that extends causal information tracing from pure language to the multi-modal setting, and ii) \\\\emph{VQA-Constraints}, a test-bed of 9.7K visual questions annotated with constraints. We use these tools to study two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important. We also show that a consistent small subset of visual tokens output by the vision encoder are responsible for transferring information from the image to these causal blocks. We validate these mechanisms by introducing MultEdit a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks. We will publicly release our dataset and code.\",\n",
       "  237: \"Image Restoration (IR), a classic low-level vision task, has witnessed significant advancements through deep models that effectively model global information. Notably, the emergence of Vision Transformers (ViTs) has further propelled these advancements. When computing, the self-attention mechanism, a cornerstone of ViTs, tends to encompass all global cues, even those from semantically unrelated objects or regions. This inclusivity introduces computational inefficiencies, particularly noticeable with high input resolution, as it requires processing irrelevant information, thereby impeding efficiency. Additionally, for IR, it is commonly noted that small segments of a degraded image, particularly those closely aligned semantically, provide particularly relevant information to aid in the restoration process, as they contribute essential contextual cues crucial for accurate reconstruction. To address these challenges, we propose boosting IR's performance by sharing the key semantics via Transformer for IR (i.e., SemanIR) in this paper. Specifically, SemanIR initially constructs a sparse yet comprehensive key-semantic dictionary within each transformer stage by establishing essential semantic connections for every degraded patch. Subsequently, this dictionary is shared across all subsequent transformer blocks within the same stage. This strategy optimizes attention calculation within each block by focusing exclusively on semantically related components stored in the key-semantic dictionary. As a result, attention calculation achieves linear computational complexity within each window. Extensive experiments across 6 IR tasks confirm the proposed SemanIR's state-of-the-art performance, quantitatively and qualitatively showcasing advancements. The visual results, code, and trained models are available at: https://github.com/Amazingren/SemanIR.\",\n",
       "  238: \"Given the ubiquity of graph data and its applications in diverse domains, building a Graph Foundation Model (GFM) that can work well across different graphs and tasks with a unified backbone has recently garnered significant interests. A major obstacle to achieving this goal stems from the fact that graphs from different domains often exhibit diverse node features. Inspired by multi-modal models that align different modalities with natural language, the text has recently been adopted to provide a unified feature space for diverse graphs. Despite the great potential of these text-space GFMs, current research in this field is hampered by two problems. First, the absence of a comprehensive benchmark with unified problem settings hinders a clear understanding of the comparative effectiveness and practical value of different text-space GFMs. Second, there is a lack of sufficient datasets to thoroughly explore the methods' full potential and verify their effectiveness across diverse settings. To address these issues, we conduct a comprehensive benchmark providing novel text-space datasets and comprehensive evaluation under unified problem settings. Empirical results provide new insights and inspire future research directions. Our code and data are publicly available from https://github.com/CurryTang/TSGFM.\",\n",
       "  239: \"Traditional long-tailed learning methods often perform poorly when dealing with inconsistencies between training and test data distributions, and they cannot flexibly adapt to different user preferences for trade-offs between head and tail classes. To address this issue, we propose a novel long-tailed learning paradigm that aims to tackle distribution shift in real-world scenarios and accommodate different user preferences for the trade-off between head and tail classes. We generate a set of diverse expert models via hypernetworks to cover all possible distribution scenarios, and optimize the model ensemble to adapt to any test distribution. Crucially, in any distribution scenario, we can flexibly output a dedicated model solution that matches the user's preference. Extensive experiments demonstrate that our method not only achieves higher performance ceilings but also effectively overcomes distribution shift while allowing controllable adjustments according to user preferences. We provide new insights and a paradigm for the long-tailed learning problem, greatly expanding its applicability in practical scenarios. The code can be found here: https://github.com/DataLab-atom/PRL.\",\n",
       "  240: 'We consider the problem of online multiclass U-calibration, where a forecaster aims to make sequential distributional predictions over $K$ classes with low U-calibration error, that is, low regret with respect to all bounded proper losses simultaneously.    Kleinberg et al. (2023) developed an algorithm with U-calibration error $\\\\mathcal{O}(K\\\\sqrt{T})$ after $T$ rounds and raised the open question of what the optimal bound is.    We resolve this question by showing that the optimal U-calibration error is $\\\\Theta(\\\\sqrt{KT})$ --- we start with a simple observation that the Follow-the-Perturbed-Leader algorithm of Daskalakis and Syrgkanis (2016) achieves this upper bound, followed by a matching lower bound constructed with a specific proper loss (which, as a side result, also proves the optimality of the algorithm of Daskalakis and Syrgkanis (2016) in the context of online learning against an adversary with finite choices).  We also strengthen our results under natural assumptions on the loss functions, including $\\\\Theta(\\\\log T)$ U-calibration error for Lipschitz proper losses, $\\\\mathcal{O}(\\\\log T)$ U-calibration error for a certain class of decomposable proper losses, U-calibration error bounds for proper losses with a low covering number, and others.',\n",
       "  241: 'Prototyping complex computer-aided design (CAD) models in modern softwares can be very time-consuming. This is due to the lack of intelligent systems that can quickly generate simpler intermediate parts. We propose Text2CAD, the first AI framework for generating text-to-parametric CAD models using designer-friendly instructions for all skill levels. Furthermore, we introduce a data annotation pipeline for generating text prompts based on natural language instructions for the DeepCAD dataset using Mistral and LLaVA-NeXT. The dataset contains $\\\\sim170$K models and $\\\\sim660$K text annotations, from abstract CAD descriptions (e.g., _generate two concentric cylinders_) to detailed specifications (e.g., _draw two circles with center_ $(x,y)$ and _radius_ $r_{1}$, $r_{2}$, \\\\textit{and extrude along the normal by} $d$...). Within the Text2CAD framework, we propose an end-to-end transformer-based auto-regressive network to generate parametric CAD models from input texts. We evaluate the performance of our model through a mixture of metrics, including visual quality, parametric precision, and geometrical accuracy. Our proposed framework shows great potential in AI-aided design applications. Project page is available at https://sadilkhan.github.io/text2cad-project/.',\n",
       "  242: 'Recent 3D large reconstruction models (LRMs) can generate high-quality 3D content in sub-seconds by integrating multi-view diffusion models with scalable multi-view reconstructors. Current works further leverage 3D Gaussian Splatting as 3D representation for improved visual quality and rendering efficiency. However, we observe that existing Gaussian reconstruction models often suffer from multi-view inconsistency and blurred textures. We attribute this to the compromise of multi-view information propagation in favor of adopting powerful yet computationally intensive architectures (\\\\eg, Transformers). To address this issue, we introduce MVGamba, a general and lightweight Gaussian reconstruction model featuring a multi-view Gaussian reconstructor based on the RNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal context containing multi-view information for cross-view self-refinement while generating a long sequence of Gaussians for fine-detail modeling with linear complexity.With off-the-shelf multi-view diffusion models integrated, MVGamba unifies 3D generation tasks from a single image, sparse images, or text prompts. Extensive experiments demonstrate that MVGamba outperforms state-of-the-art baselines in all 3D content generation scenarios with approximately only $0.1\\\\times$ of the model size. The codes are available at \\\\url{https://github.com/SkyworkAI/MVGamba}.',\n",
       "  243: 'How to balance the learning ’sensitivity-stability’ upon new task training and memory preserving is critical in CL to resolve catastrophic forgetting. Improving model generalization ability within each learning phase is one solution to help CL learning overcome the gap in the joint knowledge space. Zeroth-order loss landscape sharpness-aware minimization is a strong training regime improving model generalization in transfer learning compared with optimizer like SGD. It has also been introduced into CL to improve memory representation or learning efficiency. However, zeroth-order sharpness alone could favors sharper over flatter minima in certain scenarios, leading to a rather sensitive minima rather than a global optima. To further enhance learning stability, we propose a Continual Flatness (C-Flat) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer and flat minima based CL approaches is presented in this paper, showing that our method can boost CL performance in almost all cases. Code is available at https://github.com/WanNaa/C-Flat.',\n",
       "  244: 'We analyze the capabilities of Transformer language models in learning compositional discrete tasks. To this end, we evaluate training LLaMA models and prompting GPT-4 and Gemini on four tasks demanding to learn a composition of several discrete sub-tasks. In particular, we measure how well these models can reuse primitives observable in the sub-tasks to learn the composition task. Our results indicate that compositional learning in state-of-the-art Transformer language models is highly sample inefficient: LLaMA requires more data samples than relearning all sub-tasks from scratch to learn the compositional task; in-context prompting with few samples is unreliable and fails at executing the sub-tasks or correcting the errors in multi-round code generation. Further, by leveraging complexity theory, we support these findings with a theoretical analysis focused on the sample inefficiency of gradient descent in memorizing feedforward models. We open source our code at https://github.com/IBM/limitations-lm-algorithmic-compositional-learning.',\n",
       "  245: 'Asynchronous stochastic gradient descent (ASGD) has evolved into an indispensable optimization algorithm for training modern large-scale distributed machine learning tasks. Therefore, it is imperative to explore the generalization performance of the ASGD algorithm. However, the existing results are either pessimistic and vacuous or restricted by strict assumptions that fail to reveal the intrinsic impact of asynchronous training on generalization. In this study, we establish sharper stability and generalization bounds for ASGD under much weaker assumptions. Firstly, this paper studies the on-average model stability of ASGD and provides a non-vacuous upper bound on the generalization error, without relying on the Lipschitz assumption. Furthermore, we investigate the excess generalization error of the ASGD algorithm, revealing the effects of asynchronous delay, model initialization, number of training samples and iterations on generalization performance. Secondly, for the first time, this study explores the generalization performance of ASGD in the non-smooth case. We replace smoothness with the much weaker Hölder continuous assumption and achieve similar generalization results as in the smooth case. Finally, we validate our theoretical findings by training numerous machine learning models, including convex problems and non-convex tasks in computer vision and natural language processing.',\n",
       "  246: 'The interest in leveraging physics-based inductive bias in deep learning has resulted in recent development of hybrid deep generative models (hybrid-DGMs)  that integrates known physics-based mathematical expressions in neural generative models. To identify these hybrid-DGMs requires inferring parameters of the physics-based component along with their neural component. The identifiability of these hybrid-DGMs, however, has not yet been theoretically probed or established. How does the existing theory of the un-identifiability of general DGMs apply to hybrid-DGMs? What may be an effective approach to consutrct a hybrid-DGM with theoretically-proven identifiability? This paper provides the first theoretical probe into the identifiability of hybrid-DGMs, and present meta-learning as a novel solution to construct identifiable hybrid-DGMs. On synthetic and real-data benchmarks, we provide strong empirical evidence for the un-identifiability of existing hybrid-DGMs using unconditional priors, and strong identifiability results of the presented meta-formulations of hybrid-DGMs.',\n",
       "  247: \"Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at https://github.com/NVlabs/MaskLLM.\",\n",
       "  248: \"Self-Distillation is a special type of knowledge distillation where the student model has the same architecture as the teacher model. Despite using the same architecture and the same training data, self-distillation has been empirically observed to improve performance, especially when applied repeatedly. For such a process, there is a fundamental question of interest: How much gain is possible by applying multiple steps of self-distillation? To investigate this relative gain, we propose using the simple but canonical task of linear regression. Our analysis shows that the excess risk achieved by multi-step self-distillation can significantly improve upon a single step of self-distillation, reducing the excess risk by a factor of $d$, where $d$ is the input dimension. Empirical results on regression tasks from the UCI repository show a reduction in the learnt model's risk (MSE) by up to $47$%.\",\n",
       "  249: 'Large Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples. Recent works have indicated that the functions learned by ICL can be represented through compressed vectors derived from the transformer. However, the working mechanisms and optimization of these vectors are yet to be thoroughly explored. In this paper, we address this gap by presenting a comprehensive analysis of these compressed vectors, drawing parallels to the parameters trained with gradient descent, and introducing the concept of state vector. Inspired by the works on model soup and momentum-based gradient descent, we propose inner and momentum optimization methods that are applied to refine the state vector progressively as test-time adaptation. Moreover, we simulate state vector aggregation in the multiple example setting, where demonstrations comprising numerous examples are usually too lengthy for regular ICL, and further propose a divide-and-conquer aggregation method to address this challenge. We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks.',\n",
       "  250: 'Solving mathematical problems requires advanced reasoning abilities and presents notable challenges for large language models. Previous works usually synthesize data from proprietary models to augment existing datasets, followed by instruction tuning to achieve top-tier results. However, our analysis of these datasets reveals severe biases towards easy queries, with frequent failures to generate any correct response for the most challenging queries.Hypothesizing that difficult queries are crucial to learning complex reasoning, we propose Difficulty-Aware Rejection Tuning (DART), a method that allocates difficult queries more trials during the synthesis phase, enabling more extensive training on difficult samples.Utilizing DART, we have created new datasets for mathematical problem-solving that focus more on difficult queries and are substantially smaller than previous ones. Remarkably, our synthesis process solely relies on a 7B-sized open-weight model, without reliance on the commonly used proprietary GPT-4.We fine-tune various base models on our datasets ranging from 7B to 70B in size, resulting in a series of strong models called DART-Math.In comprehensive in-domain and out-of-domain evaluation on 6 mathematical benchmarks, DART-Math outperforms vanilla rejection tuning significantly, being superior or comparable to previous arts, despite using much smaller datasets and no proprietary models. Furthermore, our results position our synthetic datasets as the most effective and cost-efficient publicly available resources for advancing mathematical problem-solving. Our datasets, models and code are publicly available at https://github.com/hkust-nlp/dart-math.',\n",
       "  251: 'Thousands of new scientific papers are published each month. Such  information overload complicates researcher efforts to stay current with the state-of-the-art as well as to verify and correctly attribute claims. We pose the following research question: Given a text excerpt referencing a paper, could an LM act as a research assistant to correctly identify the referenced paper? We advance efforts to answer this question by building a benchmark that evaluates the abilities of LMs in citation attribution. Our benchmark, CiteME, consists  of text excerpts from recent machine learning papers, each referencing a single other paper. CiteME use reveals a large gap between frontier LMs and human performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%. We close this gap by introducing CiteAgent, an autonomous system built on the GPT-4o LM that can also search and read papers, which achieves an accuracy of 35.3% on CiteME. Overall, CiteME serves as a challenging testbed for open-ended claim attribution, driving the research community towards a future where any claim made by an LM can be  automatically verified and discarded if found to be incorrect.',\n",
       "  252: \"Existing prompt-tuning methods have demonstrated impressive performances in continual learning (CL), by selecting and updating relevant prompts in the vision-transformer models. On the contrary, this paper aims to learn each task by tuning the prompts in the direction orthogonal to the subspace spanned by previous tasks' features, so as to ensure no interference on tasks that have been learned to overcome catastrophic forgetting in CL. However, different from the orthogonal projection in the traditional CNN architecture, the prompt gradient orthogonal projection in the ViT architecture shows completely different and greater challenges, i.e., 1) the high-order and non-linear self-attention operation; 2) the drift of prompt distribution brought by the LayerNorm in the transformer block. Theoretically, we have finally deduced two consistency conditions to achieve the prompt gradient orthogonal projection, which provide a theoretical guarantee of eliminating interference on previously learned knowledge via the self-attention mechanism in visual prompt tuning. In practice, an effective null-space-based approximation solution has been proposed to implement the prompt gradient orthogonal projection. Extensive experimental results demonstrate the effectiveness of anti-forgetting on four class-incremental benchmarks with diverse pre-trained baseline models, and our approach achieves superior performances to state-of-the-art methods. Our code is available at https://github.com/zugexiaodui/VPTinNSforCL\",\n",
       "  253: 'Generative text-to-image models enable us to synthesize unlimited amounts of images in a controllable manner, spurring many recent efforts to train vision models with synthetic data. However, every synthetic image ultimately originates from the upstream data used to train the generator. Does the intermediate generator provide additional information over directly training on relevant parts of the upstream data? Grounding this question in the setting of image classification, we compare finetuning on task-relevant, targeted synthetic data generated by Stable Diffusion---a generative model trained on the LAION-2B dataset---against finetuning on targeted real images retrieved directly from LAION-2B. We show that while synthetic data can benefit some downstream tasks, it is universally matched or outperformed by real data from the simple retrieval baseline. Our analysis suggests that this underperformance is partially due to generator artifacts and inaccurate task-relevant visual details in the synthetic images. Overall, we argue that targeted retrieval is a critical baseline to consider when training with synthetic data---a baseline that current methods do not yet surpass. We release code, data, and models at https://github.com/scottgeng00/unmet-promise/.',\n",
       "  254: \"Deep predictive models of neuronal activity have recently enabled several new discoveries about the selectivity and invariance of neurons in the visual cortex.These models learn a shared set of nonlinear basis functions, which are linearly combined via a learned weight vector to represent a neuron's function.Such weight vectors, which can be thought as embeddings of neuronal function, have been proposed to define functional cell types via unsupervised clustering.However, as deep models are usually highly overparameterized, the learning problem is unlikely to have a unique solution, which raises the question if such embeddings can be used in a meaningful way for downstream analysis.In this paper, we investigate how stable neuronal embeddings are with respect to changes in model architecture and initialization. We find that $L_1$ regularization to be an important ingredient for structured embeddings and develop an adaptive regularization that adjusts the strength of regularization per neuron.  This regularization improves both predictive performance and how consistently neuronal embeddings cluster across model fits  compared to uniform regularization.To overcome overparametrization, we propose an iterative feature pruning strategy which reduces the dimensionality of performance-optimized models by half without loss of performance and improves the consistency of neuronal embeddings with respect to clustering neurons.Our results suggest that to achieve an objective taxonomy of cell types or a compact representation of the functional landscape, we need novel architectures or learning techniques that improve identifiability. The code is available https://github.com/pollytur/readout_reproducibility.\",\n",
       "  255: 'Large Transformer networks are increasingly used in settings where low inference latency is necessary to enable new applications and improve the end-user experience.However, autoregressive inference is resource intensive and requires parallelism for efficiency.Parallelism introduces collective communication that is both expensive and represents a phase when hardware resources are underutilized.Towards mitigating this, Kraken is an evolution of the standard Transformer architecture that is designed to complement existing tensor parallelism schemes for efficient inference on multi-device systems.By introducing a fixed degree of intra-layer model parallelism, the architecture allows collective operations to be overlapped with compute, decreasing latency and increasing hardware utilization.When trained on OpenWebText, Kraken models reach a similar perplexity as standard Transformers while also preserving their language modeling capabilities as evaluated on the SuperGLUE benchmark.Importantly, when tested on multi-GPU systems using TensorRT-LLM engines, Kraken speeds up Time To First Token by a mean of 35.6% across a range of model sizes, context lengths, and degrees of tensor parallelism.',\n",
       "  256: 'Model-X knockoff has garnered significant attention among various feature selection methods due to its guarantees for controlling the false discovery rate (FDR). Since its introduction in parametric design, knockoff techniques have evolved to handle arbitrary data distributions using deep learning-based generative models. However, we have observed limitations in the current implementations of the deep Model-X knockoff framework. Notably, the \"swap property\" that knockoffs require often faces challenges at the sample level, resulting in diminished selection power. To address these issues, we develop \"Deep Dependency Regularized Knockoff (DeepDRK),\" a distribution-free deep learning method that effectively balances FDR and power. In DeepDRK, we introduce a novel formulation of the knockoff model as a learning problem under multi-source adversarial attacks. By employing an innovative perturbation technique, we achieve lower FDR and higher power. Our model outperforms existing benchmarks across synthetic, semi-synthetic, and real-world datasets, particularly when sample sizes are small and data distributions are non-Gaussian.',\n",
       "  257: 'Offline optimization has recently emerged as an increasingly popular approach to mitigate the prohibitively expensive cost of online experimentation. The key idea is to learn a surrogate of the black-box function that underlines the target experiment using a static (offline) dataset of its previous input-output queries. Such an approach is, however, fraught with an out-of-distribution issue where the learned surrogate becomes inaccurate outside the offline data regimes. To mitigate this, existing offline optimizers have proposed numerous conditioning techniques to prevent the learned surrogate from being too erratic. Nonetheless, such conditioning strategies are often specific to particular surrogate or search models, which might not generalize to a different model choice. This motivates us to develop a model-agnostic approach instead, which incorporates a notion of model sharpness into the training loss of the surrogate as a regularizer. Our approach is supported by a new theoretical analysis demonstrating that reducing surrogate sharpness on the offline dataset provably reduces its generalized sharpness on unseen data. Our analysis extends existing theories from bounding generalized prediction loss (on unseen data) with loss sharpness to bounding the worst-case generalized surrogate sharpness with its empirical estimate on training data, providing a new perspective on sharpness regularization. Our extensive experimentation on a diverse range of optimization tasks also shows that reducing surrogate sharpness often leads to significant improvement, marking (up to) a noticeable 9.6% performance boost. Our code is publicly available at https://github.com/cuong-dm/IGNITE.',\n",
       "  258: 'Physical adversarial attacks can deceive deep neural networks (DNNs), leading to erroneous predictions in real-world scenarios. To uncover potential security risks, attacking the safety-critical task of person detection has garnered significant attention. However, we observe that existing attack methods overlook the pivotal role of the camera, involving capturing real-world scenes and converting them into digital images, in the physical adversarial attack workflow. This oversight leads to instability and challenges in reproducing these attacks. In this work, we revisit patch-based attacks against person detectors and introduce a camera-agnostic physical adversarial attack to mitigate this limitation. Specifically, we construct a differentiable camera Image Signal Processing (ISP) proxy network to compensate for the physical-to-digital transition gap. Furthermore, the camera ISP proxy network serves as a defense module, forming an adversarial optimization framework with the attack module. The attack module optimizes adversarial patches to maximize effectiveness, while the defense module optimizes the conditional parameters of the camera ISP proxy network to minimize attack effectiveness. These modules engage in an adversarial game, enhancing cross-camera stability. Experimental results demonstrate that our proposed Camera-Agnostic Patch (CAP) attack effectively conceals persons from detectors across various imaging hardware, including two distinct cameras and four smartphones.',\n",
       "  259: 'Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We present the Graph Diffusion Transformer (Graph DiT) for multi-conditional molecular generation. Graph DiT has a condition encoder to learn the representation of numerical and categorical properties and utilizes a Transformer-based graph denoiser to achieve molecular graph denoising under conditions. Unlike previous graph diffusion models that add noise separately on the atoms and bonds in the forward diffusion process, we propose a graph-dependent noise model for training Graph DiT, designed to accurately estimate graph-related noise in molecules. We extensively validate the Graph DiT for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution learning to condition control for molecular properties. A polymer inverse design task for gas separation with feedback from domain experts further demonstrates its practical utility. The code is available at https://github.com/liugangcode/Graph-DiT.',\n",
       "  260: \"We introduce WildGuard---an open, light-weight moderation tool for LLM safety that achieves three goals: (1) identifying malicious intent in user prompts, (2) detecting safety risks of model responses, and (3) determining model refusal rate. Together, WildGuard serves the increasing needs for automatic safety moderation and evaluation of LLM interactions, providing a one-stop tool with enhanced accuracy and broad coverage across 13 risk categories. While existing open moderation tools such as Llama-Guard2 score reasonably well in classifying straightforward model interactions, they lag far behind a prompted GPT-4, especially in identifying adversarial jailbreaks  and in evaluating models' refusals, a key measure for evaluating safety behaviors in model responses. To address these challenges, we construct WildGuardMix, a large-scale and carefully balanced multi-task safety moderation dataset with 92K labeled examples that cover vanilla (direct) prompts and adversarial jailbreaks, paired with various refusal and compliance responses. WildGuardMix is a combination of WildGuardTrain, the training data of WildGuard, and WildGuardTest, a high-quality human-annotated moderation test set with 5K labeled items covering broad risk scenarios.Through extensive evaluations on WildGuardTest and ten existing public benchmarks, we show that WildGuard establishes state-of-the-art performance in open-source safety moderation across all the three tasks compared to ten strong existing open-source moderation models (e.g., up to 25.3% improvement on refusal detection). Importantly, WildGuard matches and sometimes exceeds GPT-4 performance (e.g., up to 4.8% improvement on prompt harmfulness identification). WildGuard serves as a highly effective safety moderator in an LLM interface, reducing the success rate of jailbreak attacks from 79.8%  to 2.4%. We will make all our data, models and training/evaluation code publicly available under CC BY 4.0 license.\",\n",
       "  261: 'Current approximate posteriors in Bayesian neural networks (BNNs) exhibit a crucial limitation: they fail to maintain invariance under reparameterization, i.e. BNNs assign different posterior densities to different parametrizations of identical functions. This creates a fundamental flaw in the application of Bayesian principles as it breaks the correspondence between uncertainty over the parameters with uncertainty over the parametrized function. In this paper, we investigate this issue in the context of the increasingly popular linearized Laplace approximation. Specifically, it has been observed that linearized predictives alleviate the common underfitting problems of the Laplace approximation. We develop a new geometric view of reparametrizations from which we explain the success of linearization. Moreover, we demonstrate that these reparameterization invariance properties can be extended to the original neural network predictive using a Riemannian diffusion process giving a straightforward algorithm for approximate posterior sampling, which empirically improves posterior fit.',\n",
       "  262: 'Adaptive Risk Control (ARC) is an online calibration strategy based on set prediction that offers worst-case deterministic long-term risk control, as well as statistical marginal coverage guarantees. ARC adjusts the size of the prediction set by varying a single scalar threshold based on feedback from past decisions. In this work, we introduce Localized Adaptive Risk Control (L-ARC), an online calibration scheme that targets statistical localized risk guarantees ranging from conditional risk to marginal risk, while preserving the worst-case performance of ARC. L-ARC updates a threshold function within a reproducing kernel Hilbert space (RKHS), with the kernel determining the level of localization of the statistical risk guarantee. The theoretical results highlight a trade-off between localization of the statistical risk and convergence speed to the long-term risk target. Thanks to localization, L-ARC is demonstrated via experiments to produce prediction sets with risk guarantees across different data subpopulations, significantly improving the fairness of the calibrated model for tasks such as image segmentation and beam selection in wireless networks.',\n",
       "  263: \"Graph Neural Networks (GNNs) often perform better for high-degree nodes than low-degree nodes on node classification tasks. This degree bias can reinforce social marginalization by, e.g., privileging celebrities and other high-degree actors in social networks during social and content recommendation. While researchers have proposed numerous hypotheses for why GNN degree bias occurs, we find via a survey of 38 degree bias papers that these hypotheses are often not rigorously validated, and can even be contradictory. Thus, we provide an analysis of the origins of degree bias in message-passing GNNs with different graph filters. We prove that high-degree test nodes tend to have a lower probability of misclassification regardless of how GNNs are trained. Moreover, we show that degree bias arises from a variety of factors that are associated with a node's degree (e.g., homophily of neighbors, diversity of neighbors). Furthermore, we show that during training, some GNNs may adjust their loss on low-degree nodes more slowly than on high-degree nodes; however, with sufficiently many epochs of training, message-passing GNNs can achieve their maximum possible training accuracy, which is not significantly limited by their expressive power. Throughout our analysis, we connect our findings to previously-proposed hypotheses for the origins of degree bias, supporting and unifying some while drawing doubt to others. We validate our theoretical findings on 8 common real-world networks, and based on our theoretical and empirical insights, describe a roadmap to alleviate degree bias.\",\n",
       "  264: 'In federated learning, it is common to assume that clients are always available to participate in training, which may not be feasible with user devices in practice. Recent works analyze federated learning under more realistic participation patterns, such as cyclic client availability or arbitrary participation. However, all such works either require strong assumptions (e.g., all clients participate almost surely within a bounded window), do not achieve linear speedup and reduced communication rounds, or are not applicable in the general non-convex setting. In this work, we focus on nonconvex optimization and consider participation patterns in which the chance of participation over a fixed window of rounds is equal among all clients, which includes cyclic client availability as a special case. Under this setting, we propose a new algorithm, named Amplified SCAFFOLD, and prove that it achieves linear speedup, reduced communication, and resilience to data heterogeneity simultaneously. In particular, for cyclic participation, our algorithm is proved to enjoy $\\\\mathcal{O}(\\\\epsilon^{-2})$ communication rounds to find an $\\\\epsilon$-stationary point in the non-convex stochastic setting. In contrast, the prior work under the same setting requires $\\\\mathcal{O}(\\\\kappa^2 \\\\epsilon^{-4})$ communication rounds, where $\\\\kappa$ denotes the data heterogeneity. Therefore, our algorithm significantly reduces communication rounds due to better dependency in terms of $\\\\epsilon$ and $\\\\kappa$. Our analysis relies on a fine-grained treatment of the nested dependence between client participation and errors in the control variates, which results in tighter guarantees than previous work. We also provide experimental results with (1) synthetic data and (2) real-world data with a large number of clients $(N = 250)$, demonstrating the effectiveness of our algorithm under periodic client participation.',\n",
       "  265: 'Multiple instance learning (MIL) is an effective and widely used approach for weakly supervised machine learning. In histopathology, MIL models have achieved remarkable success in tasks like tumor detection, biomarker prediction, and outcome prognostication. However, MIL explanation methods are still lagging behind, as they are limited to small bag sizes or disregard instance interactions. We revisit MIL through the lens of explainable AI (XAI) and introduce xMIL, a refined framework with more general assumptions. We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets. Our approach consistently outperforms previous explanation attempts with particularly improved faithfulness scores on challenging biomarker prediction tasks. Finally, we showcase how xMIL explanations enable pathologists to extract insights from MIL models, representing a significant advance for knowledge discovery and model debugging in digital histopathology.',\n",
       "  266: 'We study the generalized linear contextual bandit problem within the constraints of limited adaptivity.  In this paper, we present two algorithms, B-GLinCB and RS-GLinCB, that address, respectively, two prevalent limited adaptivity settings. Given a budget $M$ on the number of policy updates, in the first setting, the algorithm needs to decide upfront $M$ rounds at which it will update its policy, while in the second setting it can adaptively perform $M$ policy updates during its course. For the first setting, we design an algorithm B-GLinCB, that incurs $\\\\tilde{O}(\\\\sqrt{T})$ regret when $M = \\\\Omega( \\\\log{\\\\log T} )$ and the arm feature vectors are generated stochastically. For the second setting, we design an algorithm RS-GLinCB that updates its policy $\\\\tilde{O}(\\\\log^2 T)$ times and achieves a regret of $\\\\tilde{O}(\\\\sqrt{T})$ even when the arm feature vectors are adversarially generated. Notably, in these bounds, we manage to eliminate the dependence on a key instance dependent parameter $\\\\kappa$, that captures non-linearity of the underlying reward model. Our novel approach for removing this dependence for generalized linear contextual bandits might be of independent interest.',\n",
       "  267: \"The paper proposes a novel evaluation metric for automatic medical report generation from X-ray images, VLScore. It aims to overcome the limitations of existing evaluation methods, which either focus solely on textual similarities, ignoring clinical aspects, or concentrate only on a single clinical aspect, the pathology, neglecting all other factors. The key idea of our metric is to measure the similarity between radiology reports while considering the corresponding image. We demonstrate the benefit of our metric through evaluation on a dataset where radiologists marked errors in pairs of reports, showing notable alignment with radiologists' judgments. In addition, we provide a new dataset for evaluating metrics. This dataset includes well-designed perturbations that distinguish between significant modifications (e.g., removal of a diagnosis) and insignificant ones. It highlights the weaknesses in current evaluation metrics and provides a clear framework for analysis.\",\n",
       "  268: 'Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.',\n",
       "  269: \"Estimating individualized treatment rules (ITRs) is fundamental in causal inference, particularly for precision medicine applications. Traditional ITR estimation methods rely on inverse probability weighting (IPW) to address confounding factors and $L_{1}$-penalization for simplicity and interpretability. However, IPW can introduce statistical bias without precise propensity score modeling, while $L_1$-penalization makes the objective non-smooth, leading to computational bias and requiring subgradient methods. In this paper, we propose a unified ITR estimation framework formulated as a constrained, weighted, and smooth convex optimization problem. The optimal ITR can be robustly and effectively computed by projected gradient descent. Our comprehensive theoretical analysis reveals that weights that balance the spectrum of a `weighted design matrix' improve both the optimization and likelihood landscapes, yielding improved computational and statistical estimation guarantees. In particular, this is achieved by distributional covariate balancing weights, which are model-free alternatives to IPW. Extensive simulations and applications demonstrate that our framework achieves significant gains in both robustness and effectiveness for ITR learning against existing methods.\",\n",
       "  270: 'Follow-the-Regularized-Leader (FTRL) is a powerful framework for various online learning problems. By designing its regularizer and learning rate to be adaptive to past observations, FTRL is known to work adaptively to various properties of an underlying environment. However, most existing adaptive learning rates are for online learning problems with a minimax regret of $\\\\Theta(\\\\sqrt{T})$ for the number of rounds $T$, and there are only a few studies on adaptive learning rates for problems with a minimax regret of $\\\\Theta(T^{2/3})$, which include several important problems dealing with indirect feedback. To address this limitation, we establish a new adaptive learning rate framework for problems with a minimax regret of $\\\\Theta(T^{2/3})$. Our learning rate is designed by matching the stability, penalty, and bias terms that naturally appear in regret upper bounds for problems with a minimax regret of $\\\\Theta(T^{2/3})$. As applications of this framework, we consider three major problems with a minimax regret of $\\\\Theta(T^{2/3})$: partial monitoring, graph bandits, and multi-armed bandits with paid observations. We show that FTRL with our learning rate and the Tsallis entropy regularizer improves existing Best-of-Both-Worlds (BOBW) regret upper bounds, which achieve simultaneous optimality in the stochastic and adversarial regimes. The resulting learning rate is surprisingly simple compared to the existing learning rates for BOBW algorithms for problems with a minimax regret of $\\\\Theta(T^{2/3})$.',\n",
       "  271: 'The interaction between Fourier transform and deep learning opens new avenues for long-term time series forecasting (LTSF). We propose a new perspective to reconsider the Fourier transform from a basis functions perspective. Specifically, the real and imaginary parts of the frequency components can be viewed as the coefficients of cosine and sine basis functions at tiered frequency levels, respectively. We argue existing Fourier-based methods do not involve basis functions thus fail to interpret frequency coefficients precisely and consider the time-frequency relationship sufficiently, leading to inconsistent starting cycles and inconsistent series length issues. Accordingly, a novel Fourier basis mapping (FBM) method addresses these issues by mixing time and frequency domain features through Fourier basis expansion. Differing from existing approaches, FBM (i) embeds the discrete Fourier transform with basis functions, and then (ii) can enable plug-and-play in various types of neural networks for better performance. FBM extracts explicit frequency features while preserving temporal characteristics, enabling the mapping network to capture the time-frequency relationships. By incorporating our unique time-frequency features, the FBM variants can enhance any type of networks like linear, multilayer-perceptron-based, transformer-based, and Fourier-based networks, achieving state-of-the-art LTSF results on diverse real-world datasets with just one or three fully connected layers. The code is available at: https://github.com/runze1223/Fourier-Basis-Mapping.',\n",
       "  272: \"We address a generalization of the bandit with knapsacks problem, where a learner aims to maximize rewards while satisfying an arbitrary set of long-term constraints. Our goal is to design best-of-both-worlds algorithms that perform optimally under both stochastic and adversarial constraints. Previous works address this problem via primal-dual methods, and require some stringent assumptions, namely the Slater's condition, and in adversarial settings, they either assume knowledge of a lower bound on the Slater's parameter, or impose strong requirements on the primal and dual regret minimizers such as requiring weak adaptivity. We propose an alternative and more natural approach based on optimistic estimations of the constraints. Surprisingly, we show that estimating the constraints with an UCB-like approach guarantees optimal performances.Our algorithm consists of two main components: (i) a regret minimizer working on moving strategy sets and (ii) an estimate of the feasible set as an optimistic weighted empirical mean of previous samples. The key challenge in this approach is designing adaptive weights that meet the different requirements for stochastic and adversarial constraints. Our algorithm is significantly simpler than previous approaches, and has a cleaner analysis. Moreover, ours is the first best-of-both-worlds algorithm providing bounds logarithmic in the number of constraints. Additionally, in stochastic settings, it provides $\\\\widetilde O(\\\\sqrt{T})$ regret without Slater's condition.\",\n",
       "  273: 'Reinforcement Learning (RL) encompasses diverse paradigms, including model-based RL, policy-based RL, and value-based RL, each tailored to approximate the model, optimal policy, and optimal value function, respectively. This work investigates the potential hierarchy of representation complexity among these RL paradigms. By utilizing computational complexity measures, including time complexity and circuit complexity, we theoretically unveil a potential representation complexity hierarchy within RL. We find that representing the model emerges as the easiest task, followed by the optimal policy, while representing the optimal value function presents the most intricate challenge. Additionally, we reaffirm this hierarchy from the perspective of the expressiveness of Multi-Layer Perceptrons (MLPs), which align more closely with practical deep RL and contribute to a completely new perspective in theoretical studying representation complexity in RL. Finally, we conduct deep RL experiments to validate our theoretical findings.',\n",
       "  274: 'Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs, especially when the resolution of the input image is high or when the interested region that could provide key information for answering the question is small. To address these challenges, we collect and introduce the large-scale Visual CoT dataset comprising 438k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Additionally, about 98k pairs of them are annotated with detailed reasoning steps. Importantly, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We also introduce the related benchmark to evaluate the MLLMs in scenarios requiring specific local region identification.Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset, benchmark, and pre-trained models are available on this website to support further research in this area.',\n",
       "  275: \"Invisible watermarks safeguard images' copyrights by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models.We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and extensive empirical evaluations, we demonstrate that pixel-level invisible watermarks are vulnerable to this regeneration attack.Our results reveal that, across four different pixel-level watermarking schemes, the proposed method consistently achieves superior performance compared to existing attack techniques, with lower detection rates and higher image quality.However, watermarks that keep the image semantically similar can be an alternative defense against our attacks.Our finding underscores the need for a shift in research/industry emphasis from invisible watermarks to semantic-preserving watermarks. Code is available at https://github.com/XuandongZhao/WatermarkAttacker\",\n",
       "  276: \"This paper studies a risk minimization problem with decision dependent data distribution. The problem pertains to the performative prediction setting in which a trained model can affect the outcome estimated by the model. Such dependency creates a feedback loop that influences the stability of optimization algorithms such as stochastic gradient descent (SGD). We present the first study on performative prediction with smooth but possibly non-convex loss. We analyze a greedy deployment scheme with SGD (SGD-GD). Note that in the literature, SGD-GD is often studied with strongly convex loss. We first propose the definition of stationary performative stable (SPS) solutions through relaxing the popular performative stable condition. We then prove that SGD-GD converges to a biased SPS solution in expectation. We consider two conditions of sensitivity on the distribution shifts: (i) the sensitivity is characterized by Wasserstein-1 distance and the loss is Lipschitz w.r.t.~data samples, or (ii) the sensitivity is characterized by total variation (TV) divergence and the loss is bounded. In both conditions, the bias levels are proportional to the stochastic gradient's variance and sensitivity level. Our analysis is extended to a lazy deployment scheme where models are deployed once per several SGD updates, and we show that it converges to an SPS solution with reduced bias. Numerical experiments corroborate our theories.\",\n",
       "  277: \"Generating natural and meaningful responses to communicate with multi-modal human inputs is a fundamental capability of Large Vision-Language Models (LVLMs). While current open-source LVLMs demonstrate promising performance in simplified scenarios such as single-turn single-image input, they fall short in real-world conversation scenarios such as following instructions in a long context history with multi-turn and multi-images. Existing LVLM benchmarks primarily focus on single-choice questions or short-form responses, which do not adequately assess the capabilities of LVLMs in real-world human-AI interaction applications. Therefore, we introduce MMDU, a comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning dataset, designed to evaluate and improve LVLMs' abilities in multi-turn and multi-image conversations. We employ the clustering algorithm to find the relevant images and textual descriptions from the open-source Wikipedia and construct the question-answer pairs by human annotators with the assistance of the GPT-4o model.MMDU has a maximum of 18k image+text tokens, 20 images, and 27 turns, which is at least 5x longer than previous benchmarks and poses challenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs using MMDU reveals that open-source LVLMs lag behind closed-source counterparts due to limited conversational instruction tuning data.We demonstrate that fine-tuning open-source LVLMs on MMDU-45k significantly address this gap, generating longer and more accurate conversations, and improving scores on MMDU and existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA: +1.2%). Our contributions pave the way for bridging the gap between current LVLM models and real-world application demands. The links to MMDU, and MMDU-45k are available in the supplementary material.\",\n",
       "  278: 'Recent years have witnessed a clear trend towards language models with an ever-increasing number of parameters, as well as the growing training overhead and memory usage. Distributed training, particularly through Sharded Data Parallelism (ShardedDP) which partitions optimizer states among workers, has emerged as a crucial technique to mitigate training time and memory usage. Yet, a major challenge in the scalability of ShardedDP is the intensive communication of weights and gradients. While compression techniques can alleviate this issue, they often result in worse accuracy. Driven by this limitation, we propose SDP4Bit (Toward 4Bit Communication Quantization in Sharded Data Parallelism for LLM Training), which effectively reduces the communication of weights and gradients to nearly 4 bits via two novel techniques: quantization on weight differences, and two-level gradient smooth quantization. Furthermore, SDP4Bit presents an algorithm-system co-design with runtime optimization to minimize the computation overhead of compression. Additional to the theoretical guarantees of convergence, we empirically evaluate the accuracy of SDP4Bit on the pre-training of GPT models with up to 6.7 billion parameters, and the results demonstrate a negligible impact on training loss. Furthermore, speed experiments show that SDP4Bit achieves up to 4.08× speedup in end-to-end throughput on a scale of 128 GPUs.',\n",
       "  279: 'Collaborative filtering (CF) has exhibited prominent results for recommender systems and been broadly utilized for real-world applications.A branch of research enhances CF methods by message passing (MP) used in graph neural networks, due to its strong capabilities of extracting knowledge from graph-structured data, like user-item bipartite graphs that naturally exist in CF. They assume that MP helps CF methods in a manner akin to its benefits for graph-based learning tasks in general (e.g., node classification). However, even though MP empirically improves CF, whether or not this assumption is correct still needs verification. To address this gap, we formally investigate why MP helps CF from multiple perspectives and show that many assumptions made by previous works are not entirely accurate. With our curated ablation studies and theoretical analyses, we discover that (i) MP improves the CF performance primarily by additional representations passed from neighbors during the forward pass instead of additional gradient updates to neighbor representations during the model back-propagation and (ii) MP usually helps low-degree nodes more than high-degree nodes.}Utilizing these novel findings, we present Test-time Aggregation for Collaborative Filtering, namely TAG-CF, a test-time augmentation framework that only conducts MP once at inference time. The key novelty of TAG-CF is that it effectively utilizes graph knowledge while circumventing most of notorious computational overheads of MP. Besides, TAG-CF is extremely versatile can be used as a plug-and-play module to enhance representations trained by different CF supervision signals. Evaluated on six datasets (i.e., five academic benchmarks and one real-world industrial dataset), TAG-CF consistently improves the recommendation performance of CF methods without graph by up to 39.2% on cold users and 31.7% on all users, with little to no extra computational overheads. Furthermore, compared with trending graph-enhanced CF methods, TAG-CF delivers comparable or even better performance with less than 1% of their total training times. Our code is publicly available at https://github.com/snap-research/Test-time-Aggregation-for-CF.',\n",
       "  280: 'In this study, we consider the infinitely many-armed bandit problems in a rested rotting setting, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged. We explore two scenarios regarding the rotting of rewards: one in which the cumulative amount of rotting is bounded by $V_T$, referred to as the slow-rotting case, and the other in which the cumulative number of rotting instances is bounded by $S_T$, referred to as the abrupt-rotting case. To address the challenge posed by rotting rewards, we introduce an algorithm that utilizes UCB with an adaptive sliding window, designed to manage the bias and variance trade-off arising due to rotting rewards. Our proposed algorithm achieves tight regret bounds for both slow and abrupt rotting scenarios. Lastly, we demonstrate the performance of our algorithm using numerical experiments.',\n",
       "  281: '$\\\\newcommand{\\\\Tr}{\\\\mathsf{Tr}}$We consider the problem of high-dimensional heavy-tailed statistical estimation in the streaming setting, which is much harder than the traditional batch setting due to memory constraints. We cast this problem as stochastic convex optimization with heavy tailed stochastic gradients, and prove that the widely used Clipped-SGD algorithm attains near-optimal sub-Gaussian statistical rates whenever the second moment of the stochastic gradient noise is finite. More precisely, with $T$ samples, we show that Clipped-SGD, for smooth and strongly convex objectives, achieves an error of $\\\\sqrt{\\\\frac{\\\\Tr(\\\\Sigma)+\\\\sqrt{\\\\Tr(\\\\Sigma)\\\\\\\\|\\\\Sigma\\\\\\\\|_2}\\\\ln(\\\\tfrac{\\\\ln(T)}{\\\\delta})}{T}}$ with probability $1-\\\\delta$, where $\\\\Sigma$ is the covariance of the clipped gradient. Note that the fluctuations (depending on $\\\\tfrac{1}{\\\\delta}$) are of lower order than the term $\\\\Tr(\\\\Sigma)$.This improves upon the current best rate of$\\\\sqrt{\\\\frac{\\\\Tr(\\\\Sigma)\\\\ln(\\\\tfrac{1}{\\\\delta})}{T}}$ for Clipped-SGD, known \\\\emph{only} for smooth and strongly convex objectives. Our results also extend to smooth convex and lipschitz convex objectives. Key to our result is a novel iterative refinement strategy for martingale concentration, improving upon the PAC-Bayes approach of \\\\citet{catoni2018dimension}.',\n",
       "  282: \"Uncertainty quantification in Large Language Models (LLMs) is crucial for applications where safety and reliability are important. In particular, uncertainty can be used to improve the trustworthiness of LLMs by detecting factually incorrect model responses, commonly called hallucinations. Critically, one should seek to capture the model's semantic uncertainty, i.e., the uncertainty over the meanings of LLM outputs, rather than uncertainty over lexical or syntactic variations that do not affect answer correctness.To address this problem, we propose Kernel Language Entropy (KLE), a novel method for uncertainty estimation in white- and black-box LLMs. KLE defines positive semidefinite unit trace kernels to encode the semantic similarities of LLM outputs and quantifies uncertainty using the von Neumann entropy. It considers pairwise semantic dependencies between answers (or semantic clusters), providing more fine-grained uncertainty estimates than previous methods based on hard clustering of answers. We theoretically prove that KLE generalizes the previous state-of-the-art method called semantic entropy and empirically demonstrate that it improves uncertainty quantification performance across multiple natural language generation datasets and LLM architectures.\",\n",
       "  283: \"Recommending out-of-vocabulary (OOV) items is a challenging problem since the in-vocabulary (IV) items have well-trained behavioral embeddings but the OOV items only have content features. Current OOV recommendation models often generate 'makeshift' embeddings for OOV items from content features and then jointly recommend with the `makeshift' OOV item embeddings and the behavioral IV item embeddings. However, merely using the 'makeshift' embedding will result in suboptimal recommendation performance due to the substantial gap between the content feature and the behavioral embeddings. To bridge the gap, we propose a novel User Sequence IMagination (USIM) fine-tuning framework, which first imagines the user sequences and then refines the generated OOV embeddings with the user behavioral embeddings. Specifically, we frame the user sequence imagination as a reinforcement learning problem and develop a recommendation-focused reward function to evaluate to what extent a user can help recommend the OOV items. Besides, we propose an embedding-driven transition function to model the embedding transition after imaging a user. USIM has been deployed on a prominent e-commerce platform for months, offering recommendations for millions of OOV items and billions of users. Extensive experiments demonstrate that USIM outperforms traditional generative models in OOV item recommendation performance across traditional collaborative filtering and GNN-based collaborative filtering models.\",\n",
       "  284: \"The pretraining data of today's strongest language models remains opaque, even when their parameters are open-sourced.In particular, little is known about the proportions of different domains, languages, or code represented in the data. While a long line of membership inference attacks aim to identify training examples on an instance level, they do not extend easily to global statistics about the corpus. In this work, we tackle a task which we call data mixture inference, which aims to uncover the distributional make-up of the pretraining data. We introduce a novel attack based on a previously overlooked source of information — byte-pair encoding (BPE) tokenizers, used by the vast majority of modern language models. Our key insight is that the ordered vocabulary learned by a BPE tokenizer naturally reveals information about the token frequencies in its training data: the first token is the most common byte pair, the second is the most common pair after merging the first token, and so on. Given a tokenizer's merge list along with data samples for each category of interest (e.g., different natural languages), we formulate a linear program that solves for the relative proportion of each category in the tokenizer's training set. Importantly, to the extent to which tokenizer training data is representative of the pretraining data, we indirectly learn about the pretraining data. In controlled experiments, we show that our attack can recover mixture ratios with high precision for tokenizers trained on known mixtures of natural languages, programming languages, and data sources. We then apply our approach to off-the-shelf tokenizers released alongside recent LMs. We confirm much publicly disclosed information about these models, and also make several new inferences: GPT-4o is much more multilingual than its predecessors, training on 10x more non-English data than GPT-3.5, Llama 3 and Claude are trained on predominantly code, and many recent models are trained on 7-16% books. We hope our work sheds light on current design practices for pretraining data, and inspires continued research into data mixture inference for LMs.\",\n",
       "  285: \"Regularization is a critical component in deep learning. The most commonly used approach, weight decay, applies a constant penalty coefficient uniformly across all parameters. This may be overly restrictive for some parameters, while insufficient for others. To address this, we present Constrained Parameter Regularization (CPR) as an alternative to traditional weight decay. Unlike the uniform application of a single penalty, CPR enforces an upper bound on a statistical measure, such as the L$_2$-norm, of individual parameter matrices. Consequently, learning becomes a constraint optimization problem, which we tackle using an adaptation of the augmented Lagrangian method. CPR introduces only a minor runtime overhead and only requires setting an upper bound. We propose simple yet efficient mechanisms for initializing this bound, making CPR rely on no hyperparameter or one, akin to weight decay. Our empirical studies on computer vision and language modeling tasks demonstrate CPR's effectiveness. The results show that CPR can outperform traditional weight decay and increase performance in pre-training and fine-tuning.\",\n",
       "  286: \"State-of-the-art neural network training methods depend on the gradient of the network function. Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks. To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL). This method works well in practice but lacks theoretical foundation.The neural tangent kernel (NTK) has proven successful in the analysis of gradient descent. Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL. First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit. To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL. We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor. Further, we illustrate our findings with numerical experiments. Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL.\",\n",
       "  287: 'Current research in adversarial robustness of LLMs focuses on \\\\textit{discrete} input manipulations in the natural language space, which can be directly transferred to \\\\textit{closed-source} models. However, this approach neglects the steady progression of \\\\textit{open-source} models. As open-source models advance in capability, ensuring their safety becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the \\\\textit{embedding space attack}, which directly attacks the \\\\textit{continuous} embedding representation of input tokens.We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Additionally, we demonstrate that models compromised by embedding attacks can be used to create discrete jailbreaks in natural language. Lastly, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source LLMs.',\n",
       "  288: 'Recent work on pruning large language models (LLMs) has shown that one can eliminate a large number of parameters without compromising performance, making pruning a promising strategy to reduce LLM model size. Existing LLM pruning strategies typically assign uniform pruning ratios across layers, limiting overall pruning ability; and recent work on layerwise pruning of LLMs is often based on heuristics that can easily lead to suboptimal performance. In this paper, we leverage Heavy-Tailed Self-Regularization (HT-SR) Theory, in particular the shape of empirical spectral densities (ESDs) of weight matrices, to design improved layerwise pruning ratios for LLMs. Our analysis reveals a wide variability in how well-trained, and thus relatedly how prunable, different layers of an LLM are. Based on this, we propose AlphaPruning, which uses shape metrics to allocate layerwise sparsity ratios in a more theoretically-principled manner. AlphaPruning can be used in conjunction with multiple existing LLM pruning methods. Our empirical results show that AlphaPruning prunes LLaMA-7B to 80% sparsity while maintaining reasonable perplexity, marking a first in the literature on LLMs.',\n",
       "  289: 'Large-scale vision-language models like CLIP have demonstrated impressive open-vocabulary capabilities for image-level tasks, excelling in recognizing what objects are present. However, they struggle with pixel-level recognition tasks like semantic segmentation, which require understanding where the objects are located. In this work, we propose a novel method, PixelCLIP, to adapt the CLIP image encoder for pixel-level understanding by guiding the model on where, which is achieved using unlabeled images and masks generated from vision foundation models such as SAM and DINO. To address the challenges of leveraging masks without semantic labels, we devise an online clustering algorithm using learnable class names to acquire general semantic concepts. PixelCLIP shows significant performance improvements over CLIP and competitive results compared to caption-supervised methods in open-vocabulary semantic segmentation.',\n",
       "  290: 'Causal discovery is a fundamental problem with applications spanning various areas in science and engineering. It is well understood that solely using observational data, one can only orient the causal graph up to its Markov equivalence class, necessitating interventional data to learn the complete causal graph. Most works in the literature design causal discovery policies with perfect interventions, i.e., they have access to infinite interventional samples. This study considers a Bayesian approach for learning causal graphs with limited interventional samples, mirroring real-world scenarios where such samples are usually costly to obtain. By leveraging the recent result of Wienöbst et al. [2023] on uniform DAG sampling in polynomial time, we can efficiently enumerate all the cut configurations and their corresponding interventional distributions of a target set, and further track their posteriors. Given any number of interventional samples, our proposed algorithm randomly intervenes on a set of target vertices that cut all the edges in the graph and returns a causal graph according to the posterior of each target set. When the number of interventional samples is large enough, we show theoretically that our proposed algorithm will return the true causal graph with high probability. We compare our algorithm against various baseline methods on simulated datasets, demonstrating its superior accuracy measured by the structural Hamming distance between the learned DAG and the ground truth. Additionally, we present a case study showing how this algorithm could be modified to answer more general causal questions without learning the whole graph. As an example, we illustrate that our method can be used to estimate the causal effect of a variable that cannot be intervened.',\n",
       "  291: 'Prior multi-frame optical flow methods typically estimate flow repeatedly in a pair-wise manner, leading to significant computational redundancy. To mitigate this, we implement a Streamlined In-batch Multi-frame (SIM) pipeline, specifically tailored to video inputs to minimize redundant calculations. It enables the simultaneous prediction of successive unidirectional flows in a single forward pass, boosting processing speed by 44.43% and reaching efficiencies on par with two-frame networks. Moreover, we investigate various spatiotemporal modeling methods for optical flow estimation within this pipeline. Notably, we propose a simple yet highly effective parameter-efficient Integrative spatiotemporal Coherence (ISC) modeling method, alongside a lightweight Global Temporal Regressor (GTR) to harness temporal cues. The proposed ISC and GTR bring powerful spatiotemporal modeling capabilities and significantly enhance accuracy, including in occluded areas, while adding modest computations to the SIM pipeline. Compared to the baseline, our approach, StreamFlow, achieves performance enhancements of 15.45% and 11.37% on the Sintel clean and final test sets respectively, with gains of 15.53% and 10.77% on occluded regions and only a 1.11% rise in latency. Furthermore, StreamFlow exhibits state-of-the-art cross-dataset testing results on Sintel and KITTI, demonstrating its robust cross-domain generalization capabilities. The code is available here.',\n",
       "  292: 'Large language models (LLMs) with billions of parameters excel at predicting the next token in a sequence. Recent work computes non-vacuous compression-based generalization bounds for LLMs, but these bounds are vacuous for large models at the billion-parameter scale. Moreover, these bounds are obtained through restrictive compression techniques, bounding compressed models that generate low-quality text. Additionally, the tightness of these existing bounds depends on the number of IID documents in a training set rather than the much larger number of non-IID constituent tokens, leaving untapped potential for tighter bounds. In this work, we instead use properties of martingales to derive generalization bounds that benefit from the vast number of tokens in LLM training sets. Since a dataset contains far more tokens than documents, our generalization bounds not only tolerate but actually benefit from far less restrictive compression schemes. With Monarch matrices, Kronecker factorizations, and post-training quantization, we achieve non-vacuous generalization bounds for LLMs as large as LLaMA2-70B. Unlike previous approaches, our work achieves the first non-vacuous bounds for models that are deployed in practice and generate high-quality text.',\n",
       "  293: 'Recently, knowledge editing on large language models (LLMs) has received considerable attention. Compared to this, editing Large Vision-Language Models (LVLMs) faces extra challenges from diverse data modalities and complicated model components, and data for LVLMs editing are limited. The existing LVLM editing benchmark, which comprises three metrics (Reliability, Locality, and Generality), falls short in the quality of synthesized evaluation images and cannot assess whether models apply edited knowledge in relevant content. Therefore, we employ more reliable data collection methods to construct a new Large $\\\\textbf{V}$ision-$\\\\textbf{L}$anguage Model $\\\\textbf{K}$nowledge $\\\\textbf{E}$diting $\\\\textbf{B}$enchmark, $\\\\textbf{VLKEB}$, and extend the Portability metric for more comprehensive evaluation. Leveraging a multi-modal knowledge graph, our image data are bound with knowledge entities. This can be further used to extract entity-related knowledge, which constitutes the base of editing data. We conduct experiments of different editing methods on five LVLMs, and thoroughly analyze how do they impact the models. The results reveal strengths and deficiencies of these methods and hopefully provide insights for future research. The codes and dataset are available at: https://github.com/VLKEB/VLKEB.',\n",
       "  294: 'In this paper, we address the challenges of offline reinforcement learning (RL) under model mismatch, where the agent aims to optimize its performance through an offline dataset that may not accurately represent the deployment environment. We identify two primary challenges under the setting: inaccurate model estimation due to limited data and performance degradation caused by the model mismatch between the dataset-collecting environment and the target deployment one. To tackle these issues, we propose a unified principle of pessimism using distributionally robust Markov decision processes. We carefully construct a robust MDP with a single uncertainty set to tackle both data sparsity and model mismatch, and demonstrate that the optimal robust policy enjoys a near-optimal sub-optimality gap under the target environment across three widely used uncertainty models: total variation, $\\\\chi^2$ divergence, and KL divergence. Our results improve upon or match the state-of-the-art performance under the total variation and KL divergence models, and provide the first result for the $\\\\chi^2$ divergence model.',\n",
       "  295: 'The study of online algorithms with machine-learned predictions has gained considerable prominence in recent years. One of the common objectives in the design and analysis of such algorithms is to attain (Pareto) optimal tradeoffs between the {\\\\em consistency} of the algorithm, i.e., its performance assuming perfect predictions, and its {\\\\em robustness}, i.e., the performance of the algorithm under adversarial predictions. In this work, we demonstrate that this optimization criterion can be extremely brittle, in that the performance of Pareto-optimal algorithms may degrade dramatically even in the presence of imperceptive prediction error. To remedy this drawback, we propose a new framework in which the smoothness in the performance of the algorithm is enforced by means of a {\\\\em user-specified profile}. This allows us to regulate the performance of the algorithm as a function of the prediction error, while simultaneouslymaintaining the analytical notion of consistency/robustness tradeoffs, adapted to the profile setting. We apply this new approach to a well-studied online problem, namely the {\\\\em one-way trading} problem. For this problem, we further address another limitation of the state-of-the-art Pareto-optimal algorithms, namely the fact that they are tailored to worst-case, and extremely pessimistic inputs. We propose a new Pareto-optimal algorithm that leverages any deviation from the worst-case input to its benefit, and introduce a new metric that allows us to compare any two Pareto-optimal algorithms via a {\\\\em dominance} relation.',\n",
       "  296: 'Graph data are inherently complex and heterogeneous, leading to a high natural diversity of distributional shifts. However, it remains unclear how to build machine learning architectures that generalize to the complex distributional shifts naturally occurring in the real world. Here, we develop GraphMETRO, a Graph Neural Network architecture that models natural diversity and captures complex distributional shifts. GraphMETRO employs a Mixture-of-Experts (MoE) architecture with a gating model and multiple expert models, where each expert model targets a specific distributional shift to produce a referential representation w.r.t. a reference model, and the gating model identifies shift components. Additionally, we design a novel objective that aligns the representations from different expert models to ensure reliable optimization. GraphMETRO achieves state-of-the-art results on four datasets from the GOOD benchmark, which is comprised of complex and natural real-world distribution shifts, improving by 67% and 4.2% on the WebKB and Twitch datasets. Code and data are available at https://github.com/Wuyxin/GraphMETRO.',\n",
       "  297: \"Skeleton-based multi-entity action recognition is a challenging task aiming to identify interactive actions or group activities involving multiple diverse entities. Existing models for individuals often fall short in this task due to the inherent distribution discrepancies among entity skeletons, leading to suboptimal backbone optimization. To this end, we introduce a Convex Hull Adaptive Shift based multi-Entity action recognition method (CHASE), which mitigates inter-entity distribution gaps and unbiases subsequent backbones. Specifically, CHASE comprises a learnable parameterized network and an auxiliary objective. The parameterized network achieves plausible, sample-adaptive repositioning of skeleton sequences through two key components. First, the Implicit Convex Hull Constrained Adaptive Shift ensures that the new origin of the coordinate system is within the skeleton convex hull. Second, the Coefficient Learning Block provides a lightweight parameterization of the mapping from skeleton sequences to their specific coefficients in convex combinations. Moreover, to guide the optimization of this network for discrepancy minimization, we propose the Mini-batch Pair-wise Maximum Mean Discrepancy as the additional objective. CHASE operates as a sample-adaptive normalization method to mitigate inter-entity distribution discrepancies, thereby reducing data bias and improving the subsequent classifier's multi-entity action recognition performance. Extensive experiments on six datasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity and Volleyball, consistently verify our approach by seamlessly adapting to single-entity backbones and boosting their performance in multi-entity scenarios. Our code is publicly available at https://github.com/Necolizer/CHASE .\",\n",
       "  298: 'Differential privacy with gradual expiration models the setting where data items arrive in a stream and at a given time $t$ the privacy loss guaranteed for a data item seen at time $(t-d)$ is $\\\\epsilon g(d)$, where $g$ is a monotonically non-decreasing function. We study the fundamental *continual (binary) counting* problem where each data item consists of a bit and the algorithm needs to output at each time step the sum of all the bits streamed so far. For a stream of length $T$ and privacy *without* expiration continual counting is possible with maximum  (over all time steps) additive error $O(\\\\log^2(T)/\\\\varepsilon)$ and the best known lower bound is $\\\\Omega(\\\\log(T)/\\\\varepsilon)$; closing this gap is a challenging open problem. We show that the situation is very different for privacy with gradual expiration by giving upper and lower bounds for a large set of expiration functions $g$. Specifically, our algorithm achieves an additive error of $O(\\\\log(T)/\\\\epsilon)$ for a large set of privacy expiration functions. We also give a lower bound that shows that if $C$ is the additive error of any $\\\\epsilon$-DP algorithm for this problem, then the product of $C$ and the privacy expiration function after $2C$ steps must be $\\\\Omega(\\\\log(T)/\\\\epsilon)$. Our algorithm matches this lower bound as its additive error is $O(\\\\log(T)/\\\\epsilon)$, even when $g(2C) = O(1)$.Our empirical evaluation shows that we achieve a slowly growing privacy loss that has significantly smaller empirical privacy loss for large values of $d$ than a natural baseline algorithm.',\n",
       "  299: 'Traditional knowledge graph embedding (KGE) models map entities and relations to unique embedding vectors in a shallow lookup manner. As the scale of data becomes larger, this manner will raise unaffordable computational costs. Anchor-based strategies have been treated as effective ways to alleviate such efficiency problems by propagation on representative entities instead of the whole graph. However, most existing anchor-based KGE models select the anchors in a primitive manner, which limits their performance. To this end, we propose a novel anchor-based strategy for KGE, i.e., a relational clustering-based anchor selection strategy (RecPiece), where two characteristics are leveraged, i.e., (1) representative ability of the cluster centroids and (2) descriptive ability of relation types in KGs. Specifically, we first perform clustering over features of factual triplets instead of entities, where cluster number is naturally set as number of relation types since each fact can be characterized by its relation in KGs. Then, representative triplets are selected around the clustering centroids, further mapped into corresponding anchor entities. Extensive experiments on six datasets show that RecPiece achieves higher performances but comparable or even fewer parameters compared to previous anchor-based KGE models, indicating that our model can select better anchors in a more scalable way.',\n",
       "  300: \"Recent work in interpretability shows that large language models (LLMs) can be adapted for new tasks in a learning-free way: it is possible to intervene on LLM representations to elicit desired behaviors for alignment. For instance, adding certain bias vectors to the outputs of certain attention heads is reported to boost the truthfulness of models. In this work, we show that localized fine-tuning serves as an effective alternative to such representation intervention methods. We introduce a framework called Localized Fine-Tuning on LLM Representations (LoFiT), which identifies a subset of attention heads that are most important for learning a specific task, then trains offset vectors to add to the model's hidden representations at those selected heads. LoFiT localizes to a sparse set of heads (3%-10%) and learns the offset vectors from limited training data, comparable to the settings used for representation intervention. For truthfulness and reasoning tasks, we find that LoFiT's intervention vectors are more effective for LLM adaptation than vectors from representation intervention methods such as Inference-time Intervention. We also find that the localization step is important: selecting a task-specific set of attention heads can lead to higher performance than intervening on heads selected for a different task. Finally, across 7 tasks we study, LoFiT achieves comparable performance to other parameter-efficient fine-tuning methods such as LoRA, despite modifying 20x-200x fewer parameters than these methods.\",\n",
       "  301: 'Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning task, demanding intelligent systems to accurately respond to natural language queries based on audio-video input pairs. Nevertheless, prevalent AVQA approaches are prone to overlearning dataset biases, resulting in poor robustness. Furthermore, current datasets may not provide a precise diagnostic for these methods. To tackle these challenges, firstly, we propose a novel dataset, MUSIC-AVQA-R, crafted in two steps: rephrasing questions within the test split of a public dataset (MUSIC-AVQA) and subsequently introducing distribution shifts to split questions. The former leads to a large, diverse test space, while the latter results in a comprehensive robustness evaluation on rare, frequent, and overall questions. Secondly, we propose a robust architecture that utilizes a multifaceted cycle collaborative debiasing strategy to overcome bias learning. Experimental results show that this architecture achieves state-of-the-art performance on MUSIC-AVQA-R, notably obtaining a significant improvement of 9.32\\\\%. Extensive ablation experiments are conducted on the two datasets mentioned to analyze the component effectiveness within the debiasing strategy. Additionally, we highlight the limited robustness of existing multi-modal QA methods through the evaluation on our dataset. We also conduct experiments combining various baselines with our proposed strategy on two datasets to verify its plug-and-play capability. Our dataset and code are available at https://github.com/reml-group/MUSIC-AVQA-R.',\n",
       "  302: 'We present Meta 3D AssetGen (AssetGen), a significant advancement in text-to-3D generation which produces faithful, high-quality meshes with texture and material control. Compared to works that bake shading in the 3D object’s appearance, AssetGen outputs physically-based rendering (PBR) materials, supporting realistic relighting. AssetGen generates first several views of the object with separate shaded and albedo appearance channels, and then reconstructs colours, metalness and roughness in 3D, using a deferred shading loss for efficient supervision. It also uses a sign-distance function to represent 3D shape more reliably and introduces acorresponding loss for direct shape supervision. This is implemented using fused kernels for high memory efficiency. After mesh extraction, a texture refinement transformer operating in UV space significantly improves sharpness and details. AssetGen achieves 17% improvement in Chamfer Distance and 40% in LPIPS over the best concurrent work for few-view reconstruction, and a human preference of 72% over the best industry competitors of comparable speed, including those that support PBR. Project page with generated assets: https://assetgen.github.io',\n",
       "  303: 'Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA. However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets. This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance. Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA. Building on these insights, we have developed HydraLoRA, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise. Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases. Our anonymous codes are submitted with the paper and will be publicly available. Code is available: https://github.com/Clin0212/HydraLoRA.',\n",
       "  304: 'Recent research has made significant progress in optimizing diffusion models for downstream objectives, which is an important pursuit in fields such as graph generation for drug design. However, directly applying these models to graph presents challenges, resulting in suboptimal performance. This paper introduces graph diffusion policy optimization (GDPO), a novel approach to optimize graph diffusion models for arbitrary (e.g., non-differentiable) objectives using reinforcement learning. GDPO is based on an eager policy gradient tailored for graph diffusion models, developed through meticulous analysis and promising improved performance. Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives. Code is available at https://github.com/sail-sg/GDPO.',\n",
       "  305: 'This work presents a unified knowledge protocol, called UKnow, which facilitates knowledge-based studies from the perspective of data. Particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types, namely, in-image, in-text, cross-image, cross-text, and image-text, and set up an efficient pipeline to help construct the multimodal knowledge graph from any data collection. Thanks to the logical information naturally contained in knowledge graph, organizing datasets under UKnow format opens up more possibilities of data usage compared to the commonly used image-text pairs. Following UKnow protocol, we collect, from public international news, a large-scale multimodal knowledge graph dataset that consists of 1,388,568 nodes (with 571,791 vision-related ones) and 3,673,817 triplets. The dataset is also annotated with rich event tags, including 11 coarse labels and 9,185 fine labels. Experiments on four benchmarks demonstrate the potential of UKnow in supporting common-sense reasoning and boosting vision-language pre-training with a single dataset, benefiting from its unified form of knowledge organization. Code, dataset, and models will be made publicly available. See Appendix to download the dataset.',\n",
       "  306: \"Deep Equilibrium Model (DEQ), which serves as a typical implicit neural network, emphasizes their memory efficiency and competitive performance compared to explicit neural networks. However, there has been relatively limited theoretical analysis on the representation of DEQ. In this paper, we utilize the Neural Collapse ($\\\\mathcal{NC}$) as a tool to systematically analyze the representation of DEQ under both balanced and imbalanced conditions. $\\\\mathcal{NC}$ is an interesting phenomenon in the neural network training process that characterizes the geometry of class features and classifier weights. While extensively studied in traditional explicit neural networks, the $\\\\mathcal{NC}$ phenomenon has not received substantial attention in the context of implicit neural networks. We theoretically show that $\\\\mathcal{NC}$ exists in DEQ under balanced conditions. Moreover, in imbalanced settings, despite the presence of minority collapse, DEQ demonstrated advantages over explicit neural networks. These advantages include the convergence of extracted features to the vertices of a simplex equiangular tight frame and self-duality properties under mild conditions, highlighting DEQ's superiority in handling imbalanced datasets. Finally, we validate our theoretical analyses through experiments in both balanced and imbalanced scenarios.\",\n",
       "  307: 'Large Language Models (LLMs) are recognized for their potential to be an important building block toward achieving artificial general intelligence due to their unprecedented capability for solving diverse tasks. Despite these achievements, LLMs often underperform in domain-specific tasks without training on relevant domain data. This phenomenon, which is often attributed to distribution shifts, makes adapting pre-trained LLMs with domain-specific data crucial. However, this adaptation raises significant privacy concerns, especially when the data involved come from sensitive domains. In this work, we extensively investigate the privacy vulnerabilities of adapted (fine-tuned) LLMs and benchmark privacy leakage across a wide range of data modalities, state-of-the-art privacy attack methods, adaptation techniques, and model architectures. We systematically evaluate and pinpoint critical factors related to privacy leakage. With our organized codebase and actionable insights, we aim to provide a standardized auditing tool for practitioners seeking to deploy customized LLM applications with faithful privacy assessments.',\n",
       "  308: \"Machine learning methods have a groundbreaking impact in many application domains, but their application on real robotic platforms is still limited.Despite the many challenges associated with combining machine learning technology with robotics, robot learning remains one of the most promising directions for enhancing the capabilities of robots. When deploying learning-based approaches on real robots, extra effort is required to address the challenges posed by various real-world factors. To investigate the key factors influencing real-world deployment and to encourage original solutions from different researchers, we organized the Robot Air Hockey Challenge at the NeurIPS 2023 conference. We selected the air hockey task as a benchmark, encompassing low-level robotics problems and high-level tactics. Different from other machine learning-centric benchmarks, participants need to tackle practical challenges in robotics, such as the sim-to-real gap, low-level control issues, safety problems, real-time requirements, and the limited availability of real-world data. Furthermore, we focus on a dynamic environment, removing the typical assumption of quasi-static motions of other real-world benchmarks.The competition's results show that solutions combining learning-based approaches with prior knowledge outperform those relying solely on data when real-world deployment is challenging.Our ablation study reveals which real-world factors may be overlooked when building a learning-based solution.The successful real-world air hockey deployment of best-performing agents sets the foundation for future competitions and follow-up research directions.\",\n",
       "  309: 'In density functional theory, charge density is the core attribute of atomic systems from which all chemical properties can be derived. Machine learning methods are promising in significantly accelerating charge density prediction, yet existing approaches either lack accuracy or scalability. We propose a recipe that can achieve both. In particular, we identify three key ingredients: (1) representing the charge density with atomic and virtual orbitals (spherical fields centered at atom/virtual coordinates); (2) using expressive and learnable orbital basis sets (basis function for the spherical fields); and (3) using high-capacity equivariant neural network architecture. Our method achieves state-of-the-art accuracy while being more than an order of magnitude faster than existing methods. Furthermore, our method enables flexible efficiency-accuracy trade-offs by adjusting the model/basis sizes.',\n",
       "  310: 'In this paper, we introduce the first learning-free model reuse task within the non-Euclidean domain, termed as Deep Graph Mating (Grama). We strive to create a child Graph Neural Network (GNN) that integrates knowledge from pre-trained parent models without requiring re-training, fine-tuning, or annotated labels. To this end, we begin by investigating the permutation invariance property of GNNs, which leads us to develop two vanilla approaches for Grama: Vanilla Parameter Interpolation (VPI) and Vanilla Alignment Prior to Interpolation (VAPI), both employing topology-independent interpolation in the parameter space. However, neither approach has achieved the anticipated results. Through theoretical analysis of VPI and VAPI, we identify critical challenges unique to Grama, including increased sensitivity to parameter misalignment and further the inherent topology-dependent complexities. Motivated by these findings, we propose the Dual-Message Coordination and Calibration (DuMCC) methodology, comprising the Parent Message Coordination (PMC) scheme to optimise the permutation matrices for parameter interpolation by coordinating aggregated messages, and the Child Message Calibration (CMC) scheme to mitigate over-smoothing identified in PMC by calibrating the message statistics within child GNNs. Experiments across diverse domains, including node and graph property prediction, 3D object recognition, and large-scale semantic parsing, demonstrate that the proposed DuMCC effectively enables training-free knowledge transfer, yielding results on par with those of pre-trained models.',\n",
       "  311: 'As language models continue to scale, Large Language Models (LLMs) have exhibited emerging capabilities in In-Context Learning (ICL), enabling them to solve language tasks by prefixing a few in-context demonstrations (ICDs) as context. Inspired by these advancements, researchers have extended these techniques to develop Large Multimodal Models (LMMs) with ICL capabilities. However, applying ICL usually faces two major challenges: 1) using more ICDs will largely increase the inference time and 2) the performance is sensitive to the selection of ICDs. These challenges are further exacerbated in LMMs due to the integration of multiple data types and the combinational complexity of multimodal ICDs. Recently, to address these challenges, some NLP studies introduce non-learnable In-Context Vectors (ICVs) which extract useful task information from ICDs into a single vector and then insert it into the LLM to help solve the corresponding task. However, although useful in simple NLP tasks, these non-learnable methods fail to handle complex multimodal tasks like Visual Question Answering (VQA). In this study, we propose \\\\underline{\\\\textbf{L}}earnable \\\\underline{\\\\textbf{I}}n-Context \\\\underline{\\\\textbf{Ve}}ctor (LIVE) to distill essential task information from demonstrations, improving ICL performance in LMMs. Experiments show that LIVE can significantly reduce computational costs while enhancing accuracy in VQA tasks compared to traditional ICL and other non-learnable ICV methods.',\n",
       "  312: 'We introduce a novel framework, Online Relational Inference (ORI), designed to efficiently identify hidden interaction graphs in evolving multi-agent interacting systems using streaming data. Unlike traditional offline methods that rely on a fixed training set, ORI employs online backpropagation, updating the model with each new data point, thereby allowing it to adapt to changing environments in real-time. A key innovation is the use of an adjacency matrix as a trainable parameter, optimized through a new adaptive learning rate technique called AdaRelation, which adjusts based on the historical sensitivity of the decoder to changes in the interaction graph. Additionally, a data augmentation method named Trajectory Mirror (TM) is introduced to improve generalization by exposing the model to varied trajectory patterns. Experimental results on both synthetic datasets and real-world data (CMU MoCap for human motion) demonstrate that ORI significantly improves the accuracy and adaptability of relational inference in dynamic settings compared to existing methods. This approach is model-agnostic, enabling seamless integration with various neural relational inference (NRI) architectures, and offers a robust solution for real-time applications in complex, evolving systems.',\n",
       "  313: 'Neural algorithmic reasoning is an emerging area of machine learning that focuses on building neural networks capable of solving complex algorithmic tasks. Recent advancements predominantly follow the standard supervised learning paradigm -- feeding an individual problem instance into the network each time and training it to approximate the execution steps of a classical algorithm. We challenge this mode and propose a novel open-book learning framework. In this framework, whether during training or testing, the network can access and utilize all instances in the training dataset when reasoning for a given instance.Empirical evaluation is conducted on the challenging CLRS Algorithmic Reasoning Benchmark, which consists of 30 diverse algorithmic tasks. Our open-book learning framework exhibits a significant enhancement in neural reasoning capabilities. Further, we notice that there is recent literature suggesting that multi-task training on CLRS can improve the reasoning accuracy of certain tasks, implying intrinsic connections between different algorithmic tasks. We delve into this direction via the open-book framework. When the network reasons for a specific task, we enable it to aggregate information from training instances of other tasks in an attention-based manner. We show that this open-book attention mechanism offers insights into the inherent relationships among various tasks in the benchmark and provides a robust tool for interpretable multi-task training.',\n",
       "  314: 'A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to classify between data samples and samples from some reference distribution. DRE-based models can directly output the likelihood for any given input, a highly desired property that is lacking in most generative techniques. Nevertheless, to date, DRE methods have failed in accurately capturing the distributions of complex high-dimensional data, like images, and have thus been drawing reduced research attention in recent years.  In this work we present classification diffusion models (CDMs), a DRE-based generative method that adopts the formalism of denoising diffusion models (DDMs) while making use of a classifier that predicts the level of noise added to a clean signal. Our method is based on an analytical connection that we derive between the MSE-optimal denoiser for removing white Gaussian noise and the cross-entropy-optimal classifier for predicting the noise level. Our method is the first DRE-based technique that can successfully generate images beyond the MNIST dataset. Furthermore, it can output the likelihood of any input in a single forward pass, achieving state-of-the-art negative log likelihood (NLL) among methods with this property.',\n",
       "  315: 'Zero-Shot Temporal Action Detection (ZSTAD) aims to classify and localize action segments in untrimmed videos for unseen action categories. Most existing ZSTAD methods utilize a foreground-based approach, limiting the integration of text and visual features due to their reliance on pre-extracted proposals. In this paper, we introduce a cross-modal ZSTAD baseline with mutual cross-attention, integrating both text and visual information throughout the detection process. Our simple approach results in superior performance compared to previous methods. Despite this improvement, we further identify a common-action bias issue that the cross-modal baseline over-focus on common sub-actions due to a lack of ability to discriminate text-related visual parts. To address this issue, we propose Text-infused attention and Foreground-aware Action Detection (Ti-FAD), which enhances the ability to focus on text-related sub-actions and distinguish relevant action segments from the background. Our extensive experiments demonstrate that Ti-FAD outperforms the state-of-the-art methods on ZSTAD benchmarks by a large margin:  41.2\\\\% (+ 11.0\\\\%) on THUMOS14 and 32.0\\\\% (+ 5.4\\\\%) on ActivityNet v1.3. Code is available at: https://github.com/YearangLee/Ti-FAD.',\n",
       "  316: 'Online Budgeted Matching (OBM) is a classic problem with important applications in online advertising, online service matching, revenue management, and beyond. Traditional online algorithms typically assume a small bid setting, where the maximum bid-to-budget ratio ($\\\\kappa$) is infinitesimally small. While recent algorithms have tried to address scenarios with non-small or general bids, they often rely on the Fractional Last Matching (FLM) assumption, which allows for accepting partial bids when the remaining budget is insufficient. This assumption, however, does not hold for many applications with indivisible bids. In this paper, we remove the FLM assumption and tackle the open problem of OBM with general bids. We first establish an upper bound of $1-\\\\kappa$ on the competitive ratio for any deterministic online algorithm. We then propose a novel meta algorithm, called MetaAd, which reduces to different algorithms with first known provable competitive ratios parameterized by the maximum bid-to-budget ratio $\\\\kappa\\\\in [0,1]$. As a by-product, we extend MetaAd to the FLM setting and get provable competitive algorithms. Finally, we apply our competitive analysis to the design learning- augmented algorithms.',\n",
       "  317: 'Investigating the marginal causal effect of an intervention on an outcome from complex data remains challenging due to the inflexibility of employed models and the lack of complexity in causal benchmark datasets, which often fail to reproduce intricate real-world data patterns. In this paper we introduce Frugal Flows, a likelihood-based machine learning model that uses normalising flows to flexibly learn the data-generating process, while also directly targeting the marginal causal quantities inferred from observational data. We provide a novel algorithm for fitting a model to observational data with a parametrically specified causal distribution, and propose that these models are exceptionally well suited for synthetic data generation to validate causal methods. Unlike existing data generation methods, Frugal Flows generate synthetic data that closely resembles the empirical dataset, while also automatically and exactly satisfying a user-defined average treatment effect. To our knowledge, Frugal Flows are the first generative model to both learn flexible data representations and also \\\\textit{exactly} parameterise quantities such as the average treatment effect and the degree of unobserved confounding. We demonstrate the above with experiments on  both simulated and real-world datasets.',\n",
       "  318: 'We introduce camera ray matching (CRAYM) into the joint optimization of camera poses and neural fields from multi-view images. The optimized field, referred to as a feature volume, can be “probed” by the camera rays for novel view synthesis (NVS) and 3D geometry reconstruction. One key reason for matching camera rays, instead of pixels as in prior works, is that the camera rays can be parameterized by the feature volume to carry both geometric and photometric information. Multi-view consistencies involving the camera rays and scene rendering can be naturally integrated into the joint optimization and network training, to impose physically meaningful constraints to improve the final quality of both the geometric reconstruction and photorealistic rendering. We formulate our per-ray optimization and matched ray coherence by focusing on camera rays passing through keypoints in the input images to elevate both the efficiency and accuracy of scene correspondences. Accumulated ray features along the feature volume provide a means to discount the coherence constraint amid erroneous ray matching. We demonstrate the effectiveness of CRAYM for both NVS and geometry reconstruction, over dense- or sparse-view settings, with qualitative and quantitative comparisons to state-of-the-art alternatives.',\n",
       "  319: 'Existing learning rate schedules that do not require specification of the optimization stopping step $T$ are greatly out-performed by learning rate schedules that depend on $T$. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available at https://github.com/facebookresearch/schedule_free. Schedule-Free AdamW is the core algorithm behind our winning entry to the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track.',\n",
       "  320: 'Due to the heterogeneous architectures and class skew, the global representation models training in resource-adaptive federated self-supervised learning face with tricky challenges: $\\\\textit{deviated representation abilities}$ and $\\\\textit{inconsistent representation spaces}$. In this work, we are the first to propose a multi-teacher knowledge distillation framework, namely $\\\\textit{FedMKD}$, to learn global representations with whole class knowledge from heterogeneous clients even under extreme class skew. Firstly, the adaptive knowledge integration mechanism is designed to learn better representations from all heterogeneous models with deviated representation abilities. Then the weighted combination of the self-supervised loss and the distillation loss can support the global model to encode all classes from clients into a unified space. Besides, the global knowledge anchored alignment module can make the local representation spaces close to the global spaces, which further improves the representation abilities of local ones. Finally, extensive experiments conducted on two datasets demonstrate the effectiveness of $\\\\textit{FedMKD}$ which outperforms state-of-the-art baselines 4.78\\\\% under linear evaluation on average.',\n",
       "  321: 'Spiking Neural Networks (SNNs) provide a sparse spike-driven mechanism which is believed to be critical for energy-efficient deep learning. Mixture-of-Experts (MoE), on the other side, aligns with the brain mechanism of distributed and sparse processing, resulting in an efficient way of enhancing model capacity and conditional computation. In this work, we consider how to incorporate SNNs’ spike-driven and MoE’s conditional computation into a unified framework. However, MoE uses softmax to get the dense conditional weights for each expert and TopK to hard-sparsify the network, which does not fit the properties of SNNs. To address this issue, we reformulate MoE in SNNs and introduce the Spiking Experts Mixture Mechanism (SEMM) from the perspective of sparse spiking activation. Both the experts and the router output spiking sequences, and their element-wise operation makes SEMM computation spike-driven and dynamic sparse-conditional. By developing SEMM into Spiking Transformer, the Experts Mixture Spiking Attention (EMSA) and the Experts Mixture Spiking Perceptron (EMSP) are proposed, which performs routing allocation for head-wise and channel-wise spiking experts, respectively. Experiments show that SEMM realizes sparse conditional computation and obtains a stable improvement on neuromorphic and static datasets with approximate computational overhead based on the Spiking Transformer baselines.',\n",
       "  322: \"We study learning-based approaches to semantic route planning, which concerns producing routes in response to rich queries that specify various criteria and preferences. Semantic routing is already widely found in industry applications, especially navigational services like Google Maps; however, existing implementations only support limited route criteria and narrow query sets as they rely on repurposing classical route optimization algorithms. We argue for a learning-based approach to semantic routing as a more scalable and general alternative. To foster interest in this important application of graph learning, we are releasing a large-scale publicly-licensed benchmark for semantic routing consisting of real-world multi-objective navigation problems---expressed via natural language queries---on the richly annotated road networks of US cities. In addition to being intractable with existing approaches to semantic routing, our benchmark poses a significant scaling challenge for graph learning methods. As a proof-of-concept, we show that---at scale---even a standard transformer network is a powerful semantic routing system and achieves non-trivial performance on our benchmark. In the process, we demonstrate a simple solution to the challenge of scaling up graph learning: an autoregressive approach that decomposes semantic routing into smaller ``next-edge'' prediction problems.\",\n",
       "  323: \"Automated scientific discovery promises to accelerate progress across scientific domains, but evaluating an agent's capacity for end-to-end scientific reasoning is challenging as running real-world experiments is often prohibitively expensive or infeasible. In this work we introduce DiscoveryWorld, a virtual environment that enables benchmarking an agent's ability to perform complete cycles of novel scientific discovery in an inexpensive, simulated, multi-modal, long-horizon, and fictional setting.DiscoveryWorld consists of 24 scientific tasks across three levels of difficulty, each with parametric variations that provide new discoveries for agents to make across runs. Tasks require an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions. Task difficulties are normed to range from straightforward to challenging for human scientists with advanced degrees. DiscoveryWorld further provides three automatic metrics for evaluating performance, including: (1) binary task completion, (2) fine-grained report cards detailing procedural scoring of task-relevant actions, and (3) the accuracy of discovered explanatory knowledge.While simulated environments such as DiscoveryWorld are low-fidelity compared to the real world, we find that strong baseline agents struggle on most DiscoveryWorld tasks, highlighting the utility of using simulated environments as proxy tasks for near-term development of scientific discovery competency in agents.\",\n",
       "  324: 'Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data.We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques.We evaluate our method on data selection for both continued pretraining and instruction tuning of language models.We show that instruction tuning using data selected by our method with a 1\\\\% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average.',\n",
       "  325: 'We provide a technique for OLO that obtains regret $G\\\\|w_\\\\star\\\\|\\\\sqrt{T\\\\log(\\\\|w_\\\\star\\\\|G\\\\sqrt{T})} + \\\\|w_\\\\star\\\\|^2 + G^2$ on $G$-Lipschitz losses for any comparison point $w_\\\\star$ without knowing either $G$ or $\\\\|w_\\\\star\\\\|$. Importantly, this matches the optimal bound $G\\\\|w_\\\\star\\\\|\\\\sqrt{T}$ available with such knowledge (up to logarithmic factors), unless either $\\\\|w_\\\\star\\\\|$ or $G$ is so large that even $G\\\\|w_\\\\star\\\\|\\\\sqrt{T}$ is roughly linear in $T$. Thus, at a  high level it matches the optimal bound in all cases in which one can achieve sublinear regret.',\n",
       "  326: 'The expressive power of transformers over inputs of unbounded size can be studied through their ability to recognize classes of formal languages. In this paper, we establish exact characterizations of transformers with hard attention (in which all attention is focused on exactly one position) and attention masking (in which each position only attends to positions on one side). With strict masking (each position cannot attend to itself) and without position embeddings, these transformers are expressively equivalent to linear temporal logic (LTL), which defines exactly the star-free languages. A key technique is the use of Boolean RASP as a convenient intermediate language between transformers and LTL. We then take numerous results known for LTL and apply them to transformers, showing how position embeddings, strict masking, and depth all increase expressive power.',\n",
       "  327: 'In this paper, we introduce Cosine Autoencoder (CosAE), a novel, generic Autoencoder that seamlessly leverages the classic Fourier series with a feed-forward neural network. CosAE represents an input image as a series of 2D Cosine time series, each defined by a tuple of learnable frequency and Fourier coefficients. This method stands in contrast to a conventional Autoencoder that often sacrifices detail in their reduced-resolution bottleneck latent spaces. CosAE, however, encodes frequency coefficients, i.e., the amplitudes and phases, in its bottleneck. This encoding enables extreme spatial compression, e.g., $64\\\\times$ downsampled feature maps in the bottleneck, without losing detail upon decoding. We showcase the advantage of CosAE via extensive experiments on flexible-resolution super-resolution and blind image restoration, two highly challenging tasks that demand the restoration network to effectively generalize to complex and even unknown image degradations. Our method surpasses state-of-the-art approaches, highlighting its capability to learn a generalizable representation for image restoration. The project page is maintained at [https://sifeiliu.net/CosAE-page/](https://sifeiliu.net/CosAE-page/).',\n",
       "  328: 'The sampling problem under local differential privacy has recently been studied with potential applications to generative models, but a fundamental analysis of its privacy-utility trade-off (PUT) remains incomplete. In this work, we define the fundamental PUT of private sampling in the minimax sense, using the $f$-divergence between original and sampling distributions as the utility measure. We characterize the exact PUT for both finite and continuous data spaces under some mild conditions on the data distributions, and  propose sampling mechanisms that are universally optimal for all $f$-divergences. Our numerical experiments demonstrate the superiority of our mechanisms over baselines, in terms of theoretical utilities for finite data space and of empirical utilities for continuous data space.',\n",
       "  329: 'Transformer-based large language models (LLMs) typically have a limited context window, resulting in significant performance degradation when processing text beyond the length of the context window. Extensive studies have been proposed to extend the context window and achieve length extrapolation of LLMs, but there is still a lack of in-depth interpretation of these approaches. In this study, we explore the positional information within and beyond the context window for deciphering the underlying mechanism of LLMs. By using a mean-based decomposition method, we disentangle positional vectors from hidden states of LLMs and analyze their formation and effect on attention. Furthermore, when texts exceed the context window, we analyze the change of positional vectors in two settings, i.e., direct extrapolation and context window extension. Based on our findings, we design two training-free context window extension methods, positional vector replacement and attention window extension. Experimental results show that our methods can effectively extend the context window length.',\n",
       "  330: 'Message Passing Graph Neural Networks are known to suffer from two problems that are sometimes believed to be diametrically opposed: over-squashing and over-smoothing. The former results from topological bottlenecks that hamper the information flow from distant nodes and are mitigated by spectral gap maximization, primarily, by means of edge additions. However, such additions often promote over-smoothing that renders nodes of different classes less distinguishable. Inspired by the Braess phenomenon, we argue that deleting edges can address over-squashing and over-smoothing simultaneously. This insight explains how edge deletions can improve generalization, thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets. To this end, we propose a computationally effective spectral gap optimization framework to add or delete edges and demonstrate its effectiveness on the long range graph benchmark and on larger heterophilous datasets.',\n",
       "  331: 'We study fairness in social influence maximization, whereby one seeks to selectseeds that spread a given information throughout a network, ensuring balancedoutreach among different communities (e.g. demographic groups). In the literature,fairness is often quantified in terms of the expected outreach within individualcommunities. In this paper, we demonstrate that such fairness metrics can bemisleading since they overlook the stochastic nature of information diffusionprocesses. When information diffusion occurs in a probabilistic manner, multipleoutreach scenarios can occur. As such, outcomes such as “In 50% of the cases, noone in group 1 gets the information, while everyone in group 2 does, and in theother 50%, it is the opposite”, which always results in largely unfair outcomes,are classified as fair by a variety of fairness metrics in the literature. We tacklethis problem by designing a new fairness metric, mutual fairness, that capturesvariability in outreach through optimal transport theory. We propose a new seed-selection algorithm that optimizes both outreach and mutual fairness, and we showits efficacy on several real datasets. We find that our algorithm increases fairnesswith only a minor decrease (and at times, even an increase) in efficiency.',\n",
       "  332: 'The Lipschitz constant plays a crucial role in certifying the robustness of neural networks to input perturbations. Since calculating the exact Lipschitz constant is NP-hard, efforts have been made to obtain tight upper bounds on the Lipschitz constant. Typically, this involves solving a large matrix verification problem, the computational cost of which grows significantly for both deeper and wider networks. In this paper, we provide a compositional approach to estimate Lipschitz constants for deep feed-forward neural networks. We first obtain an exact decomposition of the large matrix verification problem into smaller sub-problems. Then, leveraging the underlying cascade structure of the network, we develop two algorithms. The first algorithm explores the geometric features of the problem and enables us to provide Lipschitz estimates that are comparable to existing methods by solving small semidefinite programs (SDPs) that are only as large as the size of each layer. The second algorithm relaxes these sub-problems and provides a closed-form solution to each sub-problem for extremely fast estimation, altogether eliminating the need to solve SDPs. The two algorithms represent different levels of trade-offs between efficiency and accuracy. Finally, we demonstrate that our approach provides a steep reduction in computation time (as much as several thousand times faster, depending on the algorithm for deeper networks) while yielding Lipschitz bounds that are very close to or even better than those achieved by state-of-the-art approaches in a broad range of experiments. In summary, our approach considerably advances the scalability and efficiency of certifying neural network robustness, making it particularly attractive for online learning tasks.',\n",
       "  333: 'We introduce a fairness-aware dataset for job recommendation in advertising, designed to foster research in algorithmic fairness within real-world scenarios. It was collected and prepared to comply with privacy standards and business confidentiality. An additional challenge is the lack of access to protected user attributes such as gender, for which we propose a pragmatic solution to obtain a proxy estimate. Despite being anonymized and including a proxy for a sensitive attribute, our dataset preserves predictive power and maintains a realistic and challenging benchmark. This dataset addresses a significant gap in the availability of fairness-focused resources for high-impact domains like advertising -- the actual impact being having access or not to precious employment opportunities, where balancing fairness and utility is a common industrial challenge. We also explore various stages in the advertising process where unfairness can occur and introduce a method to compute a fair utility metric for the job recommendations in online systems case from a biased dataset. Experimental evaluations of bias mitigation techniques on the released dataset demonstrate potential improvements in fairness and the associated trade-offs with utility.The dataset is hosted at https://huggingface.co/datasets/criteo/FairJob. Source code for the experiments is hosted at https://github.com/criteo-research/FairJob-dataset/.',\n",
       "  334: 'Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)’s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve $\\\\le 34\\\\%$ accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracted thousands of participants and submissions. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions. CRAG is available at https://github.com/facebookresearch/CRAG/.',\n",
       "  335: 'LLMs are computationally expensive to pre-train due to their large scale.Model growth emerges as a promising approach by leveraging smaller models to accelerate the training of larger ones. However, the viability of these model growth methods in efficient LLM pre-training remains underexplored.This work identifies three critical $\\\\underline{\\\\textit{O}}$bstacles: ($\\\\textit{O}$1) lack of comprehensive evaluation, ($\\\\textit{O}$2) untested viability for scaling, and ($\\\\textit{O}$3) lack of empirical guidelines.To tackle $\\\\textit{O}$1, we summarize existing approaches into four atomic growth operators and systematically evaluate them in a standardized LLM pre-training setting.Our findings reveal that a depthwise stacking operator, called $G_{\\\\text{stack}}$, exhibits remarkable acceleration in training, leading to decreased loss and improved overall performance on eight standard NLP benchmarks compared to strong baselines. Motivated by these promising results, we conduct extensive experiments to delve deeper into $G_{\\\\text{stack}}$ to address $\\\\textit{O}$2 and $\\\\textit{O}$3.For $\\\\textit{O}$2 (untested scalability), our study shows that $G_{\\\\text{stack}}$ is scalable and consistently performs well, with experiments up to 7B LLMs after growth and pre-training LLMs with 750B tokens.For example, compared to a conventionally trained 7B model using 300B tokens, our $G_{\\\\text{stack}}$ model converges to the same loss with 194B tokens, resulting in a 54.6\\\\% speedup. We further address $\\\\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines to determine growth timing and growth factor for $G_{\\\\text{stack}}$, making it practical in general LLM pre-training.We also provide in-depth discussions and comprehensive ablation studies of $G_{\\\\text{stack}}$. Our code and pre-trained model are available at https://llm-stacking.github.io/.',\n",
       "  336: 'We introduce Annealed Multiple Choice Learning (aMCL) which combines simulated annealing with MCL. MCL is a learning framework handling ambiguous tasks by predicting a small set of plausible hypotheses. These hypotheses are trained using the Winner-takes-all (WTA) scheme, which promotes the diversity of the predictions. However, this scheme may converge toward an arbitrarily suboptimal local minimum, due to the greedy nature of WTA. We overcome this limitation using annealing, which enhances the exploration of the hypothesis space during training. We leverage insights from statistical physics and information theory to provide a detailed description of the model training trajectory. Additionally, we validate our algorithm by extensive experiments on synthetic datasets, on the standard UCI benchmark, and on speech separation.',\n",
       "  337: 'Multiple Instance Learning (MIL) has been increasingly adopted to mitigate the high costs and complexity associated with labeling individual instances, learning instead from bags of instances labeled at the bag level and enabling instance-level labeling. While existing research has primarily focused on the learnability of MIL at the bag level, there is an absence of theoretical exploration to check if a given MIL algorithm is learnable at the instance level. This paper proposes a theoretical framework based on probably approximately correct (PAC) learning theory to assess the instance-level learnability of deep multiple instance learning (Deep MIL) algorithms. Our analysis exposes significant gaps between current Deep MIL algorithms, highlighting the theoretical conditions that must be satisfied by MIL algorithms to ensure instance-level learnability. With these conditions, we interpret the learnability of the representative Deep MIL algorithms and validate them through empirical studies.',\n",
       "  338: 'Due to the lack of state dimension optimization methods, deep state space models (SSMs) have sacrificed model capacity, training search space, or stability to alleviate computational costs caused by high state dimensions. In this work, we provide a structured pruning method for SSMs, Layer-Adaptive STate pruning (LAST), which reduces the state dimension of each layer in minimizing model-level output energy loss by extending modal truncation for a single system. LAST scores are evaluated using the $\\\\mathcal{H}_{\\\\infty}$ norms of subsystems and layer-wise energy normalization. The scores serve as global pruning criteria, enabling cross-layer comparison of states and layer-adaptive pruning. Across various sequence benchmarks, LAST optimizes previous SSMs, revealing the redundancy and compressibility of their state spaces. Notably, we demonstrate that, on average, pruning 33\\\\% of states still maintains performance with 0.52\\\\% accuracy loss in multi-input multi-output SSMs without retraining. Code is available at https://github.com/msgwak/LAST.',\n",
       "  339: 'We introduce a new representation for 3D molecules based on their continuous atomic density fields. Using this representation, we propose a new model based on walk-jump sampling for unconditional 3D molecule generation in the continuous space using neural fields. Our model, FuncMol, encodes molecular fields into latent codes using a conditional neural field, samples noisy codes from a Gaussian-smoothed distribution with Langevin MCMC (walk), denoises these samples in a single step (jump), and finally decodes them into molecular fields. FuncMol performs all-atom generation of 3D molecules without assumptions on the molecular structure and scales well with the size of molecules, unlike most approaches. Our method achieves competitive results on drug-like molecules and easily scales to macro-cyclic peptides, with at least one order of magnitude faster sampling. The code is available at https://github.com/prescient-design/funcmol.',\n",
       "  340: \"Expert-designed close-ended benchmarks are indispensable in assessing the knowledge capacity of large language models (LLMs). Despite their widespread use, concerns have mounted regarding their reliability due to limited test scenarios and an unavoidable risk of data contamination. To rectify this, we present PertEval, a toolkit devised for in-depth probing of LLMs' knowledge capacity through knowledge-invariant perturbations. These perturbations employ human-like restatement techniques to generate on-the-fly test samples from static benchmarks, meticulously retaining knowledge-critical content while altering irrelevant details. Our toolkit further includes a suite of response consistency analyses that compare performance on raw vs. perturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six representative LLMs are re-evaluated using PertEval. Results reveal significantly inflated performance of the LLMs on raw benchmarks, including an absolute 25.8% overestimation for GPT-4. Additionally, through a nuanced response pattern analysis, we discover that PertEval retains LLMs' uncertainty to specious knowledge, and reveals their potential rote memorization to correct options which leads to overestimated performance. We also find that the detailed response consistency analyses by PertEval could illuminate various weaknesses in existing LLMs' knowledge mastery and guide the development of refinement. Our findings provide insights for advancing more robust and genuinely knowledgeable LLMs. Our code is available at https://github.com/aigc-apps/PertEval.\",\n",
       "  341: 'The classical Canonical Correlation Analysis (CCA) identifies the correlations between two sets of multivariate variables based on their covariance, which has been widely applied in diverse fields such as computer vision, natural language processing, and speech analysis. Despite its popularity, CCA can encounter challenges in explaining correlations between two variable sets within high-dimensional data contexts. Thus, this paper studies Sparse Canonical Correlation Analysis (SCCA) that enhances the interpretability of CCA. We first show that SCCA generalizes three well-known sparse optimization problems, sparse PCA, sparse SVD, and sparse regression, which are all classified as NP-hard problems. This result motivates us to develop strong formulations and efficient algorithms. Our main contributions include (i) the introduction of a combinatorial formulation that captures the essence of SCCA and allows the development of exact and approximation algorithms; (ii) the establishment of the complexity results for two low-rank special cases of SCCA; and (iii) the derivation of an equivalent mixed-integer semidefinite programming model that facilitates a specialized branch-and-cut algorithm with analytical cuts. The effectiveness of our proposed formulations and algorithms is validated through numerical experiments.',\n",
       "  342: 'While several recent matrix completion methods are developed to deal with non-uniform observation probabilities across matrix entries, very few allow the missingness to depend on the mostly unobserved matrix measurements, which is generally ill-posed. We aim to tackle a subclass of these ill-posed settings, characterized by a flexible separable observation probability assumption that can depend on the matrix measurements. We propose a regularized pairwise pseudo-likelihood approach for matrix completion and prove that the proposed estimator can asymptotically recover the low-rank parameter matrix up to an identifiable equivalence class of a constant shift and scaling, at a near-optimal asymptotic convergence rate of the standard well-posed (non-informative missing) setting, while effectively mitigating the impact of informative missingness. The efficacy of our method is validated via numerical experiments, positioning it as a robust tool for matrix completion to mitigate data bias.',\n",
       "  343: 'We study the gradient Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMM) in the over-parameterized setting, where a general GMM with $n>1$ components learns from data that are generated by a single ground truth Gaussian distribution. While results for the special case of 2-Gaussian mixtures are well-known, a general global convergence analysis for arbitrary $n$ remains unresolved and faces several new technical barriers since the convergence becomes sub-linear and non-monotonic. To address these challenges, we construct a novel likelihood-based convergence analysis framework and rigorously prove that gradient EM converges globally with a sublinear rate $O(1/\\\\sqrt{t})$. This is the first global convergence result for Gaussian mixtures with more than $2$ components. The sublinear convergence rate is due to the algorithmic nature of learning over-parameterized GMM with gradient EM. We also identify a new emerging technical challenge for learning general over-parameterized GMM: the existence of bad local regions that can trap gradient EM for an exponential number of steps.',\n",
       "  344: \"Generative diffusion models and many stochastic models in science and engineering naturally live in infinite dimensions before discretisation. To incorporate observed data for statistical and learning tasks, one needs to condition on observations. While recent work has treated conditioning linear processes in infinite dimensions, conditioning non-linear processes in infinite dimensions has not been explored. This paper conditions function valued stochastic processes without prior discretisation. To do so, we use an infinite-dimensional version of Girsanov's theorem to condition a function-valued stochastic process, leading to a stochastic differential equation (SDE) for the conditioned process involving the score. We apply this technique to do time series analysis for shapes of organisms in evolutionary biology, where we discretise via the Fourier basis and then learn the coefficients of the score function with score matching methods.\",\n",
       "  345: 'In this paper, we study Adam in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. We consider a general noise model which governs affine variance noise, bounded noise, and sub-Gaussian noise. We show that Adam with a specific hyper-parameter setup can find a stationary point with a $\\\\mathcal{O}(\\\\text{poly}(\\\\log T)/\\\\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to capture the smooth property of many practical objective functions more accurately.',\n",
       "  346: 'Classical algorithms for market equilibrium computation such as proportional response dynamics face scalability issues with Internet-based applications such as auctions, recommender systems, and fair division, despite having an almost linear runtime in terms of the product of buyers and goods. In this work, we provide the first quantum algorithm for market equilibrium computation with sub-linear performance. Our algorithm provides a polynomial runtime speedup in terms of the product of the number of buyers and goods while reaching the same optimization objective value as the classical algorithm. Numerical simulations of a system with 16384 buyers and goods support our theoretical results that our quantum algorithm provides a significant speedup.',\n",
       "  347: 'In the adversarial streaming model, the input is a sequence of adaptive updates that defines an underlying dataset and the goal is to approximate, collect, or compute some statistic while using space sublinear in the size of the dataset. In 2022, Ben-Eliezer, Eden, and Onak showed a dense-sparse trade-off technique that elegantly combined sparse recovery with known techniques using differential privacy and sketch switching to achieve adversarially robust algorithms for $L_p$ estimation and other algorithms on turnstile streams. However, there has been no progress since, either in terms of achievability or impossibility. In this work, we first give improved algorithms for adversarially robust $L_p$-heavy hitters, utilizing deterministic turnstile heavy-hitter algorithms with better tradeoffs. We then utilize our heavy-hitter algorithm to reduce the problem to estimating the frequency moment of the tail vector. We give a new algorithm for this problem in the classical streaming setting, which achieves additive error and uses space independent in the size of the tail. We then leverage these ingredients to give an improved algorithm for adversarially robust $L_p$ estimation on turnstile streams. We believe that our results serve as an important conceptual message, demonstrating that there is no inherent barrier at the previous state-of-the-art.',\n",
       "  348: \"Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks. Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text. In this work, we propose a novel calibration method that can be used to combat hallucinations. We add a special [IDK] (“I Don't Know”) token to the model's vocabulary and introduce an objective function that shifts probability mass to the [IDK] token for incorrect predictions. This approach allows the model to express uncertainty in its output explicitly. We evaluate our proposed method across multiple model architectures and factual downstream tasks.We find that models trained with our method are able to express uncertainty in places where they would previously make mistakes while suffering only a small loss of encoded knowledge. We further perform extensive ablation studies of multiple variations of our approach and provide a detailed analysis of the precision-recall tradeoff of our method.\",\n",
       "  349: 'Probabilistic integral circuits (PICs) have been recently introduced as probabilistic models enjoying the key ingredient behind expressive generative models: continuous latent variables (LVs). PICs are symbolic computational graphs defining continuous LV models as hierarchies of functions that are summed and multiplied together, or integrated over some LVs. They are tractable if LVs can be analytically integrated out, otherwise they can be approximated by tractable probabilistic circuits (PC) encoding a hierarchical numerical quadrature process, called QPCs.So far, only tree-shaped PICs have been explored, and training them via numerical quadrature requires memory-intensive processing at scale. In this paper, we address these issues, and present: (i) a pipeline for building DAG-shaped PICs out of arbitrary variable decompositions, (ii) a procedure for training PICs using tensorized circuit architectures, and (iii) neural functional sharing techniques to allow scalable training. In extensive experiments, we showcase the effectiveness of functional sharing and the superiority of QPCs over traditional PCs.',\n",
       "  350: 'Graph neural networks (GNNs) have been analyzed from multiple perspectives, including the WL-hierarchy, which exposes limits on their expressivity to distinguish graphs. However, characterizing the class of functions that they learn has remained unresolved. We address this fundamental question for message passing GNNs under ReLU activations, i.e., the de-facto choice for most GNNs.We first show that such GNNs learn tropical rational signomial maps or continuous piecewise linear functions, establishing an equivalence with feedforward networks (FNNs). We then elucidate the role of the choice of aggregation and update functions, and derive the first general upper and lower bounds on the geometric complexity (i.e., the number of linear regions), establishing new results for popular architectures such as GraphSAGE and GIN. We also introduce and theoretically analyze several new architectures to illuminate the relative merits of the feedforward and the message passing layers, and the tradeoffs involving depth and number of trainable parameters.  Finally, we also characterize the decision boundary for node and graph classification tasks.',\n",
       "  351: 'In this paper, we explores the expressivity and trainability of the Fourier Neural Operator (FNO). We establish a mean-field theory for the FNO, analyzing the behavior of the random FNO from an \\\\emph{edge of chaos} perspective. Our investigation into the expressivity of a random FNO involves examining the ordered-chaos phase transition of the network based on the weight distribution. This phase transition demonstrates characteristics unique to the FNO, induced by mode truncation, while also showcasing similarities to those of densely connected networks. Furthermore, we identify a connection between expressivity and trainability: the ordered and chaotic phases correspond to regions of vanishing and exploding gradients, respectively. This finding provides a practical prerequisite for the stable training of the FNO. Our experimental results corroborate our theoretical findings.',\n",
       "  352: \"The attention mechanism within the transformer architecture enables the model to weigh and combine tokens based on their relevance to the query. While self-attention has enjoyed major success, it notably treats all queries $q$ in the same way by applying the mapping $V^\\\\top\\\\text{softmax}(Kq)$, where $V,K$ are the value and key embeddings respectively. In this work, we argue that this uniform treatment hinders the ability to control contextual sparsity and relevance. As a solution, we introduce the Selective Self-Attention (SSA) layer that augments the softmax nonlinearity with a principled temperature scaling strategy. By controlling temperature, SSA adapts the contextual sparsity of the attention map to the query embedding and its position in the context window. Through theory and experiments, we demonstrate that this alleviates attention dilution, aids the optimization process, and enhances the model's ability to control softmax spikiness of individual queries. We also incorporate temperature scaling for value embeddings and show that it boosts the model's ability to suppress irrelevant/noisy tokens. Notably, SSA is a lightweight method which introduces less than 0.5\\\\% new parameters through a weight-sharing strategy and can be fine-tuned on existing LLMs. Extensive empirical evaluations demonstrate that SSA-equipped models achieve a noticeable and consistent accuracy improvement on language modeling benchmarks.\",\n",
       "  353: \"Differentially private stochastic gradient descent (DP-SGD) has been instrumental in privately training deep learning models by providing a framework to control and track the privacy loss incurred during training.  At the core of this computation lies a subsampling method that uses a privacy amplification lemma to enhance the privacy guarantees provided by the additive noise. Fixed size subsampling is appealing for its constant memory usage, unlike the variable sized minibatches in Poisson subsampling. It is also of interest in addressing class imbalance and federated learning. Current computable guarantees for fixed-size subsampling are not tight and do not consider both add/remove and replace-one adjacency relationships. We present a new and holistic Rényi differential privacy (RDP)  accountant for DP-SGD with fixed-size subsampling without replacement (FSwoR) and with replacement (FSwR). For FSwoR we consider both add/remove and replace-one adjacency, where we improve on the best current computable bound by a factor of $4$. We also show for the first time that the widely-used Poisson subsampling and FSwoR with replace-one adjacency have the same privacy to leading order in the sampling probability. Our work suggests that FSwoR is often preferable to Poisson subsampling due to constant memory usage. Our FSwR accountant includes explicit non-asymptotic upper and lower bounds and, to the  authors' knowledge, is the first such RDP analysis of fixed-size  subsampling with replacement  for DP-SGD. We analytically and empirically compare fixed size and Poisson subsampling, and show that DP-SGD gradients in a fixed-size subsampling regime exhibit lower variance in practice in addition to memory usage benefits.\",\n",
       "  354: 'We investigate the problem of universal online learning with gradient-variation regret. Universal online learning aims to achieve regret guarantees without prior knowledge of the curvature of the online functions. Moreover, we study the problem-dependent gradient-variation regret as it plays a crucial role in bridging stochastic and adversarial optimization as well as game theory. In this work, we design a universal approach with the *optimal* gradient-variation regret simultaneously for strongly convex, exp-concave, and convex functions, thus addressing an open problem highlighted by [Yan et al. [2023]](https://openreview.net/forum?id=AA1xrgAP5z). Our approach is *simple* since it is algorithmically efficient-to-implement with a two-layer online ensemble structure and only $1$ gradient query per round, and theoretically easy-to-analyze with a novel and alternative analysis to the gradient-variation regret. Concretely, previous works on gradient variations require controlling the algorithmic stability, which is challenging and leads to sub-optimal regret and less efficient algorithm design. Our analysis overcomes this issue by using a Bregman divergence negative term from linearization and a useful smoothness property.',\n",
       "  355: 'Referring 3D Segmentation is a visual-language task that segments all points of the specified object from a 3D point cloud described by a sentence of query. Previous works perform a two-stage paradigm, first conducting language-agnostic instance segmentation then matching with given text query. However, the semantic concepts from text query and visual cues are separately interacted during the training, and both instance and semantic labels for each object are required, which is time consuming and human-labor intensive. To mitigate these issues, we propose a novel Referring 3D Segmentation pipeline, Label-Efficient and Single-Stage, dubbed LESS, which is only under the supervision of efficient binary mask. Specifically, we design a Point-Word Cross-Modal Alignment module for aligning the fine-grained features of points and textual embedding. Query Mask Predictor module and Query-Sentence Alignment module are introduced for coarse-grained alignment between masks and query. Furthermore, we propose an area regularization loss, which coarsely reduces irrelevant background predictions on a large scale. Besides, a point-to-point contrastive loss is proposed concentrating on distinguishing points with subtly similar features. Through extensive experiments, we achieve state-of-the-art performance on ScanRefer dataset by surpassing the previous methods about 3.7% mIoU using only binary labels. Code is available at https://github.com/mellody11/LESS.',\n",
       "  356: 'We consider the problem of hypothesis testing for discrete distributions. In the standard model, where we have sample access to an underlying distribution $p$, extensive research has established optimal bounds for uniformity testing,  identity testing (goodness of fit), and closeness testing (equivalence or two-sample testing). We explore these problems in a setting where a predicted data distribution, possibly derived from historical data or predictive machine learning models, is available. We demonstrate that such a predictor can indeed reduce the number of samples required for all three property testing tasks. The reduction in sample complexity depends directly on the predictor’s quality, measured by its total variation distance from $p$. A key advantage of our algorithms is their adaptability to the precision of the prediction. Specifically, our algorithms can self-adjust their sample complexity based on the accuracy of the available prediction, operating without any prior knowledge of the estimation’s accuracy (i.e. they are consistent). Additionally, we never use more samples than the standard approaches require, even if the predictions provide no meaningful information (i.e. they are also robust). We provide lower bounds to indicate that the improvements in sample complexity achieved by our algorithms are information-theoretically optimal. Furthermore, experimental results show that the performance of our algorithms on real data significantly exceeds our worst-case guarantees for sample complexity, demonstrating the practicality of our approach.',\n",
       "  357: 'In the last years, the research interest in visual navigation towards objects in indoor environments has grown significantly. This growth can be attributed to the recent availability of large navigation datasets in photo-realistic simulated environments, like Gibson and Matterport3D. However, the navigation tasks supported by these datasets are often restricted to the objects present in the environment at acquisition time. Also, they fail to account for the realistic scenario in which the target object is a user-specific instance that can be easily confused with similar objects and may be found in multiple locations within the environment. To address these limitations, we propose a new task denominated Personalized Instance-based Navigation (PIN), in which an embodied agent is tasked with locating and reaching a specific personal object by distinguishing it among multiple instances of the same category. The task is accompanied by PInNED, a dedicated new dataset composed of photo-realistic scenes augmented with additional 3D objects. In each episode, the target object is presented to the agent using two modalities: a set of visual reference images on a neutral background and manually annotated textual descriptions. Through comprehensive evaluations and analyses, we showcase the challenges of the PIN task as well as the performance and shortcomings of currently available methods designed for object-driven navigation, considering modular and end-to-end agents.',\n",
       "  358: 'In cross-domain few-shot classification (CFC), recent works mainly focus on adapting a simple transformation head on top of a frozen pre-trained backbone with few labeled data to project embeddings into a task-specific metric space where classification can be performed by measuring similarities between image instance and prototype representations. Technically, an assumption implicitly adopted in such a framework is that the prototype and image instance embeddings share the same representation transformation. However, in this paper, we find that there naturally exists a gap, which resembles the modality gap, between the prototype and image instance embeddings extracted from the frozen pre-trained backbone, and simply applying the same transformation during the adaptation phase constrains exploring the optimal representation distributions and shrinks the gap between prototype and image representations. To solve this problem, we propose a simple yet effective method, contrastive prototype-image adaptation (CoPA), to adapt different transformations for prototypes and images similarly to CLIP by treating prototypes as text prompts.   Extensive experiments on Meta-Dataset demonstrate that CoPA achieves the state-of-the-art performance more efficiently. Meanwhile, further analyses also indicate that CoPA can learn better representation clusters, enlarge the gap, and achieve the minimum validation loss at the enlarged gap.',\n",
       "  359: 'Reinforcement Learning (RL) problem with general utility is a powerful decision making framework that covers standard RL with cumulative cost, exploration problems, and demonstration learning. Existing works on RL with general utility do not consider the robustness under environmental perturbation, which is important to adapt RL system in the real-world environment that differs from the training environment. To train a robust policy, we propose a robust RL framework with general utility,  which subsumes many existing RL frameworks including RL, robust RL, RL with general utility, constrained RL, robust constrained RL, pure exploration, robust entropy regularized RL, etc. Then we focus on popular convex utility functions, with which our proposed learning framework is a challenging nonconvex-nonconcave minimax optimization problem, and design a two-phase stochastic policy gradient type algorithm and obtain its sample complexity result for gradient convergence. Furthermore, for convex utility on a widely used polyhedral ambiguity set, we design an algorithm and obtain its convergence rate to a global optimal solution.',\n",
       "  360: 'Consider a hiring process with candidates coming from different universities. It is easy to order candidates with the same background, yet it can be challenging to compare them otherwise. The latter case requires additional costly assessments, leading to a potentially high total cost for the hiring organization. Given an assigned budget, what would be an optimal strategy to select the most qualified candidate?We model the above problem as a multicolor secretary problem, allowing comparisons between candidates from distinct groups at a fixed cost. Our study explores how the allocated budget enhances the success probability of online selection algorithms.',\n",
       "  361: 'Monocular 3D object detection aims for precise 3D localization and identification of objects from a single-view image. Despite its recent progress, it often struggles while handling pervasive object occlusions that tend to complicate and degrade the prediction of object dimensions, depths, and orientations. We design MonoMAE, a monocular 3D detector inspired by Masked Autoencoders that addresses the object occlusion issue by masking and reconstructing objects in the feature space. MonoMAE consists of two novel designs. The first is depth-aware masking that selectively masks certain parts of non-occluded object queries in the feature space for simulating occluded object queries for network training. It masks non-occluded object queries by balancing the masked and preserved query portions adaptively according to the depth information. The second is lightweight query completion that works with the depth-aware masking to learn to reconstruct and complete the masked object queries. With the proposed feature-space occlusion and completion, MonoMAE learns enriched 3D representations that achieve superior monocular 3D detection performance qualitatively and quantitatively for both occluded and non-occluded objects. Additionally, MonoMAE learns generalizable representations that can work well in new domains.',\n",
       "  362: 'We define maximum entropy goal-directedness (MEG), a formal measure of goal-directedness in causal models and Markov decision processes, and give algorithmsfor computing it. Measuring goal-directedness is important, as it is a criticalelement of many concerns about harm from AI. It is also of philosophical interest,as goal-directedness is a key aspect of agency. MEG is based on an adaptation ofthe maximum causal entropy framework used in inverse reinforcement learning. Itcan measure goal-directedness with respect to a known utility function, a hypothesisclass of utility functions, or a set of random variables. We prove that MEG satisfiesseveral desiderata and demonstrate our algorithms with small-scale experiments.',\n",
       "  363: 'Deep neural networks (DNNs) are widely used models for investigating biological visual representations. However, existing DNNs are mostly designed to analyze neural responses to static images, relying on feedforward structures and lacking physiological neuronal mechanisms. There is limited insight into how the visual cortex represents natural movie stimuli that contain context-rich information. To address these problems, this work proposes the long-range feedback spiking network (LoRaFB-SNet), which mimics top-down connections between cortical regions and incorporates spike information processing mechanisms inherent to biological neurons. Taking into account the temporal dependence of representations under movie stimuli, we present Time-Series Representational Similarity Analysis (TSRSA) to measure the similarity between model representations and visual cortical representations of mice. LoRaFB-SNet exhibits the highest level of representational similarity, outperforming other well-known and leading alternatives across various experimental paradigms, especially when representing long movie stimuli. We further conduct experiments to quantify how temporal structures (dynamic information) and static textures (static information) of the movie stimuli influence representational similarity, suggesting that our model benefits from long-range feedback to encode context-dependent representations just like the brain. Altogether, LoRaFB-SNet is highly competent in capturing both dynamic and static representations of the mouse visual cortex and contributes to the understanding of movie processing mechanisms of the visual system. Our codes are available at https://github.com/Grasshlw/SNN-Neural-Similarity-Movie.',\n",
       "  364: 'Deep neural networks (DNNs) often suffer from the overconfidence issue, where incorrect predictions are made with high confidence scores, hindering the applications in critical systems. In this paper, we propose a novel approach called Typicalness-Aware Learning (TAL) to address this issue and improve failure detection performance. We observe that, with the cross-entropy loss, model predictions are optimized to align with the corresponding labels via increasing logit magnitude or refining logit direction. However, regarding atypical samples, the image content and their labels may exhibit disparities. This discrepancy can lead to overfitting on atypical samples, ultimately resulting in the overconfidence issue that we aim to address.To address this issue, we have devised a metric that quantifies the typicalness of each sample, enabling the dynamic adjustment of the logit magnitude during the training process. By allowing relatively atypical samples to be adequately fitted while preserving reliable logit direction, the problem of overconfidence can be mitigated. TAL has been extensively evaluated on benchmark datasets, and the results demonstrate its superiority over existing failure detection methods. Specifically, TAL achieves a more than 5\\\\% improvement on CIFAR100 in terms of the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art. Code is available at https://github.com/liuyijungoon/TAL.',\n",
       "  365: 'Visual Question Answering~(VQA) is an important task in multimodal AI, which requires models to understand and reason on knowledge present in visual and textual data. However, most of the current VQA datasets and models are primarily focused on English and a few major world languages, with images that are Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, some datasets extend the text to other languages, either via translation or some other approaches, but usually keep the same images, resulting in narrow cultural representation. To address these limitations, we create CVQA, a new Culturally-diverse Multilingual Visual Question Answering benchmark dataset, designed to cover a rich set of languages and regions, where we engage native speakers and cultural experts in the data collection process. CVQA includes culturally-driven images and questions from across 28 countries in four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and we show that the dataset is challenging for the current state-of-the-art models. This benchmark will serve as a probing evaluation suite for assessing the cultural bias of multimodal models and hopefully encourage more research efforts towards increasing cultural awareness and linguistic diversity in this field.',\n",
       "  366: 'Code generation models have increasingly become integral to aiding software development. Although current research has thoroughly examined the correctness of the code produced by code generation models, a vital aspect that plays a pivotal role in greencomputing and sustainability efforts — the efficiency of the generated code — has often been neglected. This paper presents Effibench, a benchmark with 1,000 efficiency-critical coding problems to assess the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution, which obtains the SOTA efficiency on the LeetCode solution leaderboard. With EffiBench, we empirically examine the ability of 42 large language models (35 open-source and 7 closed-source) to generate efficient code. Our evaluation results demonstrate that the efficiency of the code generated by LLMs is generally worse than the efficiency of human-written canonical solutions. For example, GPT-4 generated code has an average \\\\textbf{3.12} times execution time that of the human-written canonical solutions. In the most extreme cases, the execution time and total memory usage of GPT-4 code are \\\\textbf{13.89} and \\\\textbf{43.92} times that of the canonical solutions. The source code of EffiBench is released on https://github.com/huangd1999/EffiBench. We also provide the LeaderBoard in https://huggingface.co/spaces/EffiBench/effibench-leaderboard.',\n",
       "  367: 'We present PutnamBench, a new multi-language benchmark for evaluating the ability of neural theorem-provers to solve competition mathematics problems. PutnamBench consists of 1692 hand-constructed formalizations of 640 theorems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problems have formalizations in Lean 4 and Isabelle; a substantial subset also has Coq formalizations. PutnamBench requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We use PutnamBench to evaluate several established neural and symbolic theorem-provers. These approaches can only solve a handful of the PutnamBench problems, establishing the benchmark as a difficult open challenge for research on neural theorem-proving. PutnamBench is available at https://github.com/trishullab/PutnamBench.',\n",
       "  368: 'Understanding cognitive processes in multi-agent interactions is a primary goal in cognitive science. It can guide the direction of artificial intelligence (AI) research toward social decision-making in multi-agent systems, which includes uncertainty from character heterogeneity. In this paper, we introduce episodic future thinking (EFT) mechanism for a reinforcement learning (RL) agent, inspired by the cognitive processes observed in animals. To enable future thinking functionality, we first develop a multi-character policy that captures diverse characters with an ensemble of heterogeneous policies. The character of an agent is defined as a different weight combination on reward components, representing distinct behavioral preferences. The future thinking agent collects observation-action trajectories of the target agents and leverages the pre-trained multi-character policy to infer their characters. Once the character is inferred, the agent predicts the upcoming actions of target agents and simulates the potential future scenario. This capability allows the agent to adaptively select the optimal action, considering the predicted future scenario in multi-agent scenarios. To evaluate the proposed mechanism, we consider the multi-agent autonomous driving scenario in which autonomous vehicles with different driving traits are on the road. Simulation results demonstrate that the EFT mechanism with accurate character inference leads to a higher reward than existing multi-agent solutions. We also confirm that the effect of reward improvement remains valid across societies with different levels of character diversity.',\n",
       "  369: 'Recent State Space Models (SSMs) such as S4, S5, and Mamba have shown remarkable computational benefits in long-range temporal dependency modeling. However, in many sequence modeling problems, the underlying process is inherently modular and it is of interest to have inductive biases that mimic this modular structure. In this paper, we introduce SlotSSMs, a novel framework for incorporating independent mechanisms into SSMs to preserve or encourage separation of information. Unlike conventional SSMs that maintain a monolithic state vector, SlotSSMs maintains the state as a collection of multiple vectors called slots. Crucially, the state transitions are performed independently per slot with sparse interactions across slots implemented via the bottleneck of self-attention. In experiments, we evaluate our model in object-centric learning, 3D visual reasoning, and long-context video understanding tasks, which involve modeling multiple objects and their long-range temporal dependencies. We find that our proposed design offers substantial performance gains over existing sequence modeling methods. Project page is available at \\\\url{https://slotssms.github.io/}',\n",
       "  370: 'Human Mesh Recovery (HMR) is the task of estimating a parameterized 3D human mesh from an image. There is a kind of methods first training a regression model for this problem, then further optimizing the pretrained regression model for any specific sample individually at test time. However, the pretrained model may not provide an ideal optimization starting point for the test-time optimization. Inspired by meta-learning, we incorporate the test-time optimization into training, performing a step of test-time optimization for each sample in the training batch before really conducting the training optimization over all the training samples. In this way, we obtain a meta-model, the meta-parameter of which is friendly to the test-time optimization. At test time, after several test-time optimization steps starting from the meta-parameter, we obtain much higher HMR accuracy than the test-time optimization starting from the simply pretrained regression model. Furthermore, we find test-time HMR objectives are different from training-time objectives, which reduces the effectiveness of the learning of the meta-model. To solve this problem, we propose a dual-network architecture that unifies the training-time and test-time objectives. Our method, armed with meta-learning and the dual networks, outperforms state-of-the-art regression-based and optimization-based HMR approaches, as validated by the extensive experiments. The codes are available at https://github.com/fmx789/Meta-HMR.',\n",
       "  371: 'Causal discovery from observational data, especially for count data, is essential across scientific and industrial contexts, such as biology, economics, and network operation maintenance. For this task, most approaches model count data using Bayesian networks or ordinal relations. However, they overlook the inherent branching structures that are frequently encountered, e.g., a browsing event might trigger an adding cart or purchasing event. This can be modeled by a binomial thinning operator (for branching) and an additive independent Poisson distribution (for noising), known as Poisson Branching Structure Causal Model (PB-SCM). There is a provably sound cumulant-based causal discovery method that allows the identification of the causal structure under a branching structure. However, we show that there still remains a gap in that there exist causal directions that are identifiable while the algorithm fails to identify them. In this work, we address this gap by exploring the identifiability of PB-SCM using the Probability Generating Function (PGF). By developing a compact and exact closed-form solution for the PGF of PB-SCM, we demonstrate that each component in this closed-form solution uniquely encodes a specific local structure, enabling the identification of the local structures by testing their corresponding component appearances in the PGF. Building on this, we propose a practical algorithm for learning causal skeletons and identifying causal directions of PB-SCM using PGF. The effectiveness of our method is demonstrated through experiments on both synthetic and real datasets.',\n",
       "  372: 'Epitope identification is vital for antibody design yet challenging due to the inherent variability in antibodies. While many deep learning methods have been developed for general protein binding site prediction tasks, whether they work for epitope prediction remains an understudied research question. The challenge is also heightened by the lack of a consistent evaluation pipeline with sufficient dataset size and epitope diversity. We introduce a filtered antibody-antigen complex structure dataset, AsEP (Antibody-specific Epitope Prediction). AsEP is the largest of its kind and provides clustered epitope groups, allowing the community to develop and test novel epitope prediction methods and evaluate their generalisability. AsEP comes with an easy-to-use interface in Python and pre-built graph representations of each antibody-antigen complex while also supporting customizable embedding methods. Using this new dataset, we benchmark several representative general protein-binding site prediction methods and find that their performances fall short of expectations for epitope prediction. To address this, we propose a novel method, WALLE, which leverages both unstructured modeling from protein language models and structural modeling from graph neural networks. WALLE demonstrate up to 3-10X performance improvement over the baseline methods. Our empirical findings suggest that epitope prediction benefits from combining sequential features provided by language models with geometrical information from graph representations. This provides a guideline for future epitope prediction method design. In addition, we reformulate the task as bipartite link prediction, allowing convenient model performance attribution and interpretability. We open source our data and code at https://github.com/biochunan/AsEP-dataset.',\n",
       "  373: 'We present a formulation of flow matching as variational inference, which we refer to as variational flow matching (VFM). We use this formulation to develop CatFlow, a flow matching method for categorical data that is easy to implement, computationally efficient, and achieves strong results on graph generation tasks. In VFM, the objective is to approximate the posterior probability path, which is a distribution over possible end points of a trajectory. VFM admits both the original flow matching objective and the CatFlow objective as special cases. We also relate VFM to score-based models, in which the dynamics are stochastic rather than deterministic, and derive a bound on the model likelihood based on a reweighted VFM objective. We evaluate CatFlow on one abstract graph generation task and two molecular generation tasks. In all cases, CatFlow exceeds or matches performance of the current state-of-the-art models.',\n",
       "  374: 'InfoNCE loss is commonly used to train dense retriever in information retrieval tasks. It is well known that a large batch is essential to stable and effective training with InfoNCE loss, which requires significant hardware resources. Due to the dependency of large batch, dense retriever has bottleneck of application and research. Recently, memory reduction methods have been broadly adopted to resolve the hardware bottleneck by decomposing forward and backward or using a memory bank. However, current methods still suffer from slow and unstable train. To address these issues, we propose Contrastive Accumulation (ContAccum), a stable and efficient memory reduction method for dense retriever trains that uses a dual memory bank structure to leverage previously generated query and passage representations. Experiments on widely used five information retrieval datasets indicate that ContAccum can surpass not only existing memory reduction methods but also high-resource scenarios. Moreover, theoretical analysis and experimental results confirm that ContAccum provides more stable dual-encoder training than current memory bank utilization methods.',\n",
       "  375: 'Road network representation learning aims to learn compressed and effective vectorized representations for road segments that are applicable to numerous tasks. In this paper, we identify the limitations of existing methods, particularly their overemphasis on the distance effect as outlined in the First Law of Geography. In response,  we propose to endow road network representation with the principles of the recent Third Law of Geography. To this end, we propose a novel graph contrastive learning framework that employs geographic configuration-aware graph augmentation and spectral negative sampling, ensuring that road segments with similar geographic configurations yield similar representations, and vice versa, aligning with the principles stated in the Third Law. The framework further fuses the Third Law with the First Law through a dual contrastive learning objective to effectively balance the implications of both laws. We evaluate our framework on two real-world datasets across three downstream tasks. The results show that the integration of the Third Law significantly improves the performance of road segment representations in downstream tasks.',\n",
       "  376: 'While Distributed Machine Learning (DML) has been widely used to achieve decent performance, it is still challenging to take full advantage of data and devices distributed at multiple vantage points to adapt and learn, especially it is non-trivial to address dynamic and divergence challenges based on the linear aggregation framework as follows: (1) heterogeneous learning data at different devices (i.e., non-IID data) resulting in model divergence and (2) in the case of time-varying communication links, the limited ability for devices to reconcile model divergence. In this paper, we contribute a non-linear class aggregation framework HyperPrism that leverages distributed mirror descent with averaging done in the mirror descent dual space and adapts the degree of Weighted Power Mean (WPM) used in each round. Moreover, HyperPrism could adaptively choose different mapping for different layers of the local model with a dedicated hypernetwork per device, achieving automatic optimization of DML in high divergence settings. We perform rigorous analysis and experimental evaluations to demonstrate the effectiveness of adaptive, mirror-mapping DML. In particular, we extend the generalizability of existing related works and position them as special cases within HyperPrism. Our experimental results show that HyperPrism can improve the convergence speed up to 98.63% and scale well to more devices compared with the state-of-the-art, all with little additional computation overhead compared to traditional linear aggregation.',\n",
       "  377: 'Training diffusion models for audiovisual sequences allows for a range of generation tasks by learning conditional distributions of various input-output combinations of the two modalities. Nevertheless, this strategy often requires training a separate model for each task which is expensive. Here, we propose a novel training approach to effectively learn arbitrary conditional distributions in the audiovisual space. Our key contribution lies in how we parameterize the diffusion timestep in the forward diffusion process. Instead of the standard fixed diffusion timestep, we propose applying variable diffusion timesteps across the temporal dimension and across modalities of the inputs. This formulation offers flexibility to introduce variable noise levels for various portions of the input, hence the term mixture of noise levels. We propose a transformer-based audiovisual latent diffusion model and show that it can be trained in a task-agnostic fashion using our approach to enable a variety of audiovisual generation tasks at inference time. Experiments demonstrate the versatility of our method in tackling cross-modal and multimodal interpolation tasks in the audiovisual space. Notably, our proposed approach surpasses baselines in generating temporally and perceptually consistent samples conditioned on the input. Project page: neurips13025.github.io',\n",
       "  378: 'Manipulating garments and fabrics has long been a critical endeavor in the development of home-assistant robots. However, due to complex dynamics and topological structures, garment manipulations pose significant challenges. Recent successes in reinforcement learning and vision-based methods offer promising avenues for learning garment manipulation. Nevertheless, these approaches are severely constrained by current benchmarks, which exhibit offer limited diversity of tasks and unrealistic simulation behavior. Therefore, we present GarmentLab, a content-rich benchmark and realistic simulation designed for deformable object and garment manipulation. Our benchmark encompasses a diverse range of garment types, robotic systems and manipulators. The abundant tasks in the benchmark further explores of the interactions between garments, deformable objects, rigid bodies, fluids, and human body. Moreover, by incorporating multiple simulation methods such as FEM and PBD, along with our proposed sim-to-real algorithms and real-world benchmark, we aim to significantly narrow the sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement learning, and imitation learning approaches on these tasks, highlighting the challenges faced by current algorithms, notably their limited generalization capabilities. Our proposed open-source environments and comprehensive analysis show promising boost to future research in garment manipulation by unlocking the full potential of these methods. We guarantee that we will open-source our code as soon as possible. You can watch the videos in supplementary files to learn more about the details of our work.',\n",
       "  379: 'Many important datasets contain samples that are missing one or more feature values. Maintaining the interpretability of machine learning models in the presence of such missing data is challenging. Singly or multiply imputing missing values complicates the model’s mapping from features to labels. On the other hand, reasoning on indicator variables that represent missingness introduces a potentially large number of additional terms, sacrificing sparsity. We solve these problems with M-GAM, a sparse, generalized, additive modeling approach that incorporates missingness indicators and their interaction terms while maintaining sparsity through $\\\\ell_0$ regularization. We show that M-GAM provides similar or superior accuracy to prior methods while significantly improving sparsity relative to either imputation or naïve inclusion of indicator variables.',\n",
       "  380: \"Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models (LLMs) while maintaining an identical sampling distribution. However, the conventional approach of training separate draft model to achieve a satisfactory token acceptance rate can be costly and impractical. In this paper, we propose a novel self-speculative decoding framework \\\\emph{Kangaroo} with \\\\emph{double} early exiting strategy, which leverages the shallow sub-network and the \\\\texttt{LM Head} of the well-trained target LLM to construct a self-drafting model. Then, the self-verification stage only requires computing the remaining layers over the \\\\emph{early-exited} hidden states in parallel. To bridge the representation gap between the sub-network and the full model, we train a lightweight and efficient adapter module on top of the sub-network. One significant challenge that comes with the proposed method is that the inference latency of the self-draft model may no longer be negligible compared to the big model. To boost the token acceptance rate while minimizing the latency of the self-drafting model, we introduce an additional \\\\emph{early exiting} mechanism for both single-sequence and the tree decoding scenarios. Specifically, we dynamically halt the small model's subsequent prediction during the drafting phase once the confidence level for the current step falls below a certain threshold. This approach reduces unnecessary computations and improves overall efficiency. Extensive experiments on multiple benchmarks demonstrate our effectiveness, where Kangaroo achieves walltime speedups up to 2.04$\\\\times$, outperforming Medusa-1 with 88.7\\\\% fewer additional parameters. The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo.\",\n",
       "  381: 'When solving long-horizon tasks, it is intriguing to decompose the high-level task into subtasks. Decomposing experiences into reusable subtasks can improve data efficiency, accelerate policy generalization, and in general provide promising solutions to multi-task reinforcement learning and imitation learning problems. However, the concept of subtasks is not sufficiently understood and modeled yet, and existing works often overlook the true structure of the data generation process: subtasks are the results of a selection mechanism on actions, rather than possible underlying confounders or intermediates. Specifically, we provide a theory to identify, and experiments to verify the existence of selection variables in such data. These selections serve as subgoals that indicate subtasks and guide policy. In light of this idea, we develop a sequential non-negative matrix factorization (seq- NMF) method to learn these subgoals and extract meaningful behavior patterns as subtasks. Our empirical results on a challenging Kitchen environment demonstrate that the learned subtasks effectively enhance the generalization to new tasks in multi-task imitation learning scenarios. The codes are provided at this link.',\n",
       "  382: 'Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for tuning large language models.The problem is fairly understood in toy settings with linear target functions or over finite small domains that limits practical interest.Taking the next step, we consider infinite domains and kernelized rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm.We propose MaxMinLCB, which emulates this trade-off as a zero-sum Stackelberg game and chooses action pairs that are informative and have favorable reward values. MaxMinLCB consistently outperforms algorithms in the literature and satisfies an anytime-valid rate-optimal regret guarantee. This is owed to our novel preference-based confidence sequences for kernelized logistic estimators, which are of independent interest.',\n",
       "  383: 'We present a differentiable representation, DMesh, for general 3D triangular meshes. DMesh considers both the geometry and connectivity information of a mesh. In our design, we first get a set of convex tetrahedra that compactly tessellates the domain based on Weighted Delaunay Triangulation (WDT), and select triangular faces on the tetrahedra to define the final mesh. We formulate probability of faces to exist on the actual surface in a differentiable manner based on the WDT. This enables DMesh to represent meshes of various topology in a differentiable way, and allows us to reconstruct the mesh under various observations, such as point clouds and multi-view images using gradient-based optimization. We publicize the source code and supplementary material at our project page (https://sonsang.github.io/dmesh-project).',\n",
       "  384: \"Drug-induced toxicity is one of the leading reasons new drugs fail clinical trials. Machine learning models that predict drug toxicity from molecular structure could help researchers prioritize less toxic drug candidates. However, current toxicity datasets are typically small and limited to a single organ system (e.g., cardio, renal, or liver). Creating these datasets often involved time-intensive expert curation by parsing drug labelling documents that can exceed 100 pages per drug. Here, we introduce UniTox, a unified dataset of 2,418 FDA-approved drugs with drug-induced toxicity summaries and ratings created by using GPT-4o to process FDA drug labels. UniTox spans eight types of toxicity: cardiotoxicity, liver toxicity, renal toxicity, pulmonary toxicity, hematological toxicity, dermatological toxicity, ototoxicity, and infertility. This is, to the best of our knowledge, the largest such systematic human in vivo database by number of drugs and toxicities, and the first covering nearly all non-combination FDA-approved medications for several of these toxicities. We recruited clinicians to validate a random sample of our GPT-4o annotated toxicities, and UniTox's toxicity ratings concord with clinician labelers 85-96\\\\% of the time. Finally, we benchmark several machine learning models trained on UniTox to demonstrate the utility of this dataset for building molecular toxicity prediction models.\",\n",
       "  385: 'The sparsely activated mixture of experts (MoE) model presents an effective alternative to densely activated (dense) models, combining improved accuracy with computational efficiency. However, training MoE models from scratch requires extensive data and computational resources, a challenge that limits their widespread adoption. To address this, we introduce MoE Jetpack, a framework designed to fine-tune the abundant and easily accessible dense checkpoints into MoE models. MoE Jetpack incorporates two key techniques: (1) checkpoint recycling, which initializes MoE models with dense checkpoints to accelerate convergence and enhance accuracy, minimizing the need for extensive pre-training; (2) the hyperspherical adaptive MoE (SpheroMoE) layer, which optimizes the MoE architecture to enhance fine-tuning performance and efficiency.Experimental results indicate that MoE Jetpack doubles the convergence speed and enhances accuracy by 2.8% on ImageNet-1K. On smaller datasets, it achieves up to 8-fold faster convergence and over 30% accuracy gains, highlighting its efficiency.The code is available at https://github.com/Adlith/MoE-Jetpack.',\n",
       "  386: 'Graph diffusion, which iteratively propagates real-valued substances among the graph, is used in numerous graph/network-involved applications. However, releasing diffusion vectors may reveal sensitive linking information in the data such as transaction information in financial network data. However, protecting the privacy of graph data is challenging due to its interconnected nature.    This work proposes a novel graph diffusion framework with edge-level different privacy guarantees by using noisy diffusion iterates.    The algorithm injects Laplace noise per diffusion iteration and adopts a degree-based thresholding function to mitigate the high sensitivity induced by low-degree nodes. Our privacy loss analysis is based on Privacy Amplification by Iteration (PABI), which to our best knowledge, is the first effort that analyzes PABI with Laplace noise and provides relevant applications.    We also introduce a novel $\\\\infty$-Wasserstein distance tracking method, which tightens the analysis of privacy leakage and makes PABI more applicable in practice.     We evaluate this framework by applying it to Personalized Pagerank computation for ranking tasks. Experiments on real-world network data demonstrate the superiority of our method under stringent privacy conditions.',\n",
       "  387: 'Recently, the diffusion-based generative paradigm has achieved impressive general image generation capabilities with text prompts due to its accurate distribution modeling and stable training process. However, generating diverse remote sensing (RS) images that are tremendously different from general images in terms of scale and perspective remains a formidable challenge due to the lack of a comprehensive remote sensing image generation dataset with various modalities, ground sample distances (GSD), and scenes. In this paper, we propose a Multi-modal, Multi-GSD, Multi-scene Remote Sensing (MMM-RS) dataset and benchmark for text-to-image generation in diverse remote sensing scenarios. Specifically, we first collect nine publicly available RS datasets and conduct standardization for all samples. To bridge RS images to textual semantic information, we utilize a large-scale pretrained vision-language model to automatically output text prompts and perform hand-crafted rectification, resulting in information-rich text-image pairs (including multi-modal images). In particular, we design some methods to obtain the images with different GSD and various environments (e.g., low-light, foggy) in a single sample. With extensive manual screening and refining annotations, we ultimately obtain a MMM-RS dataset that comprises approximately 2.1 million text-image pairs. Extensive experimental results verify that our proposed MMM-RS dataset allows off-the-shelf diffusion models to generate diverse RS images across various modalities, scenes, weather conditions, and GSD. The dataset is available at https://github.com/ljl5261/MMM-RS.',\n",
       "  388: 'Capitalizing on the complementary advantages of generative and discriminative models has always been a compelling vision in machine learning, backed by a growing body of research. This work discloses the hidden semantic structure within score-based generative models, unveiling their potential as effective discriminative priors. Inspired by our theoretical findings, we propose DUSA to exploit the structured semantic priors underlying diffusion score to facilitate the test-time adaptation of image classifiers or dense predictors. Notably, DUSA extracts knowledge from a single timestep of denoising diffusion, lifting the curse of Monte Carlo-based likelihood estimation over timesteps. We demonstrate the efficacy of our DUSA in adapting a wide variety of competitive pre-trained discriminative models on diverse test-time scenarios. Additionally, a thorough ablation study is conducted to dissect the pivotal elements in DUSA. Code is publicly available at https://github.com/BIT-DA/DUSA.',\n",
       "  389: \"We propose a novel method, \\\\textbf{TwinAct}, to tackle the challenge of decoupling actions and actors in order to customize the text-guided diffusion models (TGDMs) for few-shot action image generation. TwinAct addresses the limitations of existing methods that struggle to decouple actions from other semantics (e.g., the actor's appearance) due to the lack of an effective inductive bias with few exemplar images. Our approach introduces a common action space, which is a textual embedding space focused solely on actions, enabling precise customization without actor-related details. Specifically, TwinAct involves three key steps: 1) Building common action space based on a set of representative action phrases; 2) Imitating the customized action within the action space; and 3) Generating highly adaptable customized action images in diverse contexts with action similarity loss. To comprehensively evaluate TwinAct, we construct a novel benchmark, which provides sample images with various forms of actions. Extensive experiments demonstrate TwinAct's superiority in generating accurate, context-independent customized actions while maintaining the identity consistency of different subjects, including animals, humans, and even customized actors.\",\n",
       "  390: \"Lensless cameras offer significant advantages in size, weight, and cost compared to traditional lens-based systems. Without a focusing lens, lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements. However, current algorithms struggle with inaccurate forward imaging models and insufficient priors to reconstruct high-quality images. To overcome these limitations, we introduce a novel two-stage approach for consistent and photorealistic lensless image reconstruction. The first stage of our approach ensures data consistency by focusing on accurately reconstructing the low-frequency content with a spatially varying deconvolution method that adjusts to changes in the Point Spread Function (PSF) across the camera's field of view. The second stage enhances photorealism by incorporating a generative prior from pre-trained diffusion models. By conditioning on the low-frequency content retrieved in the first stage, the diffusion model effectively reconstructs the high-frequency details that are typically lost in the lensless imaging process, while also maintaining image fidelity. Our method achieves a superior balance between data fidelity and visual quality compared to existing methods, as demonstrated with two popular lensless systems, PhlatCam and DiffuserCam.\",\n",
       "  391: 'Learning from human preference data has emerged as the dominant paradigm for fine-tuning large language models (LLMs). The two most common families of techniques -- online reinforcement learning (RL) such as Proximal Policy Optimization (PPO) and offline contrastive methods such as Direct Preference Optimization (DPO) -- were positioned as equivalent in prior work due to the fact that both have to start from the same offline preference dataset. To further expand our theoretical understanding of the similarities and differences between online and offline techniques for preference fine-tuning, we conduct a rigorous analysis through the lens of dataset coverage, a concept that captures how the training data covers the test distribution and is widely used in RL. We prove that a global coverage condition is both necessary and sufficient for offline contrastive methods to converge to the optimal policy, but a weaker partial coverage condition suffices for online RL methods. This separation provides one explanation of why online RL methods can perform better than offline methods, especially when the offline preference data is not diverse enough. Finally, motivated by our preceding theoretical observations, we derive a hybrid preference optimization (HyPO) algorithm that uses offline data for contrastive-based preference optimization and online unlabeled data for KL regularization. Theoretically and empirically, we demonstrate that HyPO is more performant than its pure offline counterpart DPO, while still preserving its computation and memory efficiency.',\n",
       "  392: 'This paper scales object-level reconstruction to complex scenes, advancing interactive scene reconstruction. We introduce two datasets, OmniSim and InterReal, featuring 28 scenes with multiple interactive objects. To tackle the challenge of inaccurate interactive motion recovery in complex scenes, we propose LiveScene, a scene-level language-embedded interactive radiance field that efficiently reconstructs and controls multiple objects. By decomposing the interactive scene into local deformable fields, LiveScene enables separate reconstruction of individual object motions, reducing memory consumption. Additionally, our interaction-aware language embedding localizes individual interactive objects, allowing for arbitrary control using natural language. Our approach demonstrates significant superiority in novel view synthesis, interactive scene control, and language grounding performance through extensive experiments. Project page: https://livescenes.github.io.',\n",
       "  393: \"Machine unlearning is the problem of removing the effect of a subset of training data (the ``forget set'') from a trained model without damaging the model's utility e.g. to comply with users' requests to delete their data, or remove mislabeled, poisoned or otherwise problematic data.With unlearning research still being at its infancy, many fundamental open questions exist: Are there interpretable characteristics of forget sets that substantially affect the difficulty of the problem? How do these characteristics affect different state-of-the-art algorithms?With this paper, we present the first investigation aiming to answer these questions. We identify two key factors affecting unlearning difficulty and the performance of unlearning algorithms. Evaluation on forget sets that isolate these identified factors reveals previously-unknown behaviours of state-of-the-art algorithms that don't materialize on random forget sets.Based on our insights, we develop a framework coined Refined-Unlearning Meta-algorithm (RUM) that encompasses: (i) refining the forget set into homogenized subsets, according to different characteristics; and (ii) a meta-algorithm that employs existing algorithms to unlearn each subset and finally delivers a model that has unlearned the overall forget set. We find that RUM substantially improves top-performing unlearning algorithms. Overall, we view our work as an important step in (i) deepening our scientific understanding of unlearning and (ii) revealing new pathways to improving the state-of-the-art.\",\n",
       "  394: 'Simulators are a pervasive tool in reinforcement learning, but most existing algorithms cannot efficiently exploit simulator access -- particularly in high-dimensional domains that require general function approximation. We explore the power of simulators through online reinforcement learning with local simulator access (or, local planning), an RL protocol where the agent is allowed to reset to previously observed states and follow their dynamics during training. We use local simulator access to unlock new statistical guarantees that were previously out of reach:- We show that MDPs with low coverability (Xie et al. 2023) -- a general structural condition that subsumes Block MDPs and Low-Rank MDPs -- can be learned in a sample-efficient fashion with only Q⋆-realizability (realizability of the optimal state-value function); existing online RL algorithms require significantly stronger representation conditions.- As a consequence, we show that the notorious Exogenous Block MDP problem (Efroni et al. 2022) is tractable under local simulator access.The results above are achieved through a computationally inefficient algorithm. We complement them with a more computationally efficient algorithm, RVFS (Recursive Value Function Search), which achieves provable sample complexity guarantees under a strengthened statistical assumption known as pushforward coverability. RVFS can be viewed as a principled, provable counterpart to a successful empirical paradigm that combines recursive search (e.g., MCTS) with value function approximation.',\n",
       "  395: 'In this paper, we obtain the Berry–Esseen bound for multivariate normal approximation for the Polyak-Ruppert averaged iterates of the linear stochastic approximation (LSA) algorithm with decreasing step size. Moreover, we prove the non-asymptotic validity of the confidence intervals for parameter estimation with LSA based on multiplier bootstrap. This procedure updates the LSA estimate together with a set of randomly perturbed LSA estimates upon the arrival of subsequent observations. We illustrate our findings in the setting of temporal difference learning with linear function approximation.',\n",
       "  396: 'Pre-trained vision language models (VLMs), though powerful, typically lack training on decision-centric data, rendering them sub-optimal for decision-making tasks such as in-the-wild device control through Graphical User Interfaces (GUIs) when used off-the-shelf. While training with static demonstrations has shown some promise, we show that such methods fall short when controlling real GUIs due to their failure to deal with real world stochasticity and dynamism not captured in static observational data. This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline and offline-to-online RL. We first build a scalable and parallelizable Android learning environment equipped with a VLM-based general-purpose evaluator and then identify the key design choices for simple and effective RL in this domain. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.5B VLM trained with RL achieves a 49.5\\\\% absolute improvement -- from 17.7 to 67.2\\\\% success rate -- over supervised fine-tuning with static human demonstration data. It is worth noting that such improvement is achieved without any additional supervision or demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3\\\\% success rate) and the 17B CogAgent trained with AitW data (14.4\\\\%), but also our implementation of prior best autonomous RL approach based on filtered behavior cloning (57.8\\\\%), thereby establishing a new state-of-the-art for digital agents for in-the-wild device control.',\n",
       "  397: 'Diffusion distillation represents a highly promising direction for achieving faithful text-to-image generation in a few sampling steps. However, despite recent successes, existing distilled models still do not provide the full spectrum of diffusion abilities, such as real image inversion, which enables many precise image manipulation methods. This work aims to enrich distilled text-to-image diffusion models with the ability to effectively encode real images into their latent space. To this end, we introduce invertible Consistency Distillation (iCD), a generalized consistency distillation framework that facilitates both high-quality image synthesis and accurate image encoding in only 3-4 inference steps. Though the inversion problem for text-to-image diffusion models gets exacerbated by high classifier-free guidance scales, we notice that dynamic guidance significantly reduces reconstruction errors without noticeable degradation in generation performance. As a result, we demonstrate that iCD equipped with dynamic guidance may serve as a highly effective tool for zero-shot text-guided image editing, competing with more expensive state-of-the-art alternatives.',\n",
       "  398: 'The sim-to-real gap, which represents the disparity between training and testing environments, poses a significant challenge in reinforcement learning (RL). A promising approach to addressing this challenge is distributionally robust RL, often framed as a robust Markov decision process (RMDP). In this framework, the objective is to find a robust policy that achieves good performance under the worst-case scenario among all environments within a pre-specified uncertainty set centered around the training environment. Unlike previous work, which relies on a generative model or a pre-collected offline dataset enjoying good coverage of the deployment environment, we tackle robust RL via interactive data collection, where the learner interacts with the training environment only and refines the policy through trial and error. In this robust RL paradigm, two main challenges emerge: managing distributional robustness while striking a balance between exploration and exploitation during data collection. Initially, we establish that sample-efficient learning without additional assumptions is unattainable owing to the curse of support shift; i.e., the potential disjointedness of the distributional supports between the training and testing environments. To circumvent such a hardness result, we introduce the vanishing minimal value assumption to RMDPs with a total-variation (TV) distance robust set, postulating that the minimal value of the optimal robust value function is zero. We prove that such an assumption effectively eliminates the support shift issue for RMDPs with a TV distance robust set, and present an algorithm with a provable sample complexity guarantee. Our work makes the initial step to uncovering the inherent difficulty of robust RL via interactive data collection and sufficient conditions for designing a sample-efficient algorithm accompanied by sharp sample complexity analysis.',\n",
       "  399: 'As Large Language Models (LLMs) demonstrate extensive capability in learning from documents, LLM unlearning becomes an increasingly important research area to address concerns of LLMs in terms of privacy, copyright, etc. A conventional LLM unlearning task typically involves two goals: (1) The target LLM should forget the knowledge in the specified forget documents; and (2) it should retain the other knowledge that the LLM possesses, for which we assume access to a small number of retain documents. To achieve both goals, a mainstream class of LLM unlearning methods introduces an optimization framework with a combination of two objectives – maximizing the prediction loss on the forget documents while minimizing that on the retain documents, which suffers from two challenges, degenerated output and catastrophic forgetting. In this paper, we propose a novel unlearning framework called Unlearning from Logit Difference (ULD), which introduces an assistant LLM that aims to achieve the opposite of the unlearning goals: remembering the forget documents and forgetting the retain knowledge. ULD then derives the unlearned LLM by computing the logit difference between the target and the assistant LLMs. We show that such reversed objectives would naturally resolve both aforementioned challenges while significantly improving the training efficiency. Extensive experiments demonstrate that our method efficiently achieves the intended forgetting while preserving the LLM’s overall capabilities, reducing training time by more than threefold. Notably, our method loses 0% of model utility on the ToFU benchmark, whereas baseline methods may sacrifice 17% of utility on average to achieve comparable forget quality.',\n",
       "  400: 'In the realm of image quantization exemplified by VQGAN, the process encodes images into discrete tokens drawn from a codebook with a predefined size. Recent advancements, particularly with LLAMA 3, reveal that enlarging the codebook significantly enhances model performance. However, VQGAN and its derivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to grapple with challenges related to expanding the codebook size and enhancing codebook utilization. For instance, VQGAN-FC is restricted to learning a codebook with a maximum size of 16,384, maintaining a typically low utilization rate of less than 12% on ImageNet. In this work, we propose a novel image quantization model named VQGAN-LC (Large Codebook), which extends the codebook size to 100,000, achieving an utilization rate exceeding 99%. Unlike previous methods that optimize each codebook entry, our approach begins with a codebook initialized with 100,000 features extracted by a pre-trained vision encoder. Optimization then focuses on training a projector that aligns the entire codebook with the feature distributions of the encoder in VQGAN-LC. We demonstrate the superior performance of our model over its counterparts across a variety of tasks, including image reconstruction, image classification, auto-regressive image generation using GPT, and image creation with diffusion- and flow-based generative models.',\n",
       "  401: \"Releasing open-source large language models (LLMs) presents a dual-use risk since bad actors can easily fine-tune these models for harmful purposes. Even without the open release of weights, weight stealing and fine-tuning APIs make closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety measures like preventing jailbreaks and improving safety guardrails are important, such measures can easily be reversed through fine-tuning. In this work, we propose Representation Noising (\\\\textsf{\\\\small RepNoise}), a defence mechanism that operates even when attackers have access to the weights. \\\\textsf{\\\\small RepNoise} works by removing information about harmful representations such that it is difficult to recover them during fine-tuning. Importantly, our defence is also able to generalize across different subsets of harm that have not been seen during the defence process as long as they are drawn from the same distribution of the attack set. Our method does not degrade the general capability of LLMs and retains the ability to train the model on harmless tasks. We provide empirical evidence that the efficacy of our defence lies in its ``depth'': the degree to which information about harmful representations is removed across {\\\\em all layers} of the LLM. We also find areas where \\\\textsf{\\\\small RepNoise} still remains ineffective and highlight how those limitations can inform future research.\",\n",
       "  402: 'Improving out-of-distribution (OOD) generalization during in-distribution (ID) adaptation is a primary goal of robust fine-tuning of zero-shot models beyond naive fine-tuning. However, despite decent OOD generalization performance from recent robust fine-tuning methods, confidence calibration for reliable model output has not been fully addressed. This work proposes a robust fine-tuning method that improves both OOD accuracy and confidence calibration simultaneously in vision language models. Firstly, we show that both OOD classification and OOD calibration errors have a shared upper bound consisting of two terms of ID data: 1) ID calibration error and 2) the smallest singular value of the ID input covariance matrix. Based on this insight, we design a novel framework that conducts fine-tuning with a constrained multimodal contrastive loss enforcing a larger smallest singular value, which is further guided by the self-distillation of a moving-averaged model to achieve calibrated prediction as well. Starting from empirical evidence supporting our theoretical statements, we provide extensive experimental results on ImageNet distribution shift benchmarks that demonstrate the effectiveness of our theorem and its practical implementation.',\n",
       "  403: \"Recent advancements in vision-language-to-image (VL2I) diffusion generation have made significant progress. While generating images from broad vision-language inputs holds promise, it also raises concerns about potential misuse, such as copying artistic styles without permission, which could have legal and social consequences. Therefore, it's crucial to establish governance frameworks to ensure ethical and copyright integrity, especially with widely used diffusion models. To address these issues, researchers have explored various approaches, such as dataset filtering, adversarial perturbations, machine unlearning, and inference-time refusals. However, these methods often lack either scalability or effectiveness. In response, we propose a new framework called causal representation editing (CRE), which extends representation editing from large language models (LLMs) to diffusion-based models. CRE enhances the efficiency and flexibility of safe content generation by intervening at diffusion timesteps causally linked to unsafe concepts. This allows for precise removal of harmful content while preserving acceptable content quality, demonstrating superior effectiveness, precision and scalability compared to existing methods. CRE can handle complex scenarios, including incomplete or blurred representations of unsafe concepts, offering a promising solution to challenges in managing harmful content generation in diffusion-based models.\",\n",
       "  404: 'This work introduces Variational Diffusion Distillation (VDD), a novel method that distills denoising diffusion policies into Mixtures of Experts (MoE) through variational inference. Diffusion Models are the current state-of-the-art in generative modeling due to their exceptional ability to accurately learn and represent complex, multi-modal distributions. This ability allows Diffusion Models to replicate the inherent diversity in human behavior, making them the preferred models in behavior learning such as Learning from Human Demonstrations (LfD).However, diffusion models come with some drawbacks, including the intractability of likelihoods and long inference times due to their iterative sampling process. The inference times, in particular, pose a significant challenge to real-time applications such as robot control.In contrast, MoEs effectively address the aforementioned issues while retaining the ability to represent complex distributions but are notoriously difficult to train.VDD is the first method that distills pre-trained diffusion models into MoE models, and hence, combines the expressiveness of Diffusion Models with the benefits of Mixture Models.Specifically, VDD leverages a decompositional upper bound of the variational objective that allows the training of each expert separately, resulting in a robust optimization scheme for MoEs.VDD demonstrates across nine complex behavior learning tasks, that it is able to: i) accurately distill complex distributions learned by the diffusion model, ii) outperform existing state-of-the-art distillation methods, and iii) surpass conventional methods for training MoE. The code and videos are available at https://intuitive-robots.github.io/vdd-website.',\n",
       "  405: 'The learnware paradigm aims to help users leverage numerous existing high-performing models instead of starting from scratch, where a learnware consists of a well-trained model and the specification describing its capability. Numerous learnwares are accommodated by a learnware dock system. When users solve tasks with the system, models that fully match the task feature space are often rare or even unavailable. However, models with heterogeneous feature space can still be helpful. This paper finds that label information, particularly model outputs, is helpful yet previously less exploited in the accommodation of heterogeneous learnwares. We extend the specification to better leverage model pseudo-labels and subsequently enrich the unified embedding space for better specification evolvement. With label information, the learnware identification can also be improved by additionally comparing conditional distributions. Experiments demonstrate that, even without a model explicitly tailored to user tasks, the system can effectively handle tasks by leveraging models from diverse feature spaces.',\n",
       "  406: 'In this paper, we propose a novel policy optimization framework that maximizes Return on Investment (ROI) of a policy using a fixed dataset within a Markov Decision Process (MDP) equipped with a cost function. ROI, defined as the ratio between the return and the accumulated cost of a policy, serves as a measure of efficiency of the policy. Despite the importance of maximizing ROI in various applications, it remains a challenging problem due to its nature as a ratio of two long-term values: return and accumulated cost. To address this, we formulate the ROI maximizing reinforcement learning problem as a linear fractional programming. We then incorporate the stationary distribution correction (DICE) framework to develop a practical offline ROI maximization algorithm.Our proposed algorithm, ROIDICE, yields an efficient policy that offers a superior trade-off between return and accumulated cost compared to policies trained using existing frameworks.',\n",
       "  407: 'How to solve high-dimensional linear programs (LPs) efficiently is a fundamental question.Recently, there has been a surge of interest in reducing LP sizes using *random projections*, which can accelerate solving LPs independently of improving LP solvers. This paper explores a new direction of *data-driven projections*, which use projection matrices learned from data instead of random projection matrices.Given training data of $n$-dimensional LPs, we learn an $n\\\\times k$ projection matrix with $n > k$. When addressing a future LP instance, we reduce its dimensionality from $n$ to $k$ via the learned projection matrix, solve the resulting LP to obtain a $k$-dimensional solution, and apply the learned matrix to it to recover an $n$-dimensional solution.On the theoretical side, a natural question is: how much data is sufficient to ensure the quality of recovered solutions? We address this question based on the framework of *data-driven algorithm design*, which connects the amount of data sufficient for establishing generalization bounds to the *pseudo-dimension* of performance metrics. We obtain an $\\\\tilde{\\\\mathrm{O}}(nk^2)$ upper bound on the pseudo-dimension, where $\\\\tilde{\\\\mathrm{O}}$ compresses logarithmic factors. We also provide an $\\\\Omega(nk)$ lower bound, implying our result is tight up to an $\\\\tilde{\\\\mathrm{O}}(k)$ factor. On the practical side, we explore two simple methods for learning projection matrices: PCA- and gradient-based methods. While the former is relatively efficient, the latter can sometimes achieve better solution quality. Experiments demonstrate that learning projection matrices from data is indeed beneficial: it leads to significantly higher solution quality than the existing random projection while greatly reducing the time for solving LPs.',\n",
       "  408: \"Spatio-temporal compression of videos, utilizing networks such as Variational Autoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other video generative models. For instance, many LLM-like video models learn the distribution of discrete tokens derived from 3D VAEs within the VQVAE framework, while most diffusion-based video models capture the distribution of continuous latent extracted by 2D VAEs without quantization. The temporal compression is simply realized by uniform frame sampling which results in unsmooth motion between consecutive frames. Currently, there lacks of a commonly used continuous video (3D) VAE for latent diffusion-based video models in the research community. Moreover, since current diffusion-based approaches are often implemented using pre-trained text-to-image (T2I) models, directly training a video VAE without considering the compatibility with existing T2I models will result in a latent space gap between them, which will take huge computational resources for training to bridge the gap even with the T2I models as initialization. To address this issue, we propose a method for training a video VAE of latent video models, namely CV-VAE, whose latent space is compatible with that of a given image VAE, e.g., image VAE of Stable Diffusion (SD). The compatibility is achieved by the proposed novel latent space regularization, which involves formulating a regularization loss using the image VAE. Benefiting from the latent space compatibility, video models can be trained seamlessly from pre-trained T2I or video models in a truly spatio-temporally compressed latent space, rather than simply sampling video frames at equal intervals. To improve the training efficiency, we also design a novel architecture for the video VAE. With our CV-VAE, existing video models can generate four times more frames with minimal finetuning. Extensive experiments are conducted to demonstrate the effectiveness of the proposed video VAE.\",\n",
       "  409: 'Test-time prompt tuning, which learns prompts online with unlabelled test samples during the inference stage, has demonstrated great potential by learning effective prompts on-the-fly without requiring any task-specific annotations. However, its performance often degrades clearly along the tuning process when the prompts are continuously updated with the test data flow, and the degradation becomes more severe when the domain of test samples changes continuously. We propose HisTPT, a Historical Test-time Prompt Tuning technique that memorizes the useful knowledge of the learnt test samples and enables robust test-time prompt tuning with the memorized knowledge. HisTPT introduces three types of knowledge banks, namely, local knowledge bank, hard-sample knowledge bank, and global knowledge bank, each of which works with different mechanisms for effective knowledge memorization and test-time prompt optimization. In addition, HisTPT features an adaptive knowledge retrieval mechanism that regularizes the prediction of each test sample by adaptively retrieving the memorized knowledge. Extensive experiments show that HisTPT achieves superior prompt tuning performance consistently while handling different visual recognition tasks (e.g., image classification, semantic segmentation, and object detection) and test samples from continuously changing domains.',\n",
       "  410: 'Stochastic Gradient Descent (SGD) with adaptive steps is widely used to train deep neural networks and generative models. Most theoretical results assume that it is possible to obtain unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods.This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias of the gradient estimator. In particular, we establish that Adagrad, RMSProp, and AMSGRAD, an exponential moving average variant of Adam, with biased gradients, converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. Finally, we provide experimental results using Variational Autoenconders (VAE) and applications to several learning frameworks that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning.',\n",
       "  411: \"Algorithms for playing in Stackelberg games have been deployed in real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), a salient feature of reality which may significantly affect both players' optimal strategies. We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing. The leader commits to a (context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context. We focus on the online setting in which a sequence of followers arrive over time, and the context may change from round-to-round. In sharp contrast to the non-contextual version, we show that it is impossible for the leader to achieve good performance (measured by regret) in the full adversarial setting.  Motivated by our impossibility result, we show that no-regret learning is possible in two natural relaxations: the setting in which the sequence of followers is chosen stochastically and the sequence of contexts is adversarial, and the setting in which the sequence of contexts is stochastic and the sequence of followers is chosen by an adversary.\",\n",
       "  412: 'Parametric dimensionality reduction methods have gained prominence for their ability to generalize to unseen datasets, an advantage that traditional non-parametric approaches typically lack. Despite their growing popularity, there remains a prevalent misconception among practitioners about the equivalence in performance between parametric and non-parametric methods. Here, we show that these methods are not equivalent -- parametric methods retain global structure but lose significant local details. To explain this, we provide evidence that parameterized approaches lack the ability to repulse negative samples, and the choice of loss function also has an impact.Addressing these issues, we developed a new parametric method, ParamRepulsor, that incorporates Hard Negative Mining and a loss function that applies a strong repulsive force. This new method achieves state-of-the-art performance on local structure preservation for parametric methods without sacrificing the fidelity of global structural representation. Our code is available at https://github.com/hyhuang00/ParamRepulsor.',\n",
       "  413: 'Driving world models have gained increasing attention due to their ability to model complex physical dynamics. However, their superb modeling capability is yet to be fully unleashed due to the limited video diversity in current driving datasets. We introduce DrivingDojo, the first dataset tailor-made for training interactive world models with complex driving dynamics. Our dataset features video clips with a complete set of driving maneuvers, diverse multi-agent interplay, and rich open-world driving knowledge, laying a stepping stone for future world model development. We further define an action instruction following (AIF) benchmark for world models and demonstrate the superiority of the proposed dataset for generating action-controlled future predictions.',\n",
       "  414: 'Time series analysis finds wide applications in fields such as weather forecasting, anomaly detection, and behavior recognition. Previous methods attempted to model temporal variations directly using 1D time series. However, this has been quite challenging due to the discrete nature of data points in time series and the complexity of periodic variation. In terms of periodicity, taking weather and traffic data as an example, there are multi-periodic variations such as yearly, monthly, weekly, and daily, etc. In order to break through the limitations of the previous methods, we decouple the implied complex periodic variations into inclusion and overlap relationships among different level periodic components based on the observation of the multi-periodicity therein and its inclusion relationships. This explicitly represents the naturally occurring pyramid-like properties in time series, where the top level is the original time series and lower levels consist of periodic components with gradually shorter periods, which we call the periodic pyramid. To further extract complex temporal variations, we introduce self-attention mechanism into the periodic pyramid, capturing complex periodic relationships by computing attention between periodic components based on their inclusion, overlap, and adjacency relationships. Our proposed Peri-midFormer demonstrates outstanding performance in five mainstream time series analysis tasks, including short- and long-term forecasting, imputation, classification, and anomaly detection.',\n",
       "  415: 'Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for low energy consumption and high performance. However, there remains a substantial gap in performance between SNNs and Artificial Neural Networks (ANNs). To narrow this gap, we have developed QKFormer, a direct training spiking transformer with the following features: i) Linear complexity and high energy efficiency, the novel spike-form Q-K attention module efficiently models the token or channel attention through binary vectors and enables the construction of larger models. ii) Multi-scale spiking representation, achieved by a hierarchical structure with the different numbers of tokens across blocks. iii) Spiking Patch Embedding with Deformed Shortcut (SPEDS), enhances spiking information transmission and integration, thus improving overall performance. It is shown that QKFormer achieves significantly superior performance over existing state-of-the-art SNN models on various mainstream datasets. Notably, with comparable size to Spikformer (66.34 M, 74.81\\\\%), QKFormer (64.96 M) achieves a groundbreaking top-1 accuracy of 85.65\\\\% on ImageNet-1k, substantially outperforming Spikformer by 10.84\\\\%. To our best knowledge, this is the first time that directly training SNNs have exceeded 85\\\\% accuracy on ImageNet-1K.',\n",
       "  416: 'Federated learning (FL) has emerged as a prominent machine learning paradigm in edge computing environments, enabling edge devices to collaboratively optimize a global model without sharing their private data. However, existing FL frameworks suffer from efficacy deterioration due to the system heterogeneity inherent in edge computing, especially in the presence of domain shifts across local data. In this paper, we propose a heterogeneous FL framework DapperFL, to enhance model performance across multiple domains. In DapperFL, we introduce a dedicated Model Fusion Pruning (MFP) module to produce personalized compact local models for clients to address the system heterogeneity challenges. The MFP module prunes local models with fused knowledge obtained from both local and remaining domains, ensuring robustness to domain shifts. Additionally, we design a Domain Adaptive Regularization (DAR) module to further improve the overall performance of DapperFL. The DAR module employs regularization generated by the pruned model, aiming to learn robust representations across domains. Furthermore, we introduce a specific aggregation algorithm for aggregating heterogeneous local models with tailored architectures and weights. We implement DapperFL on a real-world FL platform with heterogeneous clients. Experimental results on benchmark datasets with multiple domains demonstrate that DapperFL outperforms several state-of-the-art FL frameworks by up to 2.28%, while significantly achieving model volume reductions ranging from 20% to 80%. Our code is available at: https://github.com/jyzgh/DapperFL.',\n",
       "  417: 'Model-based reinforcement learning (MBRL) is a promising route to sample-efficient policy optimization. However, a known vulnerability of reconstruction-based MBRL consists of scenarios in which detailed aspects of the world are highly predictable, but irrelevant to learning a good policy. Such scenarios can lead the model to exhaust its capacity on meaningless content, at the cost of neglecting important environment dynamics. While existing approaches attempt to solve this problem, we highlight its continuing impact on leading MBRL methods ---including DreamerV3 and DreamerPro--- with a novel environment where background distractions are intricate, predictable, and useless for planning future actions. To address this challenge we develop a method for focusing the capacity of the world model through a synergy of a pretrained segmentation model, a task-aware reconstruction loss, and adversarial learning. Our method outperforms a variety of other approaches designed to reduce the impact of distractors, and is an advance towards robust model-based reinforcement learning.',\n",
       "  418: 'The paper investigates the fundamental convergence properties of Sharpness-Aware Minimization (SAM), a recently proposed gradient-based optimization method (Foret et al., 2021) that significantly improves the generalization of deep neural networks. The convergence properties including the stationarity of accumulation points, the convergence of the sequence of gradients to the origin, the sequence of function values to the optimal value, and the sequence of iterates to the optimal solution are established for the method. The universality of the provided convergence analysis based on inexact gradient descent frameworks (Khanh et al., 2023b) allows its extensions to the normalized versions of SAM such as F-SAM (Li et al. 2024), VaSSO (Li & Giannakis, 2023), RSAM (Liu et al., 2022), and to the unnormalized versions of SAM such as USAM (Andriushchenko & Flammarion, 2022). Numerical experiments are conducted on classification tasks using deep learning models to confirm the practical aspects of our analysis.',\n",
       "  419: \"We consider the problem of online fair division of indivisible goods to players when there are a finite number of types of goods and player values are drawn from distributions with unknown means. Our goal is to maximize social welfare subject to allocating the goods fairly in expectation. When a player's value for an item is unknown at the time of allocation, we show that this problem reduces to a variant of (stochastic) multi-armed bandits, where there exists an arm for each player's value for each type of good. At each time step, we choose a distribution over arms which determines how the next item is allocated. We consider two sets of fairness constraints for this problem: envy-freeness in expectation and proportionality in expectation. Our main result is the design of an explore-then-commit algorithm that achieves $\\\\tilde{O}(T^{2/3})$ regret while maintaining either fairness constraint. This result relies on unique properties fundamental to fair-division constraints that allow faster rates of learning, despite the restricted action space.\",\n",
       "  420: \"Recent advancements in pre-trained vision models have made them pivotal in computer vision, emphasizing the need for their thorough evaluation and benchmarking. This evaluation needs to consider various factors of variation, their potential biases, shortcuts, and inaccuracies that ultimately lead to disparate performance in models. Such evaluations are conventionally done using either synthetic data from 2D or 3D rendering software or real-world images in controlled settings. Synthetic methods offer full control and flexibility, while real-world methods are limited by high costs and less adaptability. Moreover, 3D rendering can't yet fully replicate real photography, creating a realism gap.In this paper, we introduce BLURD--Benchmarking and Learning using a Unified Rendering and Diffusion Model--a novel method combining 3D rendering and Stable Diffusion to bridge this gap in representation learning. With BLURD we create a new family of datasets that allow for the creation of both 3D rendered and photo-realistic images with identical factors. BLURD, therefore, provides deeper insights into the representations learned by various CLIP backbones. The source code for creating the BLURD datasets is available at https://github.com/squaringTheCircle/BLURD\",\n",
       "  421: 'Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions $z = a  x + b  y \\\\text{ mod } p$ labeled by the vector $(a, b) \\\\in \\\\mathbb{Z}_p^2$. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is *transient*, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing highly structured representations in both attention heads and MLPs; and discuss the learned algorithms. Notably, we find an algorithmic shift in deeper models, as we go from few to many in-context examples.',\n",
       "  422: 'Event-intensity asymmetric stereo systems have emerged as a promising approach for robust 3D perception in dynamic and challenging environments by integrating event cameras with frame-based sensors in different views. However, existing methods often suffer from overfitting and poor generalization due to limited dataset sizes and lack of scene diversity in the event domain. To address these issues, we propose a zero-shot framework that utilizes monocular depth estimation and stereo matching models pretrained on diverse image datasets. Our approach introduces a visual prompting technique to align the representations of frames and events, allowing the use of off-the-shelf stereo models without additional training. Furthermore, we introduce a monocular cue-guided disparity refinement module to improve robustness across static and dynamic regions by incorporating monocular depth information from foundation models. Extensive experiments on real-world datasets demonstrate the superior zero-shot evaluation performance and enhanced generalization ability of our method compared to existing approaches.',\n",
       "  423: \"Harmful memes have proliferated on the Chinese Internet, while research on detecting Chinese harmful memes significantly lags behind due to the absence of reliable datasets and effective detectors.To this end, we present the comprehensive detection of Chinese harmful memes.We introduce ToxiCN MM, the first Chinese harmful meme dataset, which consists of 12,000 samples with fine-grained annotations for meme types. Additionally, we propose a baseline detector, Multimodal Knowledge Enhancement (MKE), designed to incorporate contextual information from meme content, thereby enhancing the model's understanding of Chinese memes.In the evaluation phase, we conduct extensive quantitative experiments and qualitative analyses on multiple baselines, including LLMs and our MKE. Experimental results indicate that detecting Chinese harmful memes is challenging for existing models, while demonstrating the effectiveness of MKE.\",\n",
       "  424: 'Large-scale training of latent diffusion models (LDMs) has enabled unprecedented quality in image generation. However, large-scale end-to-end training of these models is computationally costly, and hence most research  focuses either on finetuning  pretrained models or experiments at smaller scales.In this work we aim to improve the training efficiency and performance of LDMs with the goal of scaling to larger datasets and higher resolutions.We focus our study on two points that are critical for good performance and efficient training: (i) the mechanisms used for  semantic level (\\\\eg a text prompt, or class name) and low-level (crop size, random flip, \\\\etc) conditioning of the model, and (ii) pre-training strategies to transfer representations learned on smaller and lower-resolution datasets to larger ones.The main contributions of our work are the following: we present systematic experimental study of these points, we propose a novel conditioning mechanism that disentangles semantic and low-level conditioning, we obtain state-of-the-art performance  on CC12M for text-to-image at 512 resolution.',\n",
       "  425: 'As Large Language Models (LLMs) excel across tasks and specialized domains, scaling LLMs based on existing models has gained significant attention, which is challenged by potential performance drop when combining disparate models. Various techniques have been proposed to aggregate pre-trained LLMs, including model merging, Mixture-of-Experts, and stacking. Despite their merits, a comprehensive comparison and synergistic application of them to a diverse model zoo is yet to be adequately addressed.In light of this research gap, this paper introduces $\\\\texttt{Model-GLUE}$, a holistic LLM scaling guideline. First, our work starts with a benchmarking of existing LLM scaling techniques, especially selective merging, and variants of mixture. Utilizing the insights from the benchmark results, we formulate a strategy for the selection and aggregation of a heterogeneous model zoo characterizing different architectures and initialization.Our methodology involves clustering mergeable models, selecting a merging strategy, and integrating model clusters through model-level mixture. Finally, evidenced by our experiments on a diverse Llama-2-based model zoo, $\\\\texttt{Model-GLUE}$ shows an average performance enhancement of 5.61\\\\%, achieved without additional training.Codes are available at https://github.com/Model-GLUE/Model-GLUE.',\n",
       "  426: 'We present a generative dialogue system capable of operating in a full-duplex manner, allowing for seamless interaction. It is based on a large language model (LLM) carefully aligned to be aware of a perception module, a motor function module, and the concept of a simple finite state machine (called neural FSM) with two states. The perception and motor function modules operate in tandem, allowing the system to speak and listen to the user simultaneously. The LLM generates textual tokens for inquiry responses and makes autonomous decisions to start responding to, wait for, or interrupt the user by emitting control tokens to the neural FSM. All these tasks of the LLM are carried out as next token prediction on a serialized view of the dialogue in real-time. In automatic quality evaluations simulating real-life interaction, the proposed system reduces the average conversation response latency by more than threefold compared with LLM-based half-duplex dialogue systems while responding within less than 500 milliseconds in more than 50% of evaluated interactions. Running an LLM with only 8 billion parameters, our system exhibits an 8% higher interruption precision rate than the best available commercial LLM for voice-based dialogue.',\n",
       "  427: 'Reconstructing a continuous surface from a raw 3D point cloud is a challenging task. Latest methods employ supervised learning or pretrained priors to learn a signed distance function (SDF). However, neural networks tend to smooth local details due to the lack of ground truth signed distnaces or normals, which limits the performance of learning-based methods in reconstruction tasks. To resolve this issue, we propose a novel method, named MultiPull, to learn multi-scale implicit fields from raw point clouds to optimize accurate SDFs from coarse to fine. We achieve this by mapping 3D query points into a set of frequency features, which makes it possible to leverage multi-level features during optimization. Meanwhile, we introduce optimization constraints from the perspective of spatial distance and normal consistency, which play a key role in point cloud reconstruction based on multi-scale optimization strategies. Our experiments on widely used object and scene benchmarks demonstrate that our method outperforms the state-of-the-art methods in surface reconstruction.',\n",
       "  428: 'Motion forecasting is an essential task for autonomous driving, and utilizing information from infrastructure and other vehicles can enhance forecasting capabilities.Existing research mainly focuses on leveraging single-frame cooperative information to enhance the limited perception capability of the ego vehicle, while underutilizing the motion and interaction context of traffic participants observed from cooperative devices. In this paper, we propose a forecasting-oriented representation paradigm to utilize motion and interaction features from cooperative information. Specifically, we present V2X-Graph, a representative framework to achieve interpretable and end-to-end trajectory feature fusion for cooperative motion forecasting. V2X-Graph is evaluated on V2X-Seq in vehicle-to-infrastructure (V2I) scenarios.To further evaluate on vehicle-to-everything (V2X) scenario, we construct the first real-world V2X motion forecasting dataset V2X-Traj, which contains multiple autonomous vehicles and infrastructure in every scenario.Experimental results on both V2X-Seq and V2X-Traj show the advantage of our method. We hope both V2X-Graph and V2X-Traj will benefit the further development of cooperative motion forecasting.Find the project at https://github.com/AIR-THU/V2X-Graph.',\n",
       "  429: 'State-space graphical models and the variational autoencoder framework provide a principled apparatus for learning dynamical systems from data.  State-of-the-art probabilistic approaches are often able to scale to large problems at the cost of flexibility of the variational posterior or expressivity of the dynamics model.  However, those consolidations can be detrimental if the ultimate goal is to learn a generative model capable of explaining the spatiotemporal structure of the data and making accurate forecasts.  We introduce a low-rank structured variational autoencoding framework for nonlinear Gaussian state-space graphical models capable of capturing dense covariance structures that are important for learning dynamical systems with predictive capabilities.  Our inference algorithm exploits the covariance structures that arise naturally from sample based approximate Gaussian message passing and low-rank amortized posterior updates -- effectively performing approximate variational smoothing with time complexity scaling linearly in the state dimensionality.  In comparisons with other deep state-space model architectures our approach consistently demonstrates the ability to learn a more predictive generative model.  Furthermore, when applied to neural physiological recordings, our approach is able to learn a dynamical system capable of forecasting population spiking and behavioral correlates from a small portion of single trials.',\n",
       "  430: \"We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields. Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds. This versatility eliminates the need for patching and allows efficient processing of diverse geometries. The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs. By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training. AROMA's superior performance in simulating 1D and 2D equations underscores the efficacy of our approach in capturing complex dynamical behaviors.\",\n",
       "  431: 'Designing single-task image restoration models for specific degradation has seen great success in recent years. To achieve generalized image restoration, all-in-one methods have recently been proposed and shown potential for multiple restoration tasks using one single model. Despite the promising results, the existing all-in-one paradigm still suffers from high computational costs as well as limited generalization on unseen degradations. In this work, we introduce an alternative solution to improve the generalization of image restoration models. Drawing inspiration from recent advancements in Parameter Efficient Transfer Learning (PETL), we aim to tune only a small number of parameters to adapt pre-trained restoration models to various tasks. However, current PETL methods fail to generalize across varied restoration tasks due to their homogeneous representation nature. To this end, we propose AdaptIR, a Mixture-of-Experts (MoE) with orthogonal multi-branch design to capture local spatial, global spatial, and channel representation bases, followed by adaptive base combination to obtain heterogeneous representation for different degradations. Extensive experiments demonstrate that our AdaptIR achieves stable performance on single-degradation tasks, and excels in hybrid-degradation tasks, with training only 0.6% parameters for 8 hours.',\n",
       "  432: 'Foundation models have emerged as powerful tools across various domains including language, vision, and multimodal tasks. While prior works have addressed unsupervised semantic segmentation, they significantly lag behind supervised models. In this paper, we use a diffusion UNet encoder as a foundation vision encoder and introduce DiffCut, an unsupervised zero-shot segmentation method that solely harnesses the output features from the final self-attention block. Through extensive experimentation, we demonstrate that using these diffusion features in a graph based segmentation algorithm, significantly outperforms previous state-of-the-art methods on zero-shot segmentation. Specifically, we leverage a recursive Normalized Cut algorithm that regulates the granularity of detected objects and produces well-defined segmentation maps that precisely capture intricate image details. Our work highlights the remarkably accurate semantic knowledge embedded within diffusion UNet encoders that could then serve as foundation vision encoders for downstream tasks.',\n",
       "  433: 'Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it is intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, thus reducing per-user generation latency by more than 10x in multi-tenant settings. We validate BitDelta through experiments across Llama-2, Mistral and MPT model families, and on models up to 70B parameters, showcasing minimal performance degradation in all tested settings.',\n",
       "  434: 'The benefit of transformers in large-scale 3D point cloud perception tasks, such as 3D object detection, is limited by their quadratic computation cost when modeling long-range relationships. In contrast, linear RNNs have low computational complexity and are suitable for long-range modeling. Toward this goal, we propose a simple and effective window-based framework built on Linear group RNN (i.e., perform linear RNN for grouped features) for accurate 3D object detection, called LION. The key property is to allow sufficient feature interaction in a much larger group than transformer-based methods. However, effectively applying linear group RNN to 3D object detection in highly sparse point clouds is not trivial due to its limitation in handling spatial modeling. To tackle this problem, we simply introduce a 3D spatial feature descriptor and integrate it into the linear group RNN operators to enhance their spatial features rather than blindly increasing the number of scanning orders for voxel features. To further address the challenge in highly sparse point clouds, we propose a 3D voxel generation strategy to densify foreground features thanks to linear group RNN as a natural property of auto-regressive models. Extensive experiments verify the effectiveness of the proposed components and the generalization of our LION on different linear group RNN operators including Mamba, RWKV, and RetNet. Furthermore, it is worth mentioning that our LION-Mamba achieves state-of-the-art on Waymo, nuScenes, Argoverse V2, and ONCE datasets. Last but not least, our method supports kinds of advanced linear RNN operators (e.g., RetNet, RWKV, Mamba, xLSTM and TTT) on small but popular KITTI dataset for a quick experience with our linear RNN-based framework.',\n",
       "  435: 'Incorporating inductive bias by embedding geometric entities (such as rays) as input has proven successful in multi-view learning. However, the methods adopting this technique typically lack equivariance, which is crucial for effective 3D learning. Equivariance serves as a valuable inductive prior, aiding in the generation of robust multi-view features for 3D scene understanding. In this paper, we explore the application of equivariant multi-view learning to depth estimation, not only recognizing its significance for computer vision and robotics but also addressing the limitations of previous research. Most prior studies have either overlooked equivariance in this setting or achieved only approximate equivariance through data augmentation, which often leads to inconsistencies across different reference frames. To address this issue, we propose to embed $SE(3)$ equivariance into the Perceiver IO architecture. We employ Spherical Harmonics for positional encoding to ensure 3D rotation equivariance, and develop a specialized equivariant encoder and decoder within the Perceiver IO architecture. To validate our model, we applied it to the task of stereo depth estimation, achieving state of the art results on real-world datasets without explicit geometric constraints or extensive data augmentation.',\n",
       "  436: 'The rapid progress in artificial intelligence-generated content (AIGC), especially with diffusion models, has significantly advanced development of high-quality video generation. However, current video diffusion models exhibit demanding computational requirements and high peak memory usage, especially for generating longer and higher-resolution videos. These limitations greatly hinder the practical application of video diffusion models on standard hardware platforms. To tackle this issue, we present a novel, training-free framework named Streamlined Inference, which leverages the temporal and spatial properties of video diffusion models. Our approach integrates three core components: Feature Slicer, Operator Grouping, and Step Rehash. Specifically, Feature Slicer effectively partitions input features into sub-features and Operator Grouping processes each sub-feature with a group of consecutive operators, resulting in significant memory reduction without sacrificing the quality or speed. Step Rehash further exploits the similarity between adjacent steps in diffusion, and accelerates inference through skipping unnecessary steps. Extensive experiments demonstrate that our approach significantly reduces peak memory and computational overhead, making it feasible to generate high-quality videos on a single consumer GPU (e.g., reducing peak memory of Animatediff from 42GB to 11GB, featuring faster inference on 2080Ti).',\n",
       "  437: 'Low Rank Adaptation (LoRA) has gained massive attention in the recent generative AI research. One of the main advantages of LoRA is its ability to be fused with  pretrained models, adding no overhead during inference. However, from a mobile deployment standpoint, we can either avoid inference overhead in the fused mode but lose the ability to switch adapters rapidly, or suffer significant (up to 30% higher) inference latency while enabling rapid switching in the unfused mode. LoRA also exhibits concept-loss when multiple adapters are used concurrently. In this paper, we propose Sparse High Rank Adapters (SHiRA), a new paradigm which incurs no inference overhead, enables rapid switching, and significantly reduces concept-loss. Specifically, SHiRA can be trained by directly tuning only 1-2% of the base model weights while leaving others unchanged. This results in a highly sparse adapter which can be switched directly in the fused mode. We further provide theoretical and empirical insights on how high sparsity in SHiRA can aid multi-adapter fusion by reducing concept loss. Our extensive experiments on LVMs and LLMs demonstrate that finetuning only a small fraction of the parameters in the base model significantly outperforms LoRA while enabling both rapid switching and multi-adapter fusion. Finally, we provide a latency- and memory-efficient SHiRA implementation based on Parameter-Efficient Finetuning (PEFT) Library which trains at nearly the same speed as LoRA while consuming up to 16% lower peak GPU memory, thus making SHiRA easy to adopt for practical use cases. To demonstrate rapid switching benefits during inference, we show that loading SHiRA on a base model can be 5x-16x faster than LoRA fusion on a CPU.',\n",
       "  438: 'Text-to-Image (T2I) models are being increasingly adopted in diverse global communities where they create visual representations of their unique cultures. Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realism of generated images, overlooking the critical dimension of cultural competence. In this work, we introduce a framework to evaluate cultural competence of T2I models along two crucial dimensions: cultural awareness and cultural diversity, and present a scalable approach using a combination of structured knowledge bases and large language models to build a large dataset of cultural artifacts to enable this evaluation. In particular, we apply this approach to build CUBE (CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to evaluate cultural competence of T2I models. CUBE covers cultural artifacts associated with 8 countries across different geo-cultural regions and along 3 concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of high-quality prompts that enable the evaluation of cultural awareness, and 2) CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to evaluate cultural diversity. We also introduce cultural diversity as a novel T2I evaluation component, leveraging quality-weighted Vendi score. Our evaluations reveal significant gaps in the cultural awareness of existing models across countries and provide valuable insights into the cultural diversity of T2I outputs for underspecified prompts. Our methodology is extendable to other cultural regions and concepts and can facilitate the development of T2I models that better cater to the global population.',\n",
       "  439: 'The brain prepares for learning even before interacting with the environment, by refining and optimizing its structures through spontaneous neural activity that resembles random noise. However, the mechanism of such a process has yet to be understood, and it is unclear whether this process can benefit the algorithm of machine learning. Here, we study this issue using a neural network with a feedback alignment algorithm, demonstrating that pretraining neural networks with random noise increases the learning efficiency as well as generalization abilities without weight transport. First, we found that random noise training modifies forward weights to match backward synaptic feedback, which is necessary for teaching errors by feedback alignment. As a result, a network with pre-aligned weights learns notably faster and reaches higher accuracy than a network without random noise training, even comparable to the backpropagation algorithm. We also found that the effective dimensionality of weights decreases in a network pretrained with random noise. This pre-regularization allows the network to learn simple solutions of a low rank, reducing the generalization error during subsequent training. This also enables the network to robustly generalize a novel, out-of-distribution dataset. Lastly, we confirmed that random noise pretraining reduces the amount of meta-loss, enhancing the network ability to adapt to various tasks. Overall, our results suggest that random noise training with feedback alignment offers a straightforward yet effective method of pretraining that facilitates quick and reliable learning without weight transport.',\n",
       "  440: 'Existing high-resolution image harmonization methods typically rely on global color adjustments or the upsampling of parameter maps. However, these methods ignore local variations, leading to inharmonious appearances. To address this problem, we propose an Adaptive-Interval Color Transformation method (AICT), which predicts pixel-wise color transformations and adaptively adjusts the sampling interval to model local non-linearities of the color transformation at high resolution. Specifically, a parameter network is first designed to generate multiple position-dependent 3-dimensional lookup tables (3D LUTs), which use the color and position of each pixel to perform pixel-wise color transformations. Then, to enhance local variations adaptively, we separate a color transform into a cascade of sub-transformations using two 3D LUTs to achieve the non-uniform sampling intervals of the color transform. Finally, a global consistent weight learning method is proposed to predict an image-level weight for each color transform, utilizing global information to enhance the overall harmony. Extensive experiments demonstrate that our AICT achieves state-of-the-art performance with a lightweight architecture. The code is available at https://github.com/aipixel/AICT.',\n",
       "  441: 'Real-world image denoising remains a challenge task. This paper studies self-supervised image denoising, requiring only noisy images captured in a single shot. We revamping the blind-spot technique by leveraging the transformer’s capability for long-range pixel interactions, which is crucial for effectively removing noise dependence in relating pixel–a requirement for achieving great performance for the blind-spot technique. The proposed method integrates these elements with two key innovations: a directional self-attention (DSA) module using a half-plane grid for self-attention, creating a sophisticated blind-spot structure, and a Siamese architecture with mutual learning to mitigate the performance impactsfrom the restricted attention grid in DSA. Experiments on benchmark datasets demonstrate that our method outperforms existing self-supervised and clean-image-free methods. This combination of blind-spot and transformer techniques provides a natural synergy for tackling real-world image denoising challenges.',\n",
       "  442: 'Hierarchical clustering has usually been addressed by discrete optimization using heuristics or continuous optimization of relaxed scores for hierarchies. In this work, we propose to optimize expected scores under a probabilistic model over hierarchies. (1) We show theoretically that the global optimal values of the expected Dasgupta cost and Tree-Sampling divergence (TSD), two unsupervised metrics for hierarchical clustering, are equal to the optimal values of their discrete counterparts contrary to some relaxed scores. (2) We propose Expected Probabilistic Hierarchies (EPH), a probabilistic model to learn hierarchies in data by optimizing expected scores. EPH uses differentiable hierarchy sampling enabling end-to-end gradient descent based optimization, and an unbiased subgraph sampling approach to scale to large datasets. (3) We evaluate EPH on synthetic and real-world datasets including vector and graph datasets. EPH outperforms all other approaches quantitatively and provides meaningful hierarchies in qualitative evaluations.',\n",
       "  443: 'As the scale of data and models for video understanding rapidly expand, handling long-form video input in transformer-based models presents a practical challenge. Rather than resorting to input sampling or token dropping, which may result in information loss, token merging shows promising results when used in collaboration with transformers. However, the application of token merging for long-form video processing is not trivial. We begin with the premise that token merging should not rely solely on the similarity of video tokens; the saliency of tokens should also be considered. To address this, we explore various video token merging strategies for long-form video classification, starting with a simple extension of image token merging, moving to region-concentrated merging, and finally proposing a learnable video token merging (VTM) algorithm that dynamically merges tokens based on their saliency. Extensive experimental results show that we achieve better or comparable performances on the LVU, COIN, and Breakfast datasets. Moreover, our approach significantly reduces memory costs by 84% and boosts throughput by approximately 6.89 times compared to baseline algorithms.',\n",
       "  444: \"The alignment of large language models (LLMs) is critical for developing effective and safe language models. Traditional approaches focus on aligning models during the instruction tuning or reinforcement learning stages, referred to in this paper as `\\\\textit{post alignment}'. We argue that alignment during the pre-training phase, which we term 'native alignment', warrants investigation. Native alignment aims to prevent unaligned content from the beginning, rather than relying on post-hoc processing. This approach leverages extensively aligned pre-training data to enhance the effectiveness and usability of pre-trained models. Our study specifically explores the application of native alignment in the context of Arabic LLMs. We conduct comprehensive experiments and ablation studies to evaluate the impact of native alignment on model performance and alignment stability. Additionally, we release open-source Arabic LLMs that demonstrate state-of-the-art performance on various benchmarks, providing significant benefits to the Arabic LLM community.\",\n",
       "  445: 'Laplace approximations are popular techniques for endowing deep networks with epistemic uncertainty estimates as they can be applied without altering the predictions of the trained network, and they scale to large models and datasets. While the choice of prior strongly affects the resulting posterior distribution, computational tractability and lack of interpretability of the weight space typically limit the Laplace approximation to isotropic Gaussian priors, which are known to cause pathological behavior as depth increases. As a remedy, we directly place a prior on function space. More precisely, since Lebesgue densities do not exist on infinite-dimensional function spaces, we recast training as finding the so-called weak mode of the posterior measure under a Gaussian process (GP) prior restricted to the space of functions representable by the neural network. Through the GP prior, one can express structured and interpretable inductive biases, such as regularity or periodicity, directly in function space, while still exploiting the implicit inductive biases that allow deep networks to generalize. After model linearization, the training objective induces a negative log-posterior density to which we apply a Laplace approximation, leveraging highly scalable methods from matrix-free linear algebra. Our method provides improved results where prior knowledge is abundant (as is the case in many scientific inference tasks). At the same time, it stays competitive for black-box supervised learning problems, where neural networks typically excel.',\n",
       "  446: 'In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the effects of local training with agent heterogeneity. We show that the communication complexity of FedLSA scales polynomially with the inverse of the desired accuracy ϵ. To overcome this, we propose SCAFFLSA a new variant of FedLSA that uses control variates to correct for client drift, and establish its sample and communication complexities. We show that for statistically heterogeneous agents, its communication complexity scales logarithmically with the desired accuracy, similar to Scaffnew. An important finding is that, compared to the existing results for Scaffnew, the sample complexity scales with the inverse of the number of agents, a property referred to as linear speed-up. Achieving this linear speed-up requires completely new theoretical arguments. We apply the proposed method to federated temporal difference learning with linear function approximation and analyze the corresponding complexity improvements.',\n",
       "  447: 'We study the fundamental problem of sequential probability assignment, also known as online learning with logarithmic loss, with respect to an arbitrary, possibly nonparametric hypothesis class. Our goal is to obtain a complexity measure for the hypothesis class that characterizes the minimax regret and to determine a general, minimax optimal algorithm. Notably, the sequential $\\\\ell_{\\\\infty}$ entropy, extensively studied in the literature (Rakhlin and Sridharan, 2015, Bilodeau et al., 2020, Wu et al., 2023), was shown to not characterize minimax regret in general. Inspired by the seminal work of Shtarkov (1987)    and Rakhlin, Sridharan, and Tewari (2010), we introduce a novel complexity measure, the \\\\emph{contextual Shtarkov sum}, corresponding to the Shtarkov sum after projection onto a multiary context tree, and show that the worst case log contextual Shtarkov sum equals the minimax regret. Using the contextual Shtarkov sum, we derive the minimax optimal strategy, dubbed \\\\emph{contextual Normalized Maximum Likelihood} (cNML). Our results hold for sequential experts, beyond binary labels, which are settings rarely considered in prior work.     To illustrate the utility of this characterization, we provide a short proof of a new regret upper bound in terms of sequential $\\\\ell_{\\\\infty}$ entropy, unifying and sharpening state-of-the-art bounds by Bilodeau et al. (2020) and Wu et al. (2023).',\n",
       "  448: 'In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks.',\n",
       "  449: 'It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters. Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent (hSGD), and show in a regularized linear regression framework that it leads to an asymptotically heavy-tailed parameter distribution, even though local gradient noise is Gaussian. We give explicit upper and lower bounds on the tail-index of the resulting parameter distribution and validate these bounds in numerical experiments. Moreover, the explicit form of these bounds enables us to quantify the interplay between optimization hyperparameters and the tail-index. Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima.',\n",
       "  450: 'Transformers have shown impressive capabilities across various tasks, but their performance on compositional problems remains a topic of debate. In this work, we investigate the mechanisms of how transformers behave on unseen compositional tasks.  We discover that the parameter initialization scale plays a critical role in determining whether the model learns inferential (reasoning-based) solutions, which capture the underlying compositional primitives, or symmetric (memory-based) solutions, which simply memorize mappings without understanding the compositional structure. By analyzing the information flow and vector representations within the model, we reveal the distinct mechanisms underlying these solution types. We further find that inferential (reasoning-based) solutions exhibit low complexity bias, which we hypothesize is a key factor enabling them to learn individual mappings for single anchors. We validate our conclusions on various real-world datasets. Our findings provide valuable insights into the role of initialization scale in tuning the reasoning and memorizing ability and we propose the initialization rate $\\\\gamma$ to be a convenient tunable hyper-parameter in common deep learning frameworks, where $1/d_{\\\\mathrm{in}}^\\\\gamma$ is the standard deviation of parameters of the layer with $d_{\\\\mathrm{in}}$ input neurons.',\n",
       "  451: 'In this paper, we propose to create animatable avatars for interacting hands with 3D Gaussian Splatting (GS) and single-image inputs. Existing GS-based methods designed for single subjects often yield unsatisfactory results due to limited input views, various hand poses, and occlusions. To address these challenges, we introduce a novel two-stage interaction-aware GS framework that exploits cross-subject hand priors and refines 3D Gaussians in interacting areas. Particularly, to handle hand variations, we disentangle the 3D presentation of hands into optimization-based identity maps and learning-based latent geometric features and neural texture maps. Learning-based features are captured by trained networks to provide reliable priors for poses, shapes, and textures, while optimization-based identity maps enable efficient one-shot fitting of out-of-distribution hands. Furthermore, we devise an interaction-aware attention module and a self-adaptive Gaussian refinement module. These modules enhance image rendering quality in areas with intra- and inter-hand interactions, overcoming the limitations of existing GS-based methods. Our proposed method is validated via extensive experiments on the large-scale InterHand2.6M dataset, and it significantly improves the state-of-the-art performance in image quality. Code and models will be released upon acceptance.',\n",
       "  452: 'In dynamic submodular maximization, the goal is to maintain a high-value solution over a sequence of element insertions and deletions with a fast update time. Motivated by large-scale applications and the fact that dynamic data often exhibits patterns, we ask the following question: can predictions be used to accelerate the update time of dynamic submodular maximization algorithms? We consider the model for dynamic algorithms with predictions where predictions regarding the insertion and deletion times of elements can be used for preprocessing. Our main result is an algorithm with an $O(\\\\text{poly}(\\\\log \\\\eta, \\\\log w, \\\\log k))$ amortized update time over the sequence of updates that achieves a $1/2 - \\\\epsilon$ approximation for dynamic monotone submodular maximization under a cardinality constraint $k$, where the prediction error $\\\\eta$ is the number of elements that are not inserted and deleted within $w$ time steps of their predicted insertion and deletion times. This amortized update time is independent of the length of the stream and instead depends on the prediction error.',\n",
       "  453: 'Few-shot font generation (FFG) aims to learn the target style from a limited number of reference glyphs and generate the remaining glyphs in the target font. Previous works focus on disentangling the content and style features of glyphs, combining the content features of the source glyph with the style features of the reference glyph to generate new glyphs. However, the disentanglement is challenging due to the complexity of glyphs, often resulting in glyphs that are influenced by the style of the source glyph and prone to artifacts. We propose IF-Font, a novel paradigm which incorporates Ideographic Description Sequence (IDS) instead of the source glyph to control the semantics of generated glyphs. To achieve this, we quantize the reference glyphs into tokens, and model the token distribution of target glyphs using corresponding IDS and reference tokens. The proposed method excels in synthesizing glyphs with neat and correct strokes, and enables the creation of new glyphs based on provided IDS. Extensive experiments demonstrate that our method greatly outperforms state-of-the-art methods in both one-shot and few-shot settings, particularly when the target styles differ significantly from the training font styles. The code is available at https://github.com/Stareven233/IF-Font.',\n",
       "  454: 'We introduce DataComp for Language Models, a testbed for controlled dataset experiments with the goal of improving language models.As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations.Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing atmodel scales ranging from 412M to 7B parameters.As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set.The resulting dataset, DCLM-Baseline, enables training a 7B parameter language model from scratch to 63% 5-shot accuracy on MMLU with 2T training tokens.Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6 percentage point improvement on MMLU while being trained with half the compute.Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation. We release the \\\\dclm benchmark, framework, models, and datasets at https://www.datacomp.ai/dclm/',\n",
       "  455: 'Zero-shot optimization involves optimizing a target task that was not seen during training, aiming to provide the optimal solution without or with minimal adjustments to the optimizer. It is crucial to ensure reliable and robust performance in various applications. Current optimizers often struggle with zero-shot optimization and require intricate hyperparameter tuning to adapt to new tasks. To address this, we propose a Pretrained Optimization Model (POM) that leverages knowledge gained from optimizing diverse tasks, offering efficient solutions to zero-shot optimization through direct application or fine-tuning with few-shot samples. Evaluation on the BBOB benchmark and two robot control tasks demonstrates that POM outperforms state-of-the-art black-box optimization methods, especially for high-dimensional tasks. Fine-tuning POM with a small number of samples and budget yields significant performance improvements. Moreover, POM demonstrates robust generalization across diverse task distributions, dimensions, population sizes, and optimization horizons. For code implementation, see https://github.com/ninja-wm/POM/.',\n",
       "  456: 'Training models with longer in-context lengths is a significant challenge for multimodal machine learning due to substantial GPU memory and computational costs. This exploratory study does not present state-of-the-art models; rather, it introduces an innovative method designed to increase in-context text length in multi-modality large language models (MLLMs) efficiently. We present \\\\ModelFullName (\\\\ModelName), which processes long in-context text using visual tokens. This technique significantly reduces GPU memory usage and floating point operations (FLOPs). For instance, our method expands the pre-training in-context length from 256 to 2048 tokens with fewer FLOPs for a 56 billion parameter MOE model. Experimental results demonstrate that \\\\ModelName enhances OCR capabilities and delivers superior performance on common downstream benchmarks for in-context few-shot evaluation. Additionally, \\\\ModelName proves effective for long context inference, achieving results comparable to full text input while maintaining computational efficiency.',\n",
       "  457: 'The nadir objective vector plays a key role in solving multi-objective optimization problems (MOPs), where it is often used to normalize the objective space and guide the search. The current methods for estimating the nadir objective vector perform effectively only on specific MOPs. This paper reveals the limitations of these methods: exact methods can only work on discrete MOPs, while heuristic methods cannot deal with the MOP with a complicated feasible objective region. To fill this gap, we propose a general and rigorous method, namely boundary decomposition for nadir objective vector estimation (BDNE). BDNE scalarizes the MOP into a set of boundary subproblems. By utilizing bilevel optimization, boundary subproblems are optimized and adjusted alternately, thereby refining their optimal solutions to align with the nadir objective vector. We prove that the bilevel optimization identifies the nadir objective vector under mild conditions. We compare BDNE with existing methods on various black-box MOPs. The results conform to the theoretical analysis and show the significant potential of BDNE for real-world application.',\n",
       "  458: 'Memory models such as Recurrent Neural Networks (RNNs) and Transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models called Linear Recurrent Models. We discover that the recurrent update of these models resembles a monoid, leading us to reformulate existing models using a novel monoid-based framework that we call memoroids. We revisit the traditional approach to batching in recurrent reinforcement learning, highlighting theoretical and empirical deficiencies. We leverage memoroids to propose a batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in reinforcement learning.',\n",
       "  459: 'Images produced by diffusion models are increasingly popular in digital artwork and visual marketing. However, such generated images might replicate content from existing ones and pose the challenge of content originality. Existing Image Copy Detection (ICD) models, though accurate in detecting hand-crafted replicas, overlook the challenge from diffusion models. This motivates us to introduce ICDiff, the first ICD specialized for diffusion models. To this end, we construct a Diffusion-Replication (D-Rep) dataset and correspondingly propose a novel deep embedding method. D-Rep uses a state-of-the-art diffusion model (Stable Diffusion V1.5) to generate 40, 000 image-replica pairs, which are manually annotated into 6 replication levels ranging from 0 (no replication) to 5 (total replication). Our method, PDF-Embedding, transforms the replication level of each image-replica pair into a probability density function (PDF) as the supervision signal. The intuition is that the probability of neighboring replication levels should be continuous and smooth. Experimental results show that PDF-Embedding surpasses protocol-driven methods and non-PDF choices on the D-Rep test set. Moreover, by utilizing PDF-Embedding, we find that the replication ratios of well-known diffusion models against an open-source gallery range from 10% to 20%. The project is publicly available at https://icdiff.github.io/.',\n",
       "  460: 'Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective  aggregation scheme for LLM fine-tuning, which mitigates the \"buckets effect\" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving thousands of clients performing heterogeneous NLP tasks and client resources, our experiments validate the efficacy of FlexLoRA, with the federated global model achieving consistently better improvement over SOTA FL methods in downstream NLP task performance across various heterogeneous distributions. FlexLoRA\\'s practicality is further underscored by our theoretical analysis and its seamless integration with existing LoRA-based FL methods, offering a path toward cross-device, privacy-preserving federated tuning for LLMs.',\n",
       "  461: 'This paper presents a new gradient flow dissipation geometry over non-negative and probability measures.This is motivated by a principled construction that combines the unbalanced optimal transport and interaction forces modeled by reproducing kernels. Using a precise connection between the Hellinger geometry and the maximum mean discrepancy (MMD), we propose the interaction-force transport (IFT) gradient flows and its spherical variant via an infimal convolution of the Wasserstein and spherical MMD tensors. We then develop a particle-based optimization algorithm based on the JKO-splitting scheme of the mass-preserving spherical IFT gradient flows. Finally, we provide both theoretical global exponential convergence guarantees and improved empirical simulation results for applying the IFT gradient flows to the sampling task of MMD-minimization. Furthermore, we prove that the spherical IFT  gradient flow enjoys the best of both worlds by providing the global exponential convergence guarantee for both the MMD and KL energy.',\n",
       "  462: 'Mean-field variational inference (VI) is computationally scalable, but its highly-demanding independence requirement hinders it from being applied to wider scenarios. Although many VI methods that take correlation into account have been proposed, these methods generally are not scalable enough to capture the correlation among data instances, which often arises in applications with graph-structured data or explicit constraints. In this paper, we developed the Tree-structured Variational Inference (TreeVI), which uses a tree structure to capture the correlation of latent variables in the posterior distribution. We show that  samples from the tree-structured posterior can be reparameterized efficiently and parallelly, making its training cost just 2 or 3 times that of VI under the mean-field assumption. To capture correlation with more complicated structure, the TreeVI is further extended to the multiple-tree case. Furthermore, we show that the underlying tree structure can be automatically learned from training data. With experiments on synthetic datasets, constrained clustering, user matching and link prediction, we demonstrate that the TreeVI is superior in capturing  instance-level correlation in posteriors and enhancing the performance of downstream applications.',\n",
       "  463: 'Recent works on the parallel complexity of Boosting have established strong lower bounds on the tradeoff between the number of training rounds $p$ and the total parallel work per round $t$.These works have also presented highly non-trivial parallel algorithms that shed light on different regions of this tradeoff.Despite these advancements, a significant gap persists between the theoretical lower bounds and the performance of these algorithms across much of the tradeoff space.In this work, we essentially close this gap by providing both improved lower bounds on the parallel complexity of weak-to-strong learners, and a parallel Boosting algorithm whose performance matches these bounds across the entire $p$ vs. $t$ compromise spectrum, up to logarithmic factors.Ultimately, this work settles the parallel complexity of Boosting algorithms that are nearly sample-optimal.',\n",
       "  464: 'Frontier AI systems, including large language models (LLMs), hold increasing influence over the epistemology of human users. Such influence can reinforce prevailing societal values, potentially contributing to the lock-in of misguided moral beliefs and, consequently, the perpetuation of problematic moral practices on a broad scale. We introduce progress alignment as a technical solution to mitigate this imminent risk. Progress alignment algorithms learn to emulate the mechanics of human moral progress, thereby addressing the susceptibility of existing alignment methods to contemporary moral blindspots. To empower research in progress alignment, we introduce ProgressGym, an experimental framework allowing the learning of moral progress mechanics from history, in order to facilitate future progress in real-world moral decisions. Leveraging 9 centuries of historical text and 18 historical LLMs, ProgressGym enables codification of real-world progress alignment challenges into concrete benchmarks. Specifically, we introduce three core challenges: tracking evolving values (PG-Follow), preemptively anticipating moral progress (PG-Predict), and regulating the feedback loop between human and AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension are inapplicable to these tasks. In response, we present lifelong and extrapolative algorithms as baseline methods of progress alignment, and build an open leaderboard soliciting novel algorithms and challenges.',\n",
       "  465: 'When applying reinforcement learning from human feedback (RLHF), the reward is learned from data and, therefore, always has some error. It is common to mitigate this by regularizing the policy with KL divergence from a base model, with the hope that balancing reward with regularization will achieve desirable outcomes despite this reward misspecification. We show that when the reward function has light-tailed error, optimal policies under less restrictive KL penalties achieve arbitrarily high utility. However, if error is heavy-tailed, some policies obtain arbitrarily high reward despite achieving no more utility than the base model--a phenomenon we call catastrophic Goodhart. We adapt a discrete optimization method to measure the tails of reward models, finding that they are consistent with light-tailed error. However, the pervasiveness of heavy-tailed distributions in many real-world applications indicates that future sources of RL reward could have heavy-tailed error, increasing the likelihood of reward hacking even with KL regularization.',\n",
       "  466: \"Watermarking is a technique that involves embedding nearly unnoticeable statistical signals within generated content to help trace its source. This work focuses on a scenario where an untrusted third-party user sends prompts to a trusted language model (LLM) provider, who then generates a text from their LLM with a watermark. This setup makes it possible for a detector to later identify the source of the text if the user publishes it. The user can modify the generated text by substitutions, insertions, or deletions. Our objective is to develop a statistical method to detect if a published text is LLM-generated from the perspective of a detector. We further propose a methodology to segment the published text into watermarked and non-watermarked sub-strings. The proposed approach is built upon randomization tests and change point detection techniques. We demonstrate that our method ensures Type I and Type II error control and can accurately identify watermarked sub-strings by finding the corresponding change point locations. To validate our technique, we apply it to texts generated by several language models with prompts extracted from Google's C4 dataset and obtain encouraging numerical results. We release all code publicly at https://github.com/doccstat/llm-watermark-cpd.\",\n",
       "  467: 'Many computational tasks can be naturally expressed as a composition of  a DNN followed by a program written in a traditional programming language or an API call to an LLM. We call such composites \"neural programs\" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite.  When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components. We present an algorithm for learning neural programs, called ISED, that only relies on input-output samples of black-box components. For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as  GPT-4 and also consider benchmarks from the neurosymbolic learning literature. Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks. For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner.',\n",
       "  468: 'Interpreting hierarchical structures latent in language is a key limitation of current language models (LMs). While previous research has implicitly leveraged these hierarchies to enhance LMs, approaches for their explicit encoding are yet to be explored. To address this, we introduce a novel approach to re-train transformer encoder-based LMs as Hierarchy Transformer encoders (HiTs), harnessing the expansive nature of hyperbolic space. Our method situates the output embedding space of pre-trained LMs within a Poincaré ball with a curvature that adapts to the embedding dimension, followed by re-training on hyperbolic clustering and centripetal losses. These losses are designed to effectively cluster related entities (input as texts) and organise them hierarchically. We evaluate HiTs against pre-trained LMs, standard fine-tuned LMs, and several hyperbolic embedding baselines, focusing on their capabilities in simulating transitive inference, predicting subsumptions, and transferring knowledge across hierarchies. The results demonstrate that HiTs consistently outperform all baselines in these tasks, underscoring the effectiveness and transferability of our re-trained hierarchy encoders.',\n",
       "  469: 'Stability in recurrent neural models poses a significant challenge, particularly in developing biologically plausible neurodynamical models that can be seamlessly trained. Traditional cortical circuit models are notoriously difficult to train due to expansive nonlinearities in the dynamical system, leading to an optimization problem with nonlinear stability constraints that are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in tasks involving sequential data but lack biological plausibility and interpretability. In this work, we address these challenges by linking dynamic divisive normalization (DN) to the stability of \"oscillatory recurrent gated neural integrator circuits\\'\\' (ORGaNICs), a biologically plausible recurrent cortical circuit model that dynamically achieves DN and that has been shown to simulate a wide range of neurophysiological phenomena. By using the indirect method of Lyapunov, we prove the remarkable property of unconditional local stability for an arbitrary-dimensional ORGaNICs circuit when the recurrent weight matrix is the identity. We thus connect ORGaNICs to a system of coupled damped harmonic oscillators, which enables us to derive the circuit\\'s energy function, providing a normative principle of what the circuit, and individual neurons, aim to accomplish. Further, for a generic recurrent weight matrix, we prove the stability of the 2D model and demonstrate empirically that stability holds in higher dimensions. Finally, we show that ORGaNICs can be trained by backpropagation through time without gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which address the problems of exploding, vanishing, and oscillating gradients. By evaluating the model\\'s performance on RNN benchmarks, we find that ORGaNICs outperform alternative neurodynamical models on static image classification tasks and perform comparably to LSTMs on sequential tasks.',\n",
       "  470: 'Medical imaging tasks require an understanding of subtle and localized visual features due to the inherently detailed and area-specific nature of pathological patterns, which are crucial for clinical diagnosis. Although recent advances in medical vision-language pre-training (VLP) enable models to learn clinically relevant visual features by leveraging both medical images and their associated radiology reports, current medical VLP methods primarily focus on aligning images with entire reports. This focus hinders the learning of dense (pixel-level) visual features and is suboptimal for dense prediction tasks (e.g., medical image segmentation).To address this challenge, we propose a novel medical VLP framework, named Global to Dense level representation learning (G2D), which aims to learn global and dense visual features simultaneously using only image-text pairs without extra annotations. In particular, G2D designs a Pseudo Segmentation (PS) task, which enables the model to learn dense visual features during VLP. Notably, generating PS masks can be performed on the fly during VLP, which does not incur extra trainable parameters. With this simple yet effective idea, G2D achieves superior performance across 5 medical imaging tasks and 25 diseases. Particularly, in the segmentation task which requires dense visual features, G2D surpasses existing models even with just 1% of the training data for finetuning, compared to 100% used by other models. The code can be found in https://github.com/cheliu-computation/G2D-NeurIPS24/tree/main.',\n",
       "  471: 'Self-attention is the key mechanism of transformers, which are the essential building blocks of modern foundation models. Recent studies have shown that pure self-attention suffers from an increasing degree of rank collapse as depth increases, limiting model expressivity and further utilization of model depth. The existing literature on rank collapse, however, has mostly overlooked other critical components in transformers that may alleviate the rank collapse issue. In this paper, we provide a general analysis of rank collapse under self-attention, taking into account the effects of attention masks and layer normalization (LayerNorm). In particular, we find that although pure masked attention still suffers from exponential collapse to a rank one subspace, sparse or local masked attention can provably slow down the collapse rate. In the case of self-attention with LayerNorm, we first show that for certain classes of value matrices, collapse to a rank one subspace still happens exponentially. However, through construction of nontrivial counterexamples, we then establish that with proper choice of value matrices, a general class of sequences may not converge to a rank one subspace, and the self-attention dynamics with LayerNorm can simultaneously possess a rich set of equilibria with any possible rank between one and full. Our result refutes the previous hypothesis that LayerNorm plays no role in the rank collapse of self-attention and suggests that self-attention with LayerNorm constitutes a much more expressive, versatile nonlinear dynamical system than what was originally thought.',\n",
       "  472: 'We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization, rather than on global, worst-case constants.  Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective.  Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes.  For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness.  Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on $L$-smoothness.',\n",
       "  473: 'Open-vocabulary object perception has become an important topic in artificial intelligence, which aims to identify objects with novel classes that have not been seen during training. Under this setting, open-vocabulary object detection (OVD) in a single image has been studied in many literature. However, open-vocabulary object tracking (OVT) from a video has been studied less, and one reason is the shortage of benchmarks. In this work, we have built a new large-scale benchmark for open-vocabulary multi-object tracking namely OVT-B. OVT-B contains 1,048 categories of objects and 1,973 videos with 637,608 bounding box annotations, which is much larger than the sole open-vocabulary tracking dataset, i.e., OVTAO-val dataset (200+ categories, 900+ videos). The proposed OVT-B can be used as a new benchmark to pave the way for OVT research. We also develop a simple yet effective baseline method for OVT. It integrates the motion features for object tracking, which is an important feature for MOT but is ignored in previous OVT methods. Experimental results have verified the usefulness of the proposed benchmark and the effectiveness of our method. We have released the benchmark to the public at https://github.com/Coo1Sea/OVT-B-Dataset.',\n",
       "  474: 'Models are expected to engage in invariance learning, which involves distinguishing the core relations that remain consistent across varying environments to ensure the predictions are safe, robust and fair. While existing works consider specific algorithms to realize invariance learning, we show that model has the potential to learn invariance through standard training procedures. In other words, this paper studies the implicit bias of Stochastic Gradient Descent (SGD) over heterogeneous data and shows that the implicit bias drives the model learning towards an invariant solution. We call the phenomenon the implicit invariance learning. Specifically, we theoretically investigate the multi-environment low-rank matrix sensing problem where in each environment, the signal comprises (i) a lower-rank invariant part shared across all environments; and (ii) a significantly varying environment-dependent spurious component. The key insight is, through simply employing the large step size large-batch SGD sequentially in each environment without any explicit regularization, the oscillation caused by heterogeneity can provably prevent model learning spurious signals.  The model reaches the invariant solution after certain iterations. In contrast, model learned using pooled SGD over all data would simultaneously learn both the invariant and spurious signals. Overall, we unveil another implicit bias that is a result of the symbiosis between the heterogeneity of data and modern algorithms, which is, to the best of our knowledge, first in the literature.',\n",
       "  475: 'Vision-language tracking (VLT) enhances traditional visual object tracking by integrating language descriptions, requiring the tracker to flexibly understand complex and diverse text in addition to visual information. However, most existing vision-language trackers still overly rely on initial fixed multimodal prompts, which struggle to provide effective guidance for dynamically changing targets. Fortunately, the Complementary Learning Systems (CLS) theory suggests that the human memory system can dynamically store and utilize multimodal perceptual information, thereby adapting to new scenarios. Inspired by this, (i) we propose a Memory-based Vision-Language Tracker (MemVLT). By incorporating memory modeling to adjust static prompts, our approach can provide adaptive prompts for tracking guidance. (ii) Specifically, the memory storage and memory interaction modules are designed in accordance with CLS theory. These modules facilitate the storage and flexible interaction between short-term and long-term memories, generating prompts that adapt to target variations. (iii) Finally, we conduct extensive experiments on mainstream VLT datasets (e.g., MGIT, TNL2K, LaSOT and LaSOT$_{ext}$). Experimental results show that MemVLT achieves new state-of-the-art performance. Impressively, it achieves 69.4% AUC on the MGIT and 63.3% AUC on the TNL2K, improving the existing best result by 8.4% and 4.7%, respectively.',\n",
       "  476: 'Solving complex Partial Differential Equations (PDEs) accurately and efficiently is an essential and challenging problem in all scientific and engineering disciplines. Mesh movement methods provide the capability to improve the accuracy of the numerical solution without increasing the overall mesh degree of freedom count. Conventional sophisticated mesh movement methods are extremely expensive and struggle to handle scenarios with complex boundary geometries. However, existing learning-based methods require re-training from scratch given a different PDE type or boundary geometry, which limits their applicability, and also often suffer from robustness issues in the form of inverted elements. In this paper, we introduce the Universal Mesh Movement Network (UM2N), which -- once trained -- can be applied in a non-intrusive, zero-shot manner to move meshes with different size distributions and structures, for solvers applicable to different PDE types and boundary geometries. UM2N consists of a Graph Transformer (GT) encoder for extracting features and a Graph Attention Network (GAT) based decoder for moving the mesh. We evaluate our method on advection and Navier-Stokes based examples, as well as a real-world tsunami simulation case. Our method out-performs existing learning-based mesh movement methods in terms of the benchmarks described above. In comparison to the conventional sophisticated Monge-Ampère PDE-solver based method, our approach not only significantly accelerates mesh movement, but also proves effective in scenarios where the conventional method fails. Our project page can be found at https://erizmr.github.io/UM2N/.',\n",
       "  477: 'Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl\\'s influential approach is theoretically elegant, its generation of a counterfactual scenario often requires too much deviation from the observed scenarios to be feasible, as we show using simple examples. To mitigate this difficulty, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are more feasible with respect to the actual data distribution. Our methodology incorporates a certain amount of backtracking when needed, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. Specifically, we introduce a novel optimization framework that permits but also controls the extent of backtracking with a \"naturalness\\'\\' criterion. Empirical experiments demonstrate the effectiveness of our method. The code is available at https://github.com/GuangyuanHao/natural_counterfactuals.',\n",
       "  478: 'Large-scale foundation models have demonstrated exceptional performance in language and vision tasks. However, the numerous dense matrix-vector operations involved in these large networks pose significant computational challenges during inference. To address these challenges, we introduce the Block-Level Adaptive STructured (BLAST) matrix, designed to learn and leverage efficient structures prevalent in the weight matrices of linear layers within deep learning models. Compared to existing structured matrices, the BLAST matrix offers substantial flexibility, as it can represent various types of structures that are either learned from data or computed from pre-existing weight matrices. We demonstrate the efficiency of using the BLAST matrix for compressing both language and vision tasks, showing that (i) for medium-sized models such as ViT and GPT-2, training with BLAST weights boosts performance while reducing complexity by 70\\\\% and 40\\\\%, respectively; and (ii) for large foundation models such as Llama-7B and DiT-XL, the BLAST matrix achieves a 2x compression while exhibiting the lowest performance degradation among all tested structured matrices. Our code is available at https://github.com/changwoolee/BLAST.',\n",
       "  479: \"Data selection has emerged as a core issue for large-scale visual-language model pretaining (e.g., CLIP), particularly with noisy web-curated datasets. Three main data selection approaches are: (1) leveraging external non-CLIP models to aid data selection, (2) training new CLIP-style embedding models that are more effective at selecting high-quality data than the original OpenAI CLIP model, and (3) designing better metrics or strategies universally applicable to any CLIP embedding without requiring specific model properties (e.g., CLIPScore is one popular metric).  While the first two approaches have been extensively studied, the third remains under-explored. In this paper, we advance the third approach by proposing two new methods. Firstly, instead of classical CLIP scores that only consider the alignment between two modalities from a single sample, we introduce $\\\\textbf{negCLIPLoss}$,  a method inspired by CLIP training loss that adds the alignment between one sample and its contrastive pairs as an extra normalization term to CLIPScore for better quality measurement. Secondly, when downstream tasks are known, we propose a new norm-based metric, $\\\\textbf{NormSim}$, to measure the similarity between pretraining data and target data. We test our methods on the data selection benchmark, DataComp [Gadre et al., 2023]. Compared to the best baseline using only OpenAI's CLIP-L/14, our methods achieve a 5.3\\\\% improvement on ImageNet-1k and a 2.8\\\\% improvement on 38 downstream evaluation tasks. Moreover, both $\\\\textbf{negCLIPLoss}$ and $\\\\textbf{NormSim}$ are compatible with existing techniques. By combining our methods with the current best methods DFN [Fang et al., 2023] and HYPE [Kim et al., 2024], we can boost average performance on downstream tasks by 0.9\\\\%, achieving a new state-of-the-art on the DataComp-medium benchmark.\",\n",
       "  480: 'Gradient estimation is critical in zeroth-order optimization methods, which aims to obtain the descent direction by sampling update directions and querying function evaluations. Extensive research has been conducted including smoothing and linear interpolation. The former methods smooth the objective function, causing a biased gradient estimation, while the latter often enjoys more accurate estimates, at the cost of large amounts of samples and queries at each iteration to update variables. This paper resorts to the linear interpolation strategy and proposes to reduce the complexity of gradient estimation by reusing queries in the prior iterations while maintaining the sample size unchanged. Specifically, we model the gradient estimation as a quadratically constrained linear program problem and manage to derive the analytical solution. It innovatively decouples the required sample size from the variable dimension without extra conditions required, making it able to leverage the queries in the prior iterations. Moreover, part of the intermediate variables that contribute to the gradient estimation can be directly indexed, significantly reducing the computation complexity. Experiments on both simulation functions and real scenarios (black-box adversarial attacks neural architecture search, and parameter-efficient fine-tuning for large language models), show its efficacy and efficiency. Our code is available at https://github.com/Thinklab-SJTU/ReLIZO.git.',\n",
       "  481: 'Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying equivalent entity pairs. While existing methods heavily rely on human-generated labels, it is prohibitively expensive to incorporate cross-domain experts for annotation in real-world scenarios. The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information. However, it is nontrivial to directly apply LLMs for EA since the annotation space in real-world KGs is large. LLMs could also generate noisy labels that may mislead the alignment. To this end, we propose a unified framework, LLM4EA, to effectively leverage LLMs for EA. Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure. Moreover, we introduce an unsupervised label refiner to continuously enhance label accuracy through in-depth probabilistic reasoning. We iteratively optimize the policy based on the feedback from a base EA model. Extensive experiments demonstrate the advantages of LLM4EA on four benchmark datasets in terms of effectiveness, robustness, and efficiency.',\n",
       "  482: 'Diffusion models are capable of generating photo-realistic images that combine elements which do not appear together in natural images, demonstrating their ability to compositionally generalize. Nonetheless, the precise mechanism of compositionality and how it is acquired through training remains elusive. Here, we consider a highly reduced setting to examine whether diffusion models learn semantically meaningful and fully factorized representations of composable features. We performed extensive controlled experiments on conditional DDPMs trained to generate various forms of 2D Gaussian data. We demonstrate that the models learn factorized, semi-continuous manifold representations that are orthogonal in underlying continuous latent features of independent variations but are not aligned for different values of the same feature. With such representations, models demonstrate superior compositionality but have limited ability to interpolate over unseen values of a given feature. Our experimental results further demonstrate that diffusion models can attain compositionality with a small amount of compositional examples, suggesting a novel way to train DDPMs. Finally, we connect manifold formation in diffusion models to percolation theory in physics, thereby offering insights into the sudden onset of factorized representation learning. Our thorough toy experiments thus contribute a deeper understanding of how diffusion models capture compositional structure in data, paving the way for future research aimed at enhancing factorization and compositional generalization in generative models for real-world applications.',\n",
       "  483: 'In Economics, the concept of externality refers to any indirect effect resulting from an interaction between players and affecting a third party without compensation. Most of the models within which externality has been studied assume that agents have perfect knowledge of their environment and preferences. This is a major hindrance to the practical implementation of many proposed solutions. To adress this issue, we consider a two-players bandit game setting where the actions of one of the player affect the other one. Building upon this setup, we extend the Coase theorem [Coase, 2013], which suggests that the optimal approach for maximizing the social welfare in the presence of externality is to establish property rights, i.e., enabling transfers and bargaining between the players. Nonetheless, this fundamental result relies on the assumption that bargainers possess perfect knowledge of the underlying game. We first demonstrate that in the absence of property rights in the considered online scenario, the social welfare breaks down. We then provide a policy for the players, which allows them to learn a bargaining strategy which maximizes the total welfare, recovering the Coase theorem under uncertainty.',\n",
       "  484: \"How can we test AI performance? This question seems trivial, but it isn't. Standard benchmarks often have problems such as in-distribution and small-size test sets, oversimplified metrics, unfair comparisons, and short-term outcome pressure. As a consequence, good performance on standard benchmarks does not guarantee success in real-world scenarios. To address these problems, we present Touchstone, a large-scale collaborative segmentation benchmark of 9 types of abdominal organs. This benchmark is based on 5,195 training CT scans from 76 hospitals around the world and 5,903 testing CT scans from 11 additional hospitals. This diverse test set enhances the statistical significance of benchmark results and rigorously evaluates AI algorithms across various out-of-distribution scenarios. We invited 14 inventors of 19 AI algorithms to train their algorithms, while our team, as a third party, independently evaluated these algorithms on three test sets. In addition, we also evaluated pre-existing AI frameworks---which, differing from algorithms, are more flexible and can support different algorithms—including MONAI from NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are committed to expanding this benchmark to encourage more innovation of AI algorithms for the medical domain.\",\n",
       "  485: 'Gradient-based bilevel programming leverages unrolling differentiation (UD) or implicit function theorem (IFT) to solve hyperparameter optimization (HO) problems, and is proven effective and scalable in practice. To understand their generalization behavior, existing works establish upper bounds on the uniform stability of these algorithms, while their tightness is still unclear. To this end, this paper attempts to establish stability lower bounds for UD-based and IFT-based algorithms. A central technical challenge arises from the dependency of each outer-level update on the concurrent stage of inner optimization in bilevel programming. To address this problem, we introduce lower-bounded expansion properties to characterize the instability in update rules which can serve as general tools for lower-bound analysis. These properties guarantee the hyperparameter divergence at the outer level and the Lipschitz constant of inner output at the inner level in the context of HO.Guided by these insights, we construct a quadratic example that yields tight lower bounds for the UD-based algorithm and meaningful bounds for a representative IFT-based algorithm.Our tight result indicates that uniform stability has reached its limit in stability analysis for the UD-based algorithm.',\n",
       "  486: 'Self-training often falls short under distribution shifts due to an increased discrepancy between prediction confidence and actual accuracy. This typically necessitates computationally demanding methods such as neighborhood or ensemble-based label corrections. Drawing inspiration from insights on early learning regularization, we develop a principled method to improve self-training under distribution shifts based on temporal consistency. Specifically, we build an uncertainty-aware temporal ensemble with a simple relative thresholding. Then, this ensemble smooths noisy pseudo labels to promote selective temporal consistency. We show that our temporal ensemble is asymptotically correct and our label smoothing technique can reduce the optimality gap of self-training. Our extensive experiments validate that our approach consistently improves self-training performances by 8% to 16% across diverse distribution shift scenarios without a computational overhead. Besides, our method exhibits attractive properties, such as improved calibration performance and robustness to different hyperparameter choices.',\n",
       "  487: 'Current 4D generation methods have achieved noteworthy efficacy with the aid of advanced diffusion generative models. However, these methods lack multi-view spatial-temporal modeling and encounter challenges in integrating diverse prior knowledge from multiple diffusion models, resulting in inconsistent temporal appearance and flickers. In this paper, we propose a novel 4D generation pipeline, namely $\\\\textbf{4Diffusion}$, aimed at generating spatial-temporally consistent 4D content from a monocular video. We first design a unified diffusion model tailored for multi-view video generation by incorporating a learnable motion module into a frozen 3D-aware diffusion model to capture multi-view spatial-temporal correlations. After training on a curated dataset, our diffusion model acquires reasonable temporal consistency and inherently preserves the generalizability and spatial consistency of the 3D-aware diffusion model. Subsequently, we propose 4D-aware Score Distillation Sampling loss, which is based on our multi-view video diffusion model, to optimize 4D representation parameterized by dynamic NeRF. This aims to eliminate discrepancies arising from multiple diffusion models, allowing for generating spatial-temporally consistent 4D content. Moreover, we devise an anchor loss to enhance the appearance details and facilitate the learning of dynamic NeRF. Extensive qualitative and quantitative experiments demonstrate that our method achieves superior performance compared to previous methods.',\n",
       "  488: \"Large language models (LLMs) have demonstrated impressive capabilities across diverse languages. This study explores how LLMs handle multilingualism. Based on observed language ratio shifts among layers and the relationships between network structures and certain capabilities, we hypothesize the LLM's multilingual workflow ($\\\\texttt{MWork}$): LLMs initially understand the query, converting multilingual inputs into English for task-solving. In the intermediate layers, they employ English for thinking and incorporate multilingual knowledge with self-attention and feed-forward structures, respectively. In the final layers, LLMs generate responses aligned with the original language of the query. To verify $\\\\texttt{MWork}$, we introduce Parallel Language-specific Neuron Detection ($\\\\texttt{PLND}$) to identify activated neurons for inputs in different languages without any labeled data. Using $\\\\texttt{PLND}$, we validate $\\\\texttt{MWork}$ through extensive experiments involving the deactivation of language-specific neurons across various layers and structures. Moreover, $\\\\texttt{MWork}$ allows fine-tuning of language-specific neurons with a small dataset, enhancing multilingual abilities in a specific language without compromising others. This approach results in an average improvement of $3.6\\\\%$ for high-resource languages and $2.3\\\\%$ for low-resource languages across all tasks with just $400$ documents.\",\n",
       "  489: 'Deep neural networks provide Reinforcement Learning (RL) powerful function approximators to address large-scale decision-making problems. However, these approximators introduce challenges due to the non-stationary nature of RL training. One source of the challenges in RL is that output predictions can churn, leading to uncontrolled changes after each batch update for states not included in the batch. Although such a churn phenomenon exists in each step of network training, it remains under-explored on how churn occurs and impacts RL. In this work, we start by characterizing churn in a view of Generalized Policy Iteration with function approximation, and we discover a chain effect of churn that leads to a cycle where the churns in value estimation and policy improvement compound and bias the learning dynamics throughout the iteration. Further, we concretize the study and focus on the learning issues caused by the chain effect in different settings, including greedy action deviation in value-based methods, trust region violation in proximal policy optimization, and dual bias of policy value in actor-critic methods. We then propose a method to reduce the chain effect across different settings, called Churn Approximated ReductIoN (CHAIN), which can be easily plugged into most existing DRL algorithms. Our experiments demonstrate the effectiveness of our method in both reducing churn and improving learning performance across online and offline, value-based and policy-based RL settings.',\n",
       "  490: 'The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. These results underscore the significance of incorporating uncertainty in the evaluation of LLMs. Our implementation is available at https://github.com/smartyfh/LLM-Uncertainty-Bench.',\n",
       "  491: \"This paper introduces FUNGI, Features from UNsupervised GradIents, a method to enhance the features of transformer encoders by leveraging self-supervised gradients. Our method is simple: given any pretrained model, we first compute gradients from various self-supervised objectives for each input. These gradients are projected to a lower dimension and then concatenated with the model's output embedding. The resulting features are evaluated on k-nearest neighbor classification over 11 datasets from vision, 5 from natural language processing, and 2 from audio. Across backbones spanning various sizes and pretraining strategies, FUNGI features provide consistent performance improvements over the embeddings. We also show that using FUNGI features can benefit linear classification, clustering and image retrieval, and that they significantly improve the retrieval-based in-context scene understanding abilities of pretrained models, for example improving upon DINO by +17% for semantic segmentation - without any training. Code is available at https://github.com/WalterSimoncini/fungivision.\",\n",
       "  492: 'In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA).  To enhance generation, we propose a two-stage instruction tuning method that significantly boosts the performance of RAG.  For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs.  We also present the ChatRAG Bench, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions.  Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without relying on any synthetic data from OpenAI GPT models.   Notably, Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09 by a margin. These results demonstrate the exceptional quality of the proposed ChatQA recipe. To advance research in this field, we open-sourced the model weights, instruction tuning data, ChatRAG Bench, and retriever for the community.',\n",
       "  493: 'Scaling hyperparameter optimisation to very large datasets remains an open problem in the Gaussian process community. This paper focuses on iterative methods, which use linear system solvers, like conjugate gradients, alternating projections or stochastic gradient descent, to construct an estimate of the marginal likelihood gradient. We discuss three key improvements which are applicable across solvers: (i) a pathwise gradient estimator, which reduces the required number of solver iterations and amortises the computational cost of making predictions, (ii) warm starting linear system solvers with the solution from the previous step, which leads to faster solver convergence at the cost of negligible bias, (iii) early stopping linear system solvers after a limited computational budget, which synergises with warm starting, allowing solver progress to accumulate over multiple marginal likelihood steps. These techniques provide speed-ups of up to $72\\\\times$ when solving to tolerance, and decrease the average residual norm by up to $7\\\\times$ when stopping early.',\n",
       "  494: \"Reinforcement learning (RL) has emerged as a pivotal technique for fine-tuning large language models (LLMs) on specific tasks. However, prevailing RL fine-tuning methods predominantly rely on PPO and its variants. Though these algorithms are effective in general RL settings, they often exhibit suboptimal performance and vulnerability to distribution collapse when applied to the fine-tuning of LLMs. In this paper, we propose CORY, extending the RL fine-tuning of LLMs to a sequential cooperative multi-agent reinforcement learning framework, to leverage the inherent coevolution and emergent capabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is initially duplicated into two autonomous agents: a pioneer and an observer. The pioneer generates responses based on queries, while the observer generates responses using both the queries and the pioneer’s responses. The two agents are trained together. During training, the agents exchange roles periodically, fostering cooperation and coevolution between them. Experiments evaluate CORY's performance by fine-tuning GPT-2 and Llama-2 under subjective and objective reward functions on the IMDB Review and GSM8K datasets, respectively. Results show that CORY outperforms PPO in terms of policy optimality, resistance to distribution collapse, and training robustness, thereby underscoring its potential as a superior methodology for refining LLMs in real-world applications.\",\n",
       "  495: 'Early and accurate detection of anomalous events on the freeway, such as accidents, can improve emergency response and clearance. However, existing delays and mistakes from manual crash reporting records make it a difficult problem to solve. Current large-scale freeway traffic datasets are not designed for anomaly detection and ignore these challenges. In this paper, we introduce the first large-scale lane-level freeway traffic dataset for anomaly detection. Our dataset consists of a month of weekday radar detection sensor data collected in 4 lanes along an 18-mile stretch of Interstate 24 heading toward Nashville, TN, comprising over 3.7 million sensor measurements. We also collect official crash reports from the Tennessee Department of Transportation Traffic Management Center and manually label all other potential anomalies in the dataset. To show the potential for our dataset to be used in future machine learning and traffic research, we benchmark numerous deep learning anomaly detection models on our dataset. We find that unsupervised graph neural network autoencoders are a promising solution for this problem and that ignoring spatial relationships leads to decreased performance. We demonstrate that our methods can reduce reporting delays by over 10 minutes on average while detecting 75% of crashes. Our dataset and all preprocessing code needed to get started are publicly released at https://vu.edu/ft-aed/ to facilitate future research.',\n",
       "  496: 'Collaborative learning is an important tool to train multiple clients more effectively by enabling communication among clients. Identifying helpful clients, however, presents challenging and often introduces significant overhead. In this paper, we model client-selection and model-training as two interconnected optimization problems, proposing a novel bilevel optimization problem for collaborative learning.We introduce CoBo, a scalable and elastic, SGD-type alternating optimization algorithm  that efficiently addresses these problem with theoretical convergence guarantees. Empirically, CoBo achieves superior performance, surpassing popular personalization algorithms by 9.3% in accuracy on a task with high heterogeneity, involving datasets distributed among 80 clients.',\n",
       "  497: 'Precision matrix estimation is a ubiquitous task featuring numerous applications such as rare disease diagnosis and neural connectivity exploration. However, this task becomes challenging in small sample settings, where the number of samples is significantly less than the number of dimensions, leading to unreliable estimates. Previous approaches either fail to perform well in small sample settings or suffer from inefficient estimation processes, even when incorporating meta-learning techniques.To this end, we propose a novel approach FasMe for Fast and Sample-efficient Meta Precision Matrix Learning, which first extracts meta-knowledge through a multi-task learning diagram. Then, meta-knowledge constraints are applied using a maximum determinant matrix completion algorithm for the novel task. As a result, we reduce the sample size requirements to $O(\\\\log p/K)$ per meta-training task and $O(\\\\log\\\\vert \\\\mathcal{G}\\\\vert)$ for the meta-testing task. Moreover, the hereby proposed model only needs $O(p \\\\log\\\\epsilon^{-1})$ time and $O(p)$ memory for converging to an $\\\\epsilon$-accurate solution. On multiple synthetic and biomedical datasets, FasMe is at least ten times faster than the four baselines while promoting prediction accuracy in small sample settings.',\n",
       "  498: 'Many structured prediction and reasoning tasks can be framed as program synthesis problems, where the goal is to generate a program in a \\\\emph{domain-specific language} (DSL) that transforms input data into the desired output. Unfortunately, purely neural approaches, such as large language models (LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while purely symbolic methods based on combinatorial search scale poorly to complex problems. Motivated by these limitations, we introduce a hybrid approach, where LLM completions for a given task are used to learn a task-specific, context-free surrogate model, which is then used to guide program synthesis. We evaluate this hybrid approach on three domains, and show that it outperforms both unguided search and direct sampling from LLMs, as well as existing program synthesizers.',\n",
       "  499: 'Gaze following and social gaze prediction are fundamental tasks providing insights into human communication behaviors, intent, and social interactions. Most previous approaches addressed these tasks separately, either by designing highly specialized social gaze models that do not generalize to other social gaze tasks or by considering social gaze inference as an ad-hoc post-processing of the gaze following task. Furthermore, the vast majority of gaze following approaches have proposed models that can handle only one person at a time and are static, therefore failing to take advantage of social interactions and temporal dynamics. In this paper, we address these limitations and introduce a novel framework to jointly predict the gaze target and social gaze label for all people in the scene. It comprises (i) a temporal, transformer-based architecture that, in addition to frame tokens, handles person-specific tokens capturing the gaze information related to each individual; (ii) a new dataset, VSGaze, built from multiple gaze following and social gaze datasets by extending and validating head detections and tracks, and unifying annotation types. We demonstrate that our model can address and benefit from training on all tasks jointly, achieving state-of-the-art results for multi-person gaze following and social gaze prediction. Our annotations and code will be made publicly available.',\n",
       "  500: 'Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition,  we probe the biases of agent trust and  differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount.',\n",
       "  501: \"As the development and application of Large Language Models (LLMs) continue to advance rapidly, enhancing their trustworthiness and aligning them with human preferences has become a critical area of research. Traditional methods rely heavily on extensive data for Reinforcement Learning from Human Feedback (RLHF), but representation engineering offers a new, training-free approach. This technique leverages semantic features to control the representation of LLM's intermediate hidden states, enabling the model to meet specific requirements such as increased honesty or heightened safety awareness. However, a significant challenge arises when attempting to fulfill multiple requirements simultaneously. It proves difficult to encode various semantic contents, like honesty and safety, into a singular semantic feature, restricting its practicality.In this work, we address this challenge through Sparse Activation Control. By delving into the intrinsic mechanisms of LLMs, we manage to identify and pinpoint modules that are closely related to specific tasks within the model, i.e. attention heads. These heads display sparse characteristics that allow for near-independent control over different tasks. Our experiments, conducted on the open-source Llama series models, have yielded encouraging results. The models were able to align with human preferences on issues of safety, factualness, and bias concurrently.\",\n",
       "  502: 'EEVR (Emotion Elicitation in Virtual Reality) is a novel dataset specifically designed for language supervision-based pre-training of emotion recognition tasks, such as valence and arousal classification. It features high-quality physiological signals, including electrodermal activity (EDA) and photoplethysmography (PPG), acquired through emotion elicitation via 360-degree virtual reality (VR) videos.Additionally, it includes subject-wise textual descriptions of emotions experienced during each stimulus gathered from qualitative interviews. The dataset consists of recordings from 37 participants and is the first dataset to pair raw text with physiological signals, providing additional contextual information that objective labels cannot offer. To leverage this dataset, we introduced the Contrastive Language Signal Pre-training (CLSP) method, which jointly learns representations using pairs of physiological signals and textual descriptions. Our results show that integrating self-reported textual descriptions with physiological signals significantly improves performance on emotion recognition tasks, such as arousal and valence classification. Moreover, our pre-trained CLSP model demonstrates strong zero-shot transferability to existing datasets, outperforming supervised baseline models, suggesting that the representations learned by our method are more contextualized and generalized. The dataset also includes baseline models for arousal, valence, and emotion classification, as well as code for data cleaning and feature extraction. Further details and access to the dataset are available at https://melangelabiiitd.github.io/EEVR/.',\n",
       "  503: \"Recent years have seen a significant progress in the general-purpose problem solving abilities of large vision and language models (LVLMs), such as ChatGPT, Gemini, etc.; some of these breakthroughs even seem to enable AI models to outperform human abilities in varied tasks that demand higher-order cognitive skills. Are the current large AI models indeed capable of generalized problem solving as humans do?  A systematic analysis of AI capabilities for joint vision and text reasoning, however, is missing in the current scientific literature. In this paper, we make an effort towards filling this gap, by evaluating state-of-the-art LVLMs on their mathematical and algorithmic reasoning abilities using visuo-linguistic problems from children's Olympiads. Specifically, we consider problems from the Mathematical Kangaroo (MK) Olympiad, which is a popular international competition targeted at children from grades 1-12, that tests children's deeper mathematical abilities using puzzles that are appropriately gauged to their age and skills. Using the puzzles from MK, we created a dataset, dubbed SMART-840, consisting of 840 problems from years 2020-2024. With our dataset, we analyze LVLMs power on mathematical reasoning; their responses on our puzzles offer a direct way to compare against that of children. Our results show that modern LVLMs do demonstrate increasingly powerful reasoning skills in solving problems for higher grades, but lack the foundations to correctly answer problems designed for younger children. Further analysis shows that there is no significant correlation between the reasoning capabilities of AI models and that of young children, and their capabilities appear to be based on a different type of reasoning than the cumulative knowledge that underlies children's mathematical skills.\",\n",
       "  504: 'We introduce Lifelong ICL, a problem setting that challenges long-context language models (LMs) to learn a sequence of language tasks through in-context learning (ICL). We further introduce Task Haystack, an evaluation suite dedicated to assessing and diagnosing how long-context LMs utilizes contexts in Lifelong ICL. When given a task instruction and test inputs, long-context LMs are expectedto leverage the relevant demonstrations in the Lifelong ICL prompt, avoid distraction and interference from other tasks, and achieve test accuracies that are not significantly worse than those of the Single-task ICL baseline.Task Haystack draws inspiration from the widely-adopted “needle-in-a-haystack” (NIAH) evaluation, but presents distinct new challenges. It requires models (1) to utilize the contexts at a deeper level, rather than resorting to simple copying and pasting; (2) to navigate through long streams of evolving topics and tasks, proxying the complexities and dynamism of contexts in real-world scenarios. Additionally, Task Haystack inherits the controllability of NIAH, providing model developers with tools and visualizations to identify model vulnerabilities effectively.We benchmark 14 long-context LMs using Task Haystack, finding that frontier models like GPT-4o still struggle with the setting, failing on 15% of cases on average. Most open-weight models further lack behind by a large margin, with failure rates reaching up to 61%. In our controlled analysis, we identify factors such as distraction and recency bias as contributors to these failure cases. Further, performance declines when task instructions are paraphrased at test time or when ICL demonstrations are repeated excessively, raising concerns about the robustness, instruction understanding, and true context utilization of long-context LMs. We release our code and data to encourage future research that investigates and addresses these limitations.',\n",
       "  505: 'Understanding how language model performance varies with scale is critical to benchmark and algorithm development. Scaling laws are one approach to building this understanding, but the requirement of training models across many different scales has limited their use. We propose an alternative, observational approach that bypasses model training and instead builds scaling laws from ~100 publically available models. Building a single scaling law from multiple model families is challenging due to large variations in their training compute efficiencies and capabilities. However, we show that these variations are consistent with a simple, generalized scaling law where language model performance is a function of a low-dimensional capability space, and model families only vary in their efficiency in converting training compute to capabilities. Using this approach, we show the surprising predictability of complex scaling phenomena: we show that several emergent phenomena follow a smooth, sigmoidal behavior and are predictable from small models; we show that the agent performance of models such as GPT-4 can be precisely predicted from simpler non-agentic benchmarks; and we show how to predict the impact of post-training interventions like Chain-of-Thought and Self-Consistency as language model capabilities continue to improve.',\n",
       "  506: \"We propose a novel approach for training large language models (LLMs) to adhere to objectives defined within a latent embedding space. Our method leverages reinforcement learning (RL), treating a pre-trained LLM as an environment. Our embedding-aligned guided language (EAGLE) agent is trained to iteratively steer the LLM's generation towards optimal regions of the latent embedding space, w.r.t. some predefined criterion. We demonstrate the effectiveness of the EAGLE agent using the MovieLens 25M and Amazon Review datasets to surface content gaps that satisfy latent user demand. We also demonstrate the benefit of using an optimal design of a state-dependent action set to improve EAGLE's efficiency. Our work paves the way for controlled and grounded text generation using LLMs, ensuring consistency with domain-specific knowledge and data representations.\",\n",
       "  507: \"Boosting is a highly successful ML-born optimization setting in which one is required to computationally efficiently learn arbitrarily good models based on the access to a weak learner oracle, providing classifiers performing at least slightly differently from random guessing. A key difference with gradient-based optimization is that boosting's original model does not requires access to first order information about a loss, yet the decades long history of boosting has quickly evolved it into a first order optimization setting -- sometimes even wrongfully *defining* it as such. Owing to recent progress extending gradient-based optimization to use only a loss' zeroth ($0^{th}$) order information to learn, this begs the question: what loss functions be efficiently optimized with boosting and what is the information really needed for boosting to meet the *original* boosting blueprint's requirements ?We provide a constructive formal answer essentially showing that *any* loss function can be optimized with boosting and thus boosting can achieve a feat not yet known to be possible in the classical $0^{th}$ order setting, since loss functions are not required to be be convex, nor differentiable or Lipschitz -- and in fact not required to be continuous either. Some tools we use are rooted in quantum calculus, the mathematical field -- not to be confounded with quantum computation -- that studies calculus without passing to the limit, and thus without using first order information.\",\n",
       "  508: \"Auto-labeling is an important family of techniques that produce labeled training sets with minimum manual annotation. A prominent variant, threshold-based auto-labeling (TBAL), works by finding thresholds on a model's confidence scores above which it can accurately automatically label unlabeled data. However, many models are known to produce overconfident scores, leading to poor TBAL  performance. While a natural idea is to apply off-the-shelf calibration methods to alleviate the overconfidence issue, we show that such methods fall short. Rather than experimenting with ad-hoc choices of confidence functions, we propose a framework for studying the optimal TBAL confidence function. We develop a tractable version of the framework to obtain Colander (Confidence functions for Efficient and Reliable Auto-labeling), a new post-hoc method specifically designed to maximize performance in TBAL systems. We perform an extensive empirical evaluation of Colander and compare it against methods designed for calibration. Colander achieves up to 60% improvement on coverage over the baselines while maintaining error level below 5% and using the same amount of labeled data.\",\n",
       "  509: 'This paper studies the challenging task of makeup transfer, which aims to apply diverse makeup styles precisely and naturally to a given facial image.  Due to the absence of paired data, current methods typically synthesize sub-optimal pseudo ground truths to guide the model training, resulting in low makeup fidelity. Additionally, different makeup styles generally have varying effects on the person face, but existing methods struggle to deal with this diversity. To address these issues, we propose a novel Self-supervised Hierarchical Makeup Transfer (SHMT) method via latent diffusion models. Following a \"decoupling-and-reconstruction\" paradigm, SHMT works in a self-supervised manner, freeing itself from the misguidance of imprecise pseudo-paired data. Furthermore, to accommodate a variety of makeup styles, hierarchical texture details are decomposed via a Laplacian pyramid and selectively introduced to the content representation. Finally, we design a novel Iterative Dual Alignment (IDA) module that dynamically adjusts the injection condition of the diffusion model, allowing the alignment errors caused by the domain gap between content and makeup representations to be corrected. Extensive quantitative and qualitative analyses demonstrate the effectiveness of our method. Our code is available at https://github.com/Snowfallingplum/SHMT.',\n",
       "  510: 'Link prediction, which aims to forecast unseen connections in graphs, is a fundamental task in graph machine learning. Heuristic methods, leveraging a range of different pairwise measures such as common neighbors and shortest paths, often rival the performance of vanilla Graph Neural Networks (GNNs). Therefore, recent advancements in GNNs for link prediction (GNN4LP) have primarily focused on integrating one or a few types of pairwise information. In this work,  we reveal that different node pairs within the same dataset necessitate varied pairwise information for accurate prediction and models that only apply the same pairwise information uniformly could achieve suboptimal performance.As a result, we propose a simple mixture of experts model Link-MoE for link prediction.  Link-MoE utilizes various GNNs as experts and  strategically selects the appropriate expert for each node pair based on various types of pairwise information. Experimental results across diverse real-world datasets demonstrate substantial performance improvement from Link-MoE. Notably,  Link-Mo achieves a relative improvement of 18.71% on the MRR metric for the Pubmed dataset and 9.59% on the Hits@100 metric for the ogbl-ppa dataset, compared to the best baselines. The code is available at https://github.com/ml-ml/Link-MoE/.',\n",
       "  511: \"What data or environments to use for training to improve downstream performance is a longstanding and very topical question in reinforcement learning. In particular, Unsupervised Environment Design (UED) methods have gained recent attention as their adaptive curricula promise to enable agents to be robust to in- and out-of-distribution tasks.This work investigates how existing UED methods select training environments, focusing on task prioritisation metrics.Surprisingly, despite methods aiming to maximise regret in theory, the practical approximations do not correlate with regret but with success rate.As a result, a significant portion of an agent's experience comes from environments it has already mastered, offering little to no contribution toward enhancing its abilities. Put differently, current methods fail to predict intuitive measures of learnability. Specifically, they are unable to consistently identify those scenarios that the agent can sometimes solve, but not always.Based on our analysis, we develop a method that directly trains on scenarios with high learnability. This simple and intuitive approach outperforms existing UED methods in several binary-outcome environments, including the standard domain of Minigrid and a novel setting closely inspired by a real-world robotics problem. We further introduce a new adversarial evaluation procedure for directly measuring robustness, closely mirroring the conditional value at risk (CVaR).We open-source all our code and present visualisations of final policies here: https://github.com/amacrutherford/sampling-for-learnability.\",\n",
       "  512: 'Many practical problems involve estimating low dimensional statistical quantities with high-dimensional models and datasets. Several approaches address these estimation tasks based on the theory of influence functions, such as debiased/double ML or targeted minimum loss estimation. We introduce \\\\textit{Monte Carlo Efficient Influence Functions} (MC-EIF), a fully automated technique for approximating efficient influence functions that integrates seamlessly with existing differentiable probabilistic programming systems. MC-EIF automates efficient statistical estimation for a broad class of models and functionals that previously required rigorous custom analysis. We prove that MC-EIF is consistent, and that estimators using MC-EIF achieve optimal $\\\\sqrt{N}$ convergence rates. We show empirically that estimators using MC-EIF are at parity with estimators using analytic EIFs. Finally, we present a novel capstone example using MC-EIF for optimal portfolio selection.',\n",
       "  513: 'Anomaly detection is widely used for identifying critical errors and suspicious behaviors, but current methods lack interpretability.We leverage common properties of existing methods and recent advances in generative models to introduce counterfactual explanations for anomaly detection.Given an input, we generate its counterfactual as a diffusion-based repair that shows what a non-anomalous version $\\\\textit{should have looked like}$.A key advantage of this approach is that it enables a domain-independent formal specification of explainability desiderata, offering a unified framework for generating and evaluating explanations.We demonstrate the effectiveness of our anomaly explainability framework, AR-Pro, on vision (MVTec, VisA) and time-series (SWaT, WADI, HAI) anomaly datasets. The code used for the experiments is accessible at: https://github.com/xjiae/arpro.',\n",
       "  514: 'Adversarial training has emerged as a popular approach for training models that are robust to inference-time adversarial attacks. However, our theoretical understanding of why and when it works remains limited. Prior work has offered generalization analysis of adversarial training, but they are either restricted to the Neural Tangent Kernel (NTK) regime or they make restrictive assumptions about data such as (noisy) linear separability or robust realizability. In this work, we study the stability and generalization of adversarial training for two-layer networks without any data distribution assumptions and beyond the NTK regime. Our findings suggest that for networks with any given initialization and sufficiently large width, the generalization bound can be effectively controlled via early stopping. We further improve the generalization bound by leveraging smoothing using Moreau’s envelope.',\n",
       "  515: 'Large language models (LLMs) have the potential to generate texts that pose risks of misuse, such as plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Consequently, detecting whether a text is generated by LLMs has become increasingly important. Existing high-quality detection methods usually require access to the interior of the model to extract the intrinsic characteristics. However, since we do not have access to the interior of the black-box model, we must resort to surrogate models, which impacts detection quality. In order to achieve high-quality detection of black-box models, we would like to extract deep intrinsic characteristics of the black-box model generated texts. We view the generation process as a coupled process of prompt and intrinsic characteristics of the generative model. Based on this insight, we propose to decouple prompt and intrinsic characteristics (DPIC) for LLM-generated text detection method. Specifically, given a candidate text, DPIC employs an auxiliary LLM to reconstruct the prompt corresponding to the candidate text, then uses the prompt to regenerate text by the auxiliary LLM, which makes the candidate text and the regenerated text align with their prompts, respectively. Then, the similarity between the candidate text and the regenerated text is used as a detection feature, thus eliminating the prompt in the detection process, which allows the detector to focus on the intrinsic characteristics of the generative model. Compared to the baselines, DPIC has achieved an average improvement of 6.76\\\\% and 2.91\\\\% in detecting texts from different domains generated by GPT4 and Claude3, respectively.',\n",
       "  516: \"Multi-agent AI research promises a path to develop human-like and human-compatible intelligent technologies that complement the solipsistic view of other approaches, which mostly do not consider interactions between agents. Aiming to make progress in this direction, the Melting Pot contest 2023 focused on the problem of cooperation among interacting agents and challenged researchers to push the boundaries of multi-agent reinforcement learning (MARL) for mixed-motive games. The contest leveraged the Melting Pot environment suite to rigorously evaluate how well agents can adapt their cooperative skills to interact with novel partners in unforeseen situations. Unlike other reinforcement learning challenges, this challenge focused on social rather than environmental generalization. In particular, a population of agents performs well in Melting Pot when its component individuals are adept at finding ways to cooperate both with others in their population and with strangers. Thus Melting Pot measures cooperative intelligence.The contest attracted over 600 participants across 100+ teams globally and was a success on multiple fronts: (i) it contributed to our goal of pushing the frontiers of MARL towards building more cooperatively intelligent agents, evidenced by several submissions that outperformed established baselines; (ii) it attracted a diverse range of participants, from independent researchers to industry affiliates and academic labs, both with strong background and new interest in the area alike, broadening the field’s demographic and intellectual diversity; and (iii) analyzing the submitted agents provided important insights, highlighting areas for improvement in evaluating agents' cooperative intelligence. This paper summarizes the design aspects and results of the contest and explores the potential of Melting Pot as a benchmark for studying Cooperative AI. We further analyze the top solutions and conclude with a discussion on promising directions for future research.\",\n",
       "  517: 'Research on video generation has recently made tremendous progress, enabling high-quality videos to be generated from text prompts or images. Adding control to the video generation process is an important goal moving forward and recent approaches that condition video generation models on camera trajectories take an important step towards this goal. Yet, it remains challenging to generate a video of the same scene from multiple different camera trajectories. Solutions to this multi-video generation problem could enable large-scale 3D scene generation with editable camera trajectories, among other applications. We introduce collaborative video diffusion (CVD) as an important step towards this vision. The CVD framework includes a novel cross-video synchronization module that promotes consistency between corresponding frames of the same video rendered from different camera poses using an epipolar attention mechanism. Trained on top of a state-of-the-art camera-control module for video generation, CVD generates multiple videos rendered from different camera trajectories with significantly better consistency than baselines, as shown in extensive experiments.',\n",
       "  518: 'People who are blind perceive the world differently than those who are sighted, which can result in distinct motion characteristics. For instance, when crossing at an intersection, blind individuals may have different patterns of movement, such as veering more from a straight path or using touch-based exploration around curbs and obstacles. These behaviors may appear less predictable to motion models embedded in technologies such as autonomous vehicles. Yet, the ability of 3D motion models to capture such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable systems that can seamlessly reason over diverse human movements in their environments, our text-and-motion benchmark is available at https://blindways.github.io/.',\n",
       "  519: 'We show that L2-accurate score estimation, in the absence of strong assumptions on the data distribution, is computationally hard even when sample complexity is polynomial in the relevant problem parameters. Our reduction builds on the result of Chen et al. (ICLR 2023), who showed that the problem of generating samples from an unknown data distribution reduces to L2-accurate score estimation. Our hard-to-estimate distributions are the \"Gaussian pancakes\" distributions, originally due to Diakonikolas et al. (FOCS 2017), which have been shown to be computationally indistinguishable from the standard Gaussian under widely believed hardness assumptions from lattice-based cryptography (Bruna et al., STOC 2021; Gupte et al., FOCS 2022).',\n",
       "  520: \"The secretary problem is one of the fundamental problems in online decision making; a tight competitive ratio for this problem of $1/e \\\\approx 0.368$ has been known since the 1960s. Much more recently, the study of algorithms with predictions was introduced: The algorithm is equipped with a (possibly erroneous) additional piece of information upfront which can be used to improve the algorithm's performance. Complementing previous work on secretary problems with prior knowledge, we tackle the following question: _What is the weakest piece of information that allows us to break the $1/e$ barrier?_To this end, we introduce the secretary problem with predicted additive gap. As in the classical problem, weights are fixed by an adversary and elements appear in random order. In contrast to previous variants of predictions, our algorithm only has access to a much weaker piece of information: an _additive gap_ $c$. This gap is the difference between the highest and $k$-th highest weight in the sequence.Unlike previous pieces of advice, knowing an exact additive gap does not make the problem trivial. Our contribution is twofold. First, we show that for any index $k$ and any gap $c$, we can obtain a competitive ratio of $0.4$ when knowing the exact gap (even if we do not know $k$), hence beating the prevalent bound for the classical problem by a constant. Second, a slightly modified version of our algorithm allows to prove standard robustness-consistency properties as well as improved guarantees when knowing a range for the error of the prediction.\",\n",
       "  521: 'As large language models gain widespread adoption, running them efficiently becomes a crucial task. Recent works on LLM inference use speculative decoding to achieve extreme speedups. However, most of these works implicitly design their algorithms for high-end datacenter hardware. In this work, we ask the opposite question: how fast can we run LLMs on consumer machines? Consumer GPUs can no longer fit the largest available models and must offload them to RAM or SSD. With parameter offloading, hundreds or thousands of tokens can be processed in batches within the same time as just one token, making it a natural fit for speculative decoding. We propose SpecExec (Speculative Execution), a simple parallel decoding method that can generate up to 20 tokens per target model iteration for popular LLM families.  SpecExec takes the most probable continuations from the draft model to build a \"cache\" tree for the target model, which then gets validated in a single pass. Using SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with RAM offloading at 4--6 tokens per second with 4-bit quantization or 2--3 tokens per second with 16-bit weights. Our code is available at https://github.com/yandex-research/specexec .',\n",
       "  522: \"Large language models (LLMs) like transformers demonstrate impressive in-context learning (ICL) capabilities, allowing them to makepredictions for new tasks based on prompt exemplars without parameter updates. While existing ICL theories often assume structured training data resembling ICL tasks (e.g., x-y pairs for linear regression), LLMs are typically trained unsupervised on unstructured text, such as web content, which lacks clear parallels to tasks like word analogy. To address this gap, we examine what enables ICL in models trained on unstructured data, focusing on critical sequence model requirements and training data structure. We find that many ICL capabilities canemerge simply from co-occurrence of semantically related word pairs in unstructured data; word analogy completion, for example, can provably arise purely through co-occurrence modeling, using classical language models like continuous bag of words (CBOW), without needing positional information or attention mechanisms. However, positional information becomes crucial for logic reasoning tasks requiring generalization to unseen tokens. Finally, we identify two cases where ICL fails: one in logic reasoning tasks that require generalizing to new, unseen patterns, and another in analogy completion where relevant word pairs appear only in fixed training positions. These findings suggest that LLMs' ICL abilities depend heavily on the structural elements within their training data.\",\n",
       "  523: 'In ImageNet-condensation, the storage for auxiliary soft labels exceeds that of the condensed dataset by over 30 times.However, are large-scale soft labels necessary for large-scale dataset distillation?In this paper, we first discover that the high within-class similarity in condensed datasets necessitates the use of large-scale soft labels.This high within-class similarity can be attributed to the fact that previous methods use samples from different classes to construct a single batch for batch normalization (BN) matching.To reduce the within-class similarity, we introduce class-wise supervision during the image synthesizing process by batching the samples within classes, instead of across classes.As a result, we can increase within-class diversity and reduce the size of required soft labels.A key benefit of improved image diversity is that soft label compression can be achieved through simple random pruning, eliminating the need for complex rule-based strategies. Experiments validate our discoveries.For example, when condensing ImageNet-1K to 200 images per class, our approach compresses the required soft labels from 113 GB to 2.8 GB (40$\\\\times$ compression) with a 2.6\\\\% performance gain.Code is available at: https://github.com/he-y/soft-label-pruning-for-dataset-distillation',\n",
       "  524: 'Uncertainty Quantification (UQ) is vital for decision makers as it offers insights into the potential reliability of data and model, enabling more informed and risk-aware decision-making. Graphical models, capable of representing data with complex dependencies, are widely used across domains.Existing sampling-based UQ methods are unbiased but cannot guarantee convergence and are time-consuming on large-scale graphs. There are fast UQ methods for graphical models with closed-form solutions and convergence guarantee but with uncertainty underestimation.We propose LinUProp, a UQ method that utilizes a novel linear propagation of uncertainty to model uncertainty among related nodes additively instead of multiplicatively, to offer linear scalability, guaranteed convergence, and closed-form solutions without underestimating uncertainty.Theoretically, we decompose the expected prediction error of the graphical model and prove that the uncertainty computed by LinUProp is the generalized variance component of the decomposition.Experimentally, we demonstrate that LinUProp is consistent with the sampling-based method but with linear scalability and fast convergence.Moreover, LinUProp outperforms competitors in uncertainty-based active learning on four real-world graph datasets, achieving higher accuracy with a lower labeling budget.',\n",
       "  525: 'We consider the solvable neural scaling model with three parameters: data complexity, target complexity, and model-parameter-count. We use this neural scaling model to derive new predictions about the compute-limited, infinite-data scaling law regime.  To train the neural scaling model, we run one-pass stochastic gradient descent on a mean-squared loss.  We derive a representation of the loss curves which holds over all iteration counts and improves in accuracy as the model parameter count grows.  We then analyze the compute-optimal model-parameter-count, and identify 4 phases (+3 subphases) in the data-complexity/target-complexity phase-plane.  The phase boundaries are determined by the relative importance of model capacity, optimizer noise, and embedding of the features. We furthermore derive, with mathematical proof and extensive numerical evidence, the scaling-law exponents in all of these phases, in particular computing the optimal model-parameter-count as a function of floating point operation budget. We include a colab notebook https://tinyurl.com/2saj6bkj, nanoChinchilla, that reproduces some key results of the paper.',\n",
       "  526: 'The ability to conduct interventions plays a pivotal role in learning causal relationships among variables, thus facilitating applications across diverse scientific disciplines such as genomics, economics, and machine learning. However, in many instances within these applications, the process of generating interventional data is subject to noise: rather than data being sampled directly from the intended interventional distribution, interventions often yield data sampled from a blend of both intended and unintended interventional distributions.We consider the fundamental challenge of disentangling mixed interventional and observational data within linear Structural Equation Models (SEMs) with Gaussian additive noise without the knowledge of the true causal graph. We demonstrate that conducting interventions, whether do or soft, yields distributions with sufficient diversity and properties conducive to efficiently recovering each component within the mixture. Furthermore, we establish that the sample complexity required to disentangle mixed data inversely correlates with the extent of change induced by an intervention in the equations governing the affected variable values. As a result, the causal graph can be identified up to its interventional Markov Equivalence Class, similar to scenarios where no noise influences the generation of interventional data. We further support our theoretical findings by conducting simulations wherein we perform causal discovery from such mixed data.',\n",
       "  527: 'Large language models (LLMs) have shown impressive performance on downstream tasks by in-context learning (ICL), which heavily relies on the quality of demonstrations selected from a large set of annotated examples. Recent works claim that in-context learning is robust to noisy demonstrations in text classification. In this work, we show that, on text generation tasks, noisy annotations significantly hurt the performance of in-context learning. To circumvent the issue, we propose a simple and effective approach called Local Perplexity Ranking (LPR), which replaces the \"noisy\" candidates with their nearest neighbors that are more likely to be clean. Our method is motivated by analyzing the perplexity deviation caused by noisy labels and decomposing perplexity into inherent perplexity and matching perplexity. Our key idea behind LPR is thus to decouple the matching perplexity by performing the ranking among the neighbors in semantic space. Our approach can prevent the selected demonstrations from including mismatched input-label pairs while preserving the effectiveness of the original selection methods. Extensive experiments demonstrate the effectiveness of LPR, improving the EM score by up to 18.75 on common benchmarks with noisy annotations.',\n",
       "  528: 'Amidst the recent strides in evaluating Large Language Models for Code (Code LLMs), existing benchmarks have mainly focused on the functional correctness of generated code, neglecting the importance of their computational efficiency. To fill the gap, we present Mercury, the first code efficiency benchmark for Code LLMs. It comprises 1,889 Python tasks, each accompanied by adequate solutions that serve as real-world efficiency baselines, enabling a comprehensive analysis of the runtime distribution. Based on the distribution, we introduce a new metric Beyond, which computes a runtime-percentile-weighted Pass score to reflect functional correctness and code efficiency simultaneously. On Mercury, leading Code LLMs can achieve 65% on Pass, while less than 50% on Beyond. Given that an ideal Beyond score would be aligned with the Pass score, it indicates that while Code LLMs exhibit impressive capabilities in generating functionally correct code, there remains a notable gap in their efficiency. Finally, our empirical experiments reveal that Direct Preference Optimization (DPO) serves as a robust baseline for enhancing code efficiency compared with Supervised Fine Tuning (SFT), which paves a promising avenue for future exploration of efficient code generation. Our code and data are available on GitHub: https://github.com/Elfsong/Mercury.',\n",
       "  529: 'The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, the lack of such high-quality long videos impedes the advancement long video generation. To promote research in long video generation, we desire a new dataset with four key features essential for training long video generation models: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse contents, and (4) temporally dense captions. To achieve this, we introduce a new pipeline for filtering high-quality long-take videos and generating temporally dense captions. Specifically, we define a set of metrics to quantitatively assess video quality including scene cuts, dynamic degrees, and semantic-level scores, enabling us to filter high-quality long-take videos from a large amount of source videos. Subsequently, we develop a hierarchical video captioning pipeline to annotate long videos with temporally-dense captions. With this pipeline, we curate the first long-take video dataset, LVD-2M, comprising 2 million long-take videos, each covering more than 10 seconds and annotated with temporally dense captions. We further validate the effectiveness of LVD-2M by fine-tuning video generation models to generate long videos with dynamic motions. We believe it will significantly contribute to future research in long video generation.',\n",
       "  530: 'We consider learning a sparse model from linear measurements taken by a network of agents. Different from existing decentralized methods designed based on the LASSO regression with explicit  $\\\\ell_1$ norm regularization, we exploit the implicit regularization of decentralized optimization method applied to an over-parameterized nonconvex least squares formulation without penalization. Our first result shows that despite nonconvexity, if the network connectivity is good, the well-known decentralized gradient descent algorithm (DGD) with small initialization and early stopping can compute the statistically optimal solution. Sufficient conditions on the initialization scale,  choice of step size, network connectivity, and stopping time are further provided to achieve convergence. Our result recovers the convergence rate of gradient descent in the centralized setting, showing its tightness. Based on the analysis of DGD, we further propose a communication-efficient version, termed T-DGD, by truncating the iterates before transmission. In the high signal-to-noise ratio (SNR) regime, we show that T-DGD achieves comparable statistical accuracy to DGD, while the communication cost is logarithmic in the number of parameters. Numerical results are provided to validate the effectiveness of DGD and T-DGD for sparse learning through implicit regularization.',\n",
       "  531: 'Inference on large language models (LLMs) can be expensive in terms of thecompute and memory costs involved, especially when long sequence lengths areused. In particular, the self-attention mechanism used in LLM inference contributessignificantly to these costs, which has sparked an interest in approximating the self-attention computation to reduce such costs. In this work, we propose to approximateself-attention by focusing on the dimensionality of key vectors computed in theattention block. Our analysis reveals that key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting thisobservation, we propose Loki, a novel sparse attention method that ranks and selectstokens in the KV-cache based on attention scores computed in low-dimensionalspace. Our evaluations show that Loki is able to speed up the attention computationdue to reduced data movement (load/store) and compute costs while maintainingthe efficacy of the models better than other popular approximation methods.',\n",
       "  532: 'As the adoption of large language models increases and the need for per-user or per-task model customization grows, the parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur substantial storage and transmission costs. To further reduce stored parameters, we introduce a \"divide-and-share\" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules, and layers by sharing  parameters globally via a vector bank. As an instantiation of the paradigm to LoRA, our proposed VB-LoRA composites all the low-rank matrices of LoRA from a shared vector bank with a differentiable top-$k$ admixture module. VB-LoRA achieves extreme parameter efficiency while maintaining comparable or better performance compared to state-of-the-art PEFT methods. Extensive experiments demonstrate the effectiveness of VB-LoRA on natural language understanding, natural language generation, instruction tuning, and mathematical reasoning tasks. When fine-tuning the Llama2-13B model, VB-LoRA only uses 0.4% of LoRA\\'s stored parameters, yet achieves superior results. Our source code is available at https://github.com/leo-yangli/VB-LoRA. This method has been merged into the Hugging Face PEFT package.',\n",
       "  533: 'The cost of ranking becomes significant in the new stage of deep learning. We propose ST$_k$, a fully differentiable module with a single trainable parameter, designed to solve the Top-k problem without requiring additional time or GPU memory. Due to its fully differentiable nature, ST$_k$ can be embedded end-to-end into neural networks and optimize the Top-k problems within a unified computational graph. We apply ST$_k$ to the Average Top-k Loss (AT$_k$), which inherently faces a Top-k problem. The proposed ST$_k$ Loss outperforms AT$_k$ Loss and achieves the best average performance on multiple benchmarks, with the lowest standard deviation. With the assistance of ST$_k$ Loss, we surpass the state-of-the-art (SOTA) on both CIFAR-100-LT and Places-LT leaderboards.',\n",
       "  534: 'We study a class of optimization problems in the Wasserstein space (the space of probability measures) where the objective function is nonconvex along generalized geodesics. Specifically, the objective exhibits some difference-of-convex structure along these geodesics. The setting also encompasses sampling problems where the logarithm of the target distribution is difference-of-convex. We derive multiple convergence insights for a novel semi Forward-Backward Euler scheme under several nonconvex (and possibly nonsmooth) regimes. Notably, the semi Forward-Backward Euler is just a slight modification of the Forward-Backward Euler whose convergence is---to our knowledge---still unknown in our very general non-geodesically-convex setting.',\n",
       "  535: \"In this paper, we present the first explicit and non-asymptotic global convergence rates of the BFGS method when implemented with an inexact line search scheme satisfying the Armijo-Wolfe conditions. We show that BFGS achieves a global linear convergence rate of $(1 - \\\\frac{1}{\\\\kappa})^t$ for $\\\\mu$-strongly convex functions with $L$-Lipschitz gradients, where $\\\\kappa = \\\\frac{L}{\\\\mu}$ represents the condition number. Additionally, if the objective function's Hessian is Lipschitz, BFGS with the Armijo-Wolfe line search achieves a linear convergence rate that depends solely on the line search parameters, independent of the condition number. We also establish a global superlinear convergence rate of $\\\\mathcal{O}((\\\\frac{1}{t})^t)$. These global bounds are all valid for any starting point $x_0$ and any symmetric positive definite initial Hessian approximation matrix $B_0$, though the choice of $B_0$ impacts the number of iterations needed to achieve these rates. By synthesizing these results, we outline the first global complexity characterization of BFGS with the Armijo-Wolfe line search. Additionally, we clearly define a mechanism for selecting the step size to satisfy the Armijo-Wolfe conditions and characterize its overall complexity.\",\n",
       "  536: 'Interactive preference learning systems infer human preferences by presenting queries as pairs of options and collecting binary choices. Although binary choices are simple and widely used, they provide limited information about preference strength. To address this, we leverage human response times, which are inversely related to preference strength, as an additional signal. We propose a computationally efficient method that combines choices and response times to estimate human utility functions, grounded in the EZ diffusion model from psychology. Theoretical and empirical analyses show that for queries with strong preferences, response times complement choices by providing extra information about preference strength, leading to significantly improved utility estimation. We incorporate this estimator into preference-based linear bandits for fixed-budget best-arm identification. Simulations on three real-world datasets demonstrate that using response times significantly accelerates preference learning compared to choice-only approaches. Additional materials, such as code, slides, and talk video, are available at https://shenlirobot.github.io/pages/NeurIPS24.html.',\n",
       "  537: 'Zero-shot reinforcement learning (RL) promises to provide agents that can perform any task in an environment after an offline, reward-free pre-training phase. Methods leveraging successor measures and successor features have shown strong performance in this setting, but require access to large heterogenous datasets for pre-training which cannot be expected for most real problems. Here, we explore how the performance of zero-shot RL methods degrades when trained on small homogeneous datasets, and propose fixes inspired by conservatism, a well-established feature of performant single-task offline RL algorithms. We evaluate our proposals across various datasets, domains and tasks, and show that conservative zero-shot RL algorithms outperform their non-conservative counterparts on low quality datasets, and perform no worse on high quality datasets. Somewhat surprisingly, our proposals also outperform baselines that get to see the task during training. Our code is available via the project page https://enjeeneer.io/projects/zero-shot-rl/.',\n",
       "  538: 'Distributed learning is essential for training large-scale deep models.Asynchronous SGD (ASGD) and its variants are commonly used distributed learning methods, particularly in scenarios where the computing capabilities of workers in the cluster are heterogeneous.Momentum has been acknowledged for its benefits in both optimization and generalization in deep model training. However, existing works have found that naively incorporating momentum into ASGD can impede the convergence.In this paper, we propose a novel method called ordered momentum (OrMo) for ASGD. In OrMo, momentum is incorporated into ASGD by organizing the gradients in order based on their iteration indexes. We theoretically prove the convergence of OrMo with both constant and delay-adaptive learning rates for non-convex problems. To the best of our knowledge, this is the first work to establish the convergence analysis of ASGD with momentum without dependence on the maximum delay. Empirical results demonstrate that OrMo can achieve better convergence performance compared with ASGD and other asynchronous methods with momentum.',\n",
       "  539: 'The need for large text corpora has increased with the advent of pretrained language models and, in particular, the discovery of scaling laws for these models. Most available corpora have sufficient data only for languages with large dominant communities. However, there is no corpus available that (i) covers a wide range of minority languages; (ii) is generated by an open-source reproducible pipeline; and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, a clean, document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages. We make GlotCC and the system used to generate it— including the pipeline, language identification model, and filters—available to the research community.Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1Pipeline v. 3.0 https://github.com/cisnlp/GlotCC',\n",
       "  540: 'Recurrent Neural Networks (RNNs) are used to learn representations in partially observable environments. For agents that learn online and continually interact with the environment, it is desirable to train RNNs with real-time recurrent learning (RTRL); unfortunately, RTRL is prohibitively expensive for standard RNNs. A promising direction is to use linear recurrent architectures (LRUs), where dense recurrent weights are replaced with a complex-valued diagonal, making RTRL efficient. In this work, we build on these insights to provide a lightweight but effective approach for training RNNs in online RL. We introduce Recurrent Trace Units (RTUs), a small modification on LRUs that we nonetheless find to have significant performance benefits over LRUs when trained with RTRL. We find RTUs significantly outperform GRUs and Transformers across several partially observable environments while using significantly less computation.',\n",
       "  541: \"Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term $\\\\textbf{natural adversarial samples}$. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, ${\\\\bf NaturalBench}$, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a $\\\\textbf{vision-centric}$ design by pairing each question with two images that yield different answers, preventing ``blind'' solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can largely be solved with language priors like commonsense knowledge. We evaluate ${\\\\bf 53}$ state-of-the-art VLMs on NaturalBench, showing that models like BLIP-3, LLaVA-OneVision, Cambrian-1, InternLM-XC2, Llama3.2-Vision, Molmo, Qwen2-VL, and even the (closed-source) GPT-4o lag 50%-70% behind human performance (which is above 90%). We analyze why NaturalBench is hard from two angles: (1) ${\\\\bf Compositionality:}$ Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) ${\\\\bf Biases: }$ NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. We show that debiasing can be crucial for VLM performance. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.\",\n",
       "  542: 'Evaluating policies using off-policy data is crucial for applying reinforcement learning to real-world problems such as healthcare and autonomous driving. Previous methods for off-policy evaluation (OPE) generally suffer from high variance or irreducible bias, leading to unacceptably high prediction errors. In this work, we introduce STAR, a framework for OPE that encompasses a broad range of estimators -- which include existing OPE methods as special cases -- that achieve lower mean squared prediction errors. STAR leverages state abstraction to distill complex, potentially continuous problems into compact, discrete models which we call abstract reward processes (ARPs). Predictions from ARPs estimated from off-policy data are provably consistent (asymptotically correct). Rather than proposing a specific estimator, we present a new framework for OPE and empirically demonstrate that estimators within STAR outperform existing methods. The best STAR estimator outperforms baselines in all twelve cases studied, and even the median STAR estimator surpasses the baselines in seven out of the twelve cases.',\n",
       "  543: 'Large language models (LLMs) have achieved impressive performance across various natural language benchmarks, prompting a continual need to curate more difficult datasets for larger LLMs, which is costly and time-consuming. In this paper, we propose to automate dataset updating and provide systematical analysis regarding its effectiveness in dealing with benchmark leakage issue, difficulty control, and stability. Thus, once current benchmark has been mastered or leaked, we can update it for timely and reliable evaluation. There are two updating strategies: 1) mimicking strategy to generate similar samples based on original data, preserving stylistic and contextual essence, and 2) extending strategy that further expands existing samples at varying cognitive levels by adapting Bloom’s taxonomy of educational objectives. Extensive experiments on updated MMLU and BIG-Bench demonstrate the stability of the proposed strategies and find that the mimicking strategy can effectively alleviate issues of overestimation from benchmark leakage. In cases where the efficient mimicking strategy fails, our extending strategy still shows promising results. Additionally, by controlling the difficulty, we can better discern the models’ performance and enable fine-grained analysis — neither too difficult nor too easy an exam can fairly judge students’ learning status. To the best of our knowledge, we are the first to automate updating benchmarks for reliable and timely evaluation. Our demo leaderboard can be found at https://yingjiahao14.github.io/Automating-DatasetUpdates/.',\n",
       "  544: 'The Sharpe ratio is an important and widely-used risk-adjusted return in financial engineering. In modern portfolio management, one may require an m-sparse (no more than m active assets) portfolio to save managerial and financial costs. However, few existing methods can optimize the Sharpe ratio with the m-sparse constraint, due to the nonconvexity and the complexity of this constraint. We propose to convert the m-sparse fractional optimization problem into an equivalent m-sparse quadratic programming problem. The semi-algebraic property of the resulting objective function allows us to exploit the Kurdyka-Lojasiewicz property to develop an efficient Proximal Gradient Algorithm (PGA) that leads to a portfolio which achieves the globally optimal m-sparse Sharpe ratio under certain conditions. The convergence rates of PGA are also provided. To the best of our knowledge, this is the first proposal that achieves a globally optimal m-sparse Sharpe ratio with a theoretically-sound guarantee.',\n",
       "  545: 'To mitigate the risk of harmful outputs from large vision models (LVMs), we introduce the SafeSora dataset to promote research on aligning text-to-video generation with human values. This dataset encompasses human preferences in text-to-video generation tasks along two primary dimensions: helpfulness and harmlessness. To capture in-depth human preferences and facilitate structured reasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and harmlessness into 12 sub-categories, serving as the basis for pilot annotations. The SafeSora dataset includes 14,711 unique prompts, 57,333 unique videos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations labeled by humans. We further demonstrate the utility of the SafeSora dataset through several applications, including training the text-video moderation model and aligning LVMs with human preference by fine-tuning a prompt augmentation module or the diffusion model. These applications highlight its potential as the foundation for text-to-video alignment research, such as human preference modeling and the development and validation of alignment algorithms. Our project is available at https://sites.google.com/view/safe-sora.Warning: this paper contains example data that may be offensive or harmful.',\n",
       "  546: \"Fully connected Graph Transformers (GT) have rapidly become prominent in the static graph community as an alternative to Message-Passing models, which suffer from a lack of expressivity, oversquashing, and under-reaching.However, in a dynamic context, by interconnecting all nodes at multiple snapshots with self-attention,GT loose both structural and temporal information. In this work, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs (SLATE), a new spatio-temporal encoding to leverage the GT architecture while keeping spatio-temporal information.Specifically, we transform Discrete Time Dynamic Graphs into multi-layer graphs and take advantage of the spectral properties of their associated supra-Laplacian matrix.Our second contribution explicitly model nodes' pairwise relationships with a cross-attention mechanism, providing an accurate edge representation for dynamic link prediction.SLATE outperforms numerous state-of-the-art methods based on Message-Passing Graph Neural Networks combined with recurrent models (e.g, LSTM), and Dynamic Graph Transformers,on~9 datasets. Code is open-source and available at this link https://github.com/ykrmm/SLATE.\",\n",
       "  547: 'Safe reinforcement learning (RL) is crucial for deploying RL agents in real-world applications, as it aims to maximize long-term rewards while satisfying safety constraints. However, safe RL often suffers from sample inefficiency, requiring extensive interactions with the environment to learn a safe policy. We propose Efficient Safe Policy Optimization (ESPO), a novel approach that enhances the efficiency of safe RL through sample manipulation. ESPO employs an optimization framework with three modes: maximizing rewards, minimizing costs, and balancing the trade-off between the two. By dynamically adjusting the sampling process based on the observed conflict between reward and safety gradients, ESPO theoretically guarantees convergence, optimization stability, and improved sample complexity bounds. Experiments on the Safety-MuJoCo and Omnisafe benchmarks demonstrate that ESPO significantly outperforms existing primal-based and primal-dual-based baselines in terms of reward maximization and constraint satisfaction. Moreover, ESPO achieves substantial gains in sample efficiency, requiring 25--29\\\\% fewer samples than baselines, and reduces training time by 21--38\\\\%.',\n",
       "  548: 'Continual Imitation Learning (CiL) involves extracting and accumulating task knowledge from demonstrations across multiple stages and tasks to achieve a multi-task policy. With recent advancements in foundation models, there has been a growing interest in adapter-based CiL approaches, where adapters are established parameter-efficiently for tasks newly demonstrated. While these approaches isolate parameters for specific tasks and tend to mitigate catastrophic forgetting, they limit knowledge sharing among different demonstrations. We introduce IsCiL, an adapter-based CiL framework that addresses this limitation of knowledge sharing by incrementally learning shareable skills from different demonstrations, thus enabling sample-efficient task adaptation using the skills particularly in non-stationary CiL environments. In IsCiL, demonstrations are mapped into the state embedding space, where proper skills can be retrieved upon input states through prototype-based memory. These retrievable skills are incrementally learned on their corresponding adapters. Our CiL experiments with complex tasks in the Franka-Kitchen and Meta-World demonstrate the robust performance of IsCiL in both task adaptation and sample-efficiency. We also show a simple extension of IsCiL for task unlearning scenarios.',\n",
       "  549: 'We study the preference-based pure exploration problem for bandits with vector-valued rewards and a set of preferences imposed over them. Specifically, we aim to identify the most preferred policy over a set of arms according to the preferences induced on the reward vectors by an ordering cone $C$. First, to quantify the impact of preferences, we derive a novel lower bound on the sample complexity for identifying the most preferred arm with confidence level $1-\\\\delta$. Our lower bound shows that how the geometry of the preferences and reward vectors changes the hardness of this problem. We further explicate this geometry for Gaussian distributions of rewards, and provide a convex reformulation of the lower bound solvable with linear programming. Then, we leverage this convex reformulation of the lower bound to design the Track and Stop with Preferences (TSwP) algorithm that identifies the most preferred policy. Finally, we derive a new concentration result for vector-valued rewards, and show that TSwP achieves a matching sample complexity upper bound.',\n",
       "  550: 'This paper studies first-order policy optimization for robust average cost Markov decision processes (MDPs). Specifically, we focus on ergodic Markov chains. For robust average cost MDPs, the goal is to optimize the worst-case average cost over an uncertainty set of transition kernels. We first develop a sub-gradient of the robust average cost. Based on the sub-gradient, a robust policy mirror descent approach is further proposed. To characterize its iteration complexity, we develop a lower bound on the difference of robust average cost between two policies and further show that the robust average cost satisfies the PL-condition. We then show that with increasing step size, our robust policy mirror descent achieves a linear convergence rate in the optimality gap, and with constant step size, our algorithm converges to an $\\\\epsilon$-optimal policy with an iteration complexity of $\\\\mathcal{O}(1/\\\\epsilon)$. The convergence rate of our algorithm matches with the best convergence rate of policy-based algorithms for robust MDPs. Moreover, our algorithm is the first algorithm that converges to the global optimum with general uncertainty sets for robust average cost MDPs. We provide simulation results to demonstrate the performance of our algorithm.',\n",
       "  551: 'While numerous Video Violence Detection (VVD) methods have focused on representation learning in Euclidean space, they struggle to learn sufficiently discriminative features, leading to weaknesses in recognizing normal events that are visually similar to violent events (i.e., ambiguous violence). In contrast, hyperbolic representation learning, renowned for its ability to model hierarchical and complex relationships between events, has the potential to amplify the discrimination between visually similar events. Inspired by these, we develop a novel Dual-Space Representation Learning (DSRL) method for weakly supervised VVD to utilize the strength of both Euclidean and hyperbolic geometries, capturing the visual features of events while also exploring the intrinsic relations between events, thereby enhancing the discriminative capacity of the features. DSRL employs a novel information aggregation strategy to progressively learn event context in hyperbolic spaces, which selects aggregation nodes through layer-sensitive hyperbolic association degrees constrained by hyperbolic Dirichlet energy. Furthermore, DSRL attempts to break the cyber-balkanization of different spaces, utilizing cross-space attention to facilitate information interactions between Euclidean and hyperbolic space to capture better discriminative features for final violence detection.  Comprehensive experiments demonstrate the effectiveness of our proposed DSRL.',\n",
       "  552: 'Unobserved discrete data are ubiquitous in many scientific disciplines, and how to learn the causal structure of these latent variables is crucial for uncovering data patterns. Most studies focus on the linear latent variable model or impose strict constraints on latent structures, which fail to address cases in discrete data involving non-linear relationships or complex latent structures. To achieve this, we explore a tensor rank condition on contingency tables for an observed variable set $\\\\mathbf{X}_p$, showing that the rank is determined by the minimum support of a specific conditional set (not necessary in $\\\\mathbf{X}_p$) that d-separates all variables in $\\\\mathbf{X}_p$. By this, one can locate the latent variable through probing the rank on different observed variables set, and further identify the latent causal structure under some structure assumptions. We present the corresponding identification algorithm and conduct simulated experiments to verify the effectiveness of our method. In general, our results elegantly extend the identification boundary for causal discovery with discrete latent variables and expand the application scope of causal discovery with latent variables.',\n",
       "  553: 'Community detection is an essential tool for unsupervised data exploration and revealing the organisational structure of networked systems. With a long history in network science, community detection typically relies on objective functions, optimised with custom-tailored search algorithms, but often without leveraging recent advances in deep learning. Recently, first works have started incorporating such objectives into loss functions for deep graph clustering and pooling. We consider the map equation, a popular information-theoretic objective function for unsupervised community detection, and express it in differentiable tensor form for optimisation through gradient descent. Our formulation turns the map equation compatible with any neural network architecture, enables end-to-end learning, incorporates node features, and chooses the optimal number of clusters automatically, all without requiring explicit regularisation. Applied to unsupervised graph clustering tasks, we achieve competitive performance against state-of-the-art deep graph clustering baselines in synthetic and real-world datasets.',\n",
       "  554: 'The recent large-scale text-to-image generative models have attained unprecedented performance, while people established adaptor modules like LoRA and DreamBooth to extend this performance to even more unseen concept tokens. However, we empirically find that this workflow often fails to accurately depict the out-of-distribution concepts. This failure is highly related to the low quality of training data. To resolve this, we present a framework called Controllable Adaptor Towards Out-of-Distribution Concepts (CATOD). Our framework follows the active learning paradigm which includes high-quality data accumulation and adaptor training, enabling a finer-grained enhancement of generative results. The aesthetics score and concept-matching score are two major factors that impact the quality of synthetic results. One key component of CATOD is the weighted scoring system that automatically balances between these two scores and we also offer comprehensive theoretical analysis for this point. Then, it determines how to select data and schedule the adaptor training based on this scoring system. The extensive results show that CATOD significantly outperforms the prior approaches with an 11.10 boost on the CLIP score and a 33.08% decrease on the CMMD metric.',\n",
       "  555: 'We propose ESPACE, an LLM compression technique based on dimensionality reduction of activations. Unlike prior works on weight-centric tensor decomposition, ESPACE projects activations onto a pre-calibrated set of principal components. The activation-centrality of the approach enables retraining LLMs with no loss of expressivity; while at inference, weight decomposition is obtained as a byproduct of matrix multiplication associativity. Theoretical results on the construction of projection matrices with optimal computational accuracy are provided. Experimentally, we find ESPACE enables 50% compression of GPT3, Llama2, and Nemotron4 models with small accuracy degradation, as low as a 0.18 perplexity increase on GPT3-22B. At lower compression rates of 20% to 40%, ESPACE drives GPT3 models to outperforming their baseline, by up to a 0.38 decrease in perplexity for GPT3-8B. ESPACE also reduces GEMM execution time and prefill inference latency on existing hardware. Comparison with related works on compressing Llama2-7B via matrix factorization shows that ESPACE is a first step in advancing the state-of-the-art in tensor decomposition compression of LLMs.',\n",
       "  556: \"Current LLMs are generally aligned to follow safety requirements and tend to refuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be overcautious and refuse benign examples. In addition, state-of-the-art toxicity detectors have low TPRs at low FPR, incurring high costs in real-world applications where toxic examples are rare. In this paper, we introduce Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves. We found we can distinguish between benign and toxic prompts from the distribution of the first response token's logits. Using this idea, we build a robust detector of toxic prompts using a sparse logistic regression model on the first response token logits. Our scheme outperforms SOTA detectors under multiple metrics.\",\n",
       "  557: 'Long-horizon manipulation tasks with general instructions often implicitly encapsulate multiple sub-tasks, posing significant challenges in instruction following.While language planning is a common approach to decompose general instructions into stepwise sub-instructions, text-only guidance may lack expressiveness and lead to potential ambiguity. Considering that humans often imagine and visualize sub-instructions reasoning out before acting, the imagined subgoal images can provide more intuitive guidance and enhance the reliability of decomposition. Inspired by this, we propose PERIA(PErceive, Reason, Imagine, Act), a novel framework that integrates holistic language planning and vision planning for long-horizon manipulation tasks with complex instructions, leveraging both logical and intuitive aspects of task decomposition.Specifically, we first perform a lightweight multimodal alignment on the encoding side to empower the MLLM to perceive visual details and language instructions. The MLLM is then jointly instruction-tuned with a pretrained image-editing model to unlock capabilities of simultaneous reasoning of language instructions and generation of imagined subgoals. Furthermore, we introduce a consistency alignment loss to encourage coherent subgoal images and align with their corresponding instructions, mitigating potential hallucinations and semantic conflicts between the two planning manners.Comprehensive evaluations across three task domains demonstrate that PERIA, benefiting from holistic language and vision planning, significantly outperforms competitive baselines in both instruction following accuracy and task success rate on complex manipulation tasks.',\n",
       "  558: 'Advanced image editing techniques, particularly inpainting, are essential for seamlessly removing unwanted elements while preserving visual integrity. Traditional GAN-based methods have achieved notable success, but recent advancements in diffusion models have produced superior results due to their training on large-scale datasets, enabling the generation of remarkably realistic inpainted images.Despite their strengths, diffusion models often struggle with object removal tasks without explicit guidance, leading to unintended hallucinations of the removed object. To address this issue, we introduce CLIPAway, a novel approach leveraging CLIP embeddings to focus on background regions while excluding foreground elements. CLIPAway enhances inpainting accuracy and quality by identifying embeddings that prioritize the background, thus achieving seamless object removal. Unlike other methods that rely on specialized training datasets or costly manual annotations, CLIPAway provides a flexible, plug-and-play solution compatible with various diffusion-based inpainting techniques.',\n",
       "  559: 'Current state-of-the-art methods for differentially private model training are based on matrix factorization techniques. However, these methods suffer from high computational overhead because they require numerically solving a demanding optimization problem to determine an approximately optimal factorization prior to the actual model training. In this work, we present a new matrix factorization approach, BSR, which overcomes this computational bottleneck. By exploiting properties of the standard matrix square root, BSR allows to efficiently handle also large-scale problems. For the key scenario of stochastic gradient descent with momentum and weight decay, we even derive analytical expressions for BSR that render the computational overhead negligible. We prove bounds on the approximation quality that hold both in the centralized and in the federated learning setting. Our numerical experiments demonstrate that models trained using BSR perform on par with the best existing methods, while completely avoiding their computational overhead.',\n",
       "  560: 'Visual reprogramming (VR) leverages the intrinsic capabilities of pretrained vision models by adapting their input or output interfaces to solve downstream tasks whose labels (i.e., downstream labels) might be totally different from the labels associated with the pretrained models (i.e., pretrained labels). When adapting the output interface, label mapping methods transform the pretrained labels to downstream labels by establishing a gradient-free one-to-one correspondence between the two sets of labels.However, in this paper, we reveal that one-to-one mappings may overlook the complex relationship between pretrained and downstream labels. Motivated by this observation, we propose a Bayesian-guided Label Mapping (BLM) method. BLM constructs an iteratively-updated probabilistic label mapping matrix, with each element quantifying a pairwise relationship between pretrained and downstream labels.The assignment of values to the constructed matrix is guided by Bayesian conditional probability, considering the joint distribution of the downstream labels and the labels predicted by the pretrained model on downstream samples. Experiments conducted on both pretrained vision models (e.g., ResNeXt) and vision-language models (e.g., CLIP) demonstrate the superior performance of BLM over existing label mapping methods. The success of BLM also offers a probabilistic lens through which to understand and analyze the effectiveness of VR.Our code is available at https://github.com/tmlr-group/BayesianLM.',\n",
       "  561: 'In the realm of Multimodal Large Language Models (MLLMs), vision-language connector plays a crucial role to link the pre-trained vision encoders with Large Language Models (LLMs). Despite its importance, the vision-language connector has been relatively less explored. In this study, we aim to propose a strong vision-language connector that enables MLLM to simultaneously achieve high accuracy and low computation cost. We first reveal the existence of the visual anchors in Vision Transformer and propose a cost-effective search algorithm to progressively extract them. Building on these findings, we introduce the Anchor Former (AcFormer), a novel vision-language connector designed to leverage the rich prior knowledge obtained from these visual anchors during pretraining, guiding the aggregation of information. Through extensive experimentation, we demonstrate that the proposed method significantly reduces computational costs by nearly two-thirds, while simultaneously outperforming baseline methods. This highlights the effectiveness and efficiency of AcFormer.',\n",
       "  562: 'Species range maps (SRMs) are essential tools for research and policy-making in ecology, conservation, and environmental management. However, traditional SRMs rely on the availability of environmental covariates and high-quality observational data, both of which can be challenging to obtain due to geographic inaccessibility and resource constraints. We propose a novel approach combining millions of citizen science species observations with textual descriptions from Wikipedia, covering habitat preferences and range descriptions for tens of thousands of species. Our framework maps location, species, and text descriptions into a common space, facilitating the learning of rich spatial covariates at a global scale and enabling zero-shot range estimation from textual descriptions. Evaluated on held-out species, our zero-shot SRMs significantly outperform baselines and match the performance of SRMs obtained using tens of observations. Our approach also acts as a strong prior when combined with observational data, resulting in more accurate range estimation with less data. We present extensive quantitative and qualitative analyses of the learned representations in the context of range estimation and other spatial tasks, demonstrating the effectiveness of our approach.',\n",
       "  563: \"Three-dimensional (3D) understanding of objects and scenes play a key role in humans' ability to interact with the world and has been an active area of research in computer vision, graphics, and robotics. Large scale synthetic and object-centric 3D datasets have shown to be effective in training models that have 3D understanding of objects. However, applying a similar approach to real-world objects and scenes is difficult due to a lack of large-scale data. Videos are a potential source for real-world 3D data, but finding diverse yet corresponding views of the same content have shown to be difficult at scale. Furthermore, standard videos come with fixed viewpoints, determined at the time of capture. This restricts the ability to access scenes from a variety of more diverse and potentially useful perspectives. We argue that large scale ODIN videos can address these limitations to provide scalable corresponding frames from diverse views.  In this paper we introduce 360-1M, a 360° video dataset consisting of 1 million videos, and a process for efficiently finding corresponding frames from diverse viewpoints at scale. We train our diffusion-based model, ODIN, on 360-1M. Empowered by the largest real-world, multi-view dataset to date, ODIN is able to freely generate novel views of real-world scenes. Unlike previous methods, ODIN can move the camera through the environment, enabling the model to infer the geometry and layout of the scene. Additionally, we show improved performance on standard novel view synthesis and 3D reconstruction benchmarks.\",\n",
       "  564: 'A proper scene representation is central to the pursuit of spatial intelligence where agents can robustly reconstruct and efficiently understand 3D scenes. A scene representation is either metric, such as landmark maps in 3D reconstruction, 3D bounding boxes in object detection, or voxel grids in occupancy prediction, or topological, such as pose graphs with loop closures in SLAM or visibility graphs in SfM.  In this work, we propose to build Multiview Scene Graphs (MSG) from unposed images, representing a scene topologically with interconnected place and object nodes.   The task of building MSG is challenging for existing representation learning methods since it needs to jointly address both visual place recognition, object detection, and object association from images with limited fields of view and potentially large viewpoint changes.  To evaluate any method tackling this task, we developed an MSG dataset and annotation based on a public 3D dataset.  We also propose an evaluation metric based on the intersection-over-union score of MSG edges.   Moreover, we develop a novel baseline method built on mainstream pretrained vision models, combining visual place recognition and object association into one Transformer decoder architecture.   Experiments demonstrate that our method has superior performance compared to existing relevant baselines.',\n",
       "  565: 'We consider the fixed-confidence best arm identification (FC-BAI) problem in the Bayesian setting. This problem aims to find the arm of the largest mean with a fixed confidence level when the bandit model has been sampled from the known prior. Most studies on the FC-BAI problem have been conducted in the frequentist setting, where the bandit model is predetermined before the game starts. We show that the traditional FC-BAI algorithms studied in the frequentist setting, such as track-and-stop and top-two algorithms, result in arbitrarily suboptimal performances in the Bayesian setting. We also obtain a lower bound of the expected number of samples in the Bayesian setting and introduce a variant of successive elimination that has a matching performance with the lower bound up to a logarithmic factor. Simulations verify the theoretical results.',\n",
       "  566: 'Large Language Models (LLMs) trained on massive corpora have shown remarkable success in knowledge-intensive tasks. Yet, most of them rely on pre-stored knowledge. Inducing new general knowledge from a specific environment andperforming reasoning with the acquired knowledge—situated inductive reasoning, is crucial and challenging for machine intelligence. In this paper, we design Mars, an interactive environment devised for situated inductive reasoning. It introduces counter-commonsense game mechanisms by modifying terrain, survival setting and task dependency while adhering to certain principles. In Mars, agents need to actively interact with their surroundings, derive useful rules and perform decision-making tasks in specific contexts. We conduct experiments on various RL-based and LLM-based methods, finding that they all struggle on this challenging situated inductive reasoning benchmark. Furthermore, we explore Induction from Reflection, where we instruct agents to perform inductive reasoning from history trajectory. The superior performance underscores the importance of inductive reasoning in Mars. Through Mars, we aim to galvanize advancements in situated inductive reasoning and set the stage for developing the next generation of AI systems that can reason in an adaptive and context-sensitive way.',\n",
       "  567: \"This paper examines the issue of fairness in the estimation of graphical models (GMs), particularly Gaussian, Covariance, and Ising models. These models play a vital role in understanding complex relationships in high-dimensional data. However, standard GMs can result in biased outcomes, especially when the underlying data involves sensitive characteristics or protected groups. To address this, we introduce a comprehensive framework designed to reduce bias in the estimation of GMs related to protected attributes. Our approach involves the integration of the pairwise graph disparity error and a tailored loss function into a nonsmooth multi-objective optimization problem, striving to achieve fairness across different sensitive groups while maintaining the effectiveness of the GMs. Experimental evaluations on synthetic and real-world datasets demonstrate that our framework effectively mitigates bias without undermining GMs' performance.\",\n",
       "  568: 'Mixed time series (MiTS) comprising both continuous variables (CVs) and discrete variables (DVs) are frequently encountered yet under-explored in time series analysis. Essentially, CVs and DVs exhibit different temporal patterns and distribution types. Overlooking these heterogeneities would lead to insufficient and imbalanced representation learning, bringing biased results. This paper addresses the problem with two insights: 1) DVs may originate from intrinsic latent continuous variables (LCVs), which lose fine-grained information due to extrinsic discretization; 2) LCVs and CVs share similar temporal patterns and interact spatially. Considering these similarities and interactions, we propose a general MiTS analysis framework MiTSformer, which recovers LCVs behind DVs for sufficient and balanced spatial-temporal modeling by designing two essential inductive biases: 1) hierarchically aggregating multi-scale temporal context information to enrich the information granularity of DVs; 2) adaptively learning the aggregation processes via the adversarial guidance from CVs. Subsequently, MiTSformer captures complete spatial-temporal dependencies within and across LCVs and CVs via cascaded self- and cross-attention blocks. Empirically, MiTSformer achieves consistent SOTA on five mixed time series analysis tasks, including classification, extrinsic regression, anomaly detection, imputation, and long-term forecasting. The code is available at https://github.com/chunhuiz/MiTSformer.',\n",
       "  569: 'PAC-Bayesian analysis is a frequentist framework for incorporating prior knowledge into learning. It was inspired by Bayesian learning, which allows sequential data processing and naturally turns posteriors from one processing step into priors for the next. However, despite two and a half decades of research, the ability to update priors sequentially without losing confidence information along the way remained elusive for PAC-Bayes. While PAC-Bayes allows construction of data-informed priors, the final confidence intervals depend only on the number of points that were not used for the construction of the prior, whereas confidence information in the prior, which is related to the number of points used to construct the prior, is lost. This limits the possibility and benefit of sequential prior updates, because the final bounds depend only on the size of the final batch.We present a novel and, in retrospect, surprisingly simple and powerful PAC-Bayesian procedure that allows  sequential prior updates with no information loss. The procedure is based on a novel decomposition of the expected loss of randomized classifiers. The decomposition rewrites the loss of the posterior as an excess loss relative to a downscaled loss of the prior plus the downscaled loss of the prior, which is bounded recursively. As a side result, we also present a generalization of the split-kl and PAC-Bayes-split-kl inequalities to discrete random variables, which we use for bounding the excess losses, and which can be of independent interest. In empirical evaluation the new procedure significantly outperforms state-of-the-art.',\n",
       "  570: 'Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent sentences expressed using different lexical compositions elicit diverging representations. The degree of this divergence and its impact on encoded semantics is not very well understood. In this paper, we introduce the SUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations. Each sample in SUGARCREPE++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SUGARCREPE++ dataset. Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly to object attributes and spatial relations. Although VLMs with larger pre-training datasets, model sizes, and multiple pre-training objectives achieve better performance on SUGARCREPE++, there is a significant opportunity for improvement. We demonstrate that models excelling on compositionality datasets may not perform equally well on SUGARCREPE++. This indicates that compositionality alone might not be sufficient to fully understand semantic and lexical alterations. Given the importance of the property that the SUGARCREPE++ dataset targets, it serves as a new challenge to the vision-and-language community. Data and code is available at https://github.com/Sri-Harsha/scpp.',\n",
       "  571: \"The performance of 3D object detection in large outdoor point clouds deteriorates significantly in an unseen environment due to the inter-domain gap. To address these challenges, most existing methods for domain adaptation harness self-training schemes and attempt to bridge the gap by focusing on a single factor that causes the inter-domain gap, such as objects' sizes, shapes, and foreground density variation. However, the resulting adaptations suggest that there is still a substantial inter-domain gap left to be minimized. We argue that this is due to two limitations: 1) Biased pseudo-label collection from self-training. 2) Multiple factors jointly contributing to how the object is perceived in the unseen target domain. In this work, we propose a grouping-exploration strategy framework,  Group Explorer Domain Adaptation ($\\\\textbf{GroupEXP-DA}$), to addresses those two issues. Specifically, our grouping divides the available label sets into multiple clusters and ensures all of them have equal learning attention with the group-equivariant spatial feature, avoiding dominant types of objects causing imbalance problems. Moreover, grouping learns to divide objects by considering inherent factors in a data-driven manner, without considering each factor separately as existing works. On top of the group-equivariant spatial feature that selectively detects objects similar to the input group, we additionally introduce an explorative group update strategy that reduces the false negative detection in the target domain, further reducing the inter-domain gap. During inference, only the learned group features are necessary for making the group-equivariant spatial feature, placing our method as a simple add-on that can be applicable to most existing detectors. We show how each module contributes to substantially bridging the inter-domain gaps compared to existing works across large urban outdoor datasets such as NuScenes, Waymo, and KITTI.\",\n",
       "  572: 'With the emergence of various molecular tasks and massive datasets, how to perform efficient training has become an urgent yet under-explored issue in the area. Data pruning (DP), as an oft-stated approach to saving training burdens, filters out less influential samples to form a coreset for training. However, the increasing reliance on pretrained models for molecular tasks renders traditional in-domain DP methods incompatible. Therefore, we propose a Molecular data Pruning framework for enhanced Generalization (MolPeg), which focuses on the source-free data pruning scenario, where data pruning is applied with pretrained models. By maintaining two models with different updating paces during training, we introduce a novel scoring function to measure the informativeness of samples based on the loss discrepancy. As a plug-and-play framework, MolPeg realizes the perception of both source and target domain and consistently outperforms existing DP methods across four downstream tasks. Remarkably, it can surpass the performance obtained from full-dataset training, even when pruning up to 60-70% of the data on HIV and PCBA dataset. Our work suggests that the discovery of effective data-pruning metrics could provide a viable path to both enhanced efficiency and superior generalization in transfer learning.',\n",
       "  573: 'Online shopping is a complex multi-task, few-shot learning problem with a wide and evolving range of entities, relations, and tasks. However, existing models and benchmarks are commonly tailored to specific tasks, falling short of capturing the full complexity of online shopping. Large Language Models (LLMs), with their multi-task and few-shot learning abilities, have the potential to profoundly transform online shopping by alleviating task-specific engineering efforts and by providing users with interactive conversations. Despite the potential, LLMs face unique challenges in online shopping, such as domain-specific concepts, implicit knowledge, and heterogeneous user behaviors. Motivated by the potential and challenges, we propose Shopping MMLU, a diverse multi-task online shopping benchmark derived from real-world Amazon data. Shopping MMLU consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality, and can thus comprehensively evaluate the abilities of LLMs as general shop assistants. With Shoppping MMLU, we benchmark over 20 existing LLMs and uncover valuable insights about practices and prospects of building versatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at https://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we are hosting a competition in KDD Cup 2024 with over 500 participating teams. The winning solutions and the associated workshop can be accessed at our website https://amazon-kddcup24.github.io/.',\n",
       "  574: 'Federated Learning (FL) allows multiple clients to collaboratively train models without directly sharing their private data. While various data augmentation techniques have been actively studied in the FL environment, most of these methods share input-level or feature-level data information over communication, posing potential privacy leakage. In response to this challenge, we introduce a federated data augmentation algorithm named FedAvP that shares only the augmentation policies, not the data-related information. For data security and efficient policy search, we interpret the policy loss as a meta update loss in standard FL algorithms and utilize the first-order gradient information to further enhance privacy and reduce communication costs. Moreover, we propose a meta-learning method to search for adaptive personalized policies tailored to heterogeneous clients. Our approach outperforms existing best performing augmentation policy search methods and federated data augmentation methods, in the benchmarks for heterogeneous FL.',\n",
       "  575: 'In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. However, little is known about whether they induce similar representations inside LLMs.  We approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, we compare how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while less-defined peaks characterize the landscape of ICL representations. Our approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing us to make a step towards designing optimal methods to extract information from language models.',\n",
       "  576: 'Vision models excel in image classification but struggle to generalize to unseen data, such as classifying images from unseen domains or discovering novel categories. In this paper, we explore the relationship between logical reasoning and deep learning generalization in visual classification. A logical regularization termed L-Reg is derived which bridges a logical analysis framework to image classification. Our work reveals that L-Reg reduces the complexity of the model in terms of the feature distribution and classifier weights. Specifically, we unveil the interpretability brought by L-Reg, as it enables the model to extract the salient features, such as faces to persons, for classification. Theoretical analysis and experiments demonstrate that L-Reg enhances generalization across various scenarios, including multi-domain generalization and generalized category discovery. In complex real-world scenarios where images span unknown classes and unseen domains, L-Reg consistently improves generalization, highlighting its practical efficacy.',\n",
       "  577: 'Deep neural networks have demonstrated remarkable performance in various vision tasks, but their success heavily depends on the quality of the training data. Noisy labels are a critical issue in medical datasets and can significantly degrade model performance. Previous clean sample selection methods have not utilized the well pre-trained features of vision foundation models (VFMs) and assumed that training begins from scratch. In this paper, we propose CUFIT, a curriculum fine-tuning paradigm of VFMs for medical image classification under label noise. Our method is motivated by the fact that linear probing of VFMs is relatively unaffected by noisy samples, as it does not update the feature extractor of the VFM, thus robustly classifying the training samples. Subsequently, curriculum fine-tuning of two adapters is conducted, starting with clean sample selection from the linear probing phase. Our experimental results demonstrate that CUFIT outperforms previous methods across various medical image benchmarks. Specifically, our method surpasses previous baselines by 5.0\\\\%, 2.1\\\\%, 4.6\\\\%, and 5.8\\\\% at a 40\\\\% noise rate on the HAM10000, APTOS-2019, BloodMnist, and OrgancMnist datasets, respectively. Furthermore, we provide extensive analyses to demonstrate the impact of our method on noisy label detection. For instance, our method shows higher label precision and recall compared to previous approaches. Our work highlights the potential of leveraging VFMs in medical image classification under challenging conditions of noisy labels.',\n",
       "  578: 'A central aim in computational neuroscience is to relate the activity of large populations of neurons to an underlying dynamical system. Models of these neural dynamics should ideally be both interpretable and fit the observed data well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability by having tractable dynamics. However, it is unclear how to best fit low-rank RNNs to data consisting of noisy observations of an underlying stochastic system. Here, we propose to fit stochastic low-rank RNNs with variational sequential Monte Carlo methods. We validate our method on several datasets consisting of both continuous and spiking neural data, where we obtain lower dimensional latent dynamics than current state of the art methods. Additionally, for low-rank models with piecewise linear nonlinearities, we show how to efficiently identify all fixed points in polynomial rather than exponential cost in the number of units, making analysis of the inferred dynamics tractable for large RNNs. Our method both elucidates the dynamical systems underlying experimental recordings and provides a generative model whose trajectories match observed variability.',\n",
       "  579: 'We present a detailed study of cardinality-aware top-$k$ classification, a novel approach that aims to learn an accurate top-$k$ set predictor while maintaining a low cardinality. We introduce a new target loss function tailored to this setting that accounts for both the classification error and the cardinality of the set predicted. To optimize this loss function, we propose two families of surrogate losses: cost-sensitive comp-sum losses and cost-sensitive constrained losses. Minimizing these loss functions leads to new cardinality-aware algorithms that we describe in detail in the case of both top-$k$ and threshold-based classifiers. We establish $H$-consistency bounds for our cardinality-aware surrogate loss functions, thereby providing a strong theoretical foundation for our algorithms. We report the results of extensive experiments on CIFAR-10, CIFAR-100, ImageNet, and SVHN datasets demonstrating the effectiveness and benefits of our cardinality-aware algorithms.',\n",
       "  580: 'We study the \\\\emph{in-context learning} (ICL) ability of a \\\\emph{Linear Transformer Block} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a \\\\emph{non-zero mean}, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive  approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization ($\\\\mathsf{GD}-\\\\beta$), in the sense that every $\\\\mathsf{GD}-\\\\beta$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a $\\\\mathsf{GD}-\\\\beta$ estimator.Finally, we show that $\\\\mathsf{GD}-\\\\beta$ estimators can be efficiently optimized with gradient flow, despite a non-convex training objective.Our results reveal that LTB achieves ICL by implementing $\\\\mathsf{GD}-\\\\beta$, and they highlight the role of MLP layers in reducing approximation error.',\n",
       "  581: 'Implicit neural representation gains popularity in modeling the continuous 3D surface for 3D representation and reconstruction. In this work, we are motivated by the fact that the local 3D patches repeatedly appear on 3D shapes/surfaces if the factor of poses is removed. Based on this observation, we propose the 3D patch-level equivariant implicit function (PEIF) based on the 3D patch-level pose-invariant representation, allowing us to reconstruct 3D surfaces by estimating equivariant displacement vector fields for query points. Specifically, our model is based on the pose-normalized query/patch pairs and enhanced by the proposed intrinsic patch geometry representation, modeling the intrinsic 3D patch geometry feature by learnable multi-head memory banks. Extensive experiments show that our model achieves state-of-the-art performance on multiple surface reconstruction datasets, and also exhibits better generalization to crossdataset shapes and robustness to arbitrary rotations. Our code will be available at https://github.com/mathXin112/PEIF.git.',\n",
       "  582: \"The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages in memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires to communicate binary or lower-precision vectorsbetween workers to the center server, significantly reducing the communication cost.  Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large models. In addition, we also demonstrate that \\\\mavolion{} presents a more favorable performance-bandwidth balance compared to existing efficient distributed methods such as deep gradient compression and ternary gradients.\",\n",
       "  583: 'The online bipartite matching problem, extensively studied in the literature, deals with the allocation of online arriving vertices (items) to a predetermined set of offline vertices (agents). However, little attention has been given to the concept of class fairness, where agents are categorized into different classes, and the matching algorithm must ensure equitable distribution across these classes.We here focus on randomized algorithms for the fair matching of indivisible items, subject to various definitions of fairness. Our main contribution is the first (randomized) non-wasteful algorithm that simultaneously achieves a $1/2$ approximation to class envy-freeness (CEF) while simultaneously ensuring an equivalent approximation to the class proportionality (CPROP) and utilitarian social welfare (USW) objectives. We supplement this result by demonstrating that no non-wasteful algorithm can achieve an $\\\\alpha$-CEF guarantee for $\\\\alpha > 0.761$. In a similar vein, we provide a novel input instance for deterministic divisible matching that demonstrates a nearly tight CEF approximation.Lastly, we define the ``price of fairness,\" which represents the trade-off between optimal and fair matching. We demonstrate that increasing the level of fairness in the approximation of the solution leads to a decrease in the objective of maximizing USW, following an inverse proportionality relationship.',\n",
       "  584: 'In the field of computational advertising, the integration of ads into the outputs of large language models (LLMs) presents an opportunity to support these services without compromising content integrity. This paper introduces novel auction mechanisms for ad allocation and pricing within the textual outputs of LLMs, leveraging retrieval-augmented generation (RAG). We propose a \\\\emph{segment auction} where an ad is probabilistically retrieved for each discourse segment (paragraph, section, or entire output) according to its bid and relevance, following the RAG framework, and priced according to competing bids. We show that our auction maximizes logarithmic social welfare, a new notion of welfare that balances allocation efficiency and fairness, and we characterize the associated incentive-compatible pricing rule. These results are extended to multi-ad allocation per segment. An empirical evaluation validates the feasibility and effectiveness of our approach over several ad auction scenarios, and exhibits inherent tradeoffs in metrics as we allow the LLM more flexibility to allocate ads.',\n",
       "  585: 'Despite significant advancements in video generation and editing using diffusion models, achieving accurate and localized video editing remains a substantial challenge. Additionally, most existing video editing methods primarily focus on altering visual content, with limited research dedicated to motion editing. In this paper, we present a novel attempt to Remake a Video (ReVideo) which stands out from existing methods by allowing precise video editing in specific areas through the specification of both content and motion. Content editing is facilitated by modifying the first frame, while the trajectory-based motion control offers an intuitive user interaction experience. ReVideo addresses a new task involving the coupling and training imbalance between content and motion control. To tackle this, we develop a three-stage training strategy that progressively decouples these two aspects from coarse to fine. Furthermore, we propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations. Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories. Our method can also seamlessly extend these applications to multi-area editing without specific training, demonstrating its flexibility and robustness.',\n",
       "  586: 'The path to interpreting a language model often proceeds via analysis of circuits---sparse computational subgraphs of the model that capture specific aspects of its behavior. Recent work has automated the task of discovering circuits. Yet, these methods have practical limitations, as they either rely on inefficient search algorithms or inaccurate approximations. In this paper, we frame circuit discovery as an optimization problem and propose Edge Pruning as an effective and scalable solution. Edge Pruning leverages gradient-based pruning techniques, but instead of removing neurons or components, prunes the edges between components. Our method finds circuits in GPT-2 that use less than half the number of edges than circuits found by previous methods while being equally faithful to the full model predictions on standard circuit-finding tasks. Edge Pruning is efficient on tasks involving up to 100,000 examples, outperforming previous methods in speed and producing substantially better circuits. It also perfectly recovers the ground-truth circuits in two models compiled with Tracr. Thanks to its efficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the size of GPT-2.We use this setting for a case study, where we compare the mechanisms behind instruction prompting and in-context learning.We find two circuits with more than 99.96% sparsity that match the performance of the full model. Further analysis reveals that the mechanisms in the two settings overlap substantially. This shows that Edge Pruning is a practical and scalable tool for interpretability, which can shed light on behaviors that only emerge in large models.',\n",
       "  587: \"Existing Multimodal Large Language Models (MLLMs) increasingly emphasize complex understanding of various visual elements, including multiple objects, text information, spatial relations. Their development for comprehensive visual perception hinges on the availability of high-quality image-text datasets that offer diverse visual elements and throughout image descriptions. However, the scarcity of such hyper-detailed datasets currently hinders progress within the MLLM community. The bottleneck stems from the limited perceptual capabilities of current caption engines, which fall short in providing complete and accurate annotations. To facilitate the cutting-edge research of MLLMs on comprehensive vision perception, we thereby propose Perceptual Fusion, using a low-budget but highly effective caption engine for complete and accurate image descriptions.  Specifically, Perceptual Fusion integrates diverse perception experts as image priors to provide explicit information on visual elements and adopts an efficient MLLM as a centric pivot to mimic advanced MLLMs' perception abilities. We carefully select 1M highly representative images from uncurated LAION dataset and generate dense descriptions using our engine, dubbed DenseFusion-1M. Extensive experiments validate that our engine outperforms its counterparts, where the resulting dataset significantly improves the perception and cognition abilities of existing MLLMs across diverse vision-language benchmarks, especially with high-resolution images as inputs. The code and dataset are available at https://huggingface.co/datasets/BAAI/DenseFusion-1M.\",\n",
       "  588: 'In variational inference (VI), an approximation of the posterior distribution is selected from a family of distributions through numerical optimization. With the most common variational objective function, known as the evidence lower bound (ELBO), only convergence to a local optimum can be guaranteed. In this work, we instead establish the global convergence of a particular VI method. This VI method, which may be considered an instance of neural posterior estimation (NPE), minimizes an expectation of the inclusive (forward) KL divergence to fit a variational distribution that is parameterized by a neural network. Our convergence result relies on the neural tangent kernel (NTK) to characterize the gradient dynamics that arise from considering the variational objective in function space. In the asymptotic regime of a fixed, positive-definite neural tangent kernel, we establish conditions under which the variational objective admits a unique solution in a reproducing kernel Hilbert space (RKHS). Then, we show that the gradient descent dynamics in function space converge to this unique function. In ablation studies and practical problems, we demonstrate that our results explain the behavior of NPE in non-asymptotic finite-neuron settings, and show that NPE outperforms ELBO-based optimization, which often converges to shallow local optima.',\n",
       "  589: \"In sensitive contexts, providers of machine learning algorithms are increasingly required to give explanations for their algorithms' decisions. However, explanation receivers might not trust the provider, who potentially could output misleading or manipulated explanations. In this work, we investigate an auditing framework in which a third-party auditor or a collective of users attempts to sanity-check explanations: they can query model decisions and the corresponding local explanations, pool all the information received, and then check for basic consistency properties. We prove upper and lower bounds on the amount of queries that are needed for an auditor to succeed within this framework. Our results show that successful auditing requires a potentially exorbitant number of queries -- particularly in high dimensional cases. Our analysis also reveals that a key property is the ``locality'' of the provided explanations --- a quantity that so far has not been paid much attention to in the explainability literature. Looking forward, our results suggest that for complex high-dimensional settings, merely providing a pointwise prediction and explanation could be insufficient, as there is no way for the users to verify that the provided explanations are not completely made-up.\",\n",
       "  590: 'Physical Human-Scene Interaction (HSI) plays a crucial role in numerous applications.     However, existing HSI techniques are limited to specific object dynamics and privileged information, which prevents the development of more comprehensive applications.    To address this limitation, we introduce HumanVLA for general object rearrangement directed by practical vision and language.     A teacher-student framework is utilized to develop HumanVLA.    A state-based teacher policy is trained first using goal-conditioned reinforcement learning and adversarial motion prior.    Then, it is distilled into a vision-language-action model via behavior cloning.    We propose several key insights to facilitate the large-scale learning process.    To support general object rearrangement by physical humanoid, we introduce a novel Human-in-the-Room dataset encompassing various rearrangement tasks.    Through extensive experiments and analysis, we demonstrate the effectiveness of our approach.',\n",
       "  591: \"Reinforcement learning (RL) algorithms have been very successful at tackling complex control problems, such as AlphaGo or fusion control. However, current research mainly emphasizes solution quality, often achieved by using large models trained on large amounts of data, and does not account for the financial, environmental, and societal costs associated with developing and deploying such models. Modern neural networks are often overparameterized and a significant number of parameters can be pruned without meaningful loss in performance, resulting in more efficient use of the model's capacity lottery ticket. We present a methodology for identifying sub-networks within a larger network in reinforcement learning (RL). We call such sub-networks, neural pathways. We show empirically that even very small learned sub-networks, using less than 5%  of the large network's parameters, can provide very good quality solutions. We also demonstrate the training of multiple pathways within the same networks in a multitask setup, where each pathway is encouraged to tackle a separate task. We evaluate empirically our approach on several continuous control tasks, in both online and offline training\",\n",
       "  592: 'Large multimodal models (LMMs) have proven flexible and generalisable across many tasks and fields. Although they have strong potential to aid scientific research, their capabilities in this domain are not well characterised. A key aspect of scientific research is the ability to understand and interpret figures, which serve as a rich, compressed source of complex information. In this work, we present SciFIBench, a scientific figure interpretation benchmark consisting of 2000 questions split between two tasks across 8 categories. The questions are curated from arXiv paper figures and captions, using adversarial filtering to find hard negatives and human verification forquality control. We evaluate 28 LMMs on SciFIBench, finding it to be a challenging benchmark. Finally, we investigate the alignment and reasoning faithfulness of the LMMs on augmented question sets from our benchmark. We release SciFIBench to encourage progress in this domain.',\n",
       "  593: 'Robots are often built from standardized assemblies, (e.g. arms, legs, or fingers), but each robot must be trained from scratch to control all the actuators of all the parts together. In this paper we demonstrate a new approach that takes a single robot and its controller as input and produces a set of modular controllers for each of these assemblies such that when a new robot is built from the same parts, its control can be quickly learned by reusing the modular controllers. We achieve this with a framework called MeMo which learns (Me)aningful, (Mo)dular controllers. Specifically, we propose a novel modularity objective to learn an appropriate division of labor among the modules. We demonstrate that this objective can be optimized simultaneously with standard behavior cloning loss via noise injection. We benchmark our framework in locomotion and grasping environments on simple to complex robot morphology transfer. We also show that the modules help in task transfer. On both structure and task transfer, MeMo achieves improved training efficiency to graph neural network and Transformer baselines.',\n",
       "  594: 'Finding specific preference-guided Pareto solutions that represent different trade-offs among multiple objectives is critical yet challenging in multi-objective problems. Existing methods are restrictive in preference definitions and/or their theoretical guarantees.In this work, we introduce a Flexible framEwork for pREfeRence-guided multi-Objective learning (FERERO) by casting it as a constrained vector optimization problem.Specifically, two types of preferences are incorporated into this formulation -- the relative preference defined by the partial ordering induced by a polyhedral cone, and the absolute preference defined by constraints that are linear functions of the objectives. To solve this problem, convergent algorithms are developed with both single-loop and stochastic variants. Notably, this is the first single-loop primal algorithm for constrained optimization to our knowledge. The proposed algorithms adaptively adjust to both constraint and objective values, eliminating the need to solve different subproblems at different stages of constraint satisfaction. Experiments on multiple benchmarks demonstrate the proposed method is very competitive in finding preference-guided optimal solutions.Code is available at https://github.com/lisha-chen/FERERO/.',\n",
       "  595: 'Deep Reinforcement Learning (DRL) algorithms have achieved great success in solving many challenging tasks while their black-box nature hinders interpretability and real-world applicability, making it difficult for human experts to interpret and understand DRL policies. Existing works on interpretable reinforcement learning have shown promise in extracting decision tree (DT) based policies from DRL policies with most focus on the single-agent settings while prior attempts to introduce DT policies in multi-agent scenarios mainly focus on heuristic designs which do not provide any quantitative guarantees on the expected return.In this paper, we establish an upper bound on the return gap between the oracle expert policy and an optimal decision tree policy. This enables us to recast the DT extraction problem into a novel non-euclidean clustering problem over the local observation and action values space of each agent, with action values as cluster labels and the upper bound on the return gap as clustering loss.Both the algorithm and the upper bound are extended to multi-agent decentralized DT extractions by an iteratively-grow-DT procedure guided by an action-value function conditioned on the current DTs of other agents. Further, we propose the Return-Gap-Minimization Decision Tree (RGMDT) algorithm, which is a surprisingly simple design and is integrated with reinforcement learning through the utilization of a novel Regularized Information Maximization loss. Evaluations on tasks like D4RL show that RGMDT significantly outperforms heuristic DT-based baselines and can achieve nearly optimal returns under given DT complexity constraints (e.g., maximum number of DT nodes).',\n",
       "  596: 'Watermarking is a technical means to dissuade malfeasant usage of Large Language Models.This paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM.Its new design leaves the LLM untouched (no modification of the weights, logits or temperature).WaterMax balances robustness and  computational complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness.Its performance is both theoretically proven and experimentally validated.It outperforms all the SotA techniques under the most complete benchmark suite.',\n",
       "  597: 'Single-cell RNA sequencing (scRNA-seq) technologies enable the exploration of cellular heterogeneity and facilitate the construction of cell atlases. However, scRNA-seq data often contain a large portion of missing values (false zeros) or noisy values, hindering downstream analyses. To recover these false zeros, propagation-based imputation methods have been proposed using $k$-NN graphs. However they model only associating relationships among genes within a cell, while, according to well-known genetic evidence, there are both associating and dissociating relationships among genes. To apply this genetic evidence to gene-gene relationship modeling, this paper proposes a novel imputation method that newly employs dissociating relationships in addition to associating relationships. Our method constructs a $k$-NN graph to additionally model dissociating relationships via the negation of a given cell-gene matrix. Moreover, our method standardizes the value distribution (mean and variance) of each gene to have standard distributions regardless of the gene. Through extensive experiments, we demonstrate that the proposed method achieves exceptional performance gains over state-of-the-art methods in both cell clustering and gene expression recovery across six scRNA-seq datasets, validating the significance of using complete gene-gene relationships in accordance with genetic evidence. The source code is available at https://github.com/daehoum1/scCR.',\n",
       "  598: 'Diffusion models are powerful generative models, and this capability can also be applied to discrimination. The inner activations of a pre-trained diffusion model can serve as features for discriminative tasks, namely, diffusion feature. We discover that diffusion feature has been hindered by a hidden yet universal phenomenon that we call content shift. To be specific, there are content differences between features and the input image, such as the exact shape of a certain object. We locate the cause of content shift as one inherent characteristic of diffusion models, which suggests the broad existence of this phenomenon in diffusion feature. Further empirical study also indicates that its negative impact is not negligible even when content shift is not visually perceivable. Hence, we propose to suppress content shift to enhance the overall quality of diffusion features. Specifically, content shift is related to the information drift during the process of recovering an image from the noisy input, pointing out the possibility of turning off-the-shelf generation techniques into tools for content shift suppression. We further propose a practical guideline named GATE to efficiently evaluate the potential benefit of a technique and provide an implementation of our methodology. Despite the simplicity, the proposed approach has achieved superior results on various tasks and datasets, validating its potential as a generic booster for diffusion features. Our code is available at https://github.com/Darkbblue/diffusion-content-shift.',\n",
       "  599: \"When humans need to learn a new skill, we can acquire knowledge through written books, including textbooks, tutorials, etc. However, current research for decision-making, like reinforcement learning (RL), has primarily required numerous real interactions with the target environment to learn a skill, while failing to utilize the existing knowledge already summarized in the text. The success of Large Language Models (LLMs) sheds light on utilizing such knowledge behind the books. In this paper, we discuss a new policy learning problem called Policy Learning from tutorial Books (PLfB) upon the shoulders of LLMs’ systems, which aims to leverage rich resources such as tutorial books to derive a policy network. Inspired by how humans learn from books, we solve the problem via a three-stage framework: Understanding, Rehearsing, and Introspecting (URI). In particular, it first rehearses decision-making trajectories based on the derived knowledge after understanding the books, then introspects in the imaginary dataset to distill a policy network.  We build two benchmarks for PLfB~based on Tic-Tac-Toe and Football games. In experiment, URI's policy achieves at least 44% net win rate against GPT-based agents without any real data; In Football game, which is a complex scenario, URI's policy beat the built-in AIs with a 37% while using GPT-based agent can only achieve a 6\\\\% winning rate. The project page: https://plfb-football.github.io.\",\n",
       "  600: 'Recently, Deep Image Prior (DIP) has emerged as an effective unsupervised one-shot learner, delivering competitive results across various image recovery problems. This method only requires the noisy measurements and a forward operator, relying solely on deep networks initialized with random noise to learn and restore the structure of the data. However, DIP is notorious for its vulnerability to overfitting due to the overparameterization of the network. Building upon insights into the impact of the DIP input and drawing inspiration from the gradual denoising process in cutting-edge diffusion models, we introduce Autoencoding Sequential DIP (aSeqDIP) for image reconstruction. This method progressively denoises and reconstructs the image through a sequential optimization of network weights. This is achieved using an input-adaptive DIP objective, combined with an autoencoding regularization term. Compared to diffusion models, our method does not require training data and outperforms other DIP-based methods in mitigating noise overfitting while maintaining a similar number of parameter updates as Vanilla DIP. Through extensive experiments, we validate the effectiveness of our method in various image reconstruction tasks, such as MRI and CT reconstruction, as well as in image restoration tasks like image denoising, inpainting, and non-linear deblurring.',\n",
       "  601: 'Approximating invariant subspaces of generalized eigenvalue problems (GEPs) is a fundamental computational problem at the core of machine learning and scientific computing. It is, for example, the root of Principal Component Analysis (PCA) for dimensionality reduction, data visualization, and noise filtering, and of Density Functional Theory (DFT), arguably the most popular method to calculate the electronic structure of materials. Given Hermitian $H,S\\\\in\\\\mathbb{C}^{n\\\\times n}$, where $S$ is positive-definite,  let $\\\\Pi_k$ be the true spectral projector on the invariant subspace that is associated with the $k$ smallest (or largest) eigenvalues of the GEP $HC=SC\\\\Lambda$, for some $k\\\\in[n]$. We show that we can compute a matrix $\\\\widetilde\\\\Pi_k$ such that $\\\\lVert\\\\Pi_k-\\\\widetilde\\\\Pi_k\\\\rVert_2\\\\leq \\\\epsilon$, in $O\\\\left( n^{\\\\omega+\\\\eta}\\\\mathrm{polylog}(n,\\\\epsilon^{-1},\\\\kappa(S),\\\\mathrm{gap}_k^{-1}) \\\\right)$ bit operations in the floating point model, for some $\\\\epsilon\\\\in(0,1)$, with probability $1-1/n$. Here, $\\\\eta>0$ is arbitrarily small, $\\\\omega\\\\lesssim 2.372$ is the matrix multiplication exponent, $\\\\kappa(S)=\\\\lVert S\\\\rVert_2\\\\lVert S^{-1}\\\\rVert_2$, and $\\\\mathrm{gap}_k$ is the gap between eigenvalues $k$ and $k+1$. To achieve such provable \"forward-error\" guarantees, our methods rely on a new $O(n^{\\\\omega+\\\\eta})$ stability analysis for the Cholesky factorization, and a smoothed analysis for computing spectral gaps, which can be of independent interest.Ultimately, we obtain new matrix multiplication-type bit complexity upper bounds for PCA problems, including classical PCA and (randomized) low-rank approximation.',\n",
       "  602: 'Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.',\n",
       "  603: 'This paper introduces OpenGaussian, a method based on 3D Gaussian Splatting (3DGS) that possesses the capability for 3D point-level open vocabulary understanding. Our primary motivation stems from observing that existing 3DGS-based open vocabulary methods mainly focus on 2D pixel-level parsing. These methods struggle with 3D point-level tasks due to weak feature expressiveness and inaccurate 2D-3D feature associations. To ensure robust feature presentation and 3D point-level understanding, we first employ SAM masks without cross-frame associations to train instance features with 3D consistency. These features exhibit both intra-object consistency and inter-object distinction. Then, we propose a two-stage codebook to discretize these features from coarse to fine levels. At the coarse level, we consider the positional information of 3D points to achieve location-based clustering, which is then refined at the fine level.Finally, we introduce an instance-level 3D-2D feature association method that links 3D points to 2D masks, which are further associated with 2D CLIP features. Extensive experiments, including open vocabulary-based 3D object selection, 3D point cloud understanding, click-based 3D object selection, and ablation studies, demonstrate the effectiveness of our proposed method. The source code is available at our project page https://3d-aigc.github.io/OpenGaussian.',\n",
       "  604: 'We revisit the recently developed framework of proportionally fair clustering, where the goal is to provide group fairness guarantees that become stronger for groups of data points that are large and cohesive. Prior work applies this framework to centroid-based clustering, where points are partitioned into clusters, and the cost to each data point is measured by its distance to a centroid assigned to its cluster. However, real-life applications often do not require such centroids. We extend the theory of proportionally fair clustering to non-centroid clustering by considering a variety of cost functions, both metric and non-metric, for a data point to be placed in a cluster with other data points. Our results indicate that Greedy Capture, a clustering algorithm developed for centroid clustering, continues to provide strong proportional fairness guarantees for non-centroid clustering, although the guarantees are significantly different and establishing them requires novel proof ideas. We also design algorithms for auditing proportional fairness of a given clustering solution. We conduct experiments on real data which suggest that traditional clustering algorithms are highly unfair, while our algorithms achieve strong fairness guarantees with a moderate loss in common clustering objectives.',\n",
       "  605: \"The diffusion model performs remarkable in generating high-dimensional content but is computationally intensive, especially during training. We propose Progressive Growing of Diffusion Autoencoder (PaGoDA), a novel pipeline that reduces the training costs through three stages: training diffusion on downsampled data, distilling the pretrained diffusion, and progressive super-resolution. With the proposed pipeline, PaGoDA achieves a $64\\\\times$ reduced cost in training its diffusion model on $8\\\\times$ downsampled data; while at the inference, with the single-step, it performs state-of-the-art on ImageNet across all resolutions from $64\\\\times64$ to $512\\\\times512$, and text-to-image. PaGoDA's pipeline can be applied directly in the latent space, adding compression alongside the pre-trained autoencoder in Latent Diffusion Models (e.g., Stable Diffusion). The code is available at https://github.com/sony/pagoda.\",\n",
       "  606: \"The evolution of Artificial Intelligence (AI) has been significantly accelerated by advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), gradually showcasing potential cognitive reasoning abilities in problem-solving and scientific discovery (i.e., AI4Science) once exclusive to human intellect. To comprehensively evaluate current models' performance in cognitive reasoning abilities, we introduce OlympicArena, which includes 11,163 bilingual problems across both text-only and interleaved text-image modalities. These challenges encompass a wide range of disciplines spanning seven fields and 62 international Olympic competitions, rigorously examined for data leakage. We argue that the challenges in Olympic competition problems are ideal for evaluating AI's cognitive reasoning due to their complexity and interdisciplinary nature, which are essential for tackling complex scientific challenges and facilitating discoveries. Beyond evaluating performance across various disciplines using answer-only criteria, we conduct detailed experiments and analyses from multiple perspectives. We delve into the models' cognitive reasoning abilities, their performance across different modalities, and their outcomes in process-level evaluations, which are vital for tasks requiring complex reasoning with lengthy solutions. Our extensive evaluations reveal that even advanced models like GPT-4o only achieve a 39.97\\\\%  overall accuracy (28.67\\\\%  for mathematics and 29.71\\\\%  for physics), illustrating current AI limitations in complex reasoning and multimodal integration. Through the OlympicArena, we aim to advance AI towards superintelligence, equipping it to address more complex challenges in science and beyond. We also provide a comprehensive set of resources to support AI research, including a benchmark dataset, an open-source annotation platform, a detailed evaluation tool, and a leaderboard with automatic submission features.\",\n",
       "  607: 'The lack of transparency in the decision-making processes of deep learning systems presents a significant challenge in modern artificial intelligence (AI), as it impairs users’ ability to rely on and verify these systems. To address this challenge, Concept Bottleneck Models (CBMs) have made significant progress by incorporating human-interpretable concepts into deep learning architectures. This approach allows predictions to be traced back to specific concept patterns that users can understand and potentially intervene on. However, existing CBMs’ task predictors are not fully interpretable, preventing a thorough analysis and any form of formal verification of their decision-making process prior to deployment, thereby raising significant reliability concerns. To bridge this gap, we introduce Concept-based Memory Reasoner (CMR), a novel CBM designed to provide a human-understandable and provably-verifiable task prediction process. Our approach is to model each task prediction as a neural selection mechanism over a memory of learnable logic rules, followed by a symbolic evaluation of the selected rule. The presence of an explicit memory and the symbolic evaluation allow domain experts to inspect and formally verify the validity of certain global properties of interest for the task prediction process. Experimental results demonstrate that CMR achieves better accuracy-interpretability trade-offs to state-of-the-art CBMs, discovers logic rules consistent with ground truths, allows for rule interventions, and allows pre-deployment verification.',\n",
       "  608: 'Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description?In this paper, we present Make-An-Agent, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by Make-An-Agent onto real-world robots on locomotion tasks. Project page: https://cheryyunl.github.io/make-an-agent/.',\n",
       "  609: 'Most existing theoretical investigations of the accuracy of diffusion models, albeit significant, assume the score function has been approximated to a certain accuracy, and then use this a priori bound to control the error of generation. This article instead provides a first quantitative understanding of the whole generation process, i.e., both training and sampling. More precisely, it conducts a non-asymptotic convergence analysis of denoising score matching under gradient descent. In addition, a refined sampling error analysis for variance exploding models is also provided. The combination of these two results yields a full error analysis, which elucidates (again, but this time theoretically) how to design the training and sampling processes for effective generation. For instance, our theory implies a preference toward noise distribution and loss weighting in training that qualitatively agree with the ones used in [Karras et al., 2022]. It also provides perspectives on the choices of time and variance schedules in sampling: when the score is well trained, the design in [Song et al., 2021] is more preferable, but when it is less trained, the design in [Karras et al., 2022] becomes more preferable.',\n",
       "  610: 'Consumer electronics used to follow the miniaturization trend described by Moore’s Law. Despite increased processing power in Microcontroller Units (MCUs), MCUs used in the smallest appliances are still not capable of running even moderately big, state-of-the-art artificial neural networks (ANNs) especially in time-sensitive scenarios. In this work, we present a novel method called Scattered Online Inference (SOI) that aims to reduce the computational complexity of ANNs. SOI leverages the continuity and seasonality of time-series data and model predictions, enabling extrapolation for processing speed improvements, particularly in deeper layers. By applying compression, SOI generates more general inner partial states of ANN, allowing skipping full model recalculation at each inference.',\n",
       "  611: 'Contextual bandit algorithms aim to identify the optimal arm with the highest reward among a set of candidates, based on the accessible contextual information. Among these algorithms, neural contextual bandit methods have shown generally superior performances against linear and kernel ones, due to the representation power of neural networks. However, similar to other neural network applications, neural bandit algorithms can be vulnerable to adversarial attacks or corruptions on the received labels (i.e., arm rewards), which can lead to unexpected performance degradation without proper treatments. As a result, it is necessary to improve the robustness of neural bandit models against potential reward corruptions. In this work, we propose a novel neural contextual bandit algorithm named R-NeuralUCB, which utilizes a novel context-aware Gradient Descent (GD) training strategy to improve the robustness against adversarial reward corruptions. Under over-parameterized neural network settings, we provide regret analysis for R-NeuralUCB to quantify  reward corruption impacts, without the commonly adopted arm separateness assumption in existing neural bandit works. We also conduct experiments against baselines on real data sets under different scenarios, in order to demonstrate the effectiveness of our proposed R-NeuralUCB.',\n",
       "  612: 'Diffusion models excel in solving imaging inverse problems due to their ability to model complex image priors. However, their reliance on large, clean datasets for training limits their practical use where clean data is scarce. In this paper, we propose EMDiffusion, an expectation-maximization (EM) approach to train diffusion models from corrupted observations. Our method alternates between reconstructing clean images from corrupted data using a known diffusion model (E-step) and refining diffusion model weights based on these reconstructions (M-step). This iterative process leads the learned diffusion model to gradually converge to a local optimum, that is, to approximate the true clean data distribution. We validate our method through extensive experiments on diverse computational imaging tasks, including random inpainting, denoising, and deblurring, achieving new state-of-the-art performance.',\n",
       "  613: 'We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions. The series comprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and annotating strategy. 2) ShareCaptioner-Video, an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it. 3) ShareGPT4Video-8B, a simple yet superb LVLM that reached SOTA performance on three advancing video benchmarks. To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 1) Inter-frame precise temporal change understanding. 2) Intra-frame detailed content description. 3) Frame-number scalability for arbitrary-length videos. To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length. Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos. We annotated 4.8M aesthetically appealing videos by it and verified their effectiveness on a 10-second text2video generation task. For video understanding, we verified the effectiveness of ShareGPT4Video on several current LVLM architectures and presented our superb new LVLM ShareGPT4Video-8B. All the models, strategies, and annotations will be open-sourced and we hope this project can serve as a pivotal resource for advancing both the LVLMs and T2VMs community.',\n",
       "  614: 'Deep neural networks, costly to train and rich in intellectual property value, areincreasingly threatened by model extraction attacks that compromise their confiden-tiality. Previous attacks have succeeded in reverse-engineering model parametersup to a precision of float64 for models trained on random data with at most threehidden layers using cryptanalytical techniques. However, the process was identifiedto be very time consuming and not feasible for larger and deeper models trained onstandard benchmarks. Our study evaluates the feasibility of parameter extractionmethods of Carlini et al. [1] further enhanced by Canales-Martínez et al. [2] formodels trained on standard benchmarks. We introduce a unified codebase thatintegrates previous methods and reveal that computational tools can significantlyinfluence performance. We develop further optimisations to the end-to-end attackand improve the efficiency of extracting weight signs by up to 14.8 times com-pared to former methods through the identification of easier and harder to extractneurons. Contrary to prior assumptions, we identify extraction of weights, notextraction of weight signs, as the critical bottleneck. With our improvements, a16,721 parameter model with 2 hidden layers trained on MNIST is extracted withinonly 98 minutes compared to at least 150 minutes previously. Finally, addressingmethodological deficiencies observed in previous studies, we propose new ways ofrobust benchmarking for future model extraction attacks.',\n",
       "  615: 'The necessity to align two graphs, minimizing a structural distance metric, is prevalent in biology, chemistry, recommender systems, and social network analysis. Due to the problem’s NP-hardness, prevailing graph alignment methods follow a modular and mediated approach, solving the problem by restricting to the domain of intermediary graph representations or products like embeddings, spectra, and graph signals. Restricting the problem to this intermediate space may distort the original problem and are hence predisposed to miss high-quality solutions. In this paper, we propose an unrestricted method, FUGAL, which finds a permutation matrix that maps one graph to another by directly operating on their adjacency matrices with judicious constraint relaxation. Extensive experimentation demonstrates that FUGAL consistently surpasses state-of-the-art graph alignment methods in accuracy across all benchmark datasets without encumbering efficiency.',\n",
       "  616: 'The era of vision-language models (VLMs) trained on web-scale datasets challenges conventional formulations of “open-world\" perception. In this work, we revisit the task of few-shot object detection (FSOD) in the context of recent foundational VLMs. First, we point out that zero-shot predictions from VLMs such as GroundingDINO significantly outperform state-of-the-art few-shot detectors (48 vs. 33 AP) on COCO. Despite their strong zero-shot performance, such foundation models may still be sub-optimal. For example, trucks on the web may be defined differently from trucks for a target applications such as autonomous vehicle perception. We argue that the task of few-shot recognition can be reformulated as aligning foundation models to target concepts using a few examples. Interestingly, such examples can be multi-modal, using both text and visual cues, mimicking instructions that are often given to human annotators when defining a target concept of interest. Concretely, we propose Foundational FSOD, a new benchmark protocol that evaluates detectors pre-trained on any external data and fine-tuned on multi-modal (text and visual) K-shot examples per target class. We repurpose nuImages for Foundational FSOD, benchmark several popular open-source VLMs, and provide an empirical analysis of state-of-the-art methods. Lastly, we discuss our recent CVPR 2024 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 23.3 mAP!',\n",
       "  617: 'Optimal transport (OT) has profoundly impacted machine learning by providing theoretical and computational tools to realign datasets.In this context, given two large point clouds of sizes $n$ and $m$ in $\\\\mathbb{R}^d$, entropic OT (EOT) solvers have emerged as the most reliable tool to either solve the Kantorovich problem and output a $n\\\\times m$ coupling matrix, or to solve the Monge problem and learn a vector-valued push-forward map. While the robustness of EOT couplings/maps makes them a go-to choice in practical applications, EOT solvers remain difficult to tune because of a small but influential set of hyperparameters, notably the omnipresent entropic regularization strength $\\\\varepsilon$. Setting $\\\\varepsilon$ can be difficult, as it simultaneously impacts various performance metrics, such as compute speed, statistical performance, generalization, and bias. In this work, we propose a new class of EOT solvers (ProgOT), that can estimate both plans and transport maps.We take advantage of several opportunities to optimize the computation of EOT solutions by *dividing* mass displacement using a time discretization, borrowing inspiration from dynamic OT formulations, and *conquering* each of these steps using EOT with properly scheduled parameters. We provide experimental evidence demonstrating that ProgOT is a faster and more robust alternative to *standard solvers* when computing couplings at large scales, even outperforming neural network-based approaches. We also prove statistical consistency of our approach for estimating OT maps.',\n",
       "  618: 'Motivated by online display advertising, this work considers repeated second-price auctions, where agents sample their value from an unknown distribution with cumulative distribution function $F$. In each auction $t$, a decision-maker bound by limited observations selects $n_t$ agents from a coalition of $N$ to compete for a prize with $p$ other agents, aiming to maximize the cumulative reward of the coalition across all auctions.The problem is framed as an $N$-armed structured bandit, each number of player sent being an arm $n$, with expected reward $r(n)$ fully characterized by $F$ and $p+n$. We present two algorithms, Local-Greedy (LG) and Greedy-Grid (GG), both achieving *constant* problem-dependent regret. This relies on three key ingredients: **1.** an estimator of $r(n)$ from feedback collected from any arm $k$, **2.** concentration bounds of these estimates for $k$ within an estimation neighborhood of $n$ and **3.** the unimodality property of $r$ under standard assumptions on $F$. Additionally, GG exhibits problem-independent guarantees on top of best problem-dependent guarantees. However, by avoiding to rely on confidence intervals, LG practically outperforms GG, as well as standard unimodal bandit algorithms such as OSUB or multi-armed bandit algorithms.',\n",
       "  619: \"Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for conditional sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling.Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood Doob's h-transform. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional $h$-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On image reconstruction tasks, we achieve speedups of up to 1.6$\\\\times$, while having the best perceptual quality on natural images and reconstruction performance on medical images. Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods.\",\n",
       "  620: 'High-resolution images offer more information about scenes that can improve model accuracy. However, the dominant model architecture in computer vision, the vision transformer (ViT), cannot effectively leverage larger images without finetuning — ViTs poorly extrapolate to more patches at test time, although transformers offer sequence length flexibility. We attribute this shortcoming to the current patch position encoding methods, which create a distribution shift when extrapolating.We propose a drop-in replacement for the position encoding of plain ViTs that restricts attention heads to fixed fields of view, pointed in different directions, using 2D attention masks. Our novel method, called LookHere, provides translation-equivariance, ensures attention head diversity, and limits the distribution shift that attention heads face when extrapolating. We demonstrate that LookHere improves performance on classification (avg. 1.6%), against adversarial attack (avg. 5.4%), and decreases calibration error (avg. 1.5%) — on ImageNet without extrapolation. With extrapolation, LookHere outperforms the current SoTA position encoding method, 2D-RoPE, by 21.7% on ImageNet when trained at $224^2$ px and tested at $1024^2$ px. Additionally, we release a high-resolution test set to improve the evaluation of high-resolution image classifiers, called ImageNet-HR.',\n",
       "  621: 'In this paper, we show that applying adaptive methods directly to distributed minimax problems can result in non-convergence due to inconsistency in locally computed adaptive stepsizes. To address this challenge, we propose D-AdaST, a Distributed Adaptive minimax method with Stepsize Tracking. The key strategy is to employ an adaptive stepsize tracking protocol involving the transmission of two extra (scalar) variables. This protocol ensures the consistency among stepsizes of nodes, eliminating the steady-state error due to the lack of coordination of stepsizes among nodes that commonly exists in vanilla distributed adaptive methods, and thus guarantees exact convergence. For nonconvex-strongly-concave distributed minimax problems, we characterize the specific transient times that ensure time-scale separation of stepsizes and quasi-independence of networks, leading to a near-optimal convergence rate of $\\\\tilde{\\\\mathcal{O}} \\\\left( \\\\epsilon ^{-\\\\left( 4+\\\\delta \\\\right)} \\\\right)$ for any small $\\\\delta > 0$, matching that of the centralized counterpart. To our best knowledge, D-AdaST is the *first* distributed adaptive method achieving near-optimal convergence without knowing any problem-dependent parameters for nonconvex minimax problems. Extensive experiments are conducted to validate our theoretical results.',\n",
       "  622: \"\\\\emph{Metacognitive knowledge} refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also  have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure  to get a powerful  LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans.To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH.  (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label.  This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic,  even though this article applies it to math problems.\",\n",
       "  623: 'Covariate adjustment, also known as back-door adjustment, is a fundamental tool in causal inference. Although a sound and complete graphical identification criterion, known as the adjustment criterion (Shpitser, 2010), exists for static contexts, sequential contexts present challenges. Current practices, such as the sequential back-door adjustment (Pearl, 1995) or multi-outcome sequential back-door adjustment (Jung, 2020), are sound but incomplete; i.e., there are graphical scenarios where the causal effect is expressible via covariate adjustment, yet these criteria do not cover. In this paper, we exemplify this incompleteness and then present the sequential adjustment criterion, a sound and complete criterion for sequential covariate adjustment. We provide a constructive sequential adjustment criterion that identifies a set that satisfies the sequential adjustment criterion if and only if the causal effect can be expressed as a sequential covariate adjustment. Finally, we present an algorithm for identifying a minimal sequential covariate adjustment set, which optimizes efficiency by ensuring that no unnecessary vertices are included.',\n",
       "  624: 'Indoor radar perception has seen rising interest due to affordable costs driven by emerging automotive imaging radar developments and the benefits of reduced privacy concerns and reliability under hazardous conditions (e.g., fire and smoke). However, existing radar perception pipelines fail to account for distinctive characteristics of the multi-view radar setting. In this paper, we propose Radar dEtection TRansformer (RETR), an extension of the popular DETR architecture, tailored for multi-view radar perception. RETR inherits the advantages of DETR, eliminating the need for hand-crafted components for object detection and segmentation in the image plane. More importantly, RETR incorporates carefully designed modifications such as 1) depth-prioritized feature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss from both radar and camera coordinates; and 3) a learnable radar-to-camera transformation via reparameterization, to account for the unique multi-view radar setting. Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of 15.38+ AP for object detection and 11.91+ IoU for instance segmentation, respectively. Our implementation is available at https://github.com/merlresearch/radar-detection-transformer.',\n",
       "  625: 'Scaling deep learning models has been at the heart of recent revolutions in language modelling and image generation. Practitioners have observed a strong relationship between model size, dataset size, and performance. However, structure-based architectures such as Graph Neural Networks (GNNs) are yet to show the benefits of scale mainly due to lower efficiency of sparse operations, large data requirements, and lack of clarity about the effectiveness of various architectures. We address this drawback of GNNs by studying their scaling behavior. Specifically, we analyze message-passing networks, graph Transformers, and hybrid architectures on the largest public collection of 2D molecular graphs for supervised pretraining. For the first time, we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules and associated labels. A major factor is the diversity of the pretraining data that comprises thousands of labels per molecule derived from bio-assays, quantum simulations, transcriptomics and phenomic imaging. We further demonstrate strong finetuning scaling behavior on 38 highly competitive downstream tasks, outclassing previous large models. This gives rise to MolGPS, a new graph foundation model that allows to navigate the chemical space, outperforming the previous state-of-the-arts on 26 out the 38 downstream tasks. We hope that our work paves the way for an era where foundational GNNs drive pharmaceutical drug discovery.',\n",
       "  626: 'Given the complexities inherent in visual scenes, such as object occlusion, a comprehensive understanding often requires observation from multiple viewpoints. Existing multi-viewpoint object-centric learning methods typically employ random or sequential viewpoint selection strategies. While applicable across various scenes, these strategies may not always be ideal, as certain scenes could benefit more from specific viewpoints. To address this limitation, we propose a novel active viewpoint selection strategy. This strategy predicts images from unknown viewpoints based on information from observation images for each scene. It then compares the object-centric representations extracted from both viewpoints and selects the unknown viewpoint with the largest disparity, indicating the greatest gain in information, as the next observation viewpoint. Through experiments on various datasets, we demonstrate the effectiveness of our active viewpoint selection strategy, significantly enhancing segmentation and reconstruction performance compared to random viewpoint selection. Moreover, our method can accurately predict images from unknown viewpoints.',\n",
       "  627: \"Quantitative Relative Judgment Aggregation (QRJA) is a new research topic in (computational) social choice. In the QRJA model, agents provide judgments on the relative quality of different candidates, and the goal is to aggregate these judgments across all agents. In this work, our main conceptual contribution is to explore the interplay between QRJA in a social choice context and its application to ranking prediction. We observe that in QRJA, judges do not have to be people with subjective opinions; for example, a race can be viewed as a ``judgment'' on the contestants' relative abilities. This allows us to aggregate results from multiple races to evaluate the contestants' true qualities. At a technical level, we introduce new aggregation rules for QRJA and study their structural and computational properties. We evaluate the proposed methods on data from various real races and show that QRJA-based methods offer effective and interpretable ranking predictions.\",\n",
       "  628: \"Benchmarks for large multimodal language models (MLMs) now serve to simultaneously assess the general capabilities of models instead of evaluating for a specific capability. As a result, when a developer wants to identify which models to use for their application, they are overwhelmed by the number of benchmarks and remain uncertain about which benchmark's results are most reflective of their specific use case. This paper introduces Task-Me-Anything, a benchmark generation engine which produces a benchmark tailored to a user's needs. Task-Me-Anything maintains an extendable taxonomy of visual assets and can programmatically generate a vast number of task instances. Additionally, it algorithmically addresses user queries regarding MLM performance efficiently within a computational budget. It contains 113K images, 10K videos, 2K 3D object assets, over 365 object categories, 655 attributes, and 335 relationships. It can generate 500M image/video question-answering pairs, which focus on evaluating MLM perceptual capabilities. Task-Me-Anything reveals critical insights: open-source MLMs excel in object and attribute recognition but lack spatial and temporal understanding; each model exhibits unique strengths and weaknesses; larger models generally perform better, though exceptions exist; and GPT4O demonstrates challenges in recognizing rotating/moving objects and distinguishing colors.\",\n",
       "  629: 'Object pose estimation plays a crucial role in robotic manipulation, however, its practical applicability still suffers from limited generalizability. This paper addresses the challenge of generalizable object pose estimation, particularly focusing on category-level object pose estimation for unseen object categories. Current methods either require impractical instance-level training or are confined to predefined categories, limiting their applicability. We propose VFM-6D, a novel framework that explores harnessing existing vision and language models, to elaborate object pose estimation into two stages: category-level object viewpoint estimation and object coordinate map estimation. Based on the two-stage framework, we introduce a 2D-to-3D feature lifting module and a shape-matching module, both of which leverage pre-trained vision foundation models to improve object representation and matching accuracy. VFM-6D is trained on cost-effective synthetic data and exhibits superior generalization capabilities. It can be applied to both instance-level unseen object pose estimation and category-level object pose estimation for novel categories. Evaluations on benchmark datasets demonstrate the effectiveness and versatility of VFM-6D in various real-world scenarios.',\n",
       "  630: 'In this paper, we explore the properties of loss curvature with respect to input data in deep neural networks. Curvature of loss with respect to input (termed input loss curvature) is the trace of the Hessian of the loss with respect to the input. We investigate how input loss curvature varies between train and test sets, and its implications for train-test distinguishability. We develop a theoretical framework that derives an upper bound on the train-test distinguishability based on privacy and the size of the training set. This novel insight fuels the development of a new black box membership inference attack utilizing input loss curvature. We validate our theoretical findings through experiments in computer vision classification tasks, demonstrating that input loss curvature surpasses existing methods in membership inference effectiveness. Our analysis highlights how the performance of membership inference attack (MIA) methods varies with the size of the training set, showing that curvature-based MIA outperforms other methods on sufficiently large datasets. This condition is often met by real datasets, as demonstrated by our results on CIFAR10, CIFAR100, and ImageNet. These findings not only advance our understanding of deep neural network behavior but also improve the ability to test privacy-preserving techniques in machine learning.',\n",
       "  631: 'In image editing, Denoising Diffusion Implicit Models (DDIM) inversion has become a widely adopted method and is extensively used in various image editing approaches. The core concept of DDIM inversion stems from the deterministic sampling technique of DDIM, which allows the DDIM process to be viewed as an Ordinary Differential Equation (ODE) process that is reversible. This enables the prediction of corresponding noise from a reference image, ensuring that the restored image from this noise remains consistent with the reference image. Image editing exploits this property by modifying the cross-attention between text and images to edit specific objects while preserving the remaining regions. However, in the DDIM inversion, using the $t-1$ time step to approximate the noise prediction at time step $t$ introduces errors between the restored image and the reference image. Recent approaches have modeled each step of the DDIM inversion process as finding a fixed-point problem of an implicit function. This approach significantly mitigates the error in the restored image but lacks theoretical support regarding the existence of such fixed points. Therefore, this paper focuses on the study of fixed points in DDIM inversion and provides theoretical support. Based on the obtained theoretical insights, we further optimize the loss function for the convergence of fixed points in the original DDIM inversion, improving the visual quality of the edited image. Finally, we extend the fixed-point based image editing to the application of unsupervised image dehazing, introducing a novel text-based approach for unsupervised dehazing.',\n",
       "  632: 'Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality. Recent advancements have utilized the diffusion model to improve gesture synthesis. However, the high computational complexity of these techniques limits the application in reality. In this study, we explore the potential of state space models (SSMs).Direct application of SSMs in gesture synthesis encounters difficulties, which stem primarily from the diverse movement dynamics of various body parts. The generated gestures may also exhibit unnatural jittering issues.To address these, we implement a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures.Built upon the selective scan mechanism, we introduce MambaTalk, which integrates hybrid fusion modules, local and global scans to refine latent space representations.Subjective and objective experiments demonstrate that our method surpasses the performance of state-of-the-art models. Our project is publicly available at~\\\\url{https://kkakkkka.github.io/MambaTalk/}.',\n",
       "  633: 'Given a graph $G$ and a positive integer $k$, the Graphlet Sampling problem asks to sample a connected induced $k$-vertex subgraph of $G$ uniformly at random.Graphlet sampling enhances machine learning applications by transforming graph structures into feature vectors for tasks such as graph classification and subgraph identification, boosting neural network performance, and supporting clustered federated learning by capturing local structures and relationships.A recent work has shown that the problem admits an algorithm that preprocesses $G$ in time $O(nk^2 \\\\log k + m)$, and draws one sample in expected time $k^{O(k)} \\\\log n$, where $n=|V(G)|$ and $m=|E(G)|$. Such an algorithm relies on the assumption that the input graph fits into main memory and it does not seem to be straightforward to adapt it to very large graphs. We consider Graphlet Sampling in the semi-streaming setting, where we have a memory of $M = \\\\Omega(n \\\\log n)$ words, and $G$ can be only read through sequential passes over the edge list. We develop a semi-streaming algorithm that preprocesses $G$ in $p={O}(\\\\log n)$ passes and samples $\\\\Theta(M k^{-O(k)})$ independent uniform $k$-graphlets in $O(k)$ passes. For constant $k$, both phases run in time $O((n+m)\\\\log n)$. We also show that the tradeoff between memory and number of passes of our algorithms is near-optimal. Our extensive evaluation on very large graphs shows the effectiveness of our algorithms.',\n",
       "  634: 'Human image animation involves generating videos from a character photo, allowing user control and unlocking the potential for video and movie production. While recent approaches yield impressive results using high-quality training data, the inaccessibility of these datasets hampers fair and transparent benchmarking. Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation. To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data. For the real-world data, we compile a vast collection of real-world videos from the internet. We developed and applied careful filtering rules to ensure video quality, resulting in a curated collection of 20K high-resolution (1080P) human-centric videos. Human and camera motion annotation is accomplished using a 2D pose estimator and a SLAM-based method. To expand our synthetic dataset, we collected 10K 3D avatar assets and leveraged existing assets of body shapes, skin textures and clothings. Notably, we introduce a rule-based camera trajectory generation method, enabling the synthetic pipeline to incorporate diverse and precise camera motion annotation, which can rarely be found in real-world data. To verify the effectiveness of HumanVid, we establish a baseline model named CamAnimate, short for Camera-controllable Human Animation, that considers both human and camera motions as conditions. Through extensive experimentation, we demonstrate that such simple baseline training on our HumanVid achieves state-of-the-art performance in controlling both human pose and camera motions, setting a new benchmark. Demo, data and code could be found in the project website: https://humanvid.github.io/.',\n",
       "  635: 'The premise of semi-supervised learning (SSL) is that combining labeled and unlabeled data yields significantly more accurate models.Despite empirical successes, the theoretical understanding of SSL is still far from complete. In this work, we study SSL for high dimensional sparse Gaussian classification. To construct an accurate classifier a  key task is feature selection, detecting the few variables that separate the two classes.For this SSL setting, we analyze information theoretic lower bounds for accurate feature selection as well as computational lower bounds, assuming the low-degree likelihood hardness conjecture. Our key contribution is the identification of a regime in the problem parameters (dimension, sparsity, number of labeled and unlabeled samples) where SSL is guaranteed to be advantageous for classification.Specifically, there is a regime where it is possible to construct in polynomial time an accurate SSL classifier.However, any computationally efficient supervised or unsupervised learning schemes, that separately use only the labeled or unlabeled data would fail.  Our work highlights the provable benefits of combining labeled and unlabeled data for classification and feature selection in high dimensions. We present simulations that complement our theoretical analysis.',\n",
       "  636: 'Large language models (LLMs) have achieved significant success across various domains. However, training these LLMs typically involves substantial memory and computational costs during both forward and backward propagation. While parameter-efficient fine-tuning (PEFT) considerably reduces the training memory associated with parameters, it does not address the significant computational costs and activation memory. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs and activation memory while maintaining accuracy. DropBP randomly drops layers during backward propagation, which is essentially equivalent to training shallow submodules generated by undropped layers and residual connections. Additionally, DropBP calculates the sensitivity of each layer to assign an appropriate drop rate, thereby stabilizing the training process. DropBP is not only applicable to full fine-tuning but can also be orthogonally integrated with all types of PEFT by dropping layers during backward propagation. Specifically, DropBP can reduce training time by 44% with comparable accuracy to the baseline, accelerate convergence to the same perplexity by 1.5$\\\\times$, and enable training with a sequence length 6.2$\\\\times$ larger on a single NVIDIA-A100 GPU. Furthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100 GPU and 117% on an Intel Gaudi2 HPU. The code is available at [https://github.com/WooSunghyeon/dropbp](https://github.com/WooSunghyeon/dropbp).',\n",
       "  637: 'Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains including Embodied AI. In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, including both continuous and discrete actions. For continuous actions, a set of learned tokenizations that capture an action at various resolutions allows for sufficient modeling precision, yielding the best performance on downstream tasks. For discrete actions, semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance. We arrive at these lessons via a thorough study of seven action grounding approaches on five different environments, encompassing over 114 embodied tasks.',\n",
       "  638: \"We consider a clustering problem where a learner seeks to partition a finite set by querying a faulty oracle. This models applications where learners crowdsource information from non-expert human workers or conduct noisy experiments to determine group structure. The learner aims to exactly recover a partition by submitting queries of the form ``are $u$ and $v$ in the same group?'' for any pair of elements $u$ and $v$ in the set. Moreover, because the learner only has access to faulty sources of information, they require an error-tolerant algorithm for this task: i.e. they must fully recover the correct partition, even if up to $\\\\ell$ answers are incorrect, for some error-tolerance parameter $\\\\ell$. We study the question: for any given error-tolerance $\\\\ell$, what is the minimum number of queries needed to learn a finite set partition of $n$ elements into $k$ groups? We design algorithms for this task and prove that they achieve optimal query complexity. To analyze our algorithms, we first highlight a  connection between this task and correlation clustering. We then use this connection to build a Rényi-Ulam style analytical framework for this problem, which yields matching lower bounds. Our analysis also reveals an inherent asymmetry between the query complexity necessary to be robust against false negative errors as opposed to false positive errors.\",\n",
       "  639: 'In order to reduce the computational complexity of large language models, great efforts have been made to to improve the efficiency of transformer models such as linear attention and flash-attention. However, the model size and corresponding computational complexity are constantly scaled up in pursuit of higher performance. In this work, we present MemoryFormer, a novel transformer architecture which significantly reduces the computational complexity (FLOPs) from a new perspective.  We eliminate nearly all the computations of the transformer model except for the necessary computation required by the multi-head attention operation. This is made possible by utilizing an alternative method for feature transformation to replace the linear projection of fully-connected layers. Specifically, we first construct a group of in-memory lookup tables that store a large amount of discrete vectors to replace the weight matrix used in linear projection. We then use a hash algorithm to retrieve a correlated subset of vectors dynamically based on the input embedding. The retrieved vectors combined together will form the output embedding, which provides an estimation of the result of matrix multiplication operation in a fully-connected layer. Compared to conducting matrix multiplication, retrieving data blocks from memory is a much cheaper operation which requires little computations. We train MemoryFormer from scratch and conduct extensive experiments on various benchmarks to demonstrate the effectiveness of the proposed model.',\n",
       "  640: 'The Janus Problem is a common issue in SDS-based text-to-3D methods. Due to view encoding approach and 2D diffusion prior guidance, the 3D representation model tends to learn content with higher certainty from each perspective, leading to view inconsistency. In this work, we first model and analyze the problem, visualizing the specific causes of the Janus Problem, which are associated with discrete view encoding and shared priors in 2D lifting. Based on this, we further propose the LCGen method, which guides text-to-3D to obtain different priors with different certainty from various viewpoints, aiding in view-consistent generation. Experiments have proven that our LCGen method can be directly applied to different SDS-based text-to-3D methods, alleviating the Janus Problem without introducing additional information, increasing excessive training burden, or compromising the generation effect.',\n",
       "  641: 'Transformers have achieved significant success in natural language modeling because of their exceptional capabilities to combine contextual information and global knowledge, yet their theoretical basis remains unclear. In this paper, we first propose Sparse Contextual Bigram (SCB), a natural extension to the classical bigram model, where the generation of the next token depends on a sparse set of earlier positions determined by the last token. We investigate the training dynamics and sample complexity of learning SCB using a one-layer linear transformer with a gradient-based algorithm. We show that when trained from scratch, the training process can be split into an initial sample-intensive stage where the correlation is boosted from zero to a nontrivial value, followed by a more sample-efficient stage of further improvement. Additionally, we prove that, provided a nontrivial correlation between the downstream and pretraining tasks, finetuning from a pretrained model allows us to bypass the initial sample-intensive stage. We also empirically demonstrate that our algorithm can outperform SGD in our setting.',\n",
       "  642: \"Residual networks, as discrete approximations of Ordinary Differential Equations (ODEs), have inspired significant advancements in neural network design, including multistep methods, high-order methods, and multi-particle dynamical systems. The precision of the solution to ODEs significantly affects parameter optimization, thereby impacting model performance. In this work, we present a series of advanced explorations of Transformer architecture design to minimize the error compared to the true ``solution.'' First, we introduce a predictor-corrector learning framework to minimize truncation errors, which consists of a high-order predictor and a multistep corrector. Second, we propose an exponential moving average-based coefficient learning method to strengthen our higher-order predictor. Extensive experiments on large-scale machine translation, abstractive summarization, language modeling, and natural language understanding benchmarks demonstrate the superiority of our approach. On the WMT'14 English-German and English-French tasks, our model achieved BLEU scores of 30.95 and 44.27, respectively. Furthermore, on the OPUS multilingual machine translation task, our model surpasses a robust 3.8B DeepNet by an average of 2.9 SacreBLEU, using only 1/3 parameters. Notably, it also beats LLama models by 5.7 accuracy points on the LM Harness Evaluation.\",\n",
       "  643: 'We study the limits and capability of public-data  assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\\\\epsilon,\\\\delta)$-PA-DP has excess risk $\\\\tilde{\\\\Omega}\\\\big(\\\\min(\\\\frac{1}{\\\\sqrt{n_{\\\\text{pub}}}},\\\\frac{1}{\\\\sqrt{n}}+\\\\frac{\\\\sqrt{d}}{n\\\\epsilon} ) \\\\big)$, where $d$ is the dimension, ${n_{\\\\text{pub}}}$ is the number of public samples, ${n_{\\\\text{priv}}}$ is the number of private samples, and $n={n_{\\\\text{pub}}}+{n_{\\\\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with \\\\textit{unlabeled} public samples. In contrast to our previous result, we here show novel methods for leveraging public data in private supervised learning. For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given $\\\\tilde{O}({n_{\\\\text{priv}}}\\\\epsilon)$ unlabeled public samples, achieves the dimension independent rate $\\\\tilde{O}\\\\big(\\\\frac{1}{\\\\sqrt{{n_{\\\\text{priv}}}}} + \\\\frac{1}{\\\\sqrt{{n_{\\\\text{priv}}}\\\\epsilon}}\\\\big)$. We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate. Finally, we provide extensions of this result to general hypothesis classes with finite \\\\textit{fat-shattering dimension} with applications to neural networks and non-Euclidean geometries.',\n",
       "  644: 'This paper focuses on the optimization of overparameterized, non-convex low-rank matrix sensing (LRMS)—an essential component in contemporary statistics and machine learning. Recent years have witnessed significant breakthroughs in first-order methods, such as gradient descent, for tackling this non-convex optimization problem. However, the presence of numerous saddle points often prolongs the time required for gradient descent to overcome these obstacles. Moreover, overparameterization can markedly decelerate gradient descent methods, transitioning its convergence rate from linear to sub-linear.  In this paper, we introduce an approximated Gauss-Newton (AGN) method for tackling the non-convex LRMS problem. Notably, AGN incurs a computational cost comparable to gradient descent per iteration but converges much faster without being slowed down by saddle points. We prove that, despite the non-convexity of the objective function, AGN achieves Q-linear convergence from random initialization to the global optimal solution. The global Q-linear convergence of AGN represents a substantial enhancement over the convergence of the existing methods for the overparameterized non-convex LRMS. The code for this paper is available at \\\\url{https://github.com/hsijiaxidian/AGN}.',\n",
       "  645: \"Procedurally generated environments such as Procgen Benchmark provide a testbed for evaluating the agent's ability to robustly learn a relevant skill, by situating the agent in ever-changing levels. The diverse levels associated with varying contexts are naturally connected to curriculum learning. Existing works mainly focus on arranging the levels to explicitly form a curriculum. In this work, we take a close look at the learning process itself under the multi-level training in Procgen. Interestingly, the learning process exhibits a gradual shift from easy contexts to hard contexts, suggesting an implicit curriculum in multi-level training. Our analysis is made possible through C-Procgen, a benchmark we build upon Procgen that enables explicit control of the contexts. We believe our findings will foster a deeper understanding of learning in diverse contexts, and our benchmark will benefit future research in curriculum reinforcement learning.\",\n",
       "  646: \"When learning vision-language models (VLM) for the fashion domain, most existing works design new architectures from vanilla BERT with additional objectives, or perform dense multi-task learning with fashion-specific tasks. Though progress has been made, their architecture or objectives are often intricate and the extendibility is limited.By contrast, with simple architecture (comprising only two unimodal encoders) and just the contrastive objective, popular pre-trained VL models (e.g., CLIP) achieve superior performance in general domains, which are further easily extended to downstream tasks.However, inheriting such benefits of CLIP in the fashion domain is non-trivial in the presence of the notable domain gap. Empirically, we find that directly finetuning on fashion data leads CLIP to frequently ignore minor yet important details such as logos and composition, which are critical in fashion tasks such as retrieval and captioning.In this work, to maintain CLIP's simple architecture and objective while explicitly attending to fashion details, we propose $E^2$: Easy Regional Contrastive Learning of Expressive Fashion Representations.$E^2$ introduces only a few selection tokens and fusion blocks (just 1.9\\\\% additional parameters in total) with only contrastive losses. Despite lightweight, in our primary focus, cross-modal retrieval, $E^2$ notably outperforms existing fashion VLMs with various fashion-specific objectives.Moreover, thanks to CLIP's widespread use in downstream tasks in general domains (e.g., zero-shot composed image retrieval and image captioning), our model can easily extend these models  from general domain to the fashion domain with notable improvement.To conduct a comprehensive evaluation, we further collect data from Amazon Reviews to build a new dataset for cross-modal retrieval in the fashion domain.\",\n",
       "  647: 'In this paper, we consider the problem of online monotone DR-submodular maximization subject to long-term stochastic constraints. Specifically, at each round $t\\\\in [T]$, after committing an action $\\\\mathbf{x}_t$, a random reward $f_t(\\\\mathbf{x}_t)$ and an unbiased gradient estimate of the point $\\\\widetilde{\\\\nabla}f_t(\\\\mathbf{x}_t)$ (semi-bandit feedback) are revealed. Meanwhile, a budget of $g_t(\\\\mathbf{x}_t)$, which is linear and stochastic, is consumed of its total allotted budget $B_T$. We propose a gradient ascent based algorithm that achieves $\\\\frac{1}{2}$-regret of $\\\\mathcal{O}(\\\\sqrt{T})$ with $\\\\mathcal{O}(T^{3/4})$ constraint violation with high probability. Moreover, when first-order full-information feedback is available, we propose an algorithm that achieves $(1-1/e)$-regret of $\\\\mathcal{O}(\\\\sqrt{T})$ with $\\\\mathcal{O}(T^{3/4})$ constraint violation. These algorithms significantly improve over the state-of-the-art in terms of query complexity.',\n",
       "  648: 'With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive. However, understanding long multimodal content, as a foundational ability for real-world applications, remains underexplored. In this work, we present Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents. Our benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning. In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. Code and benchmark are released at https://github.com/OpenGVLab/MM-NIAH.',\n",
       "  649: 'Single-cell transcriptomics has revolutionized our understanding of cellular heterogeneity and drug perturbation effects. However, its high cost and the vast chemical space of potential drugs present barriers to experimentally characterizing the effect of chemical perturbations in all the myriad cell types of the human body. To overcome these limitations, several groups have proposed using machine learning methods to directly predict the effect of chemical perturbations either across cell contexts or chemical space. However, advances in this field have been hindered by a lack of well-designed evaluation datasets and benchmarks. To drive innovation in perturbation modeling, the Open Problems Perturbation Prediction (OP3) benchmark introduces a framework for predicting the effects of small molecule perturbations on cell type-specific gene expression. OP3 leverages the Open Problems in Single-cell Analysis benchmarking infrastructure and is enabled by a new single-cell perturbation dataset, encompassing 146 compounds tested on human blood cells. The benchmark includes diverse data representations, evaluation metrics, and winning methods from our \"Single-cell perturbation prediction: generalizing experimental interventions to unseen contexts\" competition at NeurIPS 2023. We envision that the OP3 benchmark and competition will drive innovation in single-cell perturbation prediction by improving the accessibility, visibility, and feasibility of this challenge, thereby promoting the impact of machine learning in drug discovery.',\n",
       "  650: 'Articulated object manipulation in real images is a fundamental step in computer and robotic vision tasks. Recently, several image editing methods based on diffusion models have been proposed to manipulate articulated objects according to text prompts. However, these methods often generate weird artifacts or even fail in real images. To this end, we introduce the Part-Aware Diffusion Model to approach the manipulation of articulated objects in real images. First, we develop Abstract 3D Models to represent and manipulate articulated objects efficiently. Then we propose dynamic feature maps to transfer the appearance of objects from input images to edited ones, meanwhile generating the novel-appearing parts reasonably. Extensive experiments are provided to illustrate the advanced manipulation capabilities of our method concerning state-of-the-art editing works. Additionally, we verify our method on 3D articulated object understanding forembodied robot scenarios and the promising results prove that our method supports this task strongly. The project page is https://mvig-rhos.com/pa_diffusion.',\n",
       "  651: \"Motivated by the problem of detecting AI-generated text, we consider the problem of watermarking the output of language models with provable guarantees. We aim for watermarks which satisfy: (a) undetectability, a cryptographic notion introduced by Christ, Gunn, & Zamir (2023) which stipulates that it is computationally hard to distinguish watermarked language model outputs from the model's actual output distribution; and (b) robustness to channels which introduce a constant fraction of adversarial insertions, substitutions, and deletions to the watermarked text. Earlier schemes could only handle stochastic substitutions and deletions, and thus we are aiming for a more natural and appealing robustness guarantee that holds with respect to edit distance.  Our main result is a watermarking scheme which achieves both (a) and (b) when the alphabet size for the language model is allowed to grow as a polynomial in the security parameter. To derive such a scheme, we follow an approach introduced by Christ & Gunn (2024), which proceeds via first constructing pseudorandom codes satisfying undetectability and robustness properties analogous to those above; our codes have the additional benefit of relying on weaker computational assumptions than used in previous work.   Then we show that there is a generic transformation from such codes over large alphabets to watermarking schemes for arbitrary language models.\",\n",
       "  652: \"We present a generalization of Nesterov's accelerated gradient descent algorithm. Our algorithm (AGNES) provably achieves acceleration for smooth convex and strongly convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to the magnitude of the gradient at every point. Nesterov's method converges at an accelerated rate if the constant of proportionality is below 1, while AGNES accommodates any signal-to-noise ratio. The noise model is motivated by applications in overparametrized machine learning. AGNES requires only two parameters in convex and three in strongly convex minimization tasks, improving on existing methods. We further provide clear geometric interpretations and heuristics for the choice of parameters.\",\n",
       "  653: 'We introduce DiffAug, a simple and efficient diffusion-based augmentation technique to train image classifiers for the crucial yet challenging goal of improved classifier robustness. Applying DiffAug to a given example consists of one forward-diffusion step followed by one reverse-diffusion step. Using both ResNet-50 and Vision Transformer architectures, we comprehensively evaluate classifiers trained with DiffAug and demonstrate the surprising effectiveness of single-step reverse diffusion in improving robustness to covariate shifts, certified adversarial accuracy and out of distribution detection. When we combine DiffAug with other augmentations such as AugMix and DeepAugment we demonstrate further improved robustness. Finally, building on this approach, we also improve classifier-guided diffusion wherein we observe improvements in: (i) classifier-generalization, (ii) gradient quality (i.e., improved perceptual alignment) and (iii) image generation performance. We thus introduce a computationally efficient technique for training with improved robustness that does not require any additional data, and effectively complements existing augmentation approaches.',\n",
       "  654: 'In long-term time series forecasting (LTSF) tasks, an increasing number of works have acknowledged that discrete time series originate from continuous dynamic systems and have attempted to model their underlying dynamics. Recognizing the chaotic nature of real-world data, our model, Attraos, incorporates chaos theory into LTSF, perceiving real-world time series as low-dimensional observations from unknown high-dimensional chaotic dynamical systems. Under the concept of attractor invariance, Attraos utilizes non-parametric Phase Space Reconstruction embedding along with a novel multi-resolution dynamic memory unit to memorize historical dynamical structures, and evolves by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets with only one-twelfth of the parameters compared to PatchTST.',\n",
       "  655: 'Conducting experiments and gathering data for machine learning models is a complex and expensive endeavor, particularly when confronted with limited information. Typically, extensive _experiments_ to obtain features and labels come with a significant acquisition cost, making it impractical to carry out all of them. Therefore, it becomes crucial to strategically determine what to acquire to maximize the predictive performance while minimizing costs. To perform this task, existing data acquisition methods assume the availability of an initial dataset that is both fully-observed and labeled, crucially overlooking the **partial observability** of features characteristic of many real-world scenarios. In response to this challenge, we present Partially Observable Cost-Aware Active-Learning (POCA), a new learning approach aimed at improving model generalization in data-scarce and data-costly scenarios through label and/or feature acquisition. Introducing $\\\\mu$POCA as an instantiation, we maximise the uncertainty reduction in the predictive model when obtaining labels and features, considering associated costs. $\\\\mu$POCA enhance traditional Active Learning metrics based solely on the observed features by generating the unobserved features through Generative Surrogate Models, particularly Large Language Models (LLMs). We empirically validate $\\\\mu$POCA across diverse tabular datasets, varying data availability, acquisition costs, and LLMs.',\n",
       "  656: \"Federated learning (FL) is a learning paradigm that enables collaborative training of models using decentralized data. Recently, the utilization of pre-trained weight initialization in FL has been demonstrated to effectively improve model performance. However, the evolving complexity of current pre-trained models, characterized by a substantial increase in parameters, markedly intensifies the challenges associated with communication rounds required for their adaptation to FL. To address these communication cost issues and increase the performance of pre-trained model adaptation in FL, we propose an innovative model interpolation-based local training technique called ``Local Superior Soups.''Our method enhances local training across different clients, encouraging the exploration of a connected low-loss basin within a few communication rounds through regularized model interpolation. This approach acts as a catalyst for the seamless adaptation of pre-trained models in in FL.We demonstrated its effectiveness and efficiency across diverse widely-used FL datasets.\",\n",
       "  657: 'Vision transformers (ViTs) perform exceptionally well in various computer vision tasks but remain vulnerable to adversarial attacks. Recent studies have shown that the transferability of adversarial examples exists for CNNs, and the same holds true for ViTs. However, existing ViT attacks aggressively regularize the largest token gradients to exact zero within each layer of the surrogate model, overlooking the interactions between layers, which limits their transferability in attacking black-box models. Therefore, in this paper, we focus on boosting the transferability of adversarial attacks on ViTs through adaptive token tuning (ATT). Specifically, we propose three optimization strategies: an adaptive gradient re-scaling strategy to reduce the overall variance of token gradients, a self-paced patch out strategy to enhance the diversity of input tokens, and a hybrid token gradient truncation strategy to weaken the effectiveness of attention mechanism. We demonstrate that scaling correction of gradient changes using gradient variance across different layers can produce highly transferable adversarial examples. In addition, introducing attentional truncation can mitigate the overfitting over complex interactions between tokens in deep ViT layers to further improve the transferability. On the other hand, using feature importance as a guidance to discard a subset of perturbation patches in each iteration, along with combining self-paced learning and progressively more sampled attacks, significantly enhances the transferability over attacks that use all perturbation patches. Extensive experiments conducted on ViTs, undefended CNNs, and defended CNNs validate the superiority of our proposed ATT attack method. On average, our approach improves the attack performance by 10.1% compared to state-of-the-art transfer-based attacks. Notably, we achieve the best attack performance with an average of 58.3% on three defended CNNs. Code is available at https://github.com/MisterRpeng/ATT.',\n",
       "  658: 'There are an increasing number of domains in which artificial intelligence (AI) systems both surpass human ability and accurately model human behavior. This introduces the possibility of algorithmically-informed teaching in these domains through more relatable AI partners and deeper insights into human decision-making. Critical to achieving this goal, however, is coherently modeling human behavior at various skill levels. Chess is an ideal model system for conducting research into this kind of human-AI alignment, with its rich history as a pivotal testbed for AI research, mature superhuman AI systems like AlphaZero, and precise measurements of skill via chess rating systems. Previous work in modeling human decision-making in chess uses completely independent models to capture human style at different skill levels, meaning they lack coherence in their ability to adapt to the full spectrum of human improvement and are ultimately limited in their effectiveness as AI partners and teaching tools. In this work, we propose a unified modeling approach for human-AI alignment in chess that coherently captures human style across different skill levels and directly captures how people improve. Recognizing the complex, non-linear nature of human learning, we introduce a skill-aware attention mechanism to dynamically integrate players’ strengths with encoded chess positions, enabling our model to be sensitive to evolving player skill. Our experimental results demonstrate that this unified framework significantly enhances the alignment between AI and human players across a diverse range of expertise levels, paving the way for deeper insights into human decision-making and AI-guided teaching tools.',\n",
       "  659: 'The deep operator networks (DeepONet), a class of neural operators that learn mappings between function spaces, have recently been developed as surrogate models for parametric partial differential equations (PDEs). In this work we propose a derivative-enhanced deep operator network (DE-DeepONet), which leverages derivative information to enhance the solution prediction accuracy and provides a more accurate approximation of solution-to-parameter derivatives, especially when training data are limited. DE-DeepONet explicitly incorporates linear dimension reduction of high dimensional parameter input into DeepONet to reduce training cost and adds derivative loss in the loss function to reduce the number of required parameter-solution pairs. We further demonstrate that the use of derivative loss can be extended to enhance other neural operators, such as the Fourier neural operator (FNO). Numerical experiments validate the effectiveness of our approach.',\n",
       "  660: 'We propose a novel stochastic bandit algorithm that employs reward estimates using a tree ensemble model. Specifically, our focus is on a soft tree model, a variant of the conventional decision tree that has undergone both practical and theoretical scrutiny in recent years. By deriving several non-trivial properties of soft trees, we extend the existing analytical techniques used for neural bandit algorithms to our soft tree-based algorithm. We demonstrate that our algorithm achieves a smaller cumulative regret compared to the existing ReLU-based neural bandit algorithms. We also show that this advantage comes with a trade-off: the hypothesis space of the soft tree ensemble model is more constrained than that of a ReLU-based neural network.',\n",
       "  661: \"Recent advancements in Vision-Language Models (VLMs) have enabled complex multimodal tasks by processing text and image data simultaneously, significantly enhancing the field of artificial intelligence. However, these models often exhibit biases that can skew outputs towards societal stereotypes, thus necessitating debiasing strategies. Existing debiasing methods focus narrowly on specific modalities or tasks, and require extensive retraining. To address these limitations, this paper introduces Selective Feature Imputation for Debiasing (SFID), a novel methodology that integrates feature pruning and low confidence imputation (LCI) to effectively reduce biases in VLMs. SFID is versatile, maintaining the semantic integrity of outputs and costly effective by eliminating the need for retraining. Our experimental results demonstrate SFID's effectiveness across various VLMs tasks including zero-shot classification, text-to-image retrieval, image captioning, and text-to-image generation, by significantly reducing gender biases without compromising performance. This approach not only enhances the fairness of VLMs applications but also preserves their efficiency and utility across diverse scenarios.\",\n",
       "  662: 'Human languages support both semantic categorization and local pragmatic interactions that require context-sensitive reasoning about meaning. While semantics and pragmatics are two fundamental aspects of language, they are typically studied independently and their co-evolution is largely under-explored. Here, we aim to bridge this gap by studying how a shared lexicon may emerge from local pragmatic interactions. To this end, we extend a recent information-theoretic framework for emergent communication in artificial agents, which integrates utility maximization, associated with pragmatics, with general communicative constraints that are believed to shape human semantic systems. Specifically, we show how to adapt this framework to train agents via unsupervised pragmatic interactions, and then evaluate their emergent lexical semantics. We test this approach in a rich visual domain of naturalistic images, and find that key human-like properties of the lexicon emerge when agents are guided by both context-specific utility and general communicative pressures, suggesting that both aspects are crucial for understanding how language may evolve in humans and in artificial agents.',\n",
       "  663: 'We investigate the radioactivity of text generated by large language models (LLM), \\\\ie whether it is possible to detect that such synthetic input was used to train a subsequent LLM.Current methods like membership inference or active IP protection either work only in settings where the suspected text is known or do not provide reliable statistical guarantees.We discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM.Our new methods, specialized for radioactivity, detects with a provable confidence weak residuals of the watermark signal in the fine-tuned LLM.We link the radioactivity contamination level to the following properties: the watermark robustness, its proportion in the training set, and the fine-tuning process.For instance, if the suspect model is open-weight, we demonstrate that training on watermarked instructions can be detected with high confidence ($p$-value $< 10^{-5}$) even when as little as $5\\\\%$ of training text is watermarked.',\n",
       "  664: 'High-dimensional Bayesian optimization (BO) tasks such as molecular design often require $>10,$$000$ function evaluations before obtaining meaningful results. While methods like sparse variational Gaussian processes (SVGPs) reduce computational requirements in these settings, the underlying approximations result in suboptimal data acquisitions that slow the progress of optimization. In this paper we modify SVGPs to better align with the goals of BO: targeting informed data acquisition over global posterior fidelity. Using the framework of utility-calibrated variational inference (Lacoste–Julien et al., 2011), we unify GP approximation and data acquisition into a joint optimization problem, thereby ensuring optimal decisions under a limited computational budget. Our approach can be used with any decision-theoretic acquisition function and is readily compatible with trust region methods like TuRBO (Eriksson et al., 2019). We derive efficient joint objectives for the expected improvement (EI) and knowledge gradient (KG) acquisition functions in both the standard and batch BO settings. On a variety of recent high dimensional benchmark tasks in control and molecular design, our approach significantly outperforms standard SVGPs and is capable of achieving comparable rewards with up to $10\\\\times$ fewer function evaluations.',\n",
       "  665: 'As the volume of data invested in statistical learning increases and concerns regarding privacy grow, the privacy leakage issue has drawn significant attention. Differential privacy has emerged as a widely accepted concept capable of mitigating privacy concerns, and numerous differentially private (DP) versions of machine learning algorithms have been developed. However, existing works on DP kernel learning algorithms have exhibited practical limitations, including scalability, restricted choice of kernels, or dependence on test data availability. We propose DP scalable kernel empirical risk minimization (ERM) algorithms and a DP kernel mean embedding (KME) release algorithm suitable for general kernels. Our approaches address the shortcomings of previous algorithms by employing Nyström methods, classical techniques in non-private scalable kernel learning. These methods provide data-dependent low-rank approximations of the kernel matrix for general kernels in a DP manner. We present excess empirical risk bounds and computational complexities for the scalable kernel DP ERM, KME algorithms, contrasting them with established methodologies. Furthermore, we develop a private data-generating algorithm capable of learning diverse kernel models. We conduct experiments to demonstrate the performance of our algorithms, comparing them with existing methods to highlight their superiority.',\n",
       "  666: 'Diffusion models have been widely utilized for image restoration. However, previous blind image restoration methods still need to assume the type of degradation model while leaving the parameters to be optimized, limiting their real-world applications. Therefore, we aim to tame generative diffusion prior for universal blind image restoration dubbed BIR-D, which utilizes an optimizable convolutional kernel to simulate the degradation model and dynamically update the parameters of the kernel in the diffusion steps, enabling it to achieve blind image restoration results even in various complex situations. Besides, based on mathematical reasoning, we have provided an empirical formula for the chosen of adaptive guidance scale, eliminating the need for a grid search for the optimal parameter. Experimentally, Our BIR-D has demonstrated superior practicality and versatility than off-the-shelf unsupervised methods across various tasks both on real-world and synthetic datasets, qualitatively and quantitatively. BIR-D is able to fulfill multi-guidance blind image restoration. Moreover, BIR-D can also restore images that undergo multiple and complicated degradations, demonstrating the practical applications. The code is available at https://github.com/Tusiwei/BIR-D.',\n",
       "  667: 'Neuro-symbolic neural networks have been extensively studied to integrate symbolic operations with neural networks, thereby improving systematic generalization. Specifically, Tensor Product Representation (TPR) framework enables neural networks to perform differentiable symbolic operations by encoding the symbolic structure of data within vector spaces. However, TPR-based neural networks often struggle to decompose unseen data into structured TPR representations, undermining their symbolic operations. To address this decomposition problem, we propose a Discrete Dictionary-based Decomposition (D3) layer designed to enhance the decomposition capabilities of TPR-based models. D3 employs discrete, learnable key-value dictionaries trained to capture symbolic features essential for decomposition operations. It leverages the prior knowledge acquired during training to generate structured TPR representations by mapping input data to pre-learned symbolic features within these dictionaries. D3 is a straightforward drop-in layer that can be seamlessly integrated into any TPR-based model without modifications. Our experimental results demonstrate that D3 significantly improves the systematic generalization of various TPR-based models while requiring fewer additional parameters. Notably, D3 outperforms baseline models on the synthetic task that demands the systematic decomposition of unseen combinatorial data.',\n",
       "  668: \"We propose a novel text-to-video (T2V) generation benchmark, ChronoMagic-Bench, to evaluate the temporal and metamorphic knowledge skills in time-lapse video generation of the T2V models (e.g. Sora and Lumiere). Compared to existing benchmarks that focus on visual quality and text relevance of generated videos, ChronoMagic-Bench focuses on the models’ ability to generate time-lapse videos with significant metamorphic amplitude and temporal coherence. The benchmark probes T2V models for their physics, biology, and chemistry capabilities, in a free-form text control. For these purposes, ChronoMagic-Bench introduces 1,649 prompts and real-world videos as references, categorized into four major types of time-lapse videos: biological, human creation, meteorological, and physical phenomena, which are further divided into 75 subcategories. This categorization ensures a comprehensive evaluation of the models’ capacity to handle diverse and complex transformations. To accurately align human preference on the benchmark, we introduce two new automatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic attributes and temporal coherence. MTScore measures the metamorphic amplitude, reflecting the degree of change over time, while CHScore assesses the temporal coherence, ensuring the generated videos maintain logical progression and continuity. Based on the ChronoMagic-Bench, we conduct comprehensive manual evaluations of eighteen representative T2V models, revealing their strengths and weaknesses across different categories of prompts, providing a thorough evaluation framework that addresses current gaps in video generation research. More encouragingly, we create a large-scale ChronoMagic-Pro dataset, containing 460k high-quality pairs of 720p time-lapse videos and detailed captions. Each caption ensures high physical content and large metamorphic amplitude, which have a far-reaching impact on the video generation community. The source data and code are publicly available on https://pku-yuangroup.github.io/ChronoMagic-Bench.\",\n",
       "  669: 'While the field of 3D scene reconstruction is dominated by NeRFs due to their photorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged, offering similar quality with real-time rendering speeds. However, both methods primarily excel with well-controlled 3D scenes, while in-the-wild data - characterized by occlusions, dynamic objects, and varying illumination - remains challenging. NeRFs can adapt to such conditions easily through per-image embedding vectors, but 3DGS struggles due to its explicit representation and lack of shared parameters. To address this, we introduce WildGaussians, a novel approach to handle occlusions and appearance changes with 3DGS. By leveraging robust DINO features and integrating an appearance modeling module within 3DGS, our method achieves state-of-the-art results. We demonstrate that WildGaussians matches the real-time rendering speed of 3DGS while surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all within a simple architectural framework.',\n",
       "  670: 'Out-of-distribution (OOD) detection aims to identify OOD inputs from unknown classes, which is important for the reliable deployment of machine learning models in the open world. Various scoring functions are proposed to distinguish it from in-distribution (ID) data. However, existing methods generally focus on excavating the discriminative information from a single input, which implicitly limits its representation dimension. In this work, we introduce a novel perspective, i.e., employing different common corruptions on the input space, to expand that. We reveal an interesting phenomenon termed confidence mutation, where the confidence of OOD data can decrease significantly under the corruptions, while the ID data shows a higher confidence expectation considering the resistance of semantic features. Based on that, we formalize a new scoring method, namely, Confidence aVerage (CoVer), which can capture the dynamic differences by simply averaging the scores obtained from different corrupted inputs and the original ones, making the OOD and ID distributions more separable in detection tasks. Extensive experiments and analyses have been conducted to understand and verify the effectiveness of CoVer.',\n",
       "  671: 'We present RelBench, a public benchmark for solving predictive tasks in relational databases with deep learning.  RelBench provides databases and tasks spanning diverse domains, scales, and database dimensions, and is intended to be a foundational infrastructure for future research in this direction. We use RelBench to conduct the first comprehensive empirical study of graph neural network (GNN) based predictive models on relational data, as recently proposed by Fey et al. 2024.  End-to-end learned GNNs are capable fully exploiting the predictive signal encoded in links between entities, marking a significant shift away from the dominant paradigm of manual feature engineering combined with tabular machine learning. To thoroughly evaluate GNNs against the prior gold-standard we conduct a user study, where an experienced data scientist manually engineers features for each task. In this study, GNNs learn better models whilst reducing human work needed by more than an order of magnitude. This result demonstrates the power of GNNs for solving predictive tasks in relational databases, opening up new research opportunities.',\n",
       "  672: 'Link prediction is a critical problem in graph learning with broad applications such as recommender systems and knowledge graph completion. Numerous research efforts have been directed at solving this problem, including approaches based on similarity metrics and Graph Neural Networks (GNN). However, most existing solutions are still rooted in conventional supervised learning, which makes it challenging to adapt over time to changing customer interests and to address the inherent dilemma of exploitation versus exploration in link prediction.To tackle these challenges, this paper reformulates link prediction as a sequential decision-making process, where each link prediction interaction occurs sequentially. We propose a novel fusion algorithm, PRB (PageRank Bandits), which is the first to combine contextual bandits with PageRank for collaborative exploitation and exploration. We also introduce a new reward formulation and provide a theoretical performance guarantee for PRB. Finally, we extensively evaluate PRB in both online and offline settings, comparing it with bandit-based and graph-based methods. The empirical success of PRB demonstrates the value of the proposed fusion approach. Our code is released at https://github.com/jiaruzouu/PRB.',\n",
       "  673: 'Recent advancements in 2D/3D generative techniques have facilitated the generation of dynamic 3D objects from monocular videos. Previous methods mainly rely on the implicit neural radiance fields (NeRF) or explicit Gaussian Splatting as the underlying representation, and struggle to achieve satisfactory spatial-temporal consistency and surface appearance. Drawing inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a novel framework combining mesh representation with geometric skinning technique to generate high-quality 4D object from a monocular video. Instead of utilizing classical texture map for appearance, we bind Gaussian splats to triangle face of mesh for differentiable optimization of both the texture and mesh vertices. In particular, DreamMesh4D begins with a coarse mesh obtained through an image-to-3D generation procedure. Sparse points are then uniformly sampled across the mesh surface, and are used to build a deformation graph to drive the motion of the 3D object for the sake of computational efficiency and providing additional constraint. For each step, transformations of sparse control points are predicted using a deformation network, and the mesh vertices as well as the surface Gaussians are deformed via a novel geometric skinning algorithm. The skinning algorithm is a hybrid approach combining LBS (linear blending skinning) and DQS (dual-quaternion skinning), mitigating drawbacks associated with both approaches. The static surface Gaussians and mesh vertices as well as the dynamic deformation network are learned via reference view photometric loss, score distillation loss as well as other regularization losses in a two-stage manner. Extensive experiments demonstrate superior performance of our method in terms of both rendering quality and spatial-temporal consistency. Furthermore, our method is compatible with modern graphic pipelines, showcasing its potential in the 3D gaming and film industry.',\n",
       "  674: 'The spiking camera is an emerging neuromorphic vision sensor that records high-speed motion scenes by asynchronously firing continuous binary spike streams. Prevailing image reconstruction methods, generating intermediate frames from these spike streams, often rely on complex step-by-step network architectures that overlook the intrinsic collaboration of spatio-temporal complementary information. In this paper, we propose an efficient spatio-temporal interactive reconstruction network to jointly perform inter-frame feature alignment and intra-frame feature filtering in a coarse-to-fine manner. Specifically, it starts by extracting hierarchical features from a concise hybrid spike representation, then refines the motion fields and target frames scale-by-scale, ultimately obtaining a full-resolution output. Meanwhile, we introduce a symmetric interactive attention block and a multi-motion field estimation block to further enhance the interaction capability of the overall network. Experiments on synthetic and real-captured data show that our approach exhibits excellent performance while maintaining low model complexity.',\n",
       "  675: 'Frequently, the burgeoning field of black-box optimization encounters challenges due to a limited understanding of the mechanisms of the objective function. To address such problems, in this work we focus on the deterministic concept of Order Oracle, which only utilizes order access between function values (possibly with some bounded noise), but without assuming access to their values. As theoretical results, we propose a new approach to create non-accelerated optimization algorithms (obtained by integrating Order Oracle into existing optimization “tools”) in non-convex, convex, and strongly convex settings that are as good as both SOTA coordinate algorithms with first-order oracle and SOTA algorithms with Order Oracle up to logarithm factor. Moreover, using the proposed approach, we provide the first accelerated optimization algorithm using the Order Oracle. And also, using an already different approach we provide the asymptotic convergence of the first algorithm with the stochastic Order Oracle concept. Finally, our theoretical results demonstrate effectiveness of proposed algorithms through numerical experiments.',\n",
       "  676: \"Inverse reinforcement learning (IRL) aims to infer a reward from expert demonstrations, motivated by the idea that the reward, rather than the policy, is the most succinct and transferable description of a task [Ng et al., 2000]. However, the reward corresponding to an optimal policy is not unique, making it unclear if an IRL-learned reward is transferable to new transition laws in the sense that its optimal policy aligns with the optimal policy corresponding to the expert's true reward. Past work has addressed this problem only under the assumption of full access to the expert's policy, guaranteeing transferability when learning from two experts with the same reward but different transition laws that satisfy a specific rank condition [Rolland et al., 2022]. In this work, we show that the conditions developed under full access to the expert's policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert. Instead of a binary rank condition, we propose principal angles as a more refined measure of similarity and dissimilarity between transition laws. Based on this, we then establish two key results: 1) a sufficient condition for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws, and 2) a sufficient condition for transferability to local changes in the transition law when learning from a single expert. Furthermore, we also provide a probably approximately correct (PAC) algorithm and an end-to-end analysis for learning transferable rewards from demonstrations of multiple experts.\",\n",
       "  677: 'Text-to-image diffusion models benefit artists with high-quality image generation. Yet their stochastic nature hinders artists from creating consistent images of the same subject. Existing methods try to tackle this challenge and generate consistent content in various ways. However, they either depend on external restricted data or require expensive tuning of the diffusion model. For this issue, we propose a novel one-shot tuning paradigm, termed OneActor. It efficiently performs consistent subject generation solely driven by prompts via a learned semantic guidance to bypass the laborious backbone tuning. We lead the way to formalize the objective of consistent subject generation from a clustering perspective, and thus design a cluster-conditioned model. To mitigate the overfitting challenge shared by one-shot tuning pipelines, we augment the tuning with auxiliary samples and devise two inference strategies: semantic interpolation and cluster guidance. These techniques are later verified to significantly improve the generation quality. Comprehensive experiments show that our method outperforms a variety of baselines with satisfactory subject consistency, superior prompt conformity as well as high image quality. Our method is capable of multi-subject generation and compatible with popular diffusion extensions. Besides, we achieve a $4\\\\times$ faster tuning speed than tuning-based baselines and, if desired, avoid increasing the inference time. Furthermore, our method can be naturally utilized to pre-train a consistent subject generation network from scratch, which will implement this research task into more practical applications. (Project page: https://johnneywang.github.io/OneActor-webpage/)',\n",
       "  678: 'As ML models become increasingly complex and integral to high-stakes domains such as finance and healthcare, they also become more susceptible to sophisticated adversarial attacks. We investigate the threat posed by undetectable backdoors, as defined in Goldwasser et al. [2022], in models developed by insidious external expert firms. When such backdoors exist, they allow the designer of the model to sell information on how to slightly perturb their input to change the outcome of the model. We develop a general strategy to plant backdoors to obfuscated neural networks, that satisfy the security properties of the celebrated notion of indistinguishability obfuscation. Applying obfuscation before releasing neural networks is a strategy that is well motivated to protect sensitive information of the external expert firm. Our method to plant backdoors ensures that even if the weights and architecture of the obfuscated model are accessible, the existence ofthe backdoor is still undetectable. Finally, we introduce the notion of undetectable backdoors to language models and extend our neural network backdoor attacks to such models based on the existence of steganographic functions.',\n",
       "  679: 'Transformer models have gained significant attention due to their power in machine learning tasks. Their extensive deployment has raised concerns about the potential leakage of sensitive information during inference. However, when being applied to Transformers, existing approaches based on secure two-party computation (2PC) bring about efficiency limitations in two folds: (1) resource-intensive matrix multiplications in linear layers, and (2) complex non-linear activation functions like $\\\\mathsf{GELU}$ and $\\\\mathsf{Softmax}$. This work presents a new two-party inference framework $\\\\mathsf{Nimbus}$ for Transformer models. Specifically, we propose a new 2PC paradigm to securely compute matrix multiplications based on an outer-product insight, which achieves $2.9\\\\times \\\\sim 12.5\\\\times$ performance improvements compared to the state-of-the-art (SOTA) protocol. Furthermore, through a new observation of utilizing the input distribution, we propose an approach of low-degree polynomial approximation for $\\\\mathsf{GELU}$ and $\\\\mathsf{Softmax}$, which improves the performance of the SOTA polynomial approximation by $2.9\\\\times \\\\sim 4.0\\\\times$, where the average accuracy loss of our approach is 0.08\\\\% compared to the non-2PC inference without privacy. Compared with the SOTA two-party inference, $\\\\mathsf{Nimbus}$ improves the end-to-end performance of $BERT_{base}$ inference by $2.7\\\\times \\\\sim 4.7\\\\times$ across different network settings.',\n",
       "  680: \"Disaggregated evaluation—estimation of performance of a machine learning model on different subpopulations—is a core task when assessing performance and group-fairness of AI systems.A key challenge is that evaluation data is scarce, and subpopulations arising from intersections of attributes (e.g., race, sex, age) are often tiny.Today, it is common for multiple clients to procure the same AI model from a model developer, and the task of disaggregated evaluation is faced by each customer individually.  This gives rise to what we call the multi-task disaggregated evaluation problem, wherein multiple clients seek to conduct a disaggregated evaluation of a given model in their own data setting (task).  In this work we develop a disaggregated evaluation method called SureMap that has high estimation accuracy for both multi-task and single-task disaggregated evaluations of blackbox models.  SureMap's efficiency gains come from(1) transforming the problem into structured simultaneous Gaussian mean estimation and (2) incorporating external data, e.g., from the AI system creator or from their other clients.  Our method combines maximum a posteriori (MAP) estimation using a well-chosen prior together with cross-validation-free tuning via Stein's unbiased risk estimate (SURE).We evaluate SureMap on disaggregated evaluation tasks in multiple domains, observing significant accuracy improvements over several strong competitors.\",\n",
       "  681: 'Binary analysis is a core component of many critical security tasks, including reverse engineering, malware analysis, and vulnerability detection. Manual analysis is often time-consuming, but identifying commonly-used or previously-seen functions can reduce the time it takes to understand a new file. However, given the complexity of assembly, and the NP-hard nature of determining function equivalence, this task is extremely difficult. Common approaches often use sophisticated disassembly and decompilation tools, graph analysis, and other expensive pre-processing steps to perform function similarity searches over some corpus. In this work, we identify a number of discrepancies between the current research environment and the underlying application need. To remedy this, we build a new benchmark, REFuSe-Bench, for binary function similarity detection consisting of high-quality datasets and tests that better reflect real-world use cases. In doing so, we address issues like data duplication and accurate labeling, experiment with real malware, and perform the first serious evaluation of ML binary function similarity models on Windows data. Our benchmark reveals that a new, simple baseline — one which looks at only the raw bytes of a function, and requires no disassembly or other pre-processing --- is able to achieve state-of-the-art performance in multiple settings. Our findings challenge conventional assumptions that complex models with highly-engineered features are being used to their full potential, and demonstrate that simpler approaches can provide significant value.',\n",
       "  682: \"The joint prediction of continuous fields and statistical estimation of the underlying discrete parameters is a common problem for many physical systems, governed by PDEs. Hitherto, it has been separately addressed by employing operator learning surrogates for field prediction while using simulation-based inference (and its variants) for statistical parameter determination. Here, we argue that solving both problems within the same framework can lead to consistent gains in accuracy and robustness. To this end, we propose a novel and flexible formulation of the operator learning problem that jointly predicts continuous quantities and infers distributions of discrete parameters, thereby amortizing the cost of both the inverse and the surrogate models to a joint pre-training step. We present the capabilities of the proposed methodology for predicting continuous and discrete biomarkers in full-body haemodynamics simulations under different levels of missing information. We also consider a test case for atmospheric large-eddy simulation of a two-dimensional dry cold bubble, where we infer both continuous time-series and information about the system's conditions. We present comparisons against different baselines to showcase significantly increased accuracy in both the inverse and the surrogate tasks.\",\n",
       "  683: 'In this work, we investigate stochastic approximation (SA) with Markovian data and nonlinear updates under constant stepsize $\\\\alpha>0$. Existing work has primarily focused on either i.i.d. data or linear update rules. We take a new perspective and carefully examine the simultaneous presence of Markovian dependency of data and nonlinear update rules, delineating how the interplay between these two structures leads to complications that are not captured by prior techniques. By leveraging the smoothness and recurrence properties of the SA updates, we develop a fine-grained analysis of the correlation between the SA iterates $\\\\theta_k$ and Markovian data $x_k$. This enables us to overcome the obstacles in existing analysis and establish for the first time the weak convergence of the joint process $(x_k, \\\\theta_k)$. Furthermore, we present a precise characterization of the asymptotic bias of the SA iterates, given by $\\\\mathbb{E}[\\\\theta_\\\\infty]-\\\\theta^\\\\ast=\\\\alpha(b_\\\\textup{m}+b_\\\\textup{n}+b_\\\\textup{c})+\\\\mathcal{O}(\\\\alpha^{3/2})$. Here, $b_\\\\textup{m}$ is associated with the Markovian noise, $b_\\\\textup{n}$ is tied to the nonlinearity of the SA operator, and notably, $b_\\\\textup{c}$ represents a multiplicative interaction between the Markovian noise and the nonlinearity of the operator, which is absent in previous works. As a by-product of our analysis, we derive finite-time bounds on higher moment $\\\\mathbb{E}[||\\\\theta_k-\\\\theta^\\\\ast||^{2p}]$ and present non-asymptotic geometric convergence rates for the iterates, along with a Central Limit Theorem.',\n",
       "  684: \"AI models are increasingly prevalent in high-stakes environments, necessitating thorough assessment of their capabilities and risks. Benchmarks are popular for measuring these attributes and for comparing model performance, tracking progress, and identifying weaknesses in foundation and non-foundation models. They can inform model selection for downstream tasks and influence policy initiatives. However, not all benchmarks are the same: their quality depends on their design and usability. In this paper, we develop an assessment framework considering 40 best practices across a benchmark's life cycle and evaluate 25 AI benchmarks against it. We find that there exist large quality differences and that commonly used benchmarks suffer from significant issues. We further find that most benchmarks do not report statistical significance of their results nor can results be easily replicated. To support benchmark developers in aligning with best practices, we provide a checklist for minimum quality assurance based on our assessment. We also develop a living repository of benchmark assessments to support benchmark comparability.\",\n",
       "  685: 'Large-scale simulation models of complex socio-technical systems provide decision-makers with high-fidelity testbeds in which policy interventions can be evaluated and what-if scenarios explored. Unfortunately, the high computational cost of such models inhibits their widespread use in policy-making settings. Surrogate models can address these computational limitations, but to do so they must behave consistently with the simulator under interventions of interest. In this paper, we build upon recent developments in causal abstractions to develop a framework for learning interventionally consistent surrogate models for large-scale, complex simulation models. We provide theoretical results showing that our proposed approach induces surrogates to behave consistently with high probability with respect to the simulator across interventions of interest, facilitating rapid experimentation with policy interventions in complex systems. We further demonstrate with empirical studies that conventionally trained surrogates can misjudge the effect of interventions and misguide decision-makers towards suboptimal interventions, while surrogates trained for interventional consistency with our method closely mimic the behaviour of the original simulator under interventions of interest.',\n",
       "  686: \"Over half of cancer patients experience long-term pain management challenges. Recently, interest has grown in systems for cancer pain treatment effectiveness assessment (TEA) and medication recommendation (MR) to optimize pharmacological care. These systems aim to improve treatment effectiveness by recommending personalized medication plans based on comprehensive patient information. Despite progress, current systems lack multidisciplinary treatment (MDT) team assessments of treatment and the patient's perception of medication, crucial for effective cancer pain management. Moreover, managing cancer pain medication requires multiple adjustments to the treatment plan based on the patient's evolving condition, a detail often missing in existing datasets. To tackle these issues, we designed the PEACE dataset specifically for cancer pain medication research. It includes detailed pharmacological care records for over 38,000 patients, covering demographics, clinical examination, treatment outcomes, medication plans, and patient self-perceptions. Unlike existing datasets, PEACE records not only long-term and multiple follow-ups both inside and outside hospitals but also includes patients' self-assessments of medication effects and the impact on their lives. We conducted a proof-of-concept study with 13 machine learning algorithms on the PEACE dataset for the TEA (classification task) and MR (regression task). These experiments provide valuable insights into the potential of the PEACE dataset for advancing personalized cancer pain management. The dataset is accessible at: [https://github.com/YTYTYD/PEACE].\",\n",
       "  687: 'This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with sparse depth annotations to facilitate future research. Models are available at https://github.com/DepthAnything/Depth-Anything-V2.',\n",
       "  688: 'Developing meaningful and efficient representations that separate the fundamental structure of the data generation mechanism is crucial in representation learning. However, Disentangled Representation Learning has not fully shown its potential on real images, because of correlated generative factors, their resolution and limited access to ground truth labels. Specifically on the latter, we investigate the possibility of leveraging synthetic data to learn general-purpose disentangled representations applicable to real data, discussing the effect of fine-tuning and what properties of disentanglement are preserved after the transfer. We provide an extensive empirical study to address these issues. In addition, we propose a new interpretable intervention-based metric, to measure the quality of factors encoding in the representation. Our results indicate that some level of disentanglement, transferring a representation from synthetic to real data, is possible and effective.',\n",
       "  689: 'Recent advances in immunology and synthetic biology have accelerated the development of deep generative methods for DNA sequence design. Two dominant approaches in this field are AutoRegressive (AR) models and Diffusion Models (DMs). However, genomic sequences are functionally heterogeneous, consisting of multiple connected regions (e.g., Promoter Regions, Exons, and Introns) where elements within each region come from the same probability distribution, but the overall sequence is non-homogeneous. This heterogeneous nature presents challenges for a single model to accurately generate genomic sequences. In this paper, we analyze the properties of AR models and DMs in heterogeneous genomic sequence generation, pointing out crucial limitations in both methods: (i) AR models capture the underlying distribution of data by factorizing and learning the transition probability but fail to capture the global property of DNA sequences. (ii) DMs learn to recover the global distribution but tend to produce errors at the base pair level. To overcome the limitations of both approaches, we propose a post-training sampling method, termed Absorb & Escape (A&E) to perform compositional generation from AR models and DMs. This approach starts with samples generated by DMs and refines the sample quality using an AR model through the alternation of the Absorb and Escape steps.  To assess the quality of generated sequences, we conduct extensive experiments on 15 species for conditional and unconditional DNA generation. The experiment results from motif distribution, diversity checks, and genome integration tests unequivocally show that A&E outperforms state-of-the-art AR models and DMs in genomic sequence generation. A&E does not suffer from the slowness of traditional MCMC to sample from composed distributions with Energy-Based Models whilst it obtains higher quality samples than single models. Our research sheds light on the limitations of current single-model approaches in DNA generation and provides a simple but effective solution for heterogeneous sequence generation. Code is available at the Github Repo.',\n",
       "  690: 'The generalization ability of deepfake detectors is vital for their applications in real-world scenarios. One effective solution to enhance this ability is to train the models with manually-blended data, which we termed \\'\\'blendfake\\'\\', encouraging models to learn generic forgery artifacts like blending boundary. Interestingly, current SoTA methods utilize blendfake $\\\\textit{without}$ incorporating any deepfake data in their training process. This is likely because previous empirical observations suggest that vanilla hybrid training (VHT), which combines deepfake and blendfake data, results in inferior performance to methods using only blendfake data (so-called \"1+1<2\"). Therefore, a critical question arises: Can we leave deepfake behind and rely solely on blendfake data to train an effective deepfake detector? Intuitively, as deepfakes also contain additional informative forgery clues ($\\\\textit{e.g.,}$ deep generative artifacts), excluding all deepfake data in training deepfake detectors seems counter-intuitive. In this paper, we rethink the role of blendfake in detecting deepfakes and formulate the process from \"real to blendfake to deepfake\" to be a $\\\\textit{progressive transition}$. Specifically, blendfake and deepfake can be explicitly delineated as the oriented pivot anchors between \"real-to-fake\" transitions. The accumulation of forgery information should be oriented and progressively increasing during this transition process. To this end, we propose an $\\\\underline{O}$riented $\\\\underline{P}$rogressive $\\\\underline{R}$egularizor (OPR) to establish the constraints that compel the distribution of anchors to be discretely arranged. Furthermore, we introduce feature bridging to facilitate the smooth transition between adjacent anchors.  Extensive experiments confirm that our design allows leveraging forgery information from both blendfake and deepfake effectively and comprehensively. Code is available at https://github.com/beautyremain/ProDet.',\n",
       "  691: 'Despite Retrieval-Augmented Generation (RAG) has shown promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGChecker, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGChecker has significantly better correlations with human judgments than other evaluation metrics. Using RAGChecker, we evaluate 8 RAG systems and conduct an in-depth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RAGChecker can guide researchers and practitioners in developing more effective RAG systems.',\n",
       "  692: \"Unsupervised 3D object detection methods have emerged to leverage vast amounts of data without requiring manual labels for training. Recent approaches rely on dynamic objects for learning to detect mobile objects but penalize the detections of static instances during training. Multiple rounds of (self) training are used to add detected static instances to the set of training targets; this procedure to improve performance is computationally expensive. To address this, we propose the method UNION. We use spatial clustering and self-supervised scene flow to obtain a set of static and dynamic object proposals from LiDAR. Subsequently, object proposals' visual appearances are encoded to distinguish static objects in the foreground and background by selecting static instances that are visually similar to dynamic objects. As a result, static and dynamic mobile objects are obtained together, and existing detectors can be trained with a single training. In addition, we extend 3D object discovery to detection by using object appearance-based cluster labels as pseudo-class labels for training object classification. We conduct extensive experiments on the nuScenes dataset and increase the state-of-the-art performance for unsupervised 3D object discovery, i.e. UNION more than doubles the average precision to 38.4. The code is available at github.com/TedLentsch/UNION.\",\n",
       "  693: 'This paper explores adaptive variance reduction methods for stochastic optimization based on the STORM technique. Existing adaptive extensions of STORM rely on strong assumptions like bounded gradients and bounded function values, or suffer an additional $\\\\mathcal{O}(\\\\log T)$ term in the convergence rate. To address these limitations, we introduce a novel adaptive STORM method that achieves an optimal convergence rate of $\\\\mathcal{O}(T^{-1/3})$ for non-convex functions with our newly designed learning rate strategy. Compared with existing approaches, our method requires weaker assumptions and attains the optimal convergence rate without the additional $\\\\mathcal{O}(\\\\log T)$ term. We also extend the proposed technique to stochastic compositional optimization, obtaining the same optimal rate of $\\\\mathcal{O}(T^{-1/3})$. Furthermore, we investigate the non-convex finite-sum problem and develop another innovative adaptive variance reduction method that achieves an optimal convergence rate of $\\\\mathcal{O}(n^{1/4} T^{-1/2} )$, where $n$ represents the number of component functions. Numerical experiments across various tasks validate the effectiveness of our method.',\n",
       "  694: 'In multi-agent reinforcement learning (MARL), parameter sharing is commonly employed to enhance sample efficiency. However, the popular approach of full parameter sharing often leads to homogeneous policies among agents, potentially limiting the performance benefits that could be derived from policy diversity. To address this critical limitation, we introduce \\\\emph{Kaleidoscope}, a novel adaptive partial parameter sharing scheme that fosters policy heterogeneity while still maintaining high sample efficiency. Specifically, Kaleidoscope maintains one set of common parameters alongside multiple sets of distinct, learnable masks for different agents, dictating the sharing of parameters. It promotes diversity among policy networks by encouraging discrepancy among these masks, without sacrificing the efficiencies of parameter sharing. This design allows Kaleidoscope to dynamically balance high sample efficiency with a broad policy representational capacity, effectively bridging the gap between full parameter sharing and non-parameter sharing across various environments. We further extend Kaleidoscope to critic ensembles in the context of actor-critic algorithms, which could help improve value estimations. Our empirical evaluations across extensive environments, including multi-agent particle environment, multi-agent MuJoCo and StarCraft multi-agent challenge v2, demonstrate the superior performance of Kaleidoscope compared with existing parameter sharing approaches, showcasing its potential for performance enhancement in MARL. The code is publicly available at \\\\url{https://github.com/LXXXXR/Kaleidoscope}.',\n",
       "  695: 'A major obstacle to the advancements of machine learning models in marine science, particularly in sonar imagery analysis, is the scarcity of AI-ready datasets.  While there have been efforts to make AI-ready sonar image dataset publicly available, they suffer from limitations in terms of environment setting and scale.  To bridge this gap, we introduce $\\\\texttt{SeafloorAI}$, the first extensive AI-ready datasets for seafloor mapping across 5 geological layers that is curated in collaboration with marine scientists. We further extend the dataset to $\\\\texttt{SeafloorGenAI}$ by incorporating the language component in order to facilitate the development of both $\\\\textit{vision}$- and $\\\\textit{language}$-capable machine learning models for sonar imagery.  The dataset consists of 62 geo-distributed data surveys spanning 17,300 square kilometers, with 696K sonar images, 827K annotated segmentation masks, 696K detailed language descriptions and approximately 7M question-answer pairs.   By making our data processing source code publicly available, we aim to engage the marine science community to enrich the data pool and inspire the machine learning community to develop more robust models.   This collaborative approach will enhance the capabilities and applications of our datasets within both fields.',\n",
       "  696: \"The recent success of interleaved Large Multimodal Models (LMMs) in few-shot learning suggests that in-context learning (ICL) with many examples can be promising for learning new tasks. However, this many-shot multimodal ICL setting has one crucial problem: it is fundamentally limited by the model's context length set at pretraining. The problem is especially prominent in the multimodal domain, which processes both text and images, requiring additional tokens. This motivates the need for a multimodal method to compress many shots into fewer tokens without finetuning. In this work, we enable LMMs to perform multimodal, many-shot in-context learning by leveraging Multimodal Task Vectors (MTV)---compact implicit representations of in-context examples compressed in the model's attention heads. Specifically, we first demonstrate the existence of such MTV in LMMs and then leverage these extracted MTV to enable many-shot in-context learning for various vision-and-language tasks. Our experiments suggest that MTV can scale in performance with the number of compressed shots and generalize to similar out-of-domain tasks without additional context length for inference. Code: https://github.com/Brandon3964/MultiModal-Task-Vector\",\n",
       "  697: \"This paper introduces the Quadratic Quantum Variational Monte Carlo (Q$^2$VMC) algorithm, an innovative algorithm in quantum chemistry that significantly enhances the efficiency and accuracy of solving the Schrödinger equation. Inspired by the discretization of imaginary-time Schrödinger evolution, Q$^2$VMC employs a novel quadratic update mechanism that integrates seamlessly with neural network-based ansatzes. Our extensive experiments showcase Q$^2$VMC's superior performance, achieving faster convergence and lower ground state energies in wavefunction optimization across various molecular systems, without additional computational cost. This study not only advances the field of computational quantum chemistry but also highlights the important role of discretized evolution in variational quantum algorithms, offering a scalable and robust framework for future quantum research.\",\n",
       "  698: 'Extracting scientific understanding from particle-physics experiments requires solving diverse learning problems with high precision and good data efficiency. We propose the Lorentz Geometric Algebra Transformer (L-GATr), a new multi-purpose architecture for high-energy physics. L-GATr represents high-energy data in a geometric algebra over four-dimensional space-time and is equivariant under Lorentz transformations, the symmetry group of relativistic kinematics. At the same time, the architecture is a Transformer, which makes it versatile and scalable to large systems. L-GATr is first demonstrated on regression and classification tasks from particle physics. We then construct the first Lorentz-equivariant generative model: a continuous normalizing flow based on an L-GATr network, trained with Riemannian flow matching. Across our experiments, L-GATr is on par with or outperforms strong domain-specific baselines.',\n",
       "  699: 'Time series forecasting has played a pivotal role across various industries, including finance, transportation, energy, healthcare, and climate. Due to the abundant seasonal information they contain, timestamps possess the potential to offer robust global guidance for forecasting techniques. However, existing works primarily focus on local observations, with timestamps being treated merely as an optional supplement that remains underutilized. When data gathered from the real world is polluted, the absence of global information will damage the robust prediction capability of these algorithms. To address these problems, we propose a novel framework named GLAFF. Within this framework, the timestamps are modeled individually to capture the global dependencies. Working as a plugin, GLAFF adaptively adjusts the combined weights for global and local information, enabling seamless collaboration with any time series forecasting backbone. Extensive experiments conducted on nine real-world datasets demonstrate that GLAFF significantly enhances the average performance of widely used mainstream forecasting models by 12.5\\\\%, surpassing the previous state-of-the-art method by 5.5\\\\%.',\n",
       "  700: 'Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more \"relevant\" tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, our models trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67x of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as Nx2 multiplication and a two-dimensional task. Our codebase is available at github.com/HanseulJo/position-coupling.',\n",
       "  701: \"Fine-grained visual classification (FGVC) involves classifying closely related subcategories. This task is inherently difficult due to the subtle differences between classes and the high intra-class variance. Moreover, FGVC datasets are typically small and challenging to gather, thus highlighting a significant need for effective data augmentation.Recent advancements in text-to-image diffusion models have introduced new possibilities for data augmentation in image classification. While these models have been used to generate training data for classification tasks, their effectiveness in full-dataset training of FGVC models remains under-explored. Recent techniques that rely on text-to-image generation or Img2Img methods, such as SDEdit, often struggle to generate images that accurately represent the class while modifying them to a degree that significantly increases the dataset's diversity. To address these challenges, we present SaSPA: Structure and Subject Preserving Augmentation. Contrary to recent methods, our method does not use real images as guidance, thereby increasing generation flexibility and promoting greater diversity. To ensure accurate class representation, we employ conditioning mechanisms, specifically by conditioning on image edges and subject representation.We conduct extensive experiments and benchmark SaSPA against both traditional and generative data augmentation techniques. SaSPA consistently outperforms all established baselines across multiple settings, including full dataset training and contextual bias. Additionally, our results reveal interesting patterns in using synthetic data for FGVC models; for instance, we find a relationship between the amount of real data used and the optimal proportion of synthetic data.\",\n",
       "  702: \"Collaborative perception is dedicated to tackling the constraints of single-agent perception, such as occlusions, based on the multiple agents' multi-view sensor inputs. However, most existing works assume an ideal condition that all agents' multi-view cameras are continuously available. In reality, cameras may be highly noisy, obscured or even failed during the collaboration. In this work, we introduce a new robust camera-insensitivity problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? To address above problems, we propose RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism. The key intuition of RCDN is to construct collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. To better model collaborative neural rendering field, RCDN first establishes a geometry BEV feature based time-invariant static field with other agents via fast hash grid modeling. Based on the static background field, the proposed time-varying dynamic field can model corresponding motion vector for foregrounds with appropriate positions. To validate RCDN, we create OPV2V-N, a new large-scale dataset with manual labelling under different camera failed scenarios. Extensive experiments conducted on OPV2V-N show that RCDN can be ported to other baselines and improve their robustness in extreme camera-insensitivity setting. Our code and datasets will be available soon.\",\n",
       "  703: 'Given an unconditional diffusion model and a predictor for a target property of interest (e.g., a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training. Existing methods, though effective in various individual applications, often lack theoretical grounding and rigorous testing on extensive benchmarks. As a result, they could even fail on simple tasks, and applying them to a new problem becomes unavoidably difficult. This paper introduces a novel algorithmic framework encompassing existing methods as special cases, unifying the study of training-free guidance into the analysis of an algorithm-agnostic design space. Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy that can be readily applied to any downstream task. We systematically benchmark across 7 diffusion models on 16 tasks with 40 targets, and improve performance by 8.5% on average. Our framework and benchmark offer a solid foundation for conditional generation in a training-free manner.',\n",
       "  704: 'Modern optimizers such as AdamW, equipped with momentum and adaptive learning rate, are designed to escape local minima and explore the vast parameter space. This exploration is beneficial for finding good loss basins when training from scratch. It is not necessarily ideal when resuming from a powerful foundation model because it can lead to large deviations from the pre-trained initialization and, consequently, worse robustness and generalization. At the same time, strong regularization on all parameters can lead to under-fitting. We hypothesize that selectively regularizing the parameter space is the key to fitting and retraining the pre-trained knowledge. This paper proposes a new weight decay technique, Selective Projection Decay (SPD), that selectively imposes a strong penalty on certain layers while allowing others to change freely. Intuitively, SPD expands and contracts the parameter search space for layers with consistent and inconsistent loss reduction, respectively. Experimentally, when equipped with SPD, Adam consistently provides better in-distribution generalization and out-of-distribution robustness performance on multiple popular vision and language benchmarks.',\n",
       "  705: 'Driving systems often rely on high-definition (HD) maps for precise environmental information, which is crucial for planning and navigation. While current HD map constructors perform well under ideal conditions, their resilience to real-world challenges, \\\\eg, adverse weather and sensor failures, is not well understood, raising safety concerns. This work introduces MapBench, the first comprehensive benchmark designed to evaluate the robustness of HD map construction methods against various sensor corruptions. Our benchmark encompasses a total of 29 types of corruptions that occur from cameras and LiDAR sensors. Extensive evaluations across 31 HD map constructors reveal significant performance degradation of existing methods under adverse weather conditions and sensor failures, underscoring critical safety concerns. We identify effective strategies for enhancing robustness, including innovative approaches that leverage multi-modal fusion, advanced data augmentation, and architectural techniques. These insights provide a pathway for developing more reliable HD map construction methods, which are essential for the advancement of autonomous driving technology. The benchmark toolkit and affiliated code and model checkpoints have been made publicly accessible.',\n",
       "  706: 'Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs’ abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry; for example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Moreover, we show how PromptEval can be useful in LLM-as-a-judge and best prompt identification applications.',\n",
       "  707: \"The rapid development of Large Language Models (LLMs) has been pivotal in advancing AI, with pre-trained LLMs being adaptable to diverse downstream tasks through fine-tuning. Federated learning (FL) further enhances fine-tuning in a privacy-aware manner by utilizing clients' local data through in-situ computation, eliminating the need for data movement. However, fine-tuning LLMs, given their massive scale of parameters, poses challenges for clients with constrained and heterogeneous resources in FL. Previous methods employed low-rank adaptation (LoRA) for efficient federated fine-tuning but utilized traditional FL aggregation strategies on LoRA adapters. This approach led to mathematically inaccurate aggregation noise, reducing fine-tuning effectiveness and failing to address heterogeneous LoRAs. In this work, we first highlight the mathematical incorrectness of LoRA aggregation in existing federated fine-tuning methods. We introduce a new approach called FLoRA that enables federated fine-tuning on heterogeneous LoRA adapters across clients through a novel stacking-based aggregation method. Our approach is noise-free and seamlessly supports heterogeneous LoRAs. Extensive experiments demonstrate FLoRA's superior performance in both homogeneous and heterogeneous settings, surpassing state-of-the-art methods. We envision this work as a milestone for efficient, privacy-preserving, and accurate federated fine-tuning of LLMs.\",\n",
       "  708: 'Reward allocation, also known as the credit assignment problem, has been an important topic in economics, engineering, and machine learning. An important concept in reward allocation is the core, which is the set of stable allocations where no agent has the motivation to deviate from the grand coalition. In previous works, computing the core requires either knowledge of the reward function in deterministic games or the reward distribution in stochastic games. However, this is unrealistic, as the reward function or distribution is often only partially known and may be subject to uncertainty. In this paper, we consider the core learning problem in stochastic cooperative games, where the reward distribution is unknown. Our goal is to learn the expected core, that is, the set of allocations that are stable in expectation, given an oracle that returns a stochastic reward for an enquired coalition each round. Within the class of strictly convex games, we present an algorithm named \\\\texttt{Common-Points-Picking} that returns a point in the expected core given a polynomial number of samples, with high probability. To analyse the algorithm, we develop a new extension of the separation hyperplane theorem for multiple convex sets.t.',\n",
       "  709: 'Text-driven image synthesis has made significant advancements with the development of diffusion models, transforming how visual content is generated from text prompts. Despite these advances, text-driven image editing, a key area in computer graphics, faces unique challenges. A major challenge is making simultaneous edits across multiple objects or attributes. Applying these methods sequentially for multi-attribute edits increases computational demands and efficiency losses.     In this paper, we address these challenges with significant contributions. Our main contribution is the development of ParallelEdits, a method that seamlessly manages simultaneous edits across multiple attributes. In contrast to previous approaches, ParallelEdits not only preserves the quality of single attribute edits but also significantly improves the performance of multitasking edits. This is achieved through innovative attention distribution mechanism and multi-branch design that operates across several processing heads.     Additionally, we introduce the PIE-Bench++ dataset, an expansion of the original PIE-Bench dataset, to better support evaluating image-editing tasks involving multiple objects and attributes simultaneously. This dataset is a benchmark for evaluating text-driven image editing methods in multifaceted scenarios.',\n",
       "  710: 'The pursuit of high perceptual quality in image restoration has driven the development of revolutionary generative models, capable of producing results often visually indistinguishable from real data.However, as their perceptual quality continues to improve, these models also exhibit a growing tendency to generate hallucinations – realistic-looking details that do not exist in the ground truth images.Hallucinations in these models create uncertainty about their reliability, raising major concerns about their practical application.This paper investigates this phenomenon through the lens of information theory, revealing a fundamental tradeoff between uncertainty and perception. We rigorously analyze the relationship between these two factors, proving that the global minimal uncertainty in generative models grows in tandem with perception. In particular, we define the inherent uncertainty of the restoration problem and show that attaining perfect perceptual quality entails at least twice this uncertainty. Additionally, we establish a relation between distortion, uncertainty and perception, through which we prove the aforementioned uncertainly-perception tradeoff induces the well-known perception-distortion tradeoff.We demonstrate our theoretical findings through experiments with super-resolution and inpainting algorithms.This work uncovers fundamental limitations of generative models in achieving both high perceptual quality and reliable predictions for image restoration. Thus, we aim to raise awareness among practitioners about this inherent tradeoff, empowering them to make informed decisions and potentially prioritize safety over perceptual performance.',\n",
       "  711: \"We initiate an investigation into the optimization properties of next-token prediction (NTP), the dominant training paradigm for modern language models. Specifically, we study the structural properties of the solutions selected by gradient-based optimizers among the many possible minimizers of the NTP objective. By framing NTP as cross-entropy minimization  across \\\\emph{distinct} contexts, each tied with a \\\\emph{sparse}  conditional probability distribution across a finite vocabulary of tokens, we introduce ``NTP-separability conditions'' that enable reaching the data-entropy lower bound. With this setup, and focusing on linear models with fixed context embeddings, we characterize the optimization bias of gradient descent (GD): Within the data subspace defined by the sparsity patterns of distinct contexts, GD selects parameters that equate the logits' differences of in-support tokens to their log-odds. In the orthogonal subspace, the GD parameters diverge in norm and select the direction that maximizes a margin specific to NTP. These findings extend previous research on implicit bias in one-hot classification to the NTP setting, highlighting key differences and prompting further research into the optimization and generalization properties of NTP, irrespective of the specific architecture used to generate the context embeddings.\",\n",
       "  712: \"Success in collaborative and competitive environments, where agents must work with or against each other, requires individuals to encode the position and trajectory of themselves and others. Decades of neurophysiological experiments have shed light on how brain regions [e.g., medial entorhinal cortex (MEC), hippocampus] encode the self's position and trajectory. However, it has only recently been discovered that MEC and hippocampus are modulated by the positions and trajectories of others. To understand how encoding spatial information of multiple agents shapes neural representations, we train a recurrent neural network (RNN) model that captures properties of MEC to path integrate trajectories of two agents simultaneously navigating the same environment. We find significant differences between these RNNs and those trained to path integrate only a single agent. At the individual unit level, RNNs trained to path integrate more than one agent develop weaker grid responses, stronger border responses, and tuning for the relative position of the two agents. At the population level, they develop more distributed and robust representations, with changes in network dynamics and manifold topology. Our results provide testable predictions and open new directions with which to study the neural computations supporting spatial navigation.\",\n",
       "  713: 'Out-of-distribution (OOD) detection is crucial for ensuring reliable deployment of machine learning models. Recent advancements focus on utilizing easily accessible auxiliary outliers (e.g., data from the web or other datasets) in training. However, we experimentally reveal that these methods still struggle to generalize their detection capabilities to unknown OOD data, due to the limited diversity of the auxiliary outliers collected. Therefore, we thoroughly examine this problem from the generalization perspective and demonstrate that a more diverse set of auxiliary outliers is essential for enhancing the detection capabilities. However, in practice, it is difficult and costly to collect sufficiently diverse auxiliary outlier data. Therefore, we propose a simple yet practical approach with a theoretical guarantee, termed Diversity-induced Mixup for OOD detection (diverseMix), which enhances the diversity of auxiliary outlier set for training in an efficient way. Extensive experiments show that diverseMix achieves superior performance on commonly used and recent challenging large-scale benchmarks, which further confirm the importance of the diversity of auxiliary outliers.',\n",
       "  714: \"Sequential recommendation (SR) aims to predict items that users may be interested in based on their historical behavior sequences. We revisit SR from a novel information-theoretic perspective and find that conventional sequential modeling methods fail to adequately capture the randomness and unpredictability of user behavior. Inspired by fuzzy information processing theory, this paper introduces the DDSR model, which uses fuzzy sets of interaction sequences to overcome the limitations and better capture the evolution of users' real interests. Formally based on diffusion transition processes in discrete state spaces, which is unlike common diffusion models such as DDPM that operate in continuous domains. It is better suited for discrete data, using structured transitions instead of arbitrary noise introduction to avoid information loss. Additionally, to address the inefficiency of matrix transformations due to the vast discrete space, we use semantic labels derived from quantization or RQ-VAE to replace item IDs, enhancing efficiency and improving cold start issues. Testing on three public benchmark datasets shows that DDSR outperforms existing state-of-the-art methods in various settings, demonstrating its potential and effectiveness in handling SR tasks.\",\n",
       "  715: 'Molecule generation ideally in its 3-D form has enjoyed wide applications in material, chemistry, life science, etc. We propose the first quantum parametric circuit for 3-D molecule generation for its potential quantum advantage especially considering the arrival of Noisy Intermediate-Scale Quantum (NISQ) era. We choose the Variational AutoEncoder (VAE) scheme for its simplicity and one-shot generation ability, which we believe is more quantum-friendly compared with the auto-regressive generative models or diffusion models as used in classic approaches. Specifically, we present a quantum encoding scheme designed for 3-D molecules with qubits complexity $\\\\mathcal{O}(C\\\\log n)$ ($n$ is the number of atoms) and adopt a von Mises-Fisher (vMF) distributed latent space to meet the inherent coherence of the quantum system. We further design to encode conditions into quantum circuits for property-specified generation. Experimentally, our model could generate plausible 3-D molecules and achieve competitive quantitative performance with significantly reduced circuit parameters compared with their classic counterparts. The source code will be released upon publication.',\n",
       "  716: 'In this paper, we study the non-asymptotic sample complexity for the pure exploration problem in contextual bandits and tabular reinforcement learning (RL): identifying an $\\\\epsilon$-optimal policy from a set of policies $\\\\Pi$ with high probability. Existing work in bandits has shown that it is possible to identify the best policy by estimating only the *difference* between the behaviors of individual policies–which can have substantially lower variance than estimating the behavior of each policy directly—yet the best-known complexities in RL fail to take advantage of this, and instead estimate the behavior of each policy directly. Does it suffice to estimate only the differences in the behaviors of policies in RL? We answer this question positively for contextual bandits, but in the negative for tabular RL, showing a separation between contextual bandits and RL. However, inspired by this, we show that it *almost* suffices to estimate only the differences in RL: if we can estimate the behavior of a *single* reference policy, it suffices to only estimate how any other policy deviates from this reference policy. We develop an algorithm which instantiates this principle and obtains, to the best of our knowledge, the tightest known bound on the sample complexity of tabular RL.',\n",
       "  717: 'While the conditional sequence modeling with the transformer architecture has demonstrated its effectiveness in dealing with offline reinforcement learning (RL) tasks, it is struggle to handle out-of-distribution states and actions.Existing work attempts to address this issue by data augmentation with the learned policy or adding extra constraints with the value-based RL algorithm. However, these studies still fail to overcome the following challenges: (1) insufficiently utilizing the historical temporal information among inter-steps, (2) overlooking the local intra-step relationships among return-to-gos (RTGs), states, and actions, (3) overfitting suboptimal trajectories with noisy labels. To address these challenges, we propose $\\\\textbf{D}$ecision $\\\\textbf{M}$amba ($\\\\textbf{DM}$), a novel multi-grained state space model (SSM) with a self-evolving policy learning strategy.DM explicitly models the historical hidden state to extract the temporal information by using the mamba architecture. To capture the relationship among RTG-state-action triplets, a fine-grained SSM module is designed and integrated into the original coarse-grained SSM in mamba, resulting in a novel mamba architecture tailored for offline RL. Finally, to mitigate the overfitting issue on noisy trajectories, a self-evolving policy is proposed by using progressive regularization. The policy evolves by using its own past knowledge to refine the suboptimal actions, thus enhancing its robustness on noisy demonstrations. Extensive experiments on various tasks show that DM outperforms other baselines substantially.',\n",
       "  718: 'Generative models at times produce \"invalid\" outputs, such as images with generation artifacts and unnatural sounds. Validity-constrained distribution learning attempts to address this problem by requiring that the learned distribution have a provably small fraction of its mass in invalid parts of space -- something which standard loss minimization does not always ensure. To this end, a learner in this model can guide the learning via \"validity queries\", which allow it to ascertain the validity of individual examples. Prior work on this problem takes a worst-case stance, showing that proper learning requires an exponential number of validity queries, and demonstrating an improper algorithm which -- while generating guarantees in a wide-range of settings -- makes a relatively large polynomial number of validity queries. In this work, we take a first step towards characterizing regimes where guaranteeing validity is easier than in the worst-case. We show that when the data distribution lies in the model class and the log-loss is minimized, the number samples required to ensure validity has a weak dependence on the validity requirement. Additionally, we show that when the validity region belongs to a VC-class, a limited number of validity queries are often sufficient.',\n",
       "  719: \"Recently, prompt learning has emerged as the state-of-the-art (SOTA) for fair text-to-image (T2I) generation. Specifically, this approach leverages readily available reference images to learn inclusive prompts for each target Sensitive Attribute (tSA), allowing for fair image generation. In this work, we first reveal that this prompt learning-based approach results in degraded sample quality. Our analysis shows that the approach's training objective--which aims to align the embedding differences of learned prompts and reference images-- could be sub-optimal, resulting in distortion of the learned prompts and degraded generated images. To further substantiate this claim, as our major contribution, we deep dive into the denoising subnetwork of the T2I model to track down the effect of these learned prompts by analyzing the cross-attention maps. In our analysis, we propose a novel prompt switching analysis: I2H and H2I. Furthermore, we propose new quantitative characterization of cross-attention maps. Our analysis reveals abnormalities in the early denoising steps, perpetuating improper global structure that results in degradation in the generated samples. Building on insights from our analysis, we propose two ideas: (i) Prompt Queuing and (ii) Attention Amplification to address the quality issue. Extensive experimental results on a wide range of tSAs show that our proposed method outperforms SOTA approach's image generation quality, while achieving competitive fairness. More resources at FairQueue Project site: https://sutd-visual-computing-group.github.io/FairQueue\",\n",
       "  720: \"Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order. Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question: have VLMs effectively tackled the CR challenge? We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM only negative text generation pipeline. Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs' LLM decoders or as improbable within the corresponding image context. To address these limitations, we introduce ConMe\\\\footnote{ConMe is an abbreviation for Confuse Me.} -- a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce `hard CR Q&A'. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually. Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs.\",\n",
       "  721: \"Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications.We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an `observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.\",\n",
       "  722: 'The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as \"hallucination.\" Initial retrieval-augmented generation (RAG) methods like the \"Retrieve-Read\" framework was inadequate for complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised Fine-Tuning (SFT) methods improved performance but required frequent retraining and risked altering foundational LLM capabilities. To cope with these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), integrating an intelligent information assistant within LLMs. This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification. Using a two-phase training approach—Curriculum Assistant Learning and Reinforced Preference Optimization—AssistRAG enhances information retrieval and decision-making. Experiments show AssistRAG significantly outperforms benchmarks, especially benefiting less advanced LLMs, by providing superior reasoning capabilities and accurate responses.',\n",
       "  723: 'High-level visual brain regions contain subareas in which neurons appear to respond more strongly to examples of a particular semantic category, like faces or bodies, rather than objects. However, recent work has shown that while this finding holds on average, some out-of-category stimuli also activate neurons in these regions. This may be due to visual features common among the preferred class also being present in other images. Here, we propose a deep-learning-based approach for visualizing these features. For each neuron, we identify relevant visual features driving its selectivity by modelling responses to images based on latent activations of a deep neural network. Given an out-of-category image which strongly activates the neuron, our method first identifies a reference image from the preferred category yielding a similar feature activation pattern. We then backpropagate latent activations of both images to the pixel level, while enhancing the identified shared dimensions and attenuating non-shared features. The procedure highlights image regions containing shared features driving responses of the model neuron. We apply the algorithm to novel recordings from body-selective regions in macaque IT cortex in order to understand why some images of objects excite these neurons. Visualizations reveal object parts which resemble parts of a macaque body, shedding light on neural preference of these objects.',\n",
       "  724: 'We introduce a novel capacity measure 2sED for statistical models based on the effective dimension. The new quantity provably bounds the generalization error under mild assumptions on the model. Furthermore, simulations on standard data sets and popular model architectures show that 2sED correlates well with the training error. For Markovian models, we show how to efficiently approximate 2sED from below through a layerwise iterative approach, which allows us to tackle deep learning models with a large number of parameters. Simulation results suggest that the approximation is good for different prominent models and data sets.',\n",
       "  725: 'Federated learning faces challenges due to the heterogeneity in data volumes and distributions at different clients, which can compromise model generalization ability to various distributions. Existing approaches to address this issue based on group distributionally robust optimization (GDRO) often lead to high communication and sample complexity.To this end, this work introduces algorithms tailored for communication-efficient Federated Group Distributionally Robust Optimization (FGDRO). Our contributions are threefold: Firstly, we introduce the FGDRO-CVaR algorithm, which optimizes the average top-K losses while reducing communication complexity to $O(1/\\\\epsilon^4)$, where $\\\\epsilon$ denotes the desired precision level. Secondly, our FGDRO-KL algorithm is crafted to optimize KL regularized FGDRO, cutting communication complexity to $O(1/\\\\epsilon^3)$. Lastly, we propose FGDRO-KL-Adam to utilize Adam-type local updates in FGDRO-KL, which not only maintains a communication cost of $O(1/\\\\epsilon^3)$ but also shows potential to surpass SGD-type local steps in practical applications.The effectiveness of our algorithms has been demonstrated on a variety of real-world tasks, including natural language processing and computer vision.',\n",
       "  726: 'We consider the problem of privately estimating a parameter $\\\\mathbb{E}[h(X_1,\\\\dots,X_k)]$, where $X_1$, $X_2$, $\\\\dots$, $X_k$ are i.i.d. data from some distribution and $h$ is a permutation-invariant function. Without privacy constraints, the standard estimators for this task are U-statistics, which commonly arise in a wide range of problems, including nonparametric signed rank tests, symmetry testing, uniformity testing, and subgraph counts in random networks, and are the unique minimum variance unbiased estimators under mild conditions. Despite the recent outpouring of interest in private mean estimation, privatizing U-statistics has received little attention. While existing private mean estimation algorithms can be applied in a black-box manner to obtain confidence intervals, we show that they can lead to suboptimal private error, e.g., constant-factor inflation in the leading term, or even $\\\\Theta(1/n)$ rather than $O(1/n^2)$ in degenerate settings. To remedy this, we propose a new thresholding-based approach that reweights different subsets of the data using _local Hájek projections_. This leads to nearly optimal private error for non-degenerate U-statistics and a strong indication of near-optimality for degenerate U-statistics.',\n",
       "  727: \"Current uncertainty quantification is memory and compute expensive, which hinders practical uptake. To counter, we develop Sketched Lanczos Uncertainty (SLU): an architecture-agnostic uncertainty score that can be applied to pre-trained neural networks with minimal overhead. Importantly, the memory use of SLU only grows logarithmically with the number of model parameters. We combine Lanczos' algorithm with dimensionality reduction techniques to compute a sketch of the leading eigenvectors of a matrix. Applying this novel algorithm to the Fisher information matrix yields a cheap and reliable uncertainty score. Empirically, SLU yields well-calibrated uncertainties, reliably detects out-of-distribution examples, and consistently outperforms existing methods in the low-memory regime.\",\n",
       "  728: 'In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms. Generative training has enabled Multimodal Large Language Models (MLLMs) to tackle various complex tasks, yet issues such as hallucinations and weak object discrimination persist. Discriminative training, exemplified by models like CLIP, excels in zero-shot image-text classification and retrieval, yet struggles with complex scenarios requiring fine-grained semantic differentiation. This paper addresses these challenges by proposing a unified approach that integrates the strengths of both paradigms. Considering interleaved image-text sequences as the general format of input samples, we introduce a structure-induced training strategy that imposes semantic relationships between input samples and the MLLM’s hidden state. This approach enhances the MLLM’s ability to capture global semantics and distinguish fine-grained semantics. By leveraging dynamic sequence alignment within the Dynamic Time Warping framework and integrating a novel kernel for fine-grained semantic differentiation, our method effectively balances generative and discriminative tasks. Extensive experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results in multiple generative tasks, especially those requiring cognitive and discrimination abilities. Additionally, our method surpasses discriminative benchmarks in interleaved and fine-grained retrieval tasks. By employing a retrieval-augmented generation strategy, our approach further enhances performance in some generative tasks within one model, offering a promising direction for future research in vision-language modeling.',\n",
       "  729: 'Weight decay is a broadly used technique for training state-of-the-art deep networks from image classification to large language models. Despite its widespread usage and being extensively studied in the classical literature, its role remains poorly understood for deep learning. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For deep networks on vision tasks trained with multipass SGD, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for large language models trained with nearly one-epoch training, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss and improved training stability. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way.',\n",
       "  730: 'Recent research indicates that large language models (LLMs) are susceptible to jailbreaking attacks that can generate harmful content. This paper introduces a novel token-level attack method, Adaptive Dense-to-Sparse Constrained Optimization (ADC), which has been shown to successfully jailbreak multiple open-source LLMs. Drawing inspiration from the difficulties of discrete token optimization, our method relaxes the discrete jailbreak optimization into a continuous optimization process while gradually increasing the sparsity of the optimizing vectors. This technique effectively bridges the gap between discrete and continuous space optimization. Experimental results demonstrate that our method is more effective and efficient than state-of-the-art token-level methods. On Harmbench, our approach achieves the highest attack success rate on seven out of eight LLMs compared to the latest jailbreak methods. \\\\textcolor{red}{Trigger Warning: This paper contains model behavior that can be offensive in nature.}',\n",
       "  731: 'Modelling partial differential equations (PDEs) is of crucial importance in science and engineering, and it includes tasks ranging from forecasting to inverse problems, such as data assimilation. However, most previous numerical and machine learning approaches that target forecasting cannot be applied out-of-the-box for data assimilation. Recently, diffusion models have emerged as a powerful tool for conditional generation, being able to flexibly incorporate observations without retraining. In this work, we perform a comparative study of score-based diffusion models for forecasting and assimilation of sparse observations.  In particular, we focus on diffusion models that are either trained in a conditional manner, or conditioned after unconditional training. We address the shortcomings of existing models by proposing 1) an autoregressive sampling approach, that significantly improves performance in forecasting, 2) a new training strategy for conditional score-based models that achieves stable performance over a range of history lengths, and 3) a hybrid model which employs flexible pre-training conditioning on initial conditions and flexible post-training conditioning to handle data assimilation. We empirically show that these modifications are crucial for successfully tackling the combination of forecasting and data assimilation, a task commonly encountered in real-world scenarios.',\n",
       "  732: \"Despite extensive research on adversarial training strategies to improve robustness, the decisions of even the most robust deep learning models can still be quite sensitive to imperceptible perturbations, creating serious risks when deploying them for high-stakes real-world applications. While detecting such cases may be critical, evaluating a model's vulnerability at a per-instance level using adversarial attacks is computationally too intensive and unsuitable for real-time deployment scenarios. The input space margin is the exact score to detect non-robust samples and is intractable for deep neural networks. This paper introduces the concept of margin consistency -- a property that links the input space margins and the logit margins in robust models -- for efficient detection of vulnerable samples. First, we establish that margin consistency is a necessary and sufficient condition to use a model's logit margin as a score for identifying non-robust samples. Next, through comprehensive empirical analysis of various robustly trained models on CIFAR10 and CIFAR100 datasets, we show that they indicate high margin consistency with a strong correlation between their input space margins and the logit margins. Then, we show that we can effectively use the logit margin to confidently detect brittle decisions with such models. Finally, we address cases where the model is not sufficiently margin-consistent by learning a pseudo-margin from the feature representation. Our findings highlight the potential of leveraging deep representations to efficiently assess adversarial vulnerability in deployment scenarios.\",\n",
       "  733: \"Data-driven artificial intelligence (AI) models have made significant advancements in weather forecasting, particularly in medium-range and nowcasting. However, most data-driven weather forecasting models are black-box systems that focus on learning data mapping rather than fine-grained physical evolution in the time dimension. Consequently, the limitations in the temporal scale of datasets prevent these models from forecasting at finer time scales. This paper proposes a physics-AI hybrid model (i.e., WeatherGFT) which generalizes weather forecasts to finer-grained temporal scales beyond training dataset. Specifically, we employ a carefully designed PDE kernel to simulate physical evolution on a small time scale (e.g., 300 seconds) and use a parallel neural networks with a learnable router for bias correction. Furthermore, we introduce a lead time-aware training framework to promote the generalization of the model at different lead times. The weight analysis of physics-AI modules indicates that physics conducts major evolution while AI performs corrections adaptively. Extensive experiments show that WeatherGFT trained on an hourly dataset, effectively generalizes forecasts across multiple time scales, including 30-minute, which is even smaller than the dataset's temporal resolution.\",\n",
       "  734: 'In recent years, there has been significant research focusing on addressing security concerns in single-modal person re-identification (ReID) systems that are based on RGB images. However, the safety of cross-modality scenarios, which are more commonly encountered in practical applications involving images captured by infrared cameras, has not received adequate attention. The main challenge in cross-modality ReID lies in effectively dealing with visual differences between different modalities. For instance, infrared images are typically grayscale, unlike visible images that contain color information. Existing attack methods have primarily focused on the characteristics of the visible image modality, overlooking the features of other modalities and the variations in data distribution among different modalities. This oversight can potentially undermine the effectiveness of these methods in image retrieval across diverse modalities. This study represents the first exploration into the security of cross-modality ReID models and proposes a universal perturbation attack specifically designed for cross-modality ReID. This attack optimizes perturbations by leveraging gradients from diverse modality data, thereby disrupting the discriminator and reinforcing the differences between modalities. We conducted experiments on three widely used cross-modality datasets, namely RegDB, SYSU, and LLCM. The results not only demonstrate the effectiveness of our method but also provide insights for future improvements in the robustness of cross-modality ReID systems.',\n",
       "  735: 'Functional magnetic resonance imaging (fMRI) is an indispensable tool in modern neuroscience, providing a non-invasive window into whole-brain dynamics at millimeter-scale spatial resolution. However, fMRI is constrained by issues such as high operation costs and immobility. With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality. Nonetheless, the complex projection from neural activity to fMRI hemodynamic responses and the spatial ambiguity of EEG pose substantial challenges both in modeling and interpretability. Relatively few studies to date have developed approaches for EEG-fMRI translation, and although they have made significant strides, the inference of fMRI signals in a given study has been limited to a small set of brain areas and to a single condition (i.e., either resting-state or a specific task). The capability to predict fMRI signals in other brain areas, as well as to generalize across conditions, remain critical gaps in the field. To tackle these challenges, we introduce a novel and generalizable framework: NeuroBOLT, i.e., Neuro-to-BOLD Transformer, which leverages multi-dimensional representation learning from temporal, spatial, and spectral domains to translate raw EEG data to the corresponding fMRI activity signals across the brain. Our experiments demonstrate that NeuroBOLT effectively reconstructs unseen resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy with the potential to generalize across varying conditions and sites, which significantly advances the integration of these two modalities.',\n",
       "  736: 'Self play via online learning is one of the premier ways to solve large-scale zero-sum games, both in theory and practice. Particularly popular algorithms include optimistic multiplicative weights update (OMWU) and optimistic gradient-descent-ascent (OGDA). While both algorithms enjoy $O(1/T)$ ergodic convergence to Nash equilibrium in two-player zero-sum games, OMWU offers several advantages, including logarithmic dependence on the size of the payoff matrix and $\\\\tilde{O}(1/T)$ convergence to coarse correlated equilibria even in general-sum games. However, in terms of last-iterate convergence in two-player zero-sum games, an increasingly popular topic in this area, OGDA guarantees that the duality gap shrinks at a rate of $(1/\\\\sqrt{T})$, while the best existing last-iterate convergence for OMWU depends on some game-dependent constant that could be arbitrarily large. This begs the question: is this potentially slow last-iterate convergence an inherent disadvantage of OMWU, or is the current analysis too loose? Somewhat surprisingly, we show that the former is true. More generally, we prove that a broad class of algorithms that do not forget the past quickly all suffer the same issue: for any arbitrarily small $\\\\delta>0$, there exists a $2\\\\times 2$ matrix game such that the algorithm admits a constant duality gap even after $1/\\\\delta$ rounds. This class of algorithms includes OMWU and other standard optimistic follow-the-regularized-leader algorithms.',\n",
       "  737: 'We present a novel diffusion-based approach for coherent 3D scene reconstruction from a single RGB image. Our method utilizes an image-conditioned 3D scene diffusion model to simultaneously denoise the 3D poses and geometries of all objects within the scene.Motivated by the ill-posed nature of the task and to obtain consistent scene reconstruction results, we learn a generative scene prior by conditioning on all scene objects simultaneously to capture scene context and by allowing the model to learn inter-object relationships throughout the diffusion process.We further propose an efficient surface alignment loss to facilitate training even in the absence of full ground-truth annotation, which is common in publicly available datasets. This loss leverages an expressive shape representation, which enables direct point sampling from intermediate shape predictions.By framing the task of single RGB image 3D scene reconstruction as a conditional diffusion process, our approach surpasses current state-of-the-art methods, achieving a 12.04\\\\% improvement in AP3D on SUN RGB-D and a 13.43\\\\% increase in F-Score on Pix3D.',\n",
       "  738: 'Most large multimodal models (LMMs) are implemented by feeding visual tokens as a sequence into the first layer of a large language model (LLM). The resulting architecture is simple but significantly increases computation and memory costs, as it has to handle a large number of additional tokens in its input layer. This paper presents a new architecture *DeepStack* for LMMs. Considering $N$ layers in the language and vision transformer of LMMs, we stack the visual tokens into $N$ groups and feed each group to its aligned transformer layer from bottom to top. Surprisingly, this simple method greatly enhances the power of LMMs to model interactions among visual tokens across layers but with minimal additional cost. We apply *DeepStack* to both language and vision transformer in LMMs, and validate the effectiveness of *DeepStack* LMMs with extensive empirical results. Using the same context length, our DeepStack 7B and 13B parameters surpass their counterparts by 2.7 and 2.9 on average across 9 benchmarks, respectively. Using only one-fifth of the context length, DeepStack rivals closely to the counterparts that use the full context length. These gains are particularly pronounced on high-resolution tasks, *e.g.*, 4.2, 11.0, and 4.0 improvements on TextVQA, DocVQA, and InfoVQA compared to LLaVA-1.5-7B, respectively. We further apply *DeepStack* to vision transformer layers, which brings us a similar amount of improvements, 3.8 on average compared with LLaVA-1.5-7B.',\n",
       "  739: 'A critical question about Large Language Models (LLMs) is whether their apparent deficiency in mathematical reasoning is inherent, or merely a result of insufficient exposure to high-quality mathematical data. To explore this, we developed an automated method for generating high-quality, supervised mathematical datasets. The method carefully mutates existing math problems, ensuring both diversity and validity of the newly generated problems. This is achieved by a neuro-symbolic data generation framework combining the intuitive informalization strengths of LLMs, and the precise symbolic reasoning of math solvers along with projected Markov chain Monte Carlo sampling in the highly-irregular symbolic space.Empirical experiments demonstrate the high quality of data generated by the proposed method, and that the LLMs, specifically LLaMA-2 and Mistral, when realigned with the generated data, surpass their state-of-the-art counterparts.',\n",
       "  740: \"Diffusion models (DMs) have become the dominant paradigm of generative modeling in a variety of domains by learning stochastic processes from noise to data. Recently, diffusion denoising bridge models (DDBMs), a new formulation of generative modeling that builds stochastic processes between fixed data endpoints based on a reference diffusion process, have achieved empirical success across tasks with coupled data distribution, such as image-to-image translation. However, DDBM's sampling process typically requires hundreds of network evaluations to achieve decent performance, which may impede their practical deployment due to high computational demands. In this work, inspired by the recent advance of consistency models in DMs, we tackle this problem by learning the consistency function of the probability-flow ordinary differential equation (PF-ODE) of DDBMs, which directly predicts the solution at a starting step given any point on the ODE trajectory. Based on a dedicated general-form ODE solver, we propose two paradigms: consistency bridge distillation and consistency bridge training, which is flexible to apply on DDBMs with broad design choices. Experimental results show that our proposed method could sample $4\\\\times$ to $50\\\\times$ faster than the base DDBM and produce better visual quality given the same step in various tasks with pixel resolution ranging from $64 \\\\times 64$ to $256 \\\\times 256$, as well as supporting downstream tasks such as semantic interpolation in the data space.\",\n",
       "  741: \"Dense Associative Memories are high storage capacity variants of the Hopfield networks that are capable of storing a large number of memory patterns in the weights of the network of a given size. Their common formulations typically require storing each pattern in a separate set of synaptic weights, which leads to the increase of the number of synaptic weights when new patterns are introduced. In this work we propose an alternative formulation of this class of models using random features, commonly used in kernel methods. In this formulation the number of network's parameters remains fixed. At the same time, new memories can be added to the network by modifying existing weights. We show that this novel network closely approximates the energy function and dynamics of conventional Dense Associative Memories and shares their desirable computational properties.\",\n",
       "  742: \"Consistent depth estimation across diverse scenes and sensors is a crucial challenge in computer vision, especially when deploying machine learning models in the real world. Traditional methods depend heavily on extensive pixel-wise labeled data, which is costly and labor-intensive to acquire, and frequently have difficulty in scale issues on various depth sensors. In response, we define Universal Depth Completion (UniDC) problem. We also present a baseline architecture, a simple yet effective approach tailored to estimate scene depth across a wide range of sensors and environments using minimal labeled data. Our approach addresses two primary challenges: generalizable knowledge of unseen scene configurations and strong adaptation to arbitrary depth sensors with various specifications. To enhance versatility in the wild, we utilize a foundation model for monocular depth estimation that provides a comprehensive understanding of 3D structures in scenes. Additionally, for fast adaptation to off-the-shelf sensors, we generate a pixel-wise affinity map based on the knowledge from the foundation model. We then adjust depth information from arbitrary sensors to the monocular depth along with the constructed affinity. Furthermore, to boost up both the adaptability and generality, we embed the learned features into hyperbolic space, which builds implicit hierarchical structures of 3D data from fewer examples. Extensive experiments demonstrate the proposed method's superior generalization capabilities for UniDC problem over state-of-the-art depth completion. Source code is publicly available at https://github.com/JinhwiPark/UniDC.\",\n",
       "  743: 'Study of the nonlinear evolution deep neural network (DNN) parameters undergo during training has uncovered regimes of distinct dynamical behavior. While a detailed understanding of these phenomena has the potential to advance improvements in training efficiency and robustness, the lack of methods for identifying when DNN models have equivalent dynamics limits the insight that can be gained from prior work. Topological conjugacy, a notion from dynamical systems theory, provides a precise definition of dynamical equivalence, offering a possible route to address this need. However, topological conjugacies have historically been challenging to compute. By leveraging advances in Koopman operator theory, we develop a framework for identifying conjugate and non-conjugate training dynamics. To validate our approach, we demonstrate that comparing Koopman eigenvalues can correctly identify a known equivalence between online mirror descent and online gradient descent. We then utilize our approach to: (a) identify non-conjugate training dynamics between shallow and wide fully connected neural networks; (b) characterize the early phase of training dynamics in convolutional neural networks; (c) uncover non-conjugate training dynamics in Transformers that do and do not undergo grokking. Our results, across a range of DNN architectures, illustrate the flexibility of our framework and highlight its potential for shedding new light on training dynamics.',\n",
       "  744: \"Foundation models in computer vision have demonstrated exceptional performance in zero-shot and few-shot tasks by extracting multi-purpose features from large-scale datasets through self-supervised pre-training methods. However, these models often overlook the severe corruption in cryogenic electron microscopy (cryo-EM) images by high-level noises. We introduce DRACO, a Denoising-Reconstruction Autoencoder for CryO-EM, inspired by the Noise2Noise (N2N) approach. By processing cryo-EM movies into odd and even images and treating them as independent noisy observations, we apply a denoising-reconstruction hybrid training scheme. We mask both images to create denoising and reconstruction tasks. For DRACO's pre-training, the quality of the dataset is essential, we hence build a high-quality, diverse dataset from an uncurated public database, including over 270,000 movies or micrographs. After pre-training, DRACO naturally serves as a generalizable cryo-EM image denoiser and a foundation model for various cryo-EM downstream tasks. DRACO demonstrates the best performance in denoising, micrograph curation, and particle picking tasks compared to state-of-the-art baselines.\",\n",
       "  745: 'Prevalent human-object interaction (HOI) detection approaches typically leverage large-scale visual-linguistic models to help recognize events involving humans and objects. Though promising, models trained via contrastive learning on text-image pairs often neglect mid/low-level visual cues and struggle at compositional reasoning. In response, we introduce DIFFUSIONHOI, a new HOI detector shedding light on text-to-image diffusion models. Unlike the aforementioned models, diffusion models excel in discerning mid/low-level visual concepts as generative models, and possess strong compositionality to handle novel concepts expressed in text inputs. Considering diffusion models usually emphasize instance objects, we first devise an inversion-based strategy to learn the expression of relation patterns between humans and objects in embedding space. These learned relation embeddings then serve as textual prompts, to steer diffusion models generate images that depict specific interactions, and extract HOI-relevant cues from images without heavy finetuning. Benefited from above, DIFFUSIONHOI achieves SOTA performance on three datasets under both regular and zero-shot setups.',\n",
       "  746: 'We provide the first useful and rigorous analysis of ensemble sampling for the stochastic linear bandit setting. In particular, we show that, under standard assumptions, for a $d$-dimensional stochastic linear bandit with an interaction horizon $T$, ensemble sampling with an ensemble of size of order $\\\\smash{d \\\\log T}$ incurs regret at most of the order $\\\\smash{(d \\\\log T)^{5/2} \\\\sqrt{T}}$. Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly with $T$---which defeats the purpose of ensemble sampling---while obtaining  near $\\\\smash{\\\\sqrt{T}}$ order regret. Our result is also the first to allow for infinite action sets.',\n",
       "  747: 'Learn-to-Defer is a paradigm that enables learning algorithms to work not in isolation but as a team with human experts. In this paradigm, we permit the system to defer a subset of its tasks to the expert. Although there are currently systems that follow this paradigm and are designed to optimize the accuracy of the final human-AI team, the general methodology for developing such systems under a set of constraints (e.g., algorithmic fairness, expert intervention budget, defer of anomaly, etc.) remains largely unexplored. In this paper, using a d-dimensional generalization to the fundamental lemma of Neyman and Pearson (d-GNP), we obtain the Bayes optimal solution for learn-to-defer systems under various constraints. Furthermore, we design a generalizable algorithm to estimate that solution and apply this algorithm to the COMPAS, Hatespeech, and ACSIncome datasets. Our algorithm shows improvements in terms of constraint violation over a set of learn-to-defer baselines and can control multiple constraint violations at once. The use of d-GNP is beyond learn-to-defer applications and can potentially obtain a solution to decision-making problems with a set of controlled expected performance measures.',\n",
       "  748: \"Large language models (LLMs) are increasingly essential in processing natural languages, yet their application is frequently compromised by biases and inaccuracies originating in their training data.In this study, we introduce \\\\textbf{Cross-Care}, the first benchmark framework dedicated to assessing biases and real world knowledge in LLMs, specifically focusing on the representation of disease prevalence across diverse demographic groups.We systematically evaluate how demographic biases embedded in pre-training corpora like $ThePile$ influence the outputs of LLMs.We expose and quantify discrepancies by juxtaposing these biases against actual disease prevalences in various U.S. demographic groups.Our results highlight substantial misalignment between LLM representation of disease prevalence and real disease prevalence rates across demographic subgroups, indicating a pronounced risk of bias propagation and a lack of real-world grounding for medical applications of LLMs.Furthermore, we observe that various alignment methods minimally resolve inconsistencies in the models' representation of disease prevalence across different languages.For further exploration and analysis, we make all data and a data visualization tool available at: \\\\url{www.crosscare.net}.\",\n",
       "  749: \"Graph Neural Networks (GNNs) have demonstrated remarkable performance across a spectrum of graph-related tasks, however concerns persist regarding their vulnerability to adversarial perturbations. While prevailing defense strategies focus primarily on pre-processing techniques and adaptive message-passing schemes, this study delves into an under-explored dimension: the impact of weight initialization and associated hyper-parameters, such as training epochs, on a model’s robustness.We introduce a theoretical framework bridging the connection between initialization strategies and a network's resilience to adversarial perturbations. Our analysis reveals a direct relationship between initial weights, number of training epochs and the model’s vulnerability, offering new insights into adversarial robustness beyond conventional defense mechanisms. While our primary focus is on GNNs, we extend our theoretical framework, providing a general upper-bound applicable to Deep Neural Networks.Extensive experiments, spanning diverse models and real-world datasets subjected to various adversarial attacks, validate our findings. We illustrate that selecting appropriate initialization not only ensures performance on clean datasets but also enhances model robustness against adversarial perturbations, with observed gaps of up to 50\\\\% compared to alternative initialization approaches.\",\n",
       "  750: 'Score matching estimators for point processes have gained widespread attention in recent years because they do not require the calculation of intensity integrals, thereby effectively addressing the computational challenges in maximum likelihood estimation (MLE). Some existing works have proposed score matching estimators for point processes. However, this work demonstrates that the incompleteness of the estimators proposed in those works renders them applicable only to specific problems, and they fail for more general point processes. To address this issue, this work introduces the weighted score matching estimator to point processes. Theoretically, we prove the consistency of the estimator we propose. Experimental results indicate that our estimator accurately estimates model parameters on synthetic data and yields results consistent with MLE on real data. In contrast, existing score matching estimators fail to perform effectively. Codes are publicly available at \\\\url{https://github.com/KenCao2007/WSM_TPP}.',\n",
       "  751: \"When learning in strategic environments, a key question is whether agents can overcome uncertainty about their preferences to achieve outcomes they could have achieved absent any uncertainty. Can they do this solely through interactions with each other? We focus this question on the ability of agents to attain the value of their Stackelberg optimal strategy and study the impact of information asymmetry. We study repeated interactions in fully strategic environments where players' actions are decided based on learning algorithms that take into account their observed histories and knowledge of the game. We study the pure Nash equilibria (PNE) of a meta-game where players choose these algorithms as their actions. We demonstrate that if one player has perfect knowledge about the game, then any initial informational gap persists. That is, while there is always a PNE in which the informed agent achieves her Stackelberg value, there is a game where no PNE of the meta-game allows the partially informed player to achieve her Stackelberg value. On the other hand, if both players start with some uncertainty about the game, the quality of information alone does not determine which agent can achieve her Stackelberg value. In this case, the concept of information asymmetry becomes nuanced and depends on the game's structure. Overall, our findings suggest that repeated strategic interactions alone cannot facilitate learning effectively enough to earn an uninformed player her Stackelberg value.\",\n",
       "  752: \"Backdoor attacks aim to inject a backdoor into a classifier such that it predicts any input with an attacker-chosen backdoor trigger as an attacker-chosen target class. Existing backdoor attacks require either retraining the classifier with some clean data or modifying the model's architecture.As a result, they are 1) not applicable when clean data is unavailable, 2) less efficient when the model is large, and 3) less stealthy due to architecture changes. In this work, we propose DFBA, a novel retraining-free and data-free backdoor attack without changing the model architecture. Technically, our proposed method modifies a few parameters of a classifier to inject a backdoor. Through theoretical analysis, we verify that our injected backdoor is provably undetectable and unremovable by various state-of-the-art defenses under mild assumptions. Our evaluation on multiple datasets further demonstrates that our injected backdoor: 1) incurs negligible classification loss, 2) achieves 100\\\\% attack success rates, and 3) bypasses six existing state-of-the-art defenses. Moreover, our comparison with a state-of-the-art non-data-free backdoor attack shows our attack is more stealthy and effective against various defenses while achieving less classification accuracy loss.We will release our code upon paper acceptance.\",\n",
       "  753: 'Recently, vision model pre-training has evolved from relying on manually annotated datasets to leveraging large-scale, web-crawled image-text data. Despite these advances, there is no pre-training method that effectively exploits the interleaved image-text data, which is very prevalent on the Internet. Inspired by the recent success of compression learning in natural language processing, we propose a novel vision model pre-training method called Latent Compression Learning (LCL) for interleaved image-text data. This method performs latent compression learning by maximizing the mutual information between the inputs and outputs of a causal attention model. The training objective can be decomposed into two basic tasks: 1) contrastive learning between visual representation and preceding context, and 2) generating subsequent text based on visual representation. Our experiments demonstrate that our method not only matches the performance of CLIP on paired pre-training datasets (e.g., LAION), but can also leverage interleaved pre-training data (e.g., MMC4) to learn robust visual representations from scratch, showcasing the potential of vision model pre-training with interleaved image-text data.',\n",
       "  754: 'Designing causal bandit algorithms depends on two central categories of assumptions: (i) the extent of information about the underlying causal graphs and (ii) the extent of information about interventional statistical models. There have been extensive recent advances in dispensing with assumptions on either category. These include assuming known graphs but unknown interventional distributions, and the converse setting of assuming unknown graphs but access to restrictive hard/$\\\\operatorname{do}$ interventions, which removes the stochasticity and ancestral dependencies. Nevertheless, the problem in its general form, i.e., _unknown_ graph and _unknown_ stochastic intervention models, remains open. This paper addresses this problem and establishes that in a graph with $N$ nodes, maximum in-degree $d$ and maximum causal path length $L$, after $T$ interaction rounds the regret upper bound scales as $\\\\tilde{\\\\mathcal{O}}((cd)^{L-\\\\frac{1}{2}}\\\\sqrt{T} + d + RN)$ where $c>1$ is a constant and  $R$ is a measure of intervention power. A universal minimax lower bound is also established, which scales as $\\\\Omega(d^{L-\\\\frac{3}{2}}\\\\sqrt{T})$. Importantly, the graph size $N$ has a diminishing effect on the regret as $T$ grows. These bounds have matching behavior in $T$, exponential dependence on $L$, and polynomial dependence on $d$ (with the gap $d\\\\ $). On the algorithmic aspect, the paper presents a novel way of designing a computationally efficient CB algorithm, addressing a challenge that the existing CB algorithms using soft interventions face.',\n",
       "  755: 'Adam has become one of the most favored optimizers in deep learning problems. Despite its success in practice, numerous mysteries persist regarding its theoretical understanding. In this paper, we study the implicit bias of Adam in linear logistic regression. Specifically, we show that when the training data are linearly separable, the iterates of Adam converge towards a linear classifier that achieves the maximum $\\\\ell_\\\\infty$-margin in direction. Notably, for a general class of diminishing learning rates, this convergence occurs within polynomial time. Our result shed light on the difference between Adam and (stochastic) gradient descent from a theoretical perspective.',\n",
       "  756: 'Large language models can memorize and repeat their training data, causing privacy and copyright risks. To mitigate memorization, we introduce a subtle modification to the next-token training objective that we call the goldfish loss. During training, a randomly sampled subsets of tokens are excluded from the loss computation. These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set. We run extensive experiments training billion-scale LLaMA-2 models, both pre-trained and trained from scratch, and demonstrate significant reductions in extractable memorization with little to no impact on downstream benchmarks.Code and checkpoints: https://github.com/ahans30/goldfish-loss',\n",
       "  757: 'With the blossom of large language models (LLM), inference efficiency becomes increasingly important. Various approximate methods are proposed to reduce the cost at inference time. Contextual Sparsity (CS) is appealing for its training-free nature and its ability to reach a higher compression ratio seemingly without significant performance degradation. However, after a comprehensive evaluation of contextual sparsity methods on various complex generation tasks, we find that although CS succeeds in prompt-understanding tasks, it significantly degrades the model performance for reasoning, deduction, and knowledge-based tasks. Despite the gap in end-to-end accuracy, we observed that sparse models and original models often share the general problem-solving logic and require only a few token corrections to recover the original model performance. This paper introduces SIRIUS, an efficient correction mechanism, which significantly boosts CS models on reasoning tasks while maintaining its efficiency gain. SIRIUS is evaluated on 6 models with 8 difficult generation tasks in reasoning, deduction, and coding and shows consistent effectiveness and efficiency. Also, we carefully develop a system implementation for SIRIUS and show that SIRIUS delivers theoretical latency reduction with roughly a 20% reduction in latency for 8B model on-chip and a 35% reduction in latency for 70B model offloading. We open-source our implementation of Sirius at https://github.com/Infini-AI-Lab/Sirius.git.',\n",
       "  758: \"This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture,  and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. Project website: https://boyuan.space/diffusion-forcing/\",\n",
       "  759: \"Brain stimulation has the potential to create desired neural population activity states. However, it is challenging to search the large space of stimulation parameters, for example, selecting which subset of electrodes to be used for stimulation. In this scenario, creating a model that maps the configuration of stimulation parameters to the brain’s response can be beneficial. Training such an expansive model usually requires more stimulation-response samples than can be collected in a given experimental session. Furthermore, changes in the properties of the recorded activity over time can make it challenging to merge stimulation-response samples across sessions. To address these challenges, we propose MiSO (MicroStimulation Optimization), a closed-loop stimulation framework to drive neural population activity toward specified states by optimizing over a large stimulation parameter space. MiSO consists of three key components: 1) a neural activity alignment method to merge stimulation-response samples across sessions, 2) a statistical model trained on the merged samples to predict the brain's response to untested stimulation parameter configurations, and 3) an online optimization algorithm to adaptively update the stimulation parameter configuration based on the model's predictions. In this study, we implemented MiSO with a factor analysis (FA) based alignment method, a convolutional neural network (CNN), and an epsilon greedy optimization algorithm. We tested MiSO in closed-loop experiments using electrical microstimulation in the prefrontal cortex of a non-human primate. Guided by the CNN predictions, MiSO successfully searched amongst thousands of stimulation parameter configurations to drive the neural population activity toward specified states. More broadly, MiSO increases the clinical viability of neuromodulation technologies by enabling the use of many-fold larger stimulation parameter spaces.\",\n",
       "  760: 'The advent of Federated Learning (FL) highlights the practical necessity for the ’right to be forgotten’ for all clients, allowing them to request data deletion from the machine learning model’s service provider. This necessity has spurred a growing demand for Federated Unlearning (FU). Feature unlearning has gained considerable attention due to its applications in unlearning sensitive, backdoor, and biased features. Existing methods employ the influence function to achieve feature unlearning, which is impractical for FL as it necessitates the participation of other clients, if not all, in the unlearning process. Furthermore, current research lacks an evaluation of the effectiveness of feature unlearning. To address these limitations, we define feature sensitivity in evaluating feature unlearning according to Lipschitz continuity. This metric characterizes the model output’s rate of change or sensitivity to perturbations in the input feature. We then propose an effective federated feature unlearning framework called Ferrari, which minimizes feature sensitivity. Extensive experimental results and theoretical analysis demonstrate the effectiveness of Ferrari across various feature unlearning scenarios, including sensitive, backdoor, and biased features. The code is publicly available at https://github.com/OngWinKent/Federated-Feature-Unlearning',\n",
       "  761: \"In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines:1. An uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur,2. Uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and3. A reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward.In experiments on medical diagnosis, troubleshooting and the `20 Questions' game, UoT achieves an average performance improvement of 38.1% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task).\",\n",
       "  762: 'Sequences of events, such as crashes in the stock market or outages in a network, contain strong temporal dependencies, whose understanding is crucial to react to and influence future events. In this paper, we study the problem of discovering the underlying causal structure from event sequences. To this end, we introduce a new causal model, where individual events of the cause trigger events of the effect with dynamic delays. We show that in contrast to existing methods based on Granger causality, our model is identifiable for both instant and delayed effects.We base our approach on the Algorithmic Markov Condition, by which we identify the true causal network as the one that minimizes the Kolmogorov complexity. As the Kolmogorov complexity is not computable, we instantiate our model using Minimum Description Length and show that the resulting score identifies the causal direction. To discover causal graphs, we introduce the Cascade algorithm, which adds edges in topological order. Extensive evaluation shows that Cascade outperforms existing methods in settings with instantaneous effects, noise, and multiple colliders, and discovers insightful causal graphs on real-world data.',\n",
       "  763: 'Large Language Models (LLMs) are trained on vast amounts of data, most of which is automatically scraped from the internet. This data includes encyclopedic documents that harbor a vast amount of general knowledge (e.g., Wikipedia) but also potentially overlap with benchmark datasets used for evaluating LLMs. Consequently, evaluating models on test splits that might have leaked into the training set is prone to misleading conclusions. To foster sound evaluation of language models, we introduce a new test dataset named RepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a collection of five splits of test sets, four of which have not been released to the internet or exposed to LLM APIs prior to this publication. Each sample in RepLiQA comprises (1) a reference document crafted by a human annotator and depicting an imaginary scenario (e.g., a news article) absent from the internet; (2) a question about the document’s topic; (3) a ground-truth answer derived directly from the information in the document; and (4) the paragraph extracted from the reference document containing the answer. As such, accurate answers can only be generated if a model can find relevant content within the provided document. We run a large-scale benchmark comprising several state-of-the-art LLMs to uncover differences in performance across models of various types and sizes in a context-conditional language modeling setting. Released splits of RepLiQA can be found here: https://huggingface.co/datasets/ServiceNow/repliqa.',\n",
       "  764: 'In linear bandits, how can a learner effectively learn when facing corrupted rewards? While significant work has explored this question, a holistic understanding across different adversarial models and corruption measures is lacking, as is a full characterization of the minimax regret bounds. In this work, we compare two types of corruptions commonly considered: strong corruption, where the corruption level depends on the learner’s chosen action, and weak corruption, where the corruption level does not depend on the learner’s chosen action. We provide a unified framework to analyze these corruptions. For stochastic linear bandits, we fully characterize the gap between the minimax regret under strong and weak corruptions. We also initiate the study of corrupted adversarial linear bandits, obtaining upper and lower bounds with matching dependencies on the corruption level. Next, we reveal a connection between corruption-robust learning and learning with gap-dependent misspecification—a setting first studied by Liu et al. (2023a), where the misspecification level of an action or policy is proportional to its suboptimality. We present a general reduction that enables any corruption-robust algorithm to handle gap-dependent misspecification. This allows us to recover the results of Liu et al. (2023a) in a black-box manner and significantly generalize them to settings like linear MDPs, yielding the first results for gap-dependent misspecification in reinforcement learning. However, this general reduction does not attain the optimal rate for gap-dependent misspecification. Motivated by this, we develop a specialized algorithm that achieves optimal bounds for gap-dependent misspecification in linear bandits, thus answering an open question posed by Liu et al. (2023a).',\n",
       "  765: 'Multivariate time series forecasting is of central importance in modern intelligent decision systems. The dynamics of multivariate time series are jointly characterized by temporal dependencies and spatial correlations. Hence, it is equally important to build the forecasting models from both perspectives. The real-world multivariate time series data often presents spatial correlations that show structures and evolve dynamically. To capture such dynamic spatial structures, the existing forecasting approaches often rely on a two-stage learning process (learning dynamic series representations and then generating spatial structures), which is sensitive to the small time-window input data and has high variance. To address this, we propose a novel forecasting model with a structured matrix basis. At its core is a dynamic spatial structure generation function whose output space is well-constrained and the generated structures have lower variance, meanwhile, it is more expressive and can offer interpretable dynamics. This is achieved via a novel structured parameterization and imposing structure regularization on the matrix basis. The resulting forecasting model can achieve up to $8.5\\\\%$ improvements over the existing methods on six benchmark datasets, and meanwhile, it enables us to gain insights into the dynamics of underlying systems.',\n",
       "  766: 'We introduce a new PAC-Bayes oracle bound for unbounded losses that extends Cramér-Chernoff bounds to the PAC-Bayesian setting. The proof technique relies on controlling the tails of certain random variables involving the Cramér transform of the loss. Our approach naturally leverages properties of Cramér-Chernoff bounds, such as exact optimization of the free parameter in many PAC-Bayes bounds. We highlight several applications of the main theorem. Firstly, we show that our bound recovers and generalizes previous results. Additionally, our approach allows working with richer assumptions that result in more informative and potentially tighter bounds. In this direction, we provide a general bound under a new model-dependent assumption from which we obtain bounds based on parameter norms and log-Sobolev inequalities. Notably, many of these bounds can be minimized to obtain distributions beyond the Gibbs posterior and provide novel theoretical coverage to existing regularization techniques.',\n",
       "  767: 'A key goal in mechanistic interpretability is circuit analysis: finding sparse subgraphs of models corresponding to specific behaviors or capabilities. However, MLP sublayers make fine-grained circuit analysis on transformer-based language models difficult. In particular, interpretable features—such as those found by sparse autoencoders (SAEs)—are typically linear combinations of extremely many neurons, each with its own nonlinearity to account for. Circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior. To address this we explore transcoders, which seek to faithfully approximate a densely activating MLP layer with a wider, sparsely-activating MLP layer. We introduce a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers. The resulting circuits neatly factorize into input-dependent and input-invariant terms. We then successfully train transcoders on language models with 120M, 410M, and 1.4B parameters, and find them to perform at least on par with SAEs in terms of sparsity, faithfulness, and human-interpretability. Finally, we apply transcoders to reverse-engineer unknown circuits in the model, and we obtain novel insights regarding the \"greater-than circuit\" in GPT2-small. Our results suggest that transcoders can prove effective in decomposing model computations involving MLPs into interpretable circuits. Code is available at https://github.com/jacobdunefsky/transcoder_circuits/',\n",
       "  768: 'Integer linear programs (ILPs) are commonly employed to model diverse practical problems such as scheduling and planning. Recently, machine learning techniques have been utilized to solve ILPs.  A straightforward idea is to train a model via supervised learning, with an ILP as the input and an optimal solution as the label. An ILP is symmetric if its variables can be permuted without changing the problem structure, resulting in numerous equivalent and optimal solutions. Randomly selecting an optimal solution as the label can introduce variability in the training data, which may hinder the model from learning stable patterns. In this work, we incorporate the intrinsic symmetry of ILPs and propose a novel training framework called SymILO. Specifically, we modify the learning task by introducing solution permutation along with neural network weights as learnable parameters and then design an alternating algorithm to jointly optimize the loss function.We conduct extensive experiments on ILPs involving different symmetries and the computational results demonstrate that our symmetry-aware approach significantly outperforms three existing methods----achieving $50.3\\\\\\\\%$, $66.5\\\\\\\\%$, and $45.4\\\\\\\\%$ average improvements, respectively.',\n",
       "  769: 'Machine learning systems often acquire biases by leveraging undesired features in the data, impacting accuracy variably across different sub-populations of the data. However, our current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics. To address this gap, this paper explores the evolution of bias in a teacher-student setup that models different data sub-populations with a Gaussian-mixture model. We provide an analytical description of the stochastic gradient descent dynamics of a linear classifier in this setup, which we prove to be exact in high dimension.Notably, our analysis identifies different properties of the sub-populations that drive bias at different timescales and hence shows a shifting preference of our classifier during training. By applying our general solution to fairness and robustness, we delineate how and when heterogeneous data and spurious features can generate and amplify bias. We empirically validate our results in more complex scenarios by training deeper networks on synthetic and real data, i.e. using CIFAR10, MNIST, and CelebA datasets.',\n",
       "  770: 'Understanding communication and information processing among brain regions of interest (ROIs) is highly dependent on long-range connectivity, which plays a crucial role in facilitating diverse functional neural integration across the entire brain. However, previous studies generally focused on the short-range dependencies within brain networks while neglecting the long-range dependencies, limiting an integrated understanding of brain-wide communication. To address this limitation, we propose Adaptive Long-range aware TransformER (ALTER), a brain graph transformer to capture long-range dependencies between brain ROIs utilizing biased random walk. Specifically, we present a novel long-range aware strategy to explicitly capture long-range dependencies between brain ROIs. By guiding the walker towards the next hop with higher correlation value, our strategy simulates the real-world brain-wide communication. Furthermore, by employing the transformer framework, ALERT adaptively integrates both short- and long-range dependencies between brain ROIs, enabling an integrated understanding of multi-level communication across the entire brain. Extensive experiments on ABIDE and ADNI datasets demonstrate that ALTER consistently outperforms generalized state-of-the-art graph learning methods (including SAN, Graphormer, GraphTrans, and LRGNN) and other graph learning based brain network analysis methods (including FBNETGEN, BrainNetGNN, BrainGNN, and BrainNETTF) in neurological disease diagnosis.',\n",
       "  771: 'In multi-sequence Magnetic Resonance Imaging (MRI), the accurate segmentation of the kidney and tumor based on traditional supervised methods typically necessitates detailed annotation for each sequence, which is both time-consuming and labor-intensive. Unsupervised Domain Adaptation (UDA) methods can effectively mitigate inter-domain differences by aligning cross-modal features, thereby reducing the annotation burden. However, most existing UDA methods are limited to one-to-one domain adaptation, which tends to be inefficient and resource-intensive when faced with multi-target domain transfer tasks. To address this challenge, we propose a novel and efficient One-to-Multiple Progressive Style Transfer Unsupervised Domain-Adaptive (PSTUDA) framework for kidney and tumor segmentation in multi-sequence MRI. Specifically, we develop a multi-level style dictionary to explicitly store the style information of each target domain at various stages, which alleviates the burden of a single generator in a multi-target transfer task and enables effective decoupling of content and style. Concurrently, we employ multiple cascading style fusion modules that utilize point-wise instance normalization to progressively recombine content and style features, which enhances cross-modal alignment and structural consistency. Experiments conducted on the private MSKT and public KiTS19 datasets demonstrate the superiority of the proposed PSTUDA over comparative methods in multi-sequence kidney and tumor segmentation. The average Dice Similarity Coefficients are increased by at least 1.8% and 3.9%, respectively. Impressively, our PSTUDA not only significantly reduces the floating-point computation by approximately 72% but also reduces the number of model parameters by about 50%, bringing higher efficiency and feasibility to practical clinical applications.',\n",
       "  772: 'The CLIP network excels in various tasks, but struggles with text-visual images i.e., images that contain both text and visual objects; it risks confusing textual and visual representations. To address this issue, we propose MirrorCLIP, a zero-shot framework, which disentangles the image features of CLIP by exploiting the difference in the mirror effect between visual objects and text in the images. Specifically, MirrorCLIP takes both original and flipped images as inputs, comparing their features dimension-wise in the latent space to generate disentangling masks. With disentangling masks, we further design filters to separate textual and visual factors more precisely, and then get disentangled representations. Qualitative experiments using stable diffusion models and class activation mapping (CAM) validate the effectiveness of our disentanglement. Moreover, our proposed MirrorCLIP reduces confusion when encountering text-visual images and achieves a substantial improvement on typographic defense, further demonstrating its superior ability of disentanglement. Our code is available at https://github.com/tcwangbuaa/MirrorCLIP',\n",
       "  773: \"Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.\",\n",
       "  774: \"Natural behaviors, even stereotyped ones, exhibit variability. Despite its role in exploring and learning, the function and neural basis of this variability is still not well understood. Given the coupling between neural activity and behavior, we ask what type of neural variability does not compromise behavioral performance. While previous studies typically curtail variability to allow for high task performance in neural networks, our approach takes the reversed perspective. We investigate how to generate maximal neural variability while at the same time having high network performance. To do so, we extend to neural activity the maximum occupancy principle (MOP) developed for behavior, and refer to this new neural principle as NeuroMOP. NeuroMOP posits that the goal of the nervous system is to maximize future action-state entropy, a reward-free, intrinsic motivation that entails creating all possible activity patterns while avoiding terminal or dangerous ones.We show that this goal can be achieved through a neural network controller that injects currents (actions) into a recurrent neural network of fixed random weights to maximize future cumulative action-state entropy. High activity variability can be induced while adhering to an energy constraint or while avoiding terminal states defined by specific neurons' activities, also in a context-dependent manner. The network solves these tasks by flexibly switching between stochastic and deterministic modes as needed and projecting noise onto a null space. Based on future maximum entropy production, NeuroMOP contributes to a novel theory of neural variability that reconciles stochastic and deterministic behaviors within a single framework.\",\n",
       "  775: 'We present a maximum entropy inverse reinforcement learning (IRL) approach for improving the sample quality of diffusion generative models, especially when the number of generation time steps is small. Similar to how IRL trains a policy based on the reward function learned from expert demonstrations, we train (or fine-tune) a diffusion model using the log probability density estimated from training data. Since we employ an energy-based model (EBM) to represent the log density, our approach boils down to the joint training of a diffusion model and an EBM. Our IRL formulation, named Diffusion by Maximum Entropy IRL (DxMI), is a minimax problem that reaches equilibrium when both models converge to the data distribution. The entropy maximization plays a key role in DxMI, facilitating the exploration of the diffusion model and ensuring the convergence of the EBM. We also propose Diffusion by Dynamic Programming (DxDP), a novel reinforcement learning algorithm for diffusion models, as a subroutine in DxMI. DxDP makes the diffusion model update in DxMI efficient by transforming the original problem into an optimal control formulation where value functions replace back-propagation in time. Our empirical studies show that diffusion models fine-tuned using DxMI can generate high-quality samples in as few as 4 and 10 steps.  Additionally, DxMI enables the training of an EBM without MCMC, stabilizing EBM training dynamics and enhancing anomaly detection performance.',\n",
       "  776: 'Restless multi-armed bandits (RMAB) extend multi-armed bandits so arm pulls impact future arm states. Despite the success of RMABs, a key limiting assumption is the separability of rewards into a sum across arms. We address this deficiency by proposing restless-multi-armed bandit with global rewards (RMAB-G), a generalization of RMABs to global non-separable rewards. To solve RMAB-G, we develop the Linear-Whittle and Shapley-Whittle indices, which extend Whittle indices from RMABs to RMAB-Gs. We prove approximation bounds which demonstrate how Linear and Shapley-Whittle indices fail for non-linear rewards. To overcome this limitation, we propose two sets of adaptive policies: the first computes indices iteratively and the second combines indices with Monte-Carlo Tree Search (MCTS). Empirically, we demonstrate that adaptive policies outperform both pre-computed index policies and baselines in synthetic and real-world food rescue datasets.',\n",
       "  777: 'This paper conducts a comprehensive study of the learning curves of kernel ridge regression (KRR) under minimal assumptions.Our contributions are three-fold: 1) we analyze the role of key properties of the kernel, such as its spectral eigen-decay, the characteristics of the eigenfunctions, and the smoothness of the kernel; 2) we demonstrate the validity of the Gaussian Equivalent Property (GEP), which states that the generalization performance of KRR remains the same when the whitened features are replaced by standard Gaussian vectors, thereby shedding light on the success of previous analyzes under the Gaussian Design Assumption; 3) we derive novel bounds that improve over existing bounds across a broad range of setting such as (in)dependent feature vectors and various combinations of eigen-decay rates in the over/underparameterized regimes.',\n",
       "  778: \"Distributional reinforcement learning (DRL) has achieved empirical success in various domains.One of the core tasks in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\\\\eta^\\\\pi$ for a given policy $\\\\pi$.The distributional temporal difference learning has been accordingly proposed, whichis an extension of the temporal difference learning (TD) in the classic RL area.In the tabular case,  Rowland et al. [2018] and Rowland et al. [2023] proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference learning (CTD) and quantile temporal difference learning (QTD), respectively.In this paper, we go a step further and analyze the finite-sample performance of distributional TD.To facilitate theoretical analysis, we propose a non-parametric distributional TD learning (NTD).For a $\\\\gamma$-discounted infinite-horizon tabular Markov decision process,we show that for NTD we need $\\\\widetilde O\\\\left(\\\\frac{1}{\\\\varepsilon^{2p}(1-\\\\gamma)^{2p+1}}\\\\right)$ iterations to achieve an $\\\\varepsilon$-optimal estimator with high probability, when the estimation error is measured by the $p$-Wasserstein distance.This sample complexity bound is minimax optimal (up to logarithmic factors) in the case of the $1$-Wasserstein distance.To achieve this, we establish a novel Freedman's inequality in Hilbert spaces, which would be of independent interest.In addition, we revisit CTD, showing that the same non-asymptotic convergence bounds hold for CTD in the case of the $p$-Wasserstein distance.\",\n",
       "  779: 'Tree ensembles are one of the most widely used model classes. However, these models are susceptible to adversarial examples, i.e., slightly perturbed examples that elicit a misprediction. There has been significant research on designing approaches to construct such examples for tree ensembles. But this is a computationally challenging problem that often must be solved a large number of times (e.g., for all examples in a training set). This is compounded by the fact that current approaches attempt to find such examples from scratch. In contrast, we exploit the fact that multiple similar problems are being solved. Specifically, our approach exploits the insight that adversarial examples for tree ensembles tend to perturb a consistent but relatively small set of features. We show that we can quickly identify this set of features and use this knowledge to speedup constructing adversarial examples.',\n",
       "  780: 'Class-agnostic object detection (OD) can be a cornerstone or a bottleneck for many downstream vision tasks. Despite considerable advancements in bottom-up and multi-object discovery methods that leverage basic visual cues to identify salient objects, consistently achieving a high recall rate remains difficult due to the diversity of object types and their contextual complexity. In this work, we investigate using vision-language models (VLMs) to enhance object detection via a self-supervised prompt learning strategy. Our initial findings indicate that manually crafted text queries often result in undetected objects, primarily because detection confidence diminishes when the query words exhibit semantic overlap. To address this, we propose a Dispersing Prompt Expansion (DiPEx) approach. DiPEx progressively learns to expand a set of distinct, non-overlapping hyperspherical prompts to enhance recall rates, thereby improving performance in downstream tasks such as out-of-distribution OD. Specifically, DiPEx initiates the process by self-training generic parent prompts and selecting the one with the highest semantic uncertainty for further expansion. The resulting child prompts are expected to inherit semantics from their parent prompts while capturing more fine-grained semantics. We apply dispersion losses to ensure high inter-class discrepancy among child prompts while preserving semantic consistency between parent-child prompt pairs. To prevent excessive growth of the prompt sets, we utilize the maximum angular coverage (MAC) of the semantic space as a criterion for early termination. We demonstrate the effectiveness of DiPEx through extensive class-agnostic OD and OOD-OD experiments on MS-COCO and LVIS, surpassing other prompting methods by up to 20.1% in AR and achieving a 21.3% AP improvement over SAM.',\n",
       "  781: 'Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks. Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM. However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs. To address this, we propose accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM. Specifically, we quantize each weight matrix into binary matrices paired with group-wise scaling factors. The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices. To reduce accuracy loss, we present a multi-objective optimization method to minimize both weight and output activation reparameterization errors. Additionally, based on varying sensitivity across layers to reparameterization, we develop an automated bit allocation strategy to further reduce memory usage and latency. Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity reductions of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3- and 2-bit precision, respectively, and more than 80% memory and energy reductions over the original LLMs. Codes and models are available at https://github.com/GATECH-EIC/ShiftAddLLM.',\n",
       "  782: \"The Spiking Neural Network (SNN) is a biologically inspired neural network infrastructure that has recently garnered significant attention. It utilizes binary spike activations to transmit information, thereby replacing multiplications with additions and resulting in high energy efficiency. However, training an SNN directly poses a challenge due to the undefined gradient of the firing spike process. Although prior works have employed various surrogate gradient training methods that use an alternative function to replace the firing process during back-propagation, these approaches ignore an intrinsic problem: gradient vanishing. To address this issue, we propose a shortcut back-propagation method in the paper, which advocates for transmitting the gradient directly from the loss to the shallow layers. This enables us to present the gradient to the shallow layers directly, thereby significantly mitigating the gradient vanishing problem. Additionally, this method does not introduce any burden during the inference phase.To strike a balance between final accuracy and ease of training, we also propose an evolutionary training framework and implement it by inducing a balance coefficient that dynamically changes with the training epoch, which further improves the network's performance. Extensive experiments conducted over static and dynamic datasets using several popular network structures reveal that our method consistently outperforms state-of-the-art methods.\",\n",
       "  783: 'Pedestrian pre-collision pose is one of the key factors to determine the degree of pedestrian-vehicle injury in collision. Human pose estimation algorithm is an effective method to estimate pedestrian emergency pose from accident video. However, the pose estimation model trained by the existing daily human pose datasets has poor robustness under specific poses such as pedestrian pre-collision pose, and it is difficult to obtain human pose datasets in the wild scenes, especially lacking scarce data such as pedestrian pre-collision pose in traffic scenes. In this paper, we collect pedestrian-vehicle collision pose from the dashcam perspective of dashcam and construct the first Pedestrian-Vehicle Collision Pose dataset (PVCP) in a semi-automatic way, including 40k+ accident frames and 20K+ pedestrian pre-collision pose annotation (2D, 3D, Mesh). Further, we construct a Pedestrian Pre-collision Pose Estimation Network (PPSENet) to estimate the collision pose and shape sequence of pedestrians from pedestrian-vehicle accident videos. The PPSENet first estimates the 2D pose from the image (Image to Pose, ITP) and then lifts the 2D pose to 3D mesh (Pose to Mesh, PTM). Due to the small size of the dataset, we introduce a pre-training model that learns the human pose prior on a large number of pose datasets, and use iterative regression to estimate the pre-collision pose and shape of pedestrians. Further, we classify the pre-collision pose sequence and introduce pose class loss, which achieves the best accuracy compared with the existing relevant \\\\textit{state-of-the-art} methods. Code and data are available for research at https://github.com/wmj142326/PVCP.',\n",
       "  784: 'We present Diffusion-KTO, a novel approach for aligning text-to-image diffusion models by formulating the alignment objective as the maximization of expected human utility. Unlike previous methods, Diffusion-KTO does not require collecting pairwise preference data nor training a complex reward model. Instead, our objective uses per-image binary feedback signals, e.g. likes or dislikes, to align the model with human preferences. After fine-tuning using Diffusion-KTO, text-to-image diffusion models exhibit improved performance compared to existing techniques, including supervised fine-tuning and Diffusion-DPO, both in terms of human judgment and automatic evaluation metrics such as PickScore and ImageReward. Overall, Diffusion-KTO unlocks the potential of leveraging readily available per-image binary preference signals and broadens the applicability of aligning text-to-image diffusion models with human preferences.',\n",
       "  785: \"This work presents BAdam, an optimization method that leverages the block coordinate descent (BCD) framework with Adam's update rule. BAdam offers a memory efficient approach to the full parameter finetuning of large language models. We conduct   a theoretical convergence analysis for BAdam in the deterministic case. Experimentally, we apply BAdam to finetune the Llama 3-8B and Llama 3-70B models using a single RTX3090-24GB GPU and 4 A100-80GB GPUs, respectively. The results confirm BAdam's efficiency in terms of memory usage, running time, and optimization capability. Furthermore, the downstream performance evaluation based on MT-bench and math benchmarks shows that BAdam outperforms existing memory efficient baselines such as LoRA. It also demonstrates that BAdam can achieve comparable or even superior performance compared to Adam. Finally, the ablation study using SGD's update rule illustrates the suitability of BCD for finetuning LLMs. Our code can be easily integrated into any PyTorch-based codebase and is available at https://github.com/Ledzy/BAdam.\",\n",
       "  786: \"Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to naively deploy on consumer hardware. While much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM), a sparsification framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. In addition, we demonstrate that the pre-computed posterior precision from the Laplace approximation can be re-used to define a cheap pruning criterion, which outperforms many existing (more expensive) approaches. We demonstrate the effectiveness of our framework, especially at high sparsity levels, across a range of different neural network architectures and datasets.\",\n",
       "  787: \"Multi-block minimax bilevel optimization has been studied recently due to its great potential in multi-task learning, robust machine learning, and few-shot learning. However, due to the complex three-level optimization structure, existing algorithms often suffer from issues such as high computing costs due to the second-order model derivatives or high memory consumption in storing all blocks' parameters. In this paper, we tackle these challenges by proposing two novel fully first-order algorithms named FOSL and MemCS. FOSL features a fully single-loop structure by updating all three variables simultaneously, and MemCS is a memory-efficient double-loop algorithm with cold-start initialization. We provide a comprehensive convergence analysis for both algorithms under full and partial block participation, and show that their sample complexities match or outperform those of the same type of methods in standard bilevel optimization. We evaluate our methods in two applications: the recently proposed multi-task deep AUC maximization and a novel rank-based robust meta-learning. Our methods consistently improve over existing methods with better performance over various datasets.\",\n",
       "  788: 'Data-driven algorithm design is a paradigm that uses statistical and machine learning techniques to select from a class of algorithms for a computational problem an algorithm that has the best expected performance with respect to some (unknown) distribution on the instances of the problem. We build upon recent work in this line of research by considering the setup where, instead of selecting a single algorithm that has the best performance, we allow the possibility of selecting an algorithm based on the instance to be solved, using neural networks. In particular, given a representative sample of instances, we learn a neural network that maps an instance of the problem to the most appropriate algorithm for that instance. We formalize this idea and derive rigorous sample complexity bounds for this learning problem, in the spirit of recent work in data-driven algorithm design. We then apply this approach to the problem of making good decisions in the branch-and-cut framework for mixed-integer optimization (e.g., which cut to add?). In other words, the neural network will take as input a mixed-integer optimization instance and output a decision that will result in a small branch-and-cut tree for that instance. Our computational results provide evidence that our particular way of using neural networks for cut selection can make a significant impact in reducing branch-and-cut tree sizes, compared to previous data-driven approaches.',\n",
       "  789: 'Large language models (LLMs) have made significant progress in natural language processing tasks and demonstrate considerable potential in the legal domain.  However, legal applications demand high standards of accuracy, reliability, and fairness. Applying existing LLMs to legal systems without careful evaluation of their potential and limitations could pose significant risks in legal practice.To this end, we introduce a standardized comprehensive Chinese legal benchmark LexEval.This benchmark is notable in the following three aspects: (1) Ability Modeling: We propose a new taxonomy of legal cognitive abilities to organize different tasks. (2) Scale: To our knowledge, LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions. (3) Data: we utilize formatted existing datasets, exam datasets and newly annotated datasets by legal experts to comprehensively evaluate the various capabilities of LLMs. LexEval not only focuses on the ability of LLMs to apply fundamental legal knowledge but also dedicates efforts to examining the ethical issues involved in their application.We evaluated 38 open-source and commercial LLMs and obtained some interesting findings.  The experiments and findings offer valuable insights into the challenges and potential solutions for developing Chinese legal systems and LLM evaluation pipelines. The LexEval dataset and leaderboard are publicly available at https://github.com/CSHaitao/LexEval and will be continuously updated.',\n",
       "  790: 'Recently, there have been explorations of generalist segmentation models that can effectively tackle a variety of image segmentation tasks within a unified in-context learning framework. However, these methods still struggle with task ambiguity in in-context segmentation, as not all in-context examples can accurately convey the task information. In order to address this issue, we present SINE, a simple image $\\\\textbf{S}$egmentation framework utilizing $\\\\textbf{in}$-context $\\\\textbf{e}$xamples. Our approach leverages a Transformer encoder-decoder structure, where the encoder provides high-quality image representations, and the decoder is designed to yield multiple task-specific output masks to eliminate task ambiguity effectively. Specifically, we introduce an In-context Interaction module to complement in-context information and produce correlations between the target image and the in-context example and a Matching Transformer that uses fixed matching and a Hungarian algorithm to eliminate differences between different tasks. In addition, we have further perfected the current evaluation system for in-context image segmentation, aiming to facilitate a holistic appraisal of these models. Experiments on various segmentation tasks show the effectiveness of the proposed method.',\n",
       "  791: 'Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features---dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features.Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy.Introducing pre-trained token embeddings to a randomly initialized model rescues its performance.Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.',\n",
       "  792: 'Neural operators, serving as physics surrogate models, have recently gained increased interest. With ever increasing problem complexity, the natural question arises: what is an efficient way to scale neural operators to larger and more complex simulations - most importantly by taking into account different types of simulation datasets. This is of special interest since, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. Whereas the flexibility of transformers has enabled unified architectures across domains, neural operators mostly follow a problem specific design, where GNNs are commonly used for Lagrangian simulations and grid-based models predominate Eulerian simulations. We introduce Universal Physics Transformers (UPTs), an efficient and unified learning paradigm for a wide range of spatio-temporal problems. UPTs operate without grid- or particle-based latent structures, enabling flexibility and scalability across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space representation at any point in space-time. We demonstrate diverse applicability and efficacy of UPTs in mesh-based fluid simulations, and steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics.',\n",
       "  793: \"Mobility impairment caused by limb loss, aging, stroke, and other movement deficiencies is a significant challenge faced by millions of individuals worldwide. Advanced assistive technologies, such as prostheses and orthoses, have the potential to greatly improve the quality of life for such individuals. A critical component in the design of these technologies is the accurate forecasting of reference joint motion for impaired limbs, which is hindered by the scarcity of joint locomotion data available for these patients. To address this, we propose ReMAP, a novel model repurposing strategy that leverages deep learning's reprogramming property, incorporating network inversion principles and retrieval-augmented mapping. Our approach adapts models originally designed for able-bodied individuals to forecast joint motion in limb-impaired patients without altering model parameters. We demonstrate the efficacy of ReMAP through extensive empirical studies on data from below-knee amputated patients, showcasing significant improvements over traditional transfer learning and fine-tuning methods. These findings have significant implications for advancing assistive technology and mobility for patients with amputations, stroke, or aging.\",\n",
       "  794: 'Recent researches have proven that pre-training on large-scale person images extracted from internet videos is an effective way in learning better representations for person re-identification. However, these researches are mostly confined to pre-training at the instance-level or single-video tracklet-level. They ignore the identity-invariance in images of the same person across different videos, which is a key focus in person re-identification. To address this issue, we propose a Cross-video Identity-cOrrelating pre-traiNing (CION) framework. Defining a noise concept that comprehensively considers both intra-identity consistency and inter-identity discrimination, CION seeks the identity correlation from cross-video images by modeling it as a progressive multi-level denoising problem. Furthermore, an identity-guided self-distillation loss is proposed to implement better large-scale pre-training by mining the identity-invariance within person images. We conduct extensive experiments to verify the superiority of our CION in terms of efficiency and performance. CION achieves significantly leading performance with even fewer training samples. For example, compared with the previous state-of-the-art ISR, CION with the same ResNet50-IBN achieves higher mAP of 93.3% and 74.3% on Market1501 and MSMT17, while only utilizing 8% training samples. Finally, with CION demonstrating superior model-agnostic ability, we contribute a model zoo named ReIDZoo to meet diverse research and application needs in this field. It contains a series of CION pre-trained models with spanning structures and parameters, totaling 32 models with 10 different structures, including GhostNet, ConvNext, RepViT, FastViT and so on. The code and models will be open-sourced.',\n",
       "  795: 'We propose to explore an interesting and promising problem, Cloud Object Detector Adaptation (CODA), where the target domain leverages detections provided by a large cloud model to build a target detector. Despite with powerful generalization capability, the cloud model still cannot achieve error-free detection in a specific target domain. In this work, we present a novel Cloud Object detector adaptation method by Integrating different source kNowledge (COIN). The key idea is to incorporate a public vision-language model (CLIP) to distill positive knowledge while refining negative knowledge for adaptation by self-promotion gradient direction alignment. To that end, knowledge dissemination, separation, and distillation are carried out successively. Knowledge dissemination combines knowledge from cloud detector and CLIP model to initialize a target detector and a CLIP detector in target domain. By matching CLIP detector with the cloud detector, knowledge separation categorizes detections into three parts: consistent, inconsistent and private detections such that divide-and-conquer strategy can be used for knowledge distillation. Consistent and private detections are directly used to train target detector; while inconsistent detections are fused based on a consistent knowledge generation network, which is trained by aligning the gradient direction of inconsistent detections to that of consistent detections, because it provides a direction toward an optimal target detector. Experiment results demonstrate that the proposed COIN method achieves the state-of-the-art performance.',\n",
       "  796: 'In-context learning (ICL) allows transformer-based language models that are pre-trained on general text to quickly learn a specific task with a few \"task demonstrations\"  without updating their parameters, significantly boosting their flexibility and generality. ICL possesses many distinct characteristics from conventional machine learning, thereby requiring new approaches to interpret this learning paradigm. Taking the viewpoint of recent works showing that transformers learn in context by formulating an internal optimizer, we propose an influence function-based attribution technique, DETAIL, that addresses the specific characteristics of ICL. We empirically verify the effectiveness of our approach for demonstration attribution while being computationally efficient. Leveraging the results, we then show how DETAIL can help improve model performance in real-world scenarios through demonstration reordering and curation. Finally, we experimentally prove the wide applicability of DETAIL by showing our attribution scores obtained on white-box models are transferable to black-box models in improving model performance.',\n",
       "  797: 'As the problem of minimizing functionals on the Wasserstein space encompasses many applications in machine learning, different optimization algorithms on $\\\\mathbb{R}^d$ have received their counterpart analog on the Wasserstein space. We focus here on lifting two explicit algorithms: mirror descent and preconditioned gradient descent. These algorithms have been introduced to better capture the geometry of the function to minimize and are provably convergent under appropriate (namely relative) smoothness and convexity conditions. Adapting these notions to the Wasserstein space, we prove guarantees of convergence of some Wasserstein-gradient-based discrete-time schemes for new pairings of objective functionals and regularizers. The difficulty here is to carefully select along which curves the functionals should be smooth and convex. We illustrate the advantages of adapting the geometry induced by the regularizer on ill conditioned optimization tasks, and showcase the improvement of choosing different discrepancies and geometries in a computational biology task of aligning single-cells.',\n",
       "  798: 'While inorganic retrosynthesis planning is essential in the field of chemical science, the application of machine learning in this area has been notably less explored compared to organic retrosynthesis planning. In this paper, we propose Retrieval-Retro for inorganic retrosynthesis planning, which implicitly extracts the precursor information of reference materials that are retrieved from the knowledge base regarding domain expertise in the field. Specifically, instead of directly employing the precursor information of reference materials, we propose implicitly extracting it with various attention layers, which enables the model to learn novel synthesis recipes more effectively.Moreover, during retrieval, we consider the thermodynamic relationship between target material and precursors, which is essential domain expertise in identifying the most probable precursor set among various options. Extensive experiments demonstrate the superiority of Retrieval-Retro in retrosynthesis planning, especially in discovering novel synthesis recipes, which is crucial for materials discovery.The source code for Retrieval-Retro is available at https://github.com/HeewoongNoh/Retrieval-Retro.',\n",
       "  799: 'Reinforcement Learning (RL) utilizing kernel ridge regression to predict the expected value function represents a powerful method with great representational capacity. This setting is a highly versatile framework amenable to analytical results. We consider kernel-based function approximation for RL in the infinite horizon average reward setting, also referred to as the undiscounted setting. We propose an optimistic algorithm, similar to acquisition function based algorithms in the special case of bandits. We establish novel no-regret performance guarantees for our algorithm, under kernel-based modelling assumptions. Additionally, we derive a novel confidence interval for the kernel-based prediction of the expected value function, applicable across various RL problems.',\n",
       "  800: 'High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce MathPile, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of “less is more”, firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates and conducted continual pre-training experiments, booting the performance on common mathematical reasoning benchmarks. We aim for our MathPile to boost language models’ mathematical reasoning abilities and open-source its different versions and processing scripts to advance the field.',\n",
       "  801: 'We introduce the first continuous-time score-based generative model that leverages fractional diffusion processes for its underlying dynamics. Although diffusion models have excelled at capturing data distributions, they still suffer from various limitations such as slow convergence, mode-collapse on imbalanced data, and lack of diversity. These issues are partially linked to the use of light-tailed Brownian motion (BM) with independent increments. In this paper, we replace BM with an approximation of its non-Markovian counterpart, fractional Brownian motion (fBM), characterized by correlated increments and Hurst index $H \\\\in (0,1)$, where $H=0.5$ recovers the classical BM. To ensure tractable inference and learning, we employ a recently popularized Markov approximation of fBM (MA-fBM) and derive its reverse-time model, resulting in *generative fractional diffusion models* (GFDM). We characterize the forward dynamics using a continuous reparameterization trick and propose *augmented score matching* to efficiently learn the score function, which is partly known in closed form, at minimal added cost. The ability to drive our diffusion model via MA-fBM offers flexibility and control. $H \\\\leq 0.5$ enters the regime of *rough paths* whereas $H>0.5$ regularizes diffusion paths and invokes long-term memory. The Markov approximation allows added control by varying the number of Markov processes linearly combined to approximate fBM. Our evaluations on real image datasets demonstrate that GFDM achieves greater pixel-wise diversity and enhanced image quality, as indicated by a lower FID, offering a promising alternative to traditional diffusion models',\n",
       "  802: 'The relationship between the number of training data points, the number of parameters, and the generalization capabilities of models has been widely studied. Previous work has shown that double descent can occur in the over-parameterized regime and that the standard bias-variance trade-off holds in the under-parameterized regime. These works provide multiple reasons for the existence of the peak. We postulate that the location of the peak depends on the technical properties of both the spectrum as well as the eigenvectors of the sample covariance. We present two simple examples that provably exhibit double descent in the under-parameterized regime and do not seem to occur for reasons provided in prior work.',\n",
       "  803: \"Pre-trained vision-language models (VLMs) have shown impressive results in various visual classification tasks.However, we often fail to fully unleash their potential when adapting them for new concept understanding due to limited information on new classes.To address this limitation, we introduce a novel adaptation framework, AWT (Augment, Weight, then Transport). AWT comprises three key components: augmenting inputs with diverse visual perspectives and enriched class descriptions through image transformations and language models; dynamically weighting inputs based on the prediction entropy; and employing optimal transport to mine semantic correlations in the vision-language space.AWT can be seamlessly integrated into various VLMs, enhancing their zero-shot capabilities without additional training and facilitating few-shot learning through an integrated multimodal adapter module.We verify AWT in multiple challenging scenarios, including zero-shot and few-shot image classification, zero-shot video action recognition, and out-of-distribution generalization. AWT consistently outperforms the state-of-the-art methods in each setting. In addition, our extensive studies further demonstrate AWT's effectiveness and adaptability across different VLMs, architectures, and scales.\",\n",
       "  804: 'Distribution variations in machine learning, driven by the dynamic nature of deployment environments, significantly impact the performance of learning models. This paper explores endogenous distribution shifts in learning systems, where deployed models influence environments and subsequently alter data distributions. This phenomenon is formulated by a decision-dependent distribution mapping within the recently proposed framework of performative prediction (PP) Perdomo et al. (2020). We investigate the performative effect in a decentralized noncooperative game, where players aim to minimize private cost functions while simultaneously managing coupled inequality constraints. Under performativity, we examine two equilibrium concepts for the studied game: performative stable equilibrium (PSE) and Nash equilibrium (NE), and establish sufficient conditions for their existence and uniqueness. Notably, we provide the first upper bound on the distance between the PSE and NE in the literature, which is challenging to evaluate due to the absence of strong convexity on the joint cost function. Furthermore, we develop a decentralized stochastic primal-dual algorithm for efficiently computing the PSE point. By carefully bounding the performative effect in theoretical analysis, we prove that the proposed algorithm achieves sublinear convergence rates for both performative regrets and constraint violation and maintains the same order of convergence rate as the case without performativity. Numerical experiments validate the effectiveness of our algorithm and theoretical results.',\n",
       "  805: 'Generative models have shown great promise in generating 3D geometric systems, which is a fundamental problem in many natural science domains such as molecule and protein design. However, existing approaches only operate on static structures, neglecting the fact that physical systems are always dynamic in nature. In this work, we propose geometric trajectory diffusion models (GeoTDM), the first diffusion model for modeling the temporal distribution of 3D geometric trajectories. Modeling such distribution is challenging as it requires capturing both the complex spatial interactions with physical symmetries and temporal correspondence encapsulated in the dynamics. We theoretically justify that diffusion models with equivariant temporal kernels can lead to density with desired symmetry, and  develop a novel transition kernel leveraging SE(3)-equivariant spatial convolution and temporal attention. Furthermore, to induce an expressive trajectory distribution for conditional generation, we introduce a generalized learnable geometric prior into the forward diffusion process to enhance temporal conditioning. We conduct extensive experiments on both unconditional and conditional generation in various scenarios, including physical simulation, molecular dynamics, and pedestrian motion. Empirical results on a wide suite of metrics demonstrate that GeoTDM can generate realistic geometric trajectories with significantly higher quality.',\n",
       "  806: \"Beyond the text detection and recognition tasks in image text spotting, video text spotting presents an augmented challenge with the inclusion of tracking. While advanced end-to-end trainable methods have shown commendable performance, the pursuit of multi-task optimization may pose the risk of producing sub-optimal outcomes for individual tasks. In this paper, we identify a main bottleneck in the state-of-the-art video text spotter: the limited recognition capability. In response to this issue, we propose to efficiently turn an off-the-shelf query-based image text spotter into a specialist on video and present a simple baseline termed GoMatching, which focuses the training efforts on tracking while maintaining strong recognition performance. To adapt the image text spotter to video datasets, we add a rescoring head to rescore each detected instance's confidence via efficient tuning, leading to a better tracking candidate pool. Additionally, we design a long-short term matching module, termed LST-Matcher, to enhance the spotter's tracking capability by integrating both long- and short-term matching results via Transformer. Based on the above simple designs, GoMatching delivers new records on ICDAR15-video, DSText, BOVText, and our proposed novel test set with arbitrary-shaped text termed ArTVideo, which demonstates GoMatching's capability to accommodate general, dense, small, arbitrary-shaped, Chinese and English text scenarios while saving considerable training budgets. The code will be released.\",\n",
       "  807: 'Despite the significant achievements of Vision Transformers (ViTs) in various vision tasks, they are constrained by the quadratic complexity. Recently, State Space Models (SSMs) have garnered widespread attention due to their global receptive field and linear complexity with respect to the input length, demonstrating substantial potential across fields including natural language processing and computer vision. To improve the performance of SSMs in vision tasks, a multi-scan strategy is widely adopted, which leads to significant redundancy of SSMs. For a better trade-off between efficiency and performance, we analyze the underlying reasons behind the success of the multi-scan strategy, where long-range dependency plays an important role. Based on the analysis, we introduce Multi-Scale Vision Mamba (MSVMamba) to preserve the superiority of SSMs in vision tasks with limited parameters. It employs a multi-scale 2D scanning technique on both original and downsampled feature maps, which not only benefits long-range dependency learning but also reduces computational costs. Additionally, we integrate a Convolutional Feed-Forward Network (ConvFFN) to address the lack of channel mixing. Our experiments demonstrate that MSVMamba is highly competitive, with the MSVMamba-Tiny model achieving 83.0% top-1 accuracy on ImageNet, 46.9% box mAP, and 42.5% instance mAP with the Mask R-CNN framework, 1x training schedule on COCO, and 47.9% mIoU with single-scale testing on ADE20K.  Code is available at https://github.com/YuHengsss/MSVMamba.',\n",
       "  808: \"Vision Transformers (ViTs) demonstrate remarkable performance in image classification through visual-token interaction learning, particularly when equipped with local information via region attention or convolutions. Although such architectures improve the feature aggregation from different granularities, they often fail to contribute to the robustness of the networks. Neural Cellular Automata (NCA) enables the modeling of global visual-token representations through local interactions, with its training strategies and architecture design conferring strong generalization ability and robustness against noisy input. In this paper, we propose Adaptor Neural Cellular Automata (AdaNCA) for Vision Transformers that uses NCA as plug-and-play adaptors between ViT layers, thus enhancing ViT's performance and robustness against adversarial samples as well as out-of-distribution inputs. To overcome the large computational overhead of standard NCAs, we propose Dynamic Interaction for more efficient interaction learning. Using our analysis of AdaNCA placement and robustness improvement, we also develop an algorithm for identifying the most effective insertion points for AdaNCA. With less than a 3% increase in parameters, AdaNCA contributes to more than 10% absolute improvement in accuracy under adversarial attacks on the ImageNet1K benchmark. Moreover, we demonstrate with extensive evaluations across eight robustness benchmarks and four ViT architectures that AdaNCA, as a plug-and-play module, consistently improves the robustness of ViTs.\",\n",
       "  809: 'We address the issue of physical implausibility in multi-view neural reconstruction. While implicit representations have gained popularity in multi-view 3D reconstruction, previous work struggles to yield physically plausible results, limiting their utility in domains requiring rigorous physical accuracy. This lack of plausibility stems from the absence of physics modeling in existing methods and their inability to recover intricate geometrical structures. In this paper, we introduce PHYRECON, the first approach to leverage both differentiable rendering and differentiable physics simulation to learn implicit surface representations. PHYRECON features a novel differentiable particle-based physical simulator built on neural implicit representations. Central to this design is an efficient transformation between SDF-based implicit representations and explicit surface points via our proposed Surface Points Marching Cubes (SP-MC), enabling differentiable learning with both rendering and physical losses. Additionally, PHYRECON models both rendering and physical uncertainty to identify and compensate for inconsistent and inaccurate monocular geometric priors. The physical uncertainty further facilitates physics-guided pixel sampling to enhance the learning of slender structures. By integrating these techniques, our model supports differentiable joint modeling of appearance, geometry, and physics. Extensive experiments demonstrate that PHYRECON significantly improves the reconstruction quality. Our results also exhibit superior physical stability in physical simulators, with at least a 40% improvement across all datasets, paving the way for future physics-based applications.',\n",
       "  810: 'We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates.Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads.These theoretical insights are validated experimentally and offer natural suggestions for alternative architectures.',\n",
       "  811: 'Optimal Transport (OT, also known as the Wasserstein distance) is a popular metric for comparing probability distributions and has been successfully used in many machine-learning applications.In the semi-discrete $2$-Wasserstein problem, we wish to compute the cheapest way to transport all the mass from a continuous distribution $\\\\mu$ to a discrete distribution $\\\\nu$ in $\\\\mathbb{R}^d$ for $d\\\\ge 1$, where the cost of transporting unit mass between points $a$ and $b$ is $d(a,b)=||a-b||^2$. When both distributions are discrete, a simple combinatorial framework has been used to find the exact solution (see e.g. [Orlin, STOC 1988]). In this paper, we propose a combinatorial framework for the semi-discrete OT, which can be viewed as an extension of the combinatorial framework for the discrete OT but requires several new ideas. We present a new algorithm that given $\\\\mu$ and $\\\\nu$ in $\\\\mathbb{R}^2$ and a parameter $\\\\varepsilon>0$, computes an $\\\\varepsilon$-additive approximate semi-discrete transport plan in $O(n^{4}\\\\log n\\\\log \\\\frac{1}{\\\\varepsilon})$ time (in the worst case), where $n$ is the support-size of the discrete distribution $\\\\nu$ and we assume that the mass of $\\\\mu$ inside a triangle can be computed in $O(1)$ time. Our algorithm is significantly faster than the known algorithms, and unlike many numerical algorithms, it does not make any assumptions on the smoothness of $\\\\mu$.As an application of our algorithm, we describe a data structure to store a large discrete distribution $\\\\mu$ (with support size $N$) using $O(N)$ space so that, given a query discrete distribution $\\\\nu$ (with support size $k$), an $\\\\varepsilon$-additive approximate transport plan can be computed in $O(k^{3}\\\\sqrt{N}\\\\log \\\\frac{1}{\\\\varepsilon})$ time in $2$ dimensions. Our algorithm and data structure extend to higher dimensions as well as to $p$-Wasserstein problem for any $p \\\\ge 1$.',\n",
       "  812: 'Airborne Laser Scanning (ALS) technology has transformed modern archaeology by unveiling hidden landscapes beneath dense vegetation. However, the lack of expert-annotated, open-access resources has hindered the analysis of ALS data using advanced deep learning techniques. We address this limitation with Archaeoscape (available at https://archaeoscape.ai/data/2024), a novel large-scale archaeological ALS dataset spanning 888 km² in Cambodia with 31,141 annotated archaeological features from the Angkorian period. Archaeoscape is over four times larger than comparable datasets, and the first ALS archaeology resource with open-access data, annotations, and models.We benchmark several recent segmentation models to demonstrate the benefits of modern vision techniques for this problem and highlight the unique challenges of discovering subtle human-made structures under dense jungle canopies. By making Archaeoscape available in open access, we hope to bridge the gap between traditional archaeology and modern computer vision methods.',\n",
       "  813: 'Modality differences have led to the development of heterogeneous architectures for vision and language models. While images typically require 2D non-causal modeling, texts utilize 1D causal modeling. This distinction poses significant challenges in constructing unified multi-modal models. This paper explores the feasibility of representing images using 1D causal modeling. We identify an \"over-focus\" issue in existing 1D causal vision models, where attention overly concentrates on a small proportion of visual tokens. The issue of \"over-focus\" hinders the model\\'s ability to extract diverse visual features and to receive effective gradients for optimization. To address this, we propose De-focus Attention Networks, which employ learnable bandpass filters to create varied attention patterns. During training, large and scheduled drop path rates, and an auxiliary loss on globally pooled features for global understanding tasks are introduced. These two strategies encourage the model to attend to a broader range of tokens and enhance network optimization. Extensive experiments validate the efficacy of our approach, demonstrating that 1D causal visual representation can perform comparably to 2D non-causal representation in tasks such as global perception, dense prediction, and multi-modal understanding. Code shall be released.',\n",
       "  814: 'Localized receptive fields—neurons that are selective for certain contiguous spatiotemporal features of their input—populate early sensory regions of the mammalian brain. Unsupervised learning algorithms that optimize explicit sparsity or independence criteria replicate features of these localized receptive fields, but fail to explain directly how localization arises through learning without efficient coding, as occurs in early layers of deep neural networks and might occur in early sensory regions of biological systems. We consider an alternative model in which localized receptive fields emerge without explicit top-down efficiency constraints—a feed-forward neural network trained on a data model inspired by the structure of natural images. Previous work identified the importance of non-Gaussian statistics to localization in this setting but left open questions about the mechanisms driving dynamical emergence. We address these questions by deriving the effective learning dynamics for a single nonlinear neuron, making precise how higher-order statistical properties of the input data drive emergent localization, and we demonstrate that the predictions of these effective dynamics extend to the many-neuron setting. Our analysis provides an alternative explanation for the ubiquity of localization as resulting from the nonlinear dynamics of learning in neural circuits',\n",
       "  815: 'In recent years, there has been a surge of interest in the use of machine-learned predictions to bypass worst-case lower bounds for classical problems in combinatorial optimization. So far, the focus has mostly been on online algorithms, where information-theoretic barriers are overcome using predictions about the unknown future. In this paper, we consider the complementary question of using learned information to overcome computational barriers in the form of approximation hardness of polynomial-time algorithms for NP-hard (offline) problems. We show that noisy predictions about the optimal solution can be used to break classical hardness results for maximization problems such as the max-cut problem and more generally, maximization versions of constraint satisfaction problems (CSPs).',\n",
       "  816: 'Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demon- strate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar.',\n",
       "  817: \"While 2D diffusion models generate realistic, high-detail images, 3D shape generation methods like Score Distillation Sampling (SDS)  built on these 2D diffusion models  produce cartoon-like, over-smoothed shapes. To help explain this discrepancy, we show that the image guidance used in Score Distillation can be understood as the velocity field of a 2D denoising generative process, up to the choice of a noise term. In particular, after a change of variables, SDS resembles a high-variance version of Denoising Diffusion Implicit Models (DDIM) with a differently-sampled noise term: SDS introduces noise i.i.d. randomly at each step, while DDIM infers it from the previous noise predictions. This excessive variance can lead to over-smoothing and unrealistic outputs. We show that a better noise approximation can be recovered by inverting DDIM in each SDS update step. This modification makes SDS's generative process for 2D images almost identical to DDIM. In 3D, it removes over-smoothing, preserves higher-frequency detail, and brings the generation quality closer to that of 2D samplers. Experimentally, our method achieves better or similar 3D generation quality compared to other state-of-the-art Score Distillation methods, all without training additional neural networks or multi-view supervision, and providing useful insights into relationship between 2D and 3D asset generation with diffusion models.\",\n",
       "  818: 'Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains and are moving towards more specialized areas. Recent advanced proprietary models such as GPT-4 and Gemini have achieved significant advancements in biomedicine, which have also raised privacy and security challenges. The construction of specialized generalists hinges largely on high-quality datasets, enhanced by techniques like supervised fine-tuning and reinforcement learning from human or AI feedback, and direct preference optimization. However, these leading technologies (e.g., preference learning) are still significantly limited in the open source community due to the scarcity of specialized data. In this paper, we present the UltraMedical collections, which consist of high-quality manual and synthetic datasets in the biomedicine domain, featuring preference annotations across multiple advanced LLMs. By utilizing these datasets, we fine-tune a suite of specialized medical models based on Llama-3 series, demonstrating breathtaking capabilities across various medical benchmarks. Moreover, we develop powerful reward models skilled in biomedical and general reward benchmark, enhancing further online preference learning within the biomedical LLM community.',\n",
       "  819: 'Machine Learning and AI have the potential to transform data-driven scientific discovery, enabling accurate predictions for several scientific phenomena. As many scientific questions are inherently causal, this paper looks at the causal inference task of treatment effect estimation, where the outcome of interest is recorded in high-dimensional observations in a Randomized Controlled Trial (RCT). Despite being the simplest possible causal setting and a perfect fit for deep learning, we theoretically find that many common choices in the literature may lead to biased estimates. To test the practical impact of these considerations, we recorded ISTAnt, the first real-world benchmark for causal inference downstream tasks on high-dimensional observations as an RCT studying how garden ants (Lasius neglectus) respond to microparticles applied onto their colony members by hygienic grooming. Comparing 6 480 models fine-tuned from state-of-the-art visual backbones, we find that the sampling and modeling choices significantly affect the accuracy of the causal estimate, and that classification accuracy is not a proxy thereof. We further validated the analysis, repeating it on a synthetically generated visual data set controlling the causal model. Our results suggest that future benchmarks should carefully consider real downstream scientific questions, especially causal ones. Further, we highlight guidelines for representation learning methods to help answer causal questions in the sciences.',\n",
       "  820: \"When training neural networks with custom objectives, such as ranking losses and shortest-path losses, a common problem is that they are, per se, non-differentiable. A popular approach is to continuously relax the objectives to provide gradients, enabling learning. However, such differentiable relaxations are often non-convex and can exhibit vanishing and exploding gradients, making them (already in isolation) hard to optimize. Here, the loss function poses the bottleneck when training a deep neural network. We present Newton Losses, a method for improving the performance of existing hard to optimize losses by exploiting their second-order information via their empirical Fisher and Hessian matrices. Instead of training the neural network with second-order techniques, we only utilize the loss function's second-order information to replace it by a Newton Loss, while training the network with gradient descent.  This makes our method computationally efficient. We apply Newton Losses to eight differentiable algorithms for sorting and shortest-paths, achieving significant improvements for less-optimized differentiable algorithms, and consistent improvements, even for well-optimized differentiable algorithms.\",\n",
       "  821: 'Principal component analysis (PCA) is one of the most fundamental tools in machine learning with broad use as a dimensionality reduction and denoising tool. In the later setting, while PCA is known to be effective at subspace recovery and is proven to aid clustering algorithms in some specific settings, its improvement of noisy data is still not well quantified in general. In this paper, we propose a novel metric called compression ratio to capture the effect of PCA on high-dimensional noisy data.We show that, for data with underlying community structure, PCA significantly reduces the distance of data points belonging to the same community while reducing inter-community distance relatively mildly. We explain this phenomenon through both theoretical proofs and experiments on real-world data. Building on this new metric, we design a straightforward algorithm that could be used to detect outliers. Roughly speaking, we argue that  points that have a  lower variance of compression ratio do not share a common signal with others (hence could be considered outliers).We provide theoretical justification for this simple outlier detection algorithm and use simulations to demonstrate that our method is competitive with popular outlier detection tools. Finally, we run experiments on real-world high-dimension noisy data (single-cell RNA-seq) to show that removing points from these datasets via our outlier detection method improves the accuracy of clustering algorithms. Our method is very competitive with popular outlier detection tools in this task.',\n",
       "  822: \"Current multimodal Large Language Models (MLLMs) suffer from ''hallucination'', occasionally generating responses that are not grounded in the input images. To tackle this challenge, one promising path is to utilize reinforcement learning from human feedback (RLHF), which steers MLLMs towards learning superior responses while avoiding inferior ones. We rethink the common practice of using binary preferences (i.e., superior, inferior), and find that adopting multi-level preferences (e.g., superior, medium, inferior) is better for two benefits: 1) It narrows the gap between adjacent levels, thereby encouraging MLLMs to discern subtle differences. 2) It further integrates cross-level comparisons (beyond adjacent-level comparisons), thus providing a broader range of comparisons with hallucination examples. To verify our viewpoint, we present the Automated Multi-level Preference (AMP) framework for MLLMs. To facilitate this framework, we first develop an automated dataset generation pipeline that provides high-quality multi-level preference datasets without any human annotators. Furthermore, we design the Multi-level Direct Preference Optimization (MDPO) algorithm to robustly conduct complex multi-level preference learning. Additionally, we propose a new hallucination benchmark, MRHal-Bench. Extensive experiments across public hallucination and general benchmarks, as well as our MRHal-Bench, demonstrate the effectiveness of our proposed method. Code is available at https://github.com/takomc/amp.\",\n",
       "  823: 'Graph class incremental learning (GCIL) requires the model to classify emerging nodes of new classes while remembering old classes. Existing methods are designed to preserve effective information of old models or graph data to alleviate forgetting, but there is no clear theoretical understanding of what matters in information preservation. In this paper, we consider that present practice suffers from high semantic and structural shifts assessed by two devised shift metrics. We provide insights into information preservation in GCIL and find that maintaining graph information can preserve information of old models in theory to calibrate node semantic and graph structure shifts. We correspond graph information into low-frequency local-global information and high-frequency information in spatial domain. Based on the analysis, we propose a framework, Graph Spatial Information Preservation (GSIP). Specifically, for low-frequency information preservation, the old node representations obtained by inputting replayed nodes into the old model are aligned with the outputs of the node and its neighbors in the new model, and then old and new outputs are globally matched after pooling. For high-frequency information preservation, the new node representations are encouraged to imitate the near-neighbor pair similarity of old node representations. GSIP achieves a 10\\\\% increase in terms of the forgetting metric compared to prior methods on large-scale datasets. Our framework can also seamlessly integrate existing replay designs. The code is available through https://github.com/Jillian555/GSIP.',\n",
       "  824: 'In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, a 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.',\n",
       "  825: \"Equivariant Graph Neural Networks (GNNs) that incorporate E(3) symmetry have achieved significant success in various scientific applications. As one of the most successful models, EGNN leverages a simple scalarization technique to perform equivariant message passing over only Cartesian vectors (i.e., 1st-degree steerable vectors), enjoying greater efficiency and efficacy compared to equivariant GNNs using higher-degree steerable vectors. This success suggests that higher-degree representations might be unnecessary. In this paper, we disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric structures, including $k$-fold rotations and regular polyhedra. We theoretically demonstrate that equivariant GNNs will always degenerate to a zero function if the degree of the output representations is fixed to 1 or other specific values. Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while maintaining EGNN's efficiency through the scalarization trick. Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric structures, but also shows substantial improvements on more complicated datasets such as $N$-body and MD17. Our theoretical findings and empirical results potentially open up new possibilities for the research of equivariant GNNs.\",\n",
       "  826: 'Sparsity is a central aspect of interpretability in machine learning. Typically, sparsity is measured in terms of the size of a model globally, such as the number of variables it uses. However, this notion of sparsity is not particularly relevant for decision making; someone subjected to a decision does not care about variables that do not contribute to the decision. In this work, we dramatically expand a notion of decision sparsity called the Sparse Explanation Value (SEV) so that its explanations are more meaningful. SEV considers movement along a hypercube towards a reference point. By allowing flexibility in that reference and by considering how distances along the hypercube translate to distances in feature space, we can derive sparser and more meaningful explanations for various types of function classes. We present cluster-based SEV and its variant tree-based SEV, introduce a method that improves credibility of explanations, and propose algorithms that optimize decision sparsity in machine learning models.',\n",
       "  827: 'In recent years, the merging of vast datasets with powerful computational resources has led to the emergence of large pre-trained models in the field of deep learning. However, the common practices often overgeneralize the applicability of these models, overlooking the task-specific resource constraints. To mitigate this issue, we propose \\\\textbf{Cluster-Learngene}, which effectively clusters critical internal modules from a large ancestry model and then inherits them to initialize descendant models of elastic scales. Specifically, based on the density characteristics of attention heads, our method adaptively clusters attention heads of each layer and position-wise feed-forward networks (FFNs) in the ancestry model as the learngene. Moreover, we introduce priority weight-sharing and learnable parameter transformations that expand the learngene to initialize descendant models of elastic scales. Through extensive experimentation, we demonstrate that Cluster-Learngene not only is more efficient compared to other initialization methods but also customizes models of elastic scales according to downstream task resources.',\n",
       "  828: 'Multi-instance partial-label learning (MIPL) is an emerging learning framework where each training sample is represented as a multi-instance bag associated with a candidate label set. Existing MIPL algorithms often overlook the margins for attention scores and predicted probabilities, leading to suboptimal generalization performance. A critical issue with these algorithms is that the highest prediction probability of the classifier may appear on a non-candidate label. In this paper, we propose an algorithm named MIPLMA, i.e., Multi-Instance Partial-Label learning with Margin Adjustment, which adjusts the margins for attention scores and predicted probabilities. We introduce a margin-aware attention mechanism to dynamically adjust the margins for attention scores and propose a margin distributionloss to constrain the margins between the predicted probabilities on candidate and non-candidate label sets. Experimental results demonstrate the superior performance of MIPLMA over existing MIPL algorithms, as well as other well-established multi-instance learning algorithms and partial-label learning algorithms.',\n",
       "  829: 'PAnoramic Semantic Segmentation (PASS) is an important task in computer vision,as it enables semantic understanding of a 360° environment. Currently,most of existing works have focused on addressing the distortion issues in 2Dpanoramic images without considering spatial properties of indoor scene. Thisrestricts PASS methods in perceiving contextual attributes to deal with the ambiguitywhen working with monocular images. In this paper, we propose a novelapproach for indoor panoramic semantic segmentation. Unlike previous works,we consider the panoramic image as a composition of segment groups: oversampledsegments, representing planar structures such as floors and ceilings, andunder-sampled segments, representing other scene elements. To optimize eachgroup, we first enhance over-sampled segments by jointly optimizing with a densedepth estimation task. Then, we introduce a transformer-based context modulethat aggregates different geometric representations of the scene, combinedwith a simple high-resolution branch, it serves as a robust hybrid decoder forestimating under-sampled segments, effectively preserving the resolution of predictedmasks while leveraging various indoor geometric properties. Experimentalresults on both real-world (Stanford2D3DS, Matterport3D) and synthetic (Structured3D)datasets demonstrate the robustness of our framework, by setting newstate-of-the-arts in almost evaluations, The code and updated results are availableat: https://github.com/caodinhduc/verticalrelativedistance.',\n",
       "  830: \"Machine unlearning (MU) has emerged to enhance the privacy and trustworthiness of deep neural networks. Approximate MU is a practical method for large-scale models. Our investigation into approximate MU starts with identifying the steepest descent direction, minimizing the output Kullback-Leibler divergence to exact MU inside a parameters' neighborhood. This probed direction decomposes into three components: weighted forgetting gradient ascent, fine-tuning retaining gradient descent, and a weight saliency matrix. Such decomposition derived from Euclidean metric encompasses most existing gradient-based MU methods. Nevertheless, adhering to Euclidean space may result in sub-optimal iterative trajectories due to the overlooked geometric structure of the output probability space. We suggest embedding the unlearning update into a manifold rendered by the remaining geometry, incorporating second-order Hessian from the remaining data. It helps prevent effective unlearning from interfering with the retained performance. However, computing the second-order Hessian for large-scale models is intractable. To efficiently leverage the benefits of Hessian modulation, we propose a fast-slow parameter update strategy to implicitly approximate the up-to-date salient unlearning direction.Free from specific modal constraints, our approach is adaptable across computer vision unlearning tasks, including classification and generation. Extensive experiments validate our efficacy and efficiency. Notably, our method successfully performs class-forgetting on ImageNet using DiT and forgets a class on CIFAR-10 using DDPM in just 50 steps, compared to thousands of steps required by previous methods. Code is available at Unified-Unlearning-w-Remain-Geometry.\",\n",
       "  831: 'Enzymes, with their specific catalyzed reactions, are necessary for all aspects of life, enabling diverse biological processes and adaptations. Predicting enzyme functions is essential for understanding biological pathways, guiding drug development, enhancing bioproduct yields, and facilitating evolutionary studies.Addressing the inherent complexities, we introduce a new approach to annotating enzymes based on their catalyzed reactions. This method provides detailed insights into specific reactions and is adaptable to newly discovered reactions, diverging from traditional classifications by protein family or expert-derived reaction classes. We employ machine learning algorithms to analyze enzyme reaction datasets, delivering a much more refined view on the functionality of enzymes.Our evaluation leverages the largest enzyme-reaction dataset to date, derived from the SwissProt and Rhea databases with entries up to January 8, 2024. We frame the enzyme-reaction prediction as a retrieval problem, aiming to rank enzymes by their catalytic ability for specific reactions. With our model, we can recruit proteins for novel reactions and predict reactions in novel proteins, facilitating enzyme discovery and function annotation https://github.com/WillHua127/ReactZyme.',\n",
       "  832: 'Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\\\\gamma$, we show that $h$-PMD which generalizes the standard PMD enjoys a faster dimension-free $\\\\gamma^h$-linear convergence rate, contingent on the computation of multi-step greedy policies. We propose an inexact version of $h$-PMD where lookahead action values are estimated. Under a generative model, we establish a sample complexity for $h$-PMD which improves over prior work. Finally, we extend our result to linear function approximation to scale to large state spaces. Under suitable assumptions, our sample complexity only involves dependence on the dimension of the feature map space instead of the state space size.',\n",
       "  833: 'A method to prove generalization results for a class of stochastic learning algorithms is presented. It applies whenever the algorithm generates a distribution, which is absolutely continuous distribution relative to some a-priori measure, and the logarithm of its density is exponentially concentrated about its mean. Applications include bounds for the Gibbs algorithm and randomizations of stable deterministic algorithms, combinations thereof and PAC-Bayesian bounds with data-dependent priors.',\n",
       "  834: 'We develop and analyze algorithms for instrumental variable regression by viewing the problem as a conditional stochastic optimization problem. In the context of least-squares instrumental variable regression, our algorithms neither require matrix inversions nor mini-batches thereby providing a fully online approach for performing instrumental variable regression with streaming data. When the true model is linear, we derive rates of convergence in expectation, that are of order $\\\\mathcal{O}(\\\\log T/T)$ and $\\\\mathcal{O}(1/T^{1-\\\\epsilon})$ for any $\\\\epsilon>0$, respectively under the availability of two-sample and one-sample oracles respectively. Importantly, under the availability of the two-sample oracle, the aforementioned rate is actually agnostic to the relationship between confounder and the instrumental variable demonstrating the flexibility of the proposed approach in alleviating the need for explicit model assumptions required in recent works based on reformulating the problem as min-max optimization problems. Experimental validation is provided to demonstrate the advantages of the proposed algorithms over classical approaches like the 2SLS method.',\n",
       "  835: 'Diffusion models have attained prominence for their ability to synthesize a probability distribution for a given dataset via a diffusion process,  enabling the generation of new data points with high fidelity. However, diffusion processes are prone to generating samples that reflect biases in a training dataset. To address this issue, we develop constrained diffusion models by imposing diffusion constraints based on desired distributions that are informed by requirements. Specifically, we cast the training of diffusion models under requirements as a constrained distribution optimization problem that aims to reduce the distribution difference between original and generated data while obeying constraints on the distribution of generated data. We show that our constrained diffusion models generate new data from a mixture data distribution that achieves the optimal trade-off among objective and constraints. To train constrained diffusion models, we develop a dual training algorithm and characterize the optimality of the trained constrained diffusion model. We empirically demonstrate the effectiveness of our constrained models in two constrained generation tasks: (i) we consider a dataset with one or more underrepresented classes where we train the model with constraints to ensure fairly sampling from all classes during inference; (ii) we fine-tune a pre-trained diffusion model to sample from a new dataset while avoiding overfitting.',\n",
       "  836: \"For classification and regression on tabular data, the dominance of gradient-boosted decision trees (GBDTs) has recently been challenged by often much slower deep learning methods with extensive hyperparameter tuning. We address this discrepancy by introducing (a) RealMLP, an improved multilayer perceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and RealMLP. We tune RealMLP and the default parameters on a meta-train benchmark with 118 datasets and compare them to hyperparameter-optimized versions on a disjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly benchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large tabular datasets (1K--500K samples) show that RealMLP offers a favorable time-accuracy tradeoff compared to other neural baselines and is competitive with GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and GBDTs with improved default parameters can achieve excellent results without hyperparameter tuning. Finally, we demonstrate that some of RealMLP's improvements can also considerably improve the performance of TabR with default parameters.\",\n",
       "  837: 'Positional encoding plays a crucial role in transformers, significantly impact- ing model performance and length generalization. Prior research has introduced absolute positional encoding (APE) and relative positional encoding (RPE) to distinguish token positions in given sequences. However, both APE and RPE remain fixed after model training regardless of input data, limiting their adaptability and flexibility. Hence, we expect that the desired positional encoding should be data-adaptive and can be dynamically adjusted with the given attention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors. Experimental validation on real-world datasets (Arxiv, Books3, and CHE) demonstrates that DAPE enhances model performances in terms of trained length and length generalization, where the improvements are statistically significant. The model visualization suggests that our model can keep both local and anti-local information. Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method.',\n",
       "  838: \"Sequential recommender systems (SRS) aim to predict users' subsequent choices based on their historical interactions and have found applications in diverse fields such as e-commerce and social media. However, in real-world systems, most users interact with only a handful of items, while the majority of items are seldom consumed. These two issues, known as the long-tail user and long-tail item challenges, often pose difficulties for existing SRS. These challenges can adversely affect user experience and seller benefits, making them crucial to address. Though a few works have addressed the challenges, they still struggle with the seesaw or noisy issues due to the intrinsic scarcity of interactions. The advancements in large language models (LLMs) present a promising solution to these problems from a semantic perspective. As one of the pioneers in this field, we propose the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR). This framework utilizes semantic embeddings derived from LLMs to enhance SRS without adding extra inference load. To address the long-tail item challenge, we design a dual-view modeling framework that combines semantics from LLMs and collaborative signals from conventional SRS. For the long-tail user challenge, we propose a retrieval augmented self-distillation method to enhance user preference representation using more informative interactions from similar users. To verify the effectiveness and versatility of our proposed enhancement framework, we conduct extensive experiments on three real-world datasets using three popular SRS models. The results consistently show that our method surpasses existing baselines. The implementation code is available in Supplementary Material.\",\n",
       "  839: 'In recent years, significant attention has been directed towards learning average-reward Markov Decision Processes (MDPs).However, existing algorithms either suffer from sub-optimal regret guarantees or computational inefficiencies.In this paper, we present the first *tractable* algorithm  with minimax optimal regret of $\\\\mathrm{O}\\\\left(\\\\sqrt{\\\\mathrm{sp}(h^*) S A T \\\\log(SAT)}\\\\right)$ where $\\\\mathrm{sp}(h^*)$ is the span of the optimal bias function $h^*$, $S\\\\times A$  is the size of the state-action space and $T$ the number of learning steps. Remarkably, our algorithm does not require prior information on $\\\\mathrm{sp}(h^*)$. Our algorithm relies on a novel subroutine, **P**rojected **M**itigated **E**xtended **V**alue **I**teration (`PMEVI`), to compute bias-constrained optimal policies efficiently. This subroutine can be applied to various previous algorithms to obtain improved regret bounds.',\n",
       "  840: \"We present adaptive gradient methods (both basic and accelerated) for solvingconvex composite optimization problems in which the main part is approximatelysmooth (a.k.a. $(\\\\delta, L)$-smooth) and can be accessed only via a(potentially biased) stochastic gradient oracle.This setting covers many interesting examples including Hölder smooth problemsand various inexact computations of the stochastic gradient.Our methods use AdaGrad stepsizes and are adaptive in the sense that they donot require knowing any problem-dependent constants except an estimate of thediameter of the feasible set but nevertheless achieve the best possibleconvergence rates as if they knew the corresponding constants.We demonstrate that AdaGrad stepsizes work in a variety of situationsby proving, in a unified manner, three types of new results.First, we establish efficiency guarantees for our methods in the classicalsetting where the oracle's variance is uniformly bounded.We then show that, under more refined assumptions on the variance,the same methods without any modifications enjoy implicit variancereduction properties allowing us to express their complexity estimates interms of the variance only at the minimizer.Finally, we show how to incorporate explicit SVRG-type variance reduction intoour methods and obtain even faster algorithms.In all three cases, we present both basic and accelerated algorithmsachieving state-of-the-art complexity bounds.As a direct corollary of our results, we obtain universal stochastic gradientmethods for Hölder smooth problems which can be used in all situations.\",\n",
       "  841: 'Recent studies developed jailbreaking attacks, which construct jailbreaking prompts to \"fool\" LLMs into responding to harmful questions.Early-stage jailbreaking attacks require access to model internals or significant human efforts. More advanced attacks utilize genetic algorithms for automatic and black-box attacks.However, the random nature of genetic algorithms significantly limits the effectiveness of these attacks.In this paper, we propose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement learning (DRL).We model jailbreaking as a search problem and design an RL agent to guide the search, which is more effective and has less randomness than stochastic search, such as genetic algorithms.Specifically, we design a customized DRL system for the jailbreaking problem, including a novel reward function and a customized proximal policy optimization (PPO) algorithm.Through extensive experiments, we demonstrate that RLbreaker is much more effective than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs. We also show that RLbreaker is robust against three SOTA defenses and its trained agents can transfer across different LLMs.We further validate the key design choices of RLbreaker via a comprehensive ablation study.',\n",
       "  842: \"Unsupervised out-of-distribution (OOD) detection aims to identify out-of-domain data by learning only from unlabeled In-Distribution (ID) training samples, which is crucial for developing a safe real-world machine learning system. Current reconstruction-based method provides a good alternative approach, by measuring the reconstruction error between the input and its corresponding generative counterpart in the pixel/feature space. However, such generative methods face the key dilemma, $i.e.$, improving the reconstruction power of the generative model, while keeping compact representation of the ID data. To address this issue, we propose the diffusion-based layer-wise semantic reconstruction approach for unsupervised OOD detection. The innovation of our approach is that we leverage the diffusion model's intrinsic data reconstruction ability to distinguish ID samples from OOD samples in the latent feature space. Moreover, to set up a comprehensive and discriminative feature representation, we devise a multi-layer semantic feature extraction strategy. Through distorting the extracted features with Gaussian noises and applying the diffusion model for feature reconstruction, the separation of ID and OOD samples is implemented according to the reconstruction errors. Extensive experimental results on multiple benchmarks built upon various datasets demonstrate that our method achieves state-of-the-art performance in terms of detection accuracy and speed.\",\n",
       "  843: \"Constructing fast samplers for unconditional diffusion and flow-matching models has received much attention recently; however, existing methods for solving inverse problems, such as super-resolution, inpainting, or deblurring, still require hundreds to thousands of iterative steps to obtain high-quality results. We propose a plug-and-play framework for constructing efficient samplers for inverse problems, requiring only pre-trained diffusion or flow-matching models. We present Conditional Conjugate Integrators, which leverage the specific form of the inverse problem to project the respective conditional diffusion/flow dynamics into a more amenable space for sampling. Our method complements popular posterior approximation methods for solving inverse problems using diffusion/flow models. We evaluate the proposed method's performance on various linear image restoration tasks across multiple datasets, employing diffusion and flow-matching models. Notably, on challenging inverse problems like 4x super-resolution on the ImageNet dataset, our method can generate high-quality samples in as few as 5 conditional sampling steps and outperforms competing baselines requiring 20-1000 steps. Our code will be publicly available at https://github.com/mandt-lab/c-pigdm.\",\n",
       "  844: 'Spiking neural networks (SNNs) represent a promising approach to developing artificial neural networks that are both energy-efficient and biologically plausible.However, applying SNNs to sequential tasks, such as text classification and time-series forecasting, has been hindered by the challenge of creating an effective and hardware-friendly spike-form positional encoding (PE) strategy.Drawing inspiration from the central pattern generators (CPGs) in the human brain,  which produce rhythmic patterned outputs without requiring rhythmic inputs, we propose a novel PE technique for SNNs, termed CPG-PE.We demonstrate that the commonly used sinusoidal PE is mathematically a specific solution to the membrane potential dynamics of a particular CPG.Moreover, extensive experiments across various domains, including time-series forecasting, natural language processing, and image classification, show that SNNs with CPG-PE outperform their conventional counterparts.Additionally, we perform analysis experiments to elucidate the mechanism through which SNNs encode positional information and to explore the function of CPGs in the human brain.This investigation may offer valuable insights into the fundamental principles of neural computation.',\n",
       "  845: 'Recent work suggests that large language models may implicitly learn world models. How should we assess this possibility? We formalize this question for the case where the underlying reality is governed by a deterministic finite automaton. This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry. We propose new evaluation metrics for world model recovery inspired by the classic Myhill-Nerode theorem from language theory. We illustrate their utility in three domains: game playing, logic puzzles, and navigation. In all domains, the generative models we consider do well on existing diagnostics for assessing world models, but our evaluation metrics reveal their world models to be far less coherent than they appear. Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead to failures. Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; our results suggest new ways to assess how close a given model is to that goal.',\n",
       "  846: 'Contextual linear optimization (CLO) uses predictive contextual features to reduce uncertainty in random cost coefficients and thereby improve average-cost performance. An example is the stochastic shortest path problem with random edge costs (e.g., traffic) and contextual features (e.g., lagged traffic, weather). Existing work on CLO assumes the data has fully observed cost coefficient vectors, but in many applications, we can only see the realized cost of a historical decision, that is, just one projection of the random cost coefficient vector, to which we refer as bandit feedback. We study a class of offline learning algorithms for CLO with bandit feedback, which we term induced empirical risk minimization (IERM), where we fit a predictive model to directly optimize the downstream performance of the policy it induces. We show a fast-rate regret bound for IERM that allows for misspecified model classes and flexible choices of the optimization estimate, and we develop computationally tractable surrogate losses. A byproduct of our theory of independent interest is fast-rate regret bound for IERM with full feedback and misspecified policy class. We compare the performance of different modeling choices numerically using a stochastic shortest path example and provide practical insights from the empirical results.',\n",
       "  847: 'The goal of *generalized* few-shot semantic segmentation (GFSS) is to recognize *novel-class* objects through training with a few annotated examples and the *base-class* model that learned the knowledge about the base classes.Unlike the classic few-shot semantic segmentation, GFSS aims to classify pixels into both base and novel classes, meaning it is a more practical setting.Current GFSS methods rely on several techniques such as using combinations of customized modules, carefully designed loss functions, meta-learning, and transductive learning.However, we found that a simple rule and standard supervised learning substantially improve the GFSS performance.In this paper, we propose a simple yet effective method for GFSS that does not use the techniques mentioned above.Also, we theoretically show that our method perfectly maintains the segmentation performance of the base-class model over most of the base classes.Through numerical experiments, we demonstrated the effectiveness of our method.It improved in novel-class segmentation performance in the $1$-shot scenario by $6.1$% on the PASCAL-$5^i$ dataset, $4.7$% on the PASCAL-$10^i$ dataset, and $1.0$% on the COCO-$20^i$ dataset.Our code is publicly available at https://github.com/IBM/BCM.',\n",
       "  848: 'Respiratory audio, such as coughing and breathing sounds, has predictive power for a wide range of healthcare applications, yet is currently under-explored. The main problem for those applications arises from the difficulty in collecting large labeled task-specific data for model development. Generalizable respiratory acoustic foundation models pretrained with unlabeled data would offer appealing advantages and possibly unlock this impasse.  However, given the safety-critical nature of healthcare applications, it is pivotal to also ensure openness and replicability for any proposed foundation model solution. To this end, we introduce OPERA, an OPEn Respiratory Acoustic foundation model pretraining and benchmarking system, as the first approach answering this need. We curate large-scale respiratory audio datasets ($\\\\sim$136K samples, over 400 hours), pretrain three pioneering foundation models, and build a benchmark consisting of 19 downstream respiratory health tasks for evaluation. Our pretrained models demonstrate superior performance (against existing acoustic models pretrained with general audio on 16 out of 19 tasks) and generalizability (to unseen datasets and new respiratory audio modalities). This highlights the great promise of respiratory acoustic foundation models and encourages more studies using OPERA as an open resource to accelerate research on respiratory audio for health. The system is accessible from https://github.com/evelyn0414/OPERA.',\n",
       "  849: \"Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves 42.7% on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks near 24% on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM backbone with 17.9%. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain.\",\n",
       "  850: 'Public pretraining is a promising approach to improve differentially private model training. However, recent work has noted that many positive research results studying this paradigm only consider in-distribution tasks, and may not apply to settings where there is distribution shift between the pretraining and finetuning data---a scenario that is likely when finetuning private tasks due to the sensitive nature of the data. In this work, we show empirically across three tasks that even in settings with large distribution shift, where both zero-shot performance from public data and training from scratch with private data give unusably weak results, public features can in fact improve private training accuracy by up to 67\\\\% over private training from scratch. We provide a theoretical explanation for this phenomenon, showing that if the public and private data share a low-dimensional representation, public representations can improve the sample complexity of private training even if it is \\\\emph{impossible} to learn the private task from the public data alone. Altogether, our results provide evidence that public data can indeed make private training practical in realistic settings of extreme distribution shift.',\n",
       "  851: 'Global convolutions have shown increasing promise as powerful general-purpose sequence models. However, training long convolutions is challenging, and kernel parameterizations must be able to learn long-range dependencies without overfitting. This work introduces reparameterized multi-resolution convolutions ($\\\\texttt{MRConv}$), a novel approach to parameterizing global convolutional kernels for long-sequence modeling. By leveraging multi-resolution convolutions, incorporating structural reparameterization and introducing learnable kernel decay, $\\\\texttt{MRConv}$ learns expressive long-range kernels that perform well across various data modalities. Our experiments demonstrate state-of-the-art performance on the Long Range Arena, Sequential CIFAR, and Speech Commands tasks among convolution models and linear-time transformers. Moreover, we report improved performance on ImageNet classification by replacing 2D convolutions with 1D $\\\\texttt{MRConv}$ layers.',\n",
       "  852: \"Bayes' rule naturally allows for inference refinement in a streaming fashion, without the need to recompute posteriors from scratch whenever new data arrives. In principle, Bayesian streaming is straightforward: we update our prior with the available data and use the resulting posterior as a prior when processing the next data chunk. In practice, however, this recipe entails i) approximating an intractable posterior at each time step; and ii) encapsulating results appropriately to allow for posterior propagation. For continuous state spaces, variational inference (VI) is particularly convenient due to its scalability and the tractability of variational posteriors, For discrete state spaces, however, state-of-the-art VI results in analytically intractable approximations that are ill-suited for streaming settings. To enable streaming Bayesian inference over discrete parameter spaces, we propose streaming Bayes GFlowNets (abbreviated as SB-GFlowNets) by leveraging the recently proposed GFlowNets --- a powerful class of amortized samplers for discrete compositional objects. Notably, SB-GFlowNet approximates the initial posterior using a standard GFlowNet and subsequently updates it using a tailored procedure that requires only the newly observed data. Our case studies in linear preference learning and phylogenetic inference showcase the effectiveness of SB-GFlowNets in sampling from an unnormalized posterior in a streaming setting. As expected, we also observe that SB-GFlowNets is significantly faster than repeatedly training a GFlowNet from scratch to sample from the full posterior.\",\n",
       "  853: 'This paper concerns imitation learning (IL) in cooperative multi-agent systems.The learning problem under consideration poses several challenges, characterized by high-dimensional state and action spaces and intricate inter-agent dependencies. In a single-agent setting, IL was shown to be done efficiently via an inverse soft-Q learning process. However, extending this framework to a multi-agent context introduces the need to simultaneously learn both local value functions to capture local observations and individual actions, and a joint value function for exploiting centralized learning.In this work, we introduce a new multi-agent IL algorithm designed to address these challenges. Our approach enables thecentralized learning by leveraging mixing networks to aggregate  decentralized Q functions.We further establish conditions for the mixing networks under which the multi-agent IL objective function exhibits convexity within the Q function space.We present  extensive experiments conducted on some challenging multi-agent game environments, including an advanced version of the Star-Craft multi-agent challenge (SMACv2), which demonstrates the effectiveness of our algorithm.',\n",
       "  854: 'The human brain exhibits a strong ability to spontaneously associate different visual attributes of the same or similar visual scene, such as associating sketches and graffiti with real-world visual objects, usually without supervising information. In contrast, in the field of artificial intelligence, controllable generation methods like ControlNet heavily rely on annotated training datasets such as depth maps, semantic segmentation maps, and poses, which limits the method’s scalability. Inspired by the neural mechanisms that may contribute to the brain’s associative power, specifically the cortical modularization and hippocampal pattern completion, here we propose a self-supervised controllable generation (SCG) framework. Firstly, we introduce an equivariance constraint to promote inter-module independence and intra-module correlation in a modular autoencoder network, thereby achieving functional specialization. Subsequently, based on these specialized modules, we employ a self-supervised pattern completion approach for controllable generation training. Experimental results demonstrate that the proposed modular autoencoder effectively achieves functional specialization, including the modular processing of color, brightness, and edge detection, and exhibits brain-like features including orientation selectivity, color antagonism, and center-surround receptive fields. Through self-supervised training, associative generation capabilities spontaneously emerge in SCG, demonstrating excellent zero-shot generalization ability to various tasks such as superresolution, dehaze and associative or conditional generation on painting, sketches, and ancient graffiti. Compared to the previous representative method ControlNet, our proposed approach not only demonstrates superior robustness in more challenging high-noise scenarios but also possesses more promising scalability potential due to its self-supervised manner. Codes are released on Github and Gitee.',\n",
       "  855: 'Given a relational database (RDB), how can we predict  missing column values in some target table of interest? Although RDBs store vast amounts of rich, informative data spread across interconnected tables, the progress of predictive machine learning models as applied to such tasks arguably falls well behind advances in other domains such as computer vision or natural language processing.  This deficit stems, at least in part, from the lack of established/public RDB benchmarks as needed for training and evaluation purposes.  As a result, related model development thus far often defaults to tabular approaches trained on ubiquitous single-table benchmarks, or on the relational side, graph-based alternatives such as GNNs applied to a completely different set of graph datasets devoid of tabular characteristics.  To more precisely target RDBs lying at the nexus of these two complementary regimes, we explore a broad class of baseline models predicated on: (i) converting multi-table datasets into graphs using various strategies equipped with efficient subsampling, while preserving tabular characteristics; and (ii) trainable models with well-matched inductive biases that output predictions based on these input subgraphs.  Then, to address the dearth of suitable public benchmarks and reduce siloed comparisons, we assemble a diverse collection of (i) large-scale RDB datasets and (ii) coincident predictive tasks.  From a delivery standpoint, we  operationalize the above four dimensions (4D) of exploration within a unified, scalable open-source toolbox called 4DBInfer; please see https://github.com/awslabs/multi-table-benchmark .',\n",
       "  856: 'Topological Data Analysis (TDA) provides a pipeline to extract quantitative and powerful topological descriptors from structured objects. This enables the definition of topological loss functions, which assert to which extent a given object exhibits some topological properties. One can then use these losses to perform topological optimization via gradient descent routines. While theoretically sounded, topological optimization faces an important challenge: gradients tend to be extremely sparse, in the sense that the loss function typically depends (locally) on only very few coordinates of the input object, yielding dramatically slow optimization schemes in practice. In this work, focusing on the central case of topological optimization for point clouds, we propose to overcome this limitation using diffeomorphic interpolation, turning sparse gradients into smooth vector fields defined on the whole space. In particular, this approach combines efficiently with subsampling techniques routinely used in TDA, as the diffeomorphism derived from the gradient computed on the subsample can be used to update the coordinates of the full and possibly large input object. We then illustrate the usefulness of our approach on black-box autoencoder (AE) regularization, where we aim at applying some topological priors on the latent spaces associated to fixed, black-box AE models without modifying their (unknown) architectures and parameters. We empirically show that, while vanilla topological optimization has to be re-run every time that new data comes out of the black-box models, learning a diffeomorphic flow can be done once and then re-applied to new data in linear time. Moreover, reverting the flow allows us to generate data by sampling the topologically-optimized latent space directly, allowing for better interpretability of the model.',\n",
       "  857: 'A key source of complexity in next-generation AI models is the size of model outputs, making it time-consuming to parse and provide reliable feedback on. To ensure such models are aligned, we will need to bolster our understanding of scalable oversight and how to scale up human feedback. To this end, we study the challenges of scalable oversight in the context of goal-conditioned hierarchical reinforcement learning. Hierarchical structure is a promising entrypoint into studying how to scale up human feedback, which in this work we assume can only be provided for model outputs below a threshold size. In the cardinal feedback setting, we develop an apt sub-MDP reward and algorithm that allows us to acquire and scale up low-level feedback for learning with sublinear regret. In the ordinal feedback setting, we show the necessity of both high- and low-level feedback, and develop a hierarchical experimental design algorithm that efficiently acquires both types of feedback for learning. Altogether, our work aims to consolidate the foundations of scalable oversight, formalizing and studying the various challenges thereof.',\n",
       "  858: 'Recently, diffusion models have emerged as a powerful class of generative models. Despite their success, there is still limited understanding of their semantic spaces. This makes it challenging to achieve precise and disentangled image generation without additional training, especially in an unsupervised way. In this work, we improve the understanding of their semantic spaces from intriguing observations: among a certain range of noise levels, (1) the learned posterior mean predictor (PMP) in the diffusion model is locally linear, and (2) the singular vectors of its Jacobian lie in low-dimensional semantic subspaces. We provide a solid theoretical basis to justify the linearity and low-rankness in the PMP. These insights allow us to propose an unsupervised, single-step, training-free LOw-rank COntrollable image editing (LOCO Edit) method for precise local editing in diffusion models. LOCO Edit identified editing directions with nice properties: homogeneity, transferability, composability, and linearity. These properties of LOCO Edit benefit greatly from the low-dimensional semantic subspace.Our method can further be extended to unsupervised or text-supervised editing in various text-to-image diffusion models (T-LOCO Edit). Finally, extensive empirical experiments demonstrate the effectiveness and efficiency of LOCO Edit. The code and the arXiv version can be found on the project website.',\n",
       "  859: \"Data, the seminal opportunity and challenge in modern machine learning, currently constrains the scalability of representation learning and impedes the pace of model evolution.In this work, we investigate the efficiency properties of data from both optimization and generalization perspectives.Our theoretical and empirical analysis reveals an unexpected finding: for a given task, utilizing a publicly available, task- and architecture-agnostic model (referred to as the `prior model' in this paper) can effectively produce efficient data.Building on this insight, we propose the Representation Learning Accelerator (ReLA), which promotes the formation and utilization of efficient data, thereby accelerating representation learning.Utilizing a ResNet-18 pre-trained on CIFAR-10 as a prior model to inform ResNet-50 training on ImageNet-1K reduces computational costs by $50\\\\%$ while maintaining the same accuracy as the model trained with the original BYOL, which requires $100\\\\%$ cost.Our code is available at: \\\\url{https://github.com/LINs-lab/ReLA}.\",\n",
       "  860: 'Beyond traditional binary relational facts, n-ary relational knowledge graphs (NKGs) are comprised of n-ary relational facts containing more than two entities, which are closer to real-world facts with broader applications. However, the construction of NKGs remains at a coarse-grained level, which is always in a single schema, ignoring the order and variable arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging and output merging to accomplish fine-grained n-ary relation extraction in different arity. Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, with high flexibility and practicality. The experimental results demonstrate that Text2NKG achieves state-of-the-art performance in F1 scores on the fine-grained n-ary relation extraction benchmark. Our code and datasets are publicly available.',\n",
       "  861: 'Recent advances in low light image enhancement have been dominated by Retinex-based learning framework, leveraging convolutional neural networks (CNNs) and Transformers. However, the vanilla Retinex theory primarily addresses global illumination degradation and neglects local issues such as noise and blur in dark conditions. Moreover, CNNs and Transformers struggle to capture global degradation due to their limited receptive fields. While state space models (SSMs) have shown promise in the long-sequence modeling, they face challenges in combining local invariants and global context in visual data. In this paper, we introduce MambaLLIE, an implicit Retinex-aware low light enhancer featuring a global-then-local state space design. We first propose a Local-Enhanced State Space Module (LESSM) that incorporates an augmented local bias within a 2D selective scan mechanism, enhancing the original SSMs by preserving local 2D dependency. Additionally, an Implicit Retinex-aware Selective Kernel module (IRSK) dynamically selects features using spatially-varying operations, adapting to varying inputs through an adaptive kernel selection process. Our Global-then-Local State Space Block (GLSSB) integrates LESSM and IRSK with layer normalization (LN) as its core. This design enables MambaLLIE to achieve comprehensive global long-range modeling and flexible local feature aggregation. Extensive experiments demonstrate that MambaLLIE significantly outperforms state-of-the-art CNN and Transformer-based methods. Our code is available at https://github.com/wengjiangwei/MambaLLIE.',\n",
       "  862: 'Recommender systems aim to predict personalized rankings based on user preference data. With the rise of Language Models (LMs), LM-based recommenders have been widely explored due to their extensive world knowledge and powerful reasoning abilities. Most of the LM-based recommenders convert historical interactions into language prompts, pairing with a positive item as the target response and fine-tuning LM with a language modeling loss. However, the current objective fails to fully leverage preference data and is not optimized for personalized ranking tasks, which hinders the performance of LM-based recommenders. Inspired by the current advancement of Direct Preference Optimization (DPO) in human preference alignment and the success of softmax loss in recommendations, we propose Softmax-DPO (\\\\textbf{S-DPO}) to instill ranking information into the LM to help LM-based recommenders distinguish preferred items from negatives, rather than solely focusing on positives. Specifically, we incorporate multiple negatives in user preference data and devise an alternative version of DPO loss tailored for LM-based recommenders, which is extended from the traditional full-ranking Plackett-Luce (PL) model to partial rankings and connected to softmax sampling strategies. Theoretically, we bridge S-DPO with the softmax loss over negative sampling and find that it has an inherent benefit of mining hard negatives, which assures its exceptional capabilities in recommendation tasks. Empirically, extensive experiments conducted on three real-world datasets demonstrate the superiority of S-DPO to effectively model user preference and further boost recommendation performance while providing better rewards for preferred items. Our codes are available at https://github.com/chenyuxin1999/S-DPO.',\n",
       "  863: 'Standard reinforcement learning (RL) agents never intelligently explore like a human (i.e. taking into account complex domain priors and adapting quickly based on previous exploration). Across episodes, RL agents struggle to perform even simple exploration strategies, for example systematic search that avoids exploring the same location multiple times. This poor exploration limits performance on challenging domains. Meta-RL is a potential solution, as unlike standard RL, meta-RL can learn to explore, and potentially learn highly complex strategies far beyond those of standard RL, strategies such as experimenting in early episodes to learn new skills, or conducting experiments to learn about the current environment.Traditional meta-RL focuses on the problem of learning to optimally balance exploration and exploitation to maximize the cumulative reward of the episode sequence (e.g., aiming to maximize the total wins in a tournament -- while also improving as a player).We identify a new challenge with state-of-the-art cumulative-reward meta-RL methods.When optimal behavior requires exploration that sacrifices immediate reward to enable higher subsequent reward, existing state-of-the-art cumulative-reward meta-RL methods become stuck on the local optimum of failing to explore.Our method, First-Explore, overcomes this limitation by learning two policies: one to solely explore, and one to solely exploit. When exploring requires forgoing early-episode reward, First-Explore significantly outperforms existing cumulative meta-RL methods. By identifying and solving the previously unrecognized problem of forgoing reward in early episodes, First-Explore represents a significant step towards developing meta-RL algorithms capable of human-like exploration on a broader range of domains.',\n",
       "  864: 'Learning generalist embodied agents, able to solve multitudes of tasks in different domains is a long-standing problem. Reinforcement learning (RL) is hard to scale up as it requires a complex reward design for each task. In contrast, language can specify tasks in a more natural way. Current foundation vision-language models (VLMs) generally require fine-tuning or other adaptations to be adopted in embodied contexts, due to the significant domain gap. However, the lack of multimodal data in such domains represents an obstacle to developing foundation models for embodied applications. In this work, we overcome these problems by presenting multimodal-foundation world models, able to connect and align the representation of foundation VLMs with the latent space of generative world models for RL, without any language annotations. The resulting agent learning framework, GenRL, allows one to specify tasks through vision and/or language prompts, ground them in the embodied domain’s dynamics, and learn the corresponding behaviors in imagination.As assessed through large-scale multi-task benchmarking in locomotion and manipulation domains, GenRL enables multi-task generalization from language and visual prompts. Furthermore, by introducing a data-free policy learning strategy, our approach lays the groundwork for foundational policy learning using generative world models. Website, code and data: https://mazpie.github.io/genrl/',\n",
       "  865: \"Persona-driven role-playing (PRP) aims to build AI characters that can respond to user queries by faithfully sticking with \\\\emph{all} (factual) statements in persona documents.Unfortunately, existing faithfulness criteria for PRP are limited to coarse-grained LLM-based scoring without a clear definition or formulation.This paper presents a pioneering exploration to quantify PRP faithfulness evaluation as a fine-grained and explainable criterion, which also serves as a reliable reference for faithfulness optimization.Our criterion first discriminates persona statements into \\\\emph{active} and \\\\emph{passive} constraints by identifying the query-statement relevance.Then, we incorporate all constraints following the principle that the AI character's response should be (a) entailed by active constraints and (b) not contradicted by passive constraints.We translate this principle mathematically into a novel Active-Passive-Constraint (APC) score, a constraint-wise sum of statement-to-response natural language inference (NLI) scores weighted by constraint-query relevance scores. In practice, we build the APC scoring system by symbolically distilling small NLI and relevance discriminators (300M parameters) from GPT-4 for efficiency, and both show high consistency with GPT-4's discrimination.We validate the quality of the APC score against human evaluation based on example personas with tens of statements, and the results show a high correlation.As the APC score could faithfully reflect the PRP quality, we further leverage it as a reward system in direct preference optimization (DPO) for better AI characters. Our experiments offer a fine-grained and explainable comparison between existing PRP techniques, revealing their advantages and limitations.We further find APC-based DPO to be one of the most competitive techniques for sticking with all constraints and can be well incorporated with other techniques.We then extend the scale of the experiments to real persons with hundreds of statements and reach a consistent conclusion. Finally, we provide comprehensive analyses and case studies to support the effectiveness of APC and APC-based DPO.\",\n",
       "  866: 'One key challenge to video restoration is to model the transition dynamics of video frames governed by motion. In this work, we propose Turtle to learn the truncated causal history model for efficient and high-performing video restoration. Unlike traditional methods that process a range of contextual frames in parallel, Turtle enhances efficiency by storing and summarizing a truncated history of the input frame latent representation into an evolving historical state. This is achieved through a sophisticated similarity-based retrieval mechanism that implicitly accounts for inter-frame motion and alignment. The causal design in Turtle enables recurrence in inference through state-memorized historical features while allowing parallel training by sampling truncated video clips. We report new state-of-the-art results on a multitude of video restoration benchmark tasks, including video desnowing, nighttime video deraining, video raindrops and rain streak removal, video super-resolution, real-world and synthetic video deblurring, and blind video denoising while reducing the computational cost compared to existing best contextual methods on all these tasks.',\n",
       "  867: 'Denoising Diffusion Probabilistic Models (DDPM) have recently gained significant attention. DDPMs compose a Markovian process that begins in the data domain and gradually adds noise until reaching pure white noise. DDPMs generate high-quality samples from complex data distributions by defining an inverse process and training a deep neural network to learn this mapping. However, these models are inefficient because they require many diffusion steps to produce aesthetically pleasing samples. Additionally, unlike generative adversarial networks (GANs), the latent space of diffusion models is less interpretable. In this work, we propose to generalize the denoising diffusion process into an Upsampling Diffusion Probabilistic Model (UDPM). In the forward process, we reduce the latent variable dimension through downsampling, followed by the traditional noise perturbation. As a result, the reverse process gradually denoises and upsamples the latent variable to produce a sample from the data distribution. We formalize the Markovian diffusion processes of UDPM and demonstrate its generation capabilities on the popular FFHQ, AFHQv2, and CIFAR10 datasets. UDPM generates images with as few as three network evaluations, whose overall computational cost is less than a single DDPM or EDM step while achieving an FID score of 6.86. This surpasses current state-of-the-art efficient diffusion models that use a single denoising step for sampling. Additionally, UDPM offers an interpretable and interpolable latent space, which gives it an advantage over traditional DDPMs. Our code is available online: \\\\url{https://github.com/shadyabh/UDPM/}',\n",
       "  868: \"Many imitation learning (IL) algorithms use inverse reinforcement learning (IRL) to infer a reward function that aligns with the demonstration.However, the inferred reward functions often fail to capture the underlying task objectives.In this paper, we propose a novel framework for IRL-based IL that prioritizes task alignment over conventional data alignment. Our framework is a semi-supervised approach that leverages expert demonstrations as weak supervision to derive a set of candidate reward functions that align with the task rather than only with the data. It then adopts an adversarial mechanism to train a policy with this set of reward functions to gain a collective validation of the policy's ability to accomplish the task. We provide theoretical insights into this framework's ability to mitigate task-reward misalignment and present a practical implementation. Our experimental results show that our framework outperforms conventional IL baselines in complex and transfer learning scenarios.\",\n",
       "  869: 'Although recent advancements in large language models (LLMs) have significantly improved their performance on various tasks, they still face challenges with complex and symbolic multi-step reasoning, particularly in mathematical reasoning. To bolster the mathematical reasoning capabilities of LLMs, most existing efforts concentrate on seeking assistance from either domain experts or GPT-4 for high-quality process-supervised data, which is not only expensive but also labor-intensive. In our study, we propose an innovative framework, AlphaMath, that bypasses the need for process annotations (from humans or GPTs) by leveraging Monte Carlo Tree Search (MCTS). This framework focuses on unleashing the potential of a well-pretrained LLM to autonomously enhance its mathematical reasoning. Specifically, we integrate a value model with the LLM, automatically generating both process supervision and step-level evaluation signals in MCTS. Furthermore, we propose an efficient inference strategy—step-level beam search, where the value model is crafted to assist the policy model (i.e., LLM) in navigating more effective reasoning paths, rather than solely relying on prior probabilities. The experimental results on both in-domain and out-of-domain datasets demonstrate that even without GPT-4 or human-annotated process supervision, our AlphaMath framework achieves comparable or superior results to previous state-of-the-art methods.',\n",
       "  870: \"Operator learning problems arise in many key areas of scientific computing where Partial Differential Equations (PDEs) are used to model physical systems. In such scenarios, the operators map between Banach or Hilbert spaces. In this work, we tackle the problem of learning operators between Banach spaces, in contrast to the vast majority of past works considering only Hilbert spaces. We focus on learning holomorphic operators -- an important class of problems with many applications. We combine arbitrary approximate encoders and decoders with standard feedforward Deep Neural Network (DNN) architectures -- specifically, those with constant width exceeding the depth -- under standard $\\\\ell^2$-loss minimization. We first identify a family of  DNNs such that the resulting Deep Learning (DL) procedure achieves optimal generalization bounds for such operators. For standard fully-connected architectures, we then show that there are uncountably many minimizers of the training problem that yield equivalent optimal performance. The DNN architectures we consider are `problem agnostic', with width and depth only depending on the amount of training data $m$ and not on regularity assumptions of the target operator. Next, we show that DL is optimal for this problem: no recovery procedure can surpass these generalization bounds up to log terms. Finally, we present numerical results demonstrating the practical performance on challenging problems including the parametric diffusion, Navier-Stokes-Brinkman and Boussinesq PDEs.\",\n",
       "  871: \"We study a multi-agent imitation learning (MAIL) problem where we take the perspective of a learner attempting to coordinate a group of agents based on demonstrations of an expert doing so. Most prior work in MAIL essentially reduces the problem to matching the behavior of the expert within the support of the demonstrations. While doing so is sufficient to drive the value gap between the learner and the expert to zero under the assumption that agents are non-strategic, it does not guarantee robustness to deviations by strategic agents. Intuitively, this is because strategic deviations can depend on a counterfactual quantity: the coordinator's recommendations outside of the state distribution their recommendations induce. In response, we initiate the study of an alternative objective for MAIL in Markov Games we term the regret gap that explicitly accounts for potential deviations by agents in the group. We first perform an in-depth exploration of the relationship between the value and regret gaps. First, we show that while the value gap can be efficiently minimized via a direct extension of single-agent IL algorithms, even value equivalence can lead to an arbitrarily large regret gap. This implies that achieving regret equivalence is harder than achieving value equivalence in MAIL. We then provide a pair of efficient reductions to no-regret online convex optimization that are capable of minimizing the regret gap (a) under a coverage assumption on the expert (MALICE) or (b) with access to a queryable expert (BLADES).\",\n",
       "  872: 'State-of-the-art deep learning models for tabular data have recently achieved acceptable performance to be deployed in industrial settings. However, the robustness of these models remains scarcely explored. Contrary to computer vision, there are no effective attacks to properly evaluate the adversarial robustness of deep tabular models due to intrinsic properties of tabular data, such as categorical features, immutability, and feature relationship constraints. To fill this gap, we first propose CAPGD, a gradient attack that overcomes the failures of existing gradient attacks with adaptive mechanisms. This new attack does not require parameter tuning and further degrades the accuracy, up to 81\\\\% points compared to the previous gradient attacks. Second, we design CAA, an efficient evasion attack that combines our CAPGD attack and MOEVA, the best search-based attack.  We demonstrate the effectiveness of our attacks on five architectures and four critical use cases. Our empirical study demonstrates that CAA outperforms all existing attacks in 17 over the 20 settings, and leads to a drop in the accuracy by up to 96.1\\\\% points and 21.9\\\\% points compared to CAPGD and MOEVA respectively while being up to five times faster than MOEVA. Given the effectiveness and efficiency of our new attacks, we argue that they should become the minimal test for any new defense or robust architectures in tabular machine learning.',\n",
       "  873: \"Can Transformers predict new syllogisms by composing established ones? More generally, what type of targets can be learned by such models from scratch? Recent works show that Transformers can be Turing-complete in terms of expressivity, but this does not address the learnability objective. This paper puts forward the notion of 'globality degree' of a target distribution to capture when weak learning is efficiently achievable by regular Transformers. This measure shows a contrast with the expressivity results of Transformers captured by $TC^0/TC^1$ classes (further studied here), since the globality relates to correlations with the more limited $NC^0$ class. We show here experimentally and theoretically under additional assumptions that distributions with high globality cannot be learned efficiently. In particular, syllogisms cannot be composed on long chains.  Further, we develop scratchpad techniques and show that: (i) agnostic scratchpads cannot break the globality barrier, (ii) educated scratchpads can break the globality with intermediate steps, although not all such scratchpads can generalize out-of-distribution (OOD), (iii) a notion of 'inductive scratchpad', that composes the prior information more efficiently, can both break the globality barrier and improve the OOD generalization. In particular, some of our inductive scratchpads can achieve length generalizations of up to $6\\\\times$ for some arithmetic tasks depending on the input formatting.\",\n",
       "  874: 'Contrastive Language-Image Pre-training (CLIP) achieves impressive performance on tasks like image classification and image-text retrieval by learning on large-scale image-text datasets. However, CLIP struggles with dense prediction tasks due to the poor grasp of the fine-grained details. Although existing works pay attention to this issue, they achieve limited improvements and usually sacrifice the important visual-semantic consistency. To overcome these limitations, we propose FineCLIP, which keeps the global contrastive learning to preserve the visual-semantic consistency and further enhances the fine-grained understanding through two innovations: 1) A real-time self-distillation scheme that facilitates the transfer of representation capability from global to local features. 2) A semantically-rich regional contrastive learning paradigm with generated region-text pairs, boosting the local representation capabilities with abundant fine-grained knowledge. Both cooperate to fully leverage diverse semantics and multi-grained complementary information.To validate the superiority of our FineCLIP and the rationality of each design, we conduct extensive experiments on challenging dense prediction and image-level tasks. All the observations demonstrate the effectiveness of FineCLIP.',\n",
       "  875: 'We focus on generative AI for a type of data that still represent one of the most prevalent form of data: tabular data. We introduce a new powerful class of forest-based models fit for such tasks and a simple training algorithm with strong convergence guarantees in a boosting model that parallels that of the original weak / strong supervised learning setting. This algorithm can be implemented by a few tweaks to the most popular induction scheme for decision tree induction (i.e. supervised learning) with two classes. Experiments on the quality of generated data display substantial improvements compared to the state of the art. The losses our algorithm minimize and the structure of our models make them practical for related tasks that require fast estimation of a density given a generative model and an observation (even partially specified): such tasks include missing data imputation and density estimation. Additional experiments on these tasks reveal that our models can be notably good contenders to diverse state of the art methods, relying on models as diverse as (or mixing elements of) trees, neural nets, kernels or graphical models.',\n",
       "  876: 'We present sDBSCAN, a scalable density-based clustering algorithm in high dimensions with cosine distance. sDBSCAN leverages recent advancements in random projections given a significantly large number of random vectors to quickly identify core points and their neighborhoods, the primary hurdle of density-based clustering. Theoretically, sDBSCAN preserves the DBSCAN’s clustering structure under mild conditions with high probability. To facilitate sDBSCAN, we present sOPTICS, a scalable visual tool to guide the parameter setting of sDBSCAN. We also extend sDBSCAN and sOPTICS to L2, L1, χ2, and Jensen-Shannon distances via random kernel features. Empirically, sDBSCAN is significantly faster and provides higher accuracy than competitive DBSCAN variants on real-world million-point data sets. On these data sets, sDBSCAN and sOPTICS run in a few minutes, while the scikit-learn counterparts and other clustering competitors demand several hours orcannot run on our hardware due to memory constraints. Our code is available at https://github.com/NinhPham/sDbscan.',\n",
       "  877: 'This paper considers the distributed convex-concave minimax optimization under the second-order similarity.We propose stochastic variance-reduced optimistic gradient sliding (SVOGS) method, which takes the advantage of the finite-sum structure in the objective by involving the mini-batch client sampling and variance reduction.We prove SVOGS can achieve the $\\\\varepsilon$-duality gap within communication rounds of ${\\\\mathcal O}(\\\\delta D^2/\\\\varepsilon)$, communication complexity of ${\\\\mathcal O}(n+\\\\sqrt{n}\\\\delta D^2/\\\\varepsilon)$,and local gradient calls of $\\\\tilde{\\\\mathcal O}(n+(\\\\sqrt{n}\\\\delta+L)D^2/\\\\varepsilon\\\\log(1/\\\\varepsilon))$, where $n$ is the number of nodes, $\\\\delta$ is the degree of the second-order similarity, $L$ is the smoothness parameter and $D$ is the diameter of the constraint set.We can verify that all of above complexity (nearly) matches the corresponding lower bounds.For the specific $\\\\mu$-strongly-convex-$\\\\mu$-strongly-convex case, our algorithm has the upper bounds on communication rounds,  communication complexity, and local gradient calls of $\\\\mathcal O(\\\\delta/\\\\mu\\\\log(1/\\\\varepsilon))$, ${\\\\mathcal O}((n+\\\\sqrt{n}\\\\delta/\\\\mu)\\\\log(1/\\\\varepsilon))$, and $\\\\tilde{\\\\mathcal O}(n+(\\\\sqrt{n}\\\\delta+L)/\\\\mu)\\\\log(1/\\\\varepsilon))$ respectively, which are also nearly tight.Furthermore, we conduct the numerical experiments to show the empirical advantages of proposed method.',\n",
       "  878: \"This paper presents M$^3$GPT, an advanced $\\\\textbf{M}$ultimodal, $\\\\textbf{M}$ultitask  framework for $\\\\textbf{M}$otion comprehension and generation.  M$^3$GPT operates on three fundamental principles. The first focuses on creating a unified representation space for various motion-relevant modalities. We employ discrete vector quantization for multimodal conditional signals, such as text, music and motion/dance, enabling seamless integration into a large language model (LLM) with a single vocabulary.The second involves modeling motion generation directly in the raw motion space. This strategy circumvents the information loss associated with a discrete tokenizer, resulting in more detailed and comprehensive motion generation. Third, M$^3$GPT learns to model the connections and synergies among various motion-relevant tasks. Text, the most familiar and well-understood modality for LLMs, is utilized as a bridge to establish connections between different motion tasks, facilitating mutual reinforcement. To our knowledge, M$^3$GPT is the first model capable of comprehending and generating motions based on multiple signals.Extensive experiments highlight M$^3$GPT's superior performance across various motion-relevant tasks and its powerful zero-shot generalization capabilities for extremely challenging tasks. Project page: \\\\url{https://github.com/luomingshuang/M3GPT}.\",\n",
       "  879: 'Traditional 3D shape reconstruction techniques from multi-view images, such as structure from motion and multi-view stereo, face challenges in reconstructing transparent objects. Recent advances in neural radiance fields and its variants primarily address opaque or transparent objects, encountering difficulties to reconstruct both transparent and opaque objects simultaneously. This paper introduces $\\\\alpha$-NeuS$\\\\textemdash$an extension of NeuS$\\\\textemdash$that proves NeuS is unbiased for materials from fully transparent to fully opaque. We find that transparent and opaque surfaces align with the non-negative local minima and the zero iso-surface, respectively, in the learned distance field of NeuS. Traditional iso-surfacing extraction algorithms, such as marching cubes, which rely on fixed iso-values, are ill-suited for such data. We develop a method to extract the transparent and opaque surface simultaneously based on DCUDF. To validate our approach, we construct a benchmark that includes both real-world and synthetic scenes, demonstrating its practical utility and effectiveness. Our data and code are publicly available at https://github.com/728388808/alpha-NeuS.',\n",
       "  880: \"In many Deep Reinforcement Learning (RL) problems, decisions in a trained policy vary in significance for the expected safety and performance of the policy. Since RL policies are very complex, testing efforts should concentrate on states in which the agent's decisions have the highest impact on the expected outcome. In this paper, we propose a novel model-based method to rigorously compute a ranking of state importance across the entire state space. We then focus our testing efforts on the highest-ranked states. In this paper, we focus on testing for safety. However, the proposed methods can be easily adapted to test for performance. In each iteration, our testing framework computes optimistic and pessimistic safety estimates. These estimates provide lower and upper bounds on the expected outcomes of the policy execution across all modeled states in the state space. Our approach divides the state space into safe and unsafe regions upon convergence, providing clear insights into the policy's weaknesses. Two important properties characterize our approach. (1) Optimal Test-Case Selection: At any time in the testing process, our approach evaluates the policy in the states that are most critical for safety. (2) Guaranteed Safety: Our approach can provide formal verification guarantees over the entire state space by sampling only a fraction of the policy. Any safety properties assured by the pessimistic estimate are formally proven to hold for the policy. We provide a detailed evaluation of our framework on several examples, showing that our method discovers unsafe policy behavior with low testing effort.\",\n",
       "  881: \"Video transformers are slow to train due to extremely large numbers of input tokens, even though many video tokens are repeated over time. Existing methods to remove uninformative tokens either have significant overhead, negating any speedup, or require tuning for different datasets and examples. We present Run-Length Tokenization (RLT), a simple approach to speed up video transformers inspired by run-length encoding for data compression. RLT efficiently finds and removes `runs' of patches that are repeated over time before model inference, then replaces them with a single patch and a positional encoding to represent the resulting token's new length. Our method is content-aware, requiring no tuning for different datasets, and fast, incurring negligible overhead. RLT yields a large speedup in training, reducing the wall-clock time to fine-tune a video transformer by 30% while matching baseline model performance. RLT also works without training, increasing model throughput by 35% with only 0.1% drop in accuracy.RLT speeds up training at 30 FPS by more than 100%, and on longer video datasets, can reduce the token count by up to 80\\\\%. Our project page is at  rccchoudhury.github.io/projects/rlt.\",\n",
       "  882: 'Deep Multi-agent Reinforcement Learning (MARL) relies on neural networks with numerous parameters in multi-agent scenarios, often incurring substantial computational overhead. Consequently, there is an urgent need to expedite training and enable model compression in MARL. This paper proposes the utilization of dynamic sparse training (DST), a technique proven effective in deep supervised learning tasks, to alleviate the computational burdens in MARL training. However, a direct adoption of DST fails to yield satisfactory MARL agents, leading to breakdowns in value learning within deep sparse value-based MARL models. Motivated by this challenge, we introduce an innovative Multi-Agent Sparse Training (MAST) framework aimed at simultaneously enhancing the reliability of learning targets and the rationality of sample distribution to improve value learning in sparse models. Specifically, MAST incorporates the Soft Mellowmax Operator with a hybrid TD-($\\\\lambda$) schema to establish dependable learning targets. Additionally, it employs a dual replay buffer mechanism to enhance the distribution of training samples. Building upon these aspects, MAST utilizes gradient-based topology evolution to exclusively train multiple MARL agents using sparse networks. Our comprehensive experimental investigation across various value-based MARL algorithms on multiple benchmarks demonstrates, for the first time, significant reductions in redundancy of up to $20\\\\times$ in Floating Point Operations (FLOPs) for both training and inference, with less than 3% performance degradation.',\n",
       "  883: 'Recently, various strategies for distributed training of large language models (LLMs) have been proposed.By categorizing them into basic strategies and composite strategies, we have discovered that existing basic strategies provide limited options in specific scenarios, leaving considerable room for optimization in training speed.In this paper, we rethink the impact of memory and communication costs on the training speed of LLMs, taking into account the impact of intra- and inter-group communication performance disparities, and then propose a new set of basic strategies named the \\\\textbf{Pa}rtial \\\\textbf{R}edundancy \\\\textbf{O}ptimizer (PaRO).PaRO Data Parallelism (PaRO-DP) accelerates LLM training through refined model state partitioning and tailored training procedures. At the same time, PaRO Collective Communications (PaRO-CC) speeds up collective communication operations by rearranging the topology. We also propose a guideline for choosing different DP strategies based on simple quantitative calculations, which yields minimal ranking errors.Our experiments demonstrate that PaRO improves the training speed of LLMs by up to 266\\\\% that of ZeRO-3 as basic DP strategies.Moreover, employing PaRO-CC independently for model parallel strategies, such as Megatron, can also boost the training speed by 17\\\\%.',\n",
       "  884: \"As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and (2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Most open-source LLMs, e.g., CodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than commercial LLMs, e.g., GPT-4, in complex games, yet the recently released Llama-3-70b-Instruct makes up for this shortcoming. In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. We further characterize the game-theoretic properties of LLMs, such as equilibrium and Pareto Efficiency in repeated games. Detailed error profiles are provided for a better understanding of LLMs' behavior. We hope our research provides standardized protocols and serves as a foundation to spur further explorations in the strategic reasoning of LLMs.\",\n",
       "  885: 'Automated seizure detection (ASD) using intracranial electroencephalography (iEEG) is critical for effective epilepsy treatment. However, the significant domain shift of iEEG signals across subjects poses a major challenge, limiting their applicability in real-world clinical scenarios. In this paper, we address this issue by analyzing the primary cause behind the failure of existing iEEG models for subject-independent seizure detection, and identify a critical universal seizure pattern: seizure events consistently exhibit higher average amplitude compared to adjacent normal events. To mitigate the domain shifts and preserve the universal seizure patterns, we propose a novel self-comparison mechanism. This mechanism effectively aligns iEEG signals across subjects and time intervals. Building upon these findings, we propose Difference Matrix-based Neural Network (DMNet), a subject-independent seizure detection model, which leverages self-comparison based on two constructed (contextual, channel-level) references to mitigate shifts of iEEG, and utilize a simple yet effective difference matrix to encode the universal seizure patterns. Extensive experiments show that DMNet significantly outperforms previous SOTAs while maintaining high efficiency on a real-world clinical dataset collected by us and two public datasets for subject-independent seizure detection. Moreover, the visualization results demonstrate that the generated difference matrix can effectively capture the seizure activity changes during the seizure evolution process. Additionally, we deploy our method in an online diagnosis system to illustrate its effectiveness in real clinical applications.',\n",
       "  886: 'Tokenizer, serving as a translator to map the intricate visual data into a compact latent space, lies at the core of visual generative models. Based on the finding that existing tokenizers are tailored to either image or video inputs, this paper presents OmniTokenizer, a transformer-based tokenizer for joint image and video tokenization. OmniTokenizer is designed with a spatial-temporal decoupled architecture, which integrates window attention and causal attention for spatial and temporal modeling, respectively. To exploit the complementary nature of image and video data, we further propose a progressive training strategy, where OmniTokenizer is first trained on image data on a fixed resolution to develop the spatial encoding capacity and then jointly trained on image and video data on multiple resolutions to learn the temporal dynamics. OmniTokenizer, for the first time, handles both image and video inputs within a unified framework and proves the possibility of realizing their synergy. Extensive experiments demonstrate that OmniTokenizer achieves state-of-the-art (SOTA) reconstruction performance on various image and video datasets, e.g., 1.11 reconstruction FID on ImageNet and 42 reconstruction FVD on UCF-101, beating the previous SOTA methods by 13% and 26%, respectively. Additionally, we also show that when integrated with OmniTokenizer, both language model-based approaches and diffusion models can realize advanced visual synthesis performance, underscoring the superiority and versatility of our method.',\n",
       "  887: 'Alignment techniques are critical in ensuring that large language models (LLMs) output helpful and harmless content by enforcing the LLM-generated content to align with human preferences. However, the existence of noisy preferences (NPs), where the responses are mistakenly labelled as chosen or rejected, could spoil the alignment, thus making the LLMs generate useless and even malicious content. Existing methods mitigate the issue of NPs from the loss perspective by adjusting the alignment loss based on a clean validation dataset.Orthogonal to these loss-oriented methods, we propose perplexity-aware correction (PerpCorrect) from the data perspective for robust alignment which detects and corrects NPs based on the differences between the perplexity of the chosen and rejected responses (dubbed as PPLDiff). Intuitively, a higher PPLDiff indicates a higher probability of the NP because a rejected/chosen response which is mistakenly labelled as chosen/rejected is less preferable to be generated by an aligned LLM, thus having a higher/lower perplexity.PerpCorrect works in three steps: (1) PerpCorrect aligns a surrogate LLM using the clean validation data to make the PPLDiff able to distinguish clean preferences (CPs) and NPs. (2) PerpCorrect further aligns the surrogate LLM by incorporating the reliable clean training data whose PPLDiff is extremely small and reliable noisy training data whose PPLDiff is extremely large after correction to boost the discriminatory power.(3) Detecting and correcting NPs according to the PPLDiff obtained by the aligned surrogate LLM to obtain a denoised training dataset for robust alignment.Comprehensive experiments validate that our proposed PerpCorrect can achieve state-of-the-art alignment performance under NPs.Notably, PerpCorrect demonstrates practical utility by requiring only a modest amount of validation data and being compatible with various alignment techniques. Our code is available at PerpCorrect.',\n",
       "  888: 'Many algorithms and observed phenomena in deep learning appear to be affected by parameter symmetries --- transformations of neural network parameters that do not change the underlying neural network function. These include linear mode connectivity, model merging, Bayesian neural network inference, metanetworks, and several other characteristics of optimization or loss-landscapes. However, theoretical analysis of the relationship between parameter space symmetries and these phenonmena is difficult. In this work, we empirically investigate the impact of neural parameter symmetries by introducing new neural network architectures that have reduced parameter space symmetries. We develop two methods, with some provable guarantees, of modifying standard neural networks to reduce parameter space symmetries.  With these new methods, we conduct a comprehensive experimental study consisting of multiple tasks aimed at assessing the effect of removing parameter symmetries. Our experiments reveal several interesting observations on the empirical impact of parameter symmetries; for instance, we observe linear mode connectivity between our networks without alignment of weight spaces, and we find that our networks allow for faster and more effective Bayesian neural network training.',\n",
       "  889: 'Message-passing graph neural networks (MPNNs) have emerged as a powerful paradigm for graph-based machine learning. Despite their effectiveness, MPNNs face challenges such as under-reaching and over-squashing, where limited receptive fields and structural bottlenecks hinder information flow in the graph. While graph transformers hold promise in addressing these issues, their scalability is limited due to quadratic complexity regarding the number of nodes, rendering them impractical for larger graphs. Here, we propose implicitly rewired message-passing neural networks (IPR-MPNNs), a novel approach that integrates implicit probabilistic graph rewiring into MPNNs. By introducing a small number of virtual nodes, i.e., adding additional nodes to a given graph and connecting them to existing nodes, in a differentiable, end-to-end manner, IPR-MPNNs enable long-distance message propagation, circumventing quadratic complexity. Theoretically, we demonstrate that IPR-MPNNs surpass the expressiveness of traditional MPNNs.  Empirically, we validate our approach by showcasing its ability to mitigate under-reaching and over-squashing effects, achieving state-of-the-art performance across multiple graph datasets. Notably, IPR-MPNNs outperform graph transformers while maintaining significantly faster computational efficiency.',\n",
       "  890: 'One-shot Federated Learning (OFL) significantly reduces communication costs in FL by aggregating trained models only once. However, the performance of advanced OFL methods is far behind the normal FL. In this work, we provide a causal view to find that this performance drop of OFL methods comes from the isolation problem, which means that local isolatedly trained models in OFL may easily fit to spurious correlations due to the data heterogeneity. From the causal perspective, we observe that the spurious fitting can be alleviated by augmenting intermediate features from other clients. Built upon our observation, we propose a novel learning approach to endow OFL with superb performance and low communication and storage costs, termed as FuseFL. Specifically, FuseFL decomposes neural networks into several blocks, and progressively trains and fuses each block following a bottom-up manner for feature augmentation, introducing no additional communication costs. Comprehensive experiments demonstrate that FuseFL outperforms existing OFL and ensemble FL by a significant margin. We conduct comprehensive experiments to show that FuseFL supports high scalability of clients, heterogeneous model training, and low memory costs. Our work is the first attempt using causality to analyze and alleviate data heterogeneity of OFL.',\n",
       "  891: 'Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control. In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate task-specific refinement, such as Reinforcement Learning (RL). To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent. The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks. This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline. Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency. In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it achieves SOTA performance across all designated MineDojo tasks.',\n",
       "  892: 'Second-order optimization algorithms, such as the Newton method and the natural gradient descent (NGD) method exhibit excellent convergence properties for training deep neural networks, but the high computational cost limits its practical application. In this paper, we focus on the NGD method and propose a novel layer-wise natural gradient descent (LNGD) method to further reduce computational costs and accelerate the training process. Specifically, based on the block diagonal approximation of the Fisher information matrix, we first propose the layer-wise sample method to compute each block matrix without performing a complete back-propagation. Then, each block matrix is approximated as a Kronecker product of two smaller matrices, one of which is a diagonal matrix, while keeping the traces equal before and after approximation. By these two steps, we provide a new approximation for the Fisher information matrix, which can effectively reduce the computational cost while preserving the main information of each block matrix. Moreover, we propose a new adaptive layer-wise learning rate to further accelerate training. Based on these new approaches, we propose the LNGD optimizer. The global convergence analysis of LNGD is established under some assumptions. Experiments on image classification and machine translation tasks show that our method is quite competitive compared to the state-of-the-art methods.',\n",
       "  893: 'Machine learning applied to blockchain graphs offers significant opportunities for enhanced data analysis and applications. However, the potential of this field is constrained by the lack of a large-scale, cross-chain dataset that includes hierarchical graph-level data. To address this issue, we present novel datasets that provide detailed label information at the token level and integrate interactions between tokens across multiple blockchain platforms. We model transactions within each token as local graphs and the relationships between tokens as global graphs, collectively forming a \"Graphs of Graphs\" (GoG) approach. This innovative approach facilitates a deeper understanding of systemic structures and hierarchical interactions, which are essential for applications such as link prediction, anomaly detection, and token classification. We conduct a series of experiments demonstrating that this dataset delivers new insights and challenges for exploring GoG within the blockchain domain. Our work promotes advancements and opens new avenues for research in both the blockchain and graph communities. Source code and datasets are available at https://github.com/Xtra-Computing/Cryptocurrency-Graphs-of-graphs.',\n",
       "  894: 'Multimodal Sentiment Analysis (MSA) is an important research area that aims to understand and recognize human sentiment through multiple modalities. The complementary information provided by multimodal fusion promotes better sentiment analysis compared to utilizing only a single modality. Nevertheless, in real-world applications, many unavoidable factors may lead to situations of uncertain modality missing, thus hindering the effectiveness of multimodal modeling and degrading the model’s performance. To this end, we propose a Hierarchical Representation Learning Framework (HRLF) for the MSA task under uncertain missing modalities. Specifically, we propose a fine-grained representation factorization module that sufficiently extracts valuable sentiment information by factorizing modality into sentiment-relevant and modality-specific representations through crossmodal translation and sentiment semantic reconstruction. Moreover, a hierarchical mutual information maximization mechanism is introduced to incrementally maximize the mutual information between multi-scale representations to align and reconstruct the high-level semantics in the representations. Ultimately, we propose a hierarchical adversarial learning mechanism that further aligns and adapts the latent distribution of sentiment-relevant representations to produce robust joint multimodal representations. Comprehensive experiments on three datasets demonstrate that HRLF significantly improves MSA performance under uncertain modality missing cases.',\n",
       "  895: 'The quantum approximate optimization algorithm (QAOA) is a general-purpose algorithm for combinatorial optimization that has been a promising avenue for near-term quantum advantage. In this paper, we analyze the performance of the QAOA on the spiked tensor model, a statistical estimation problem that exhibits a large computational-statistical gap classically. We prove that the weak recovery threshold of $1$-step QAOA matches that of $1$-step tensor power iteration. Additional heuristic calculations suggest that the weak recovery threshold of $p$-step QAOA matches that of $p$-step tensor power iteration when $p$ is a fixed constant. This further implies that multi-step QAOA with tensor unfolding could achieve, but not surpass, the asymptotic classical computation threshold $\\\\Theta(n^{(q-2)/4})$ for spiked $q$-tensors. Meanwhile, we characterize the asymptotic overlap distribution for $p$-step QAOA, discovering an intriguing sine-Gaussian law verified through simulations. For some $p$ and $q$, the QAOA has an effective recovery threshold that is a constant factor better than tensor power iteration.Of independent interest, our proof techniques employ the Fourier transform to handle difficult combinatorial sums, a novel approach differing from prior QAOA analyses on spin-glass models without planted structure.',\n",
       "  896: 'Previous work on Universal Transformers (UTs) has demonstrated the importance of parameter sharing across layers. By allowing recurrence in depth, UTs have advantages over standard Transformers in learning compositional generalizations, but layer-sharing comes with a practical limitation of parameter-compute ratio: it drastically reduces the parameter count compared to the non-shared model with the same dimensionality. Naively scaling up the layer size to compensate for the loss of parameters makes its computational resource requirements prohibitive. In practice, no previous work has succeeded in proposing a shared-layer Transformer design that is competitive in parameter count-dominated tasks such as language modeling. Here we propose MoEUT (pronounced \"moot\"), an effective mixture-of-experts (MoE)-based shared-layer Transformer architecture, which combines several recent advances in MoEs for both feedforward and attention layers of standard Transformers together with novel layer-normalization and grouping schemes that are specific and crucial to UTs. The resulting UT model, for the first time, slightly outperforms standard Transformers on language modeling tasks such as BLiMP and PIQA, while using significantly less compute and memory.',\n",
       "  897: \"Existing Multimodal Large Language Models (MLLMs) follow the paradigm that perceives visual information by aligning visual features with the input space of Large Language Models (LLMs) and concatenating visual tokens with text tokens to form a unified sequence input for LLMs. These methods demonstrate promising results on various vision-language tasks but are limited by the high computational effort due to the extended input sequence resulting from the involvement of visual tokens. In this paper, instead of input space alignment, we propose a novel parameter space alignment paradigm that represents visual information as model weights. For each input image, we use a vision encoder to extract visual features, convert features into perceptual weights, and merge the perceptual weights with LLM's weights. In this way, the input of LLM does not require visual tokens, which reduces the length of the input sequence and greatly improves efficiency. Following this paradigm, we propose VLoRA with the perceptual weights generator. The perceptual weights generator is designed to convert visual features to perceptual weights with low-rank property, exhibiting a form similar to LoRA. The experimental results show that our VLoRA achieves comparable performance on various benchmarks for MLLMs, while significantly reducing the computational costs for both training and inference. Code and models are released at \\\\url{https://github.com/FeipengMa6/VLoRA}.\",\n",
       "  898: 'Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks. These models typically leverage reconstructive autoencoders like VQGAN or VAE to encode pixels into a more compact latent space and learn the data distribution in the latent space instead of directly from pixels. However, this practice raises a pertinent question: Is it truly the optimal choice? In response, we begin with an intriguing observation: despite sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in image generation. This finding contrasts sharply with the field of NLP, where the autoregressive model GPT has established a commanding presence. To address this discrepancy, we introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Furthermore, we propose a simple but effective discrete image tokenizer to stabilize the latent space for image generative modeling by applying K-Means on the latent features of self-supervised learning models. Experimental results show that image autoregressive modeling with our tokenizer (DiGIT) benefits both image understanding and image generation with the next token prediction principle, which is inherently straightforward for GPT models but challenging for other generative models. Remarkably, for the first time, a GPT-style autoregressive model for images outperforms LDMs, which also exhibits substantial improvement akin to GPT when scaling up model size. Our findings underscore the potential of an optimized latent space and the integration of discrete tokenization in advancing the capabilities of image generative models. The code is available at \\\\url{https://github.com/DAMO-NLP-SG/DiGIT}.',\n",
       "  899: \"We introduce ReXTime, a benchmark designed to rigorously test AI models' ability to perform temporal reasoning within video events.Specifically, ReXTime focuses on reasoning across time, i.e. human-like understanding when the question and its corresponding answer occur in different video segments. This form of reasoning, requiring advanced understanding of cause-and-effect relationships across video segments, poses significant challenges to even the frontier multimodal large language models. To facilitate this evaluation, we develop an automated pipeline for generating temporal reasoning question-answer pairs, significantly reducing the need for labor-intensive manual annotations. Our benchmark includes 921 carefully vetted validation samples and 2,143 test samples, each manually curated for accuracy and relevance. Evaluation results show that while frontier large language models outperform academic models, they still lag behind human performance by a significant 14.3\\\\% accuracy gap. Additionally, our pipeline creates a training dataset of 9,695 machine generated samples without manual effort, which empirical studies suggest can enhance the across-time reasoning via fine-tuning.\",\n",
       "  900: 'In this paper, we present a novel data-free method for merging neural networks in weight space. Our method optimizes for the permutations of network neurons while ensuring global coherence across all layers, and it outperforms recent layer-local approaches in a set of challenging scenarios. We then generalize the formulation to the $N$-models scenario to enforce cycle consistency of the permutations with guarantees, allowing circular compositions of permutations to be computed without accumulating error along the path.     We qualitatively and quantitatively motivate the need for such a constraint, showing its benefits when merging homogeneous sets of models in scenarios spanning varying architectures and datasets. We finally show that, when coupled with activation renormalization, the approach yields the best results in the task.',\n",
       "  901: \"Benchmarking vision-based driving policies is challenging. On one hand, open-loop evaluation with real data is easy, but these results do not reflect closed-loop performance. On the other, closed-loop evaluation is possible in simulation, but is hard to scale due to its significant computational demands. Further, the simulators available today exhibit a large domain gap to real data. This has resulted in an inability to draw clear conclusions from the rapidly growing body of research on end-to-end autonomous driving. In this paper, we present NAVSIM, a middle ground between these evaluation paradigms, where we use large datasets in combination with a non-reactive simulator to enable large-scale real-world benchmarking. Specifically, we gather simulation-based metrics, such as progress and time to collision, by unrolling bird's eye view abstractions of the test scenes for a short simulation horizon. Our simulation is non-reactive, i.e., the evaluated policy and environment do not influence each other. As we demonstrate empirically, this decoupling allows open-loop metric computation while being better aligned with closed-loop evaluations than traditional displacement errors. NAVSIM enabled a new competition held at CVPR 2024, where 143 teams submitted 463 entries, resulting in several new insights. On a large set of challenging scenarios, we observe that simple methods with moderate compute requirements such as TransFuser can match recent large-scale end-to-end driving architectures such as UniAD. Our modular framework can potentially be extended with new datasets, data curation strategies, and metrics, and will be continually maintained to host future challenges. Our code is available at https://github.com/autonomousvision/navsim.\",\n",
       "  902: 'Trajectory prediction is a crucial technology to help systems avoid traffic accidents, ensuring safe autonomous driving. Previous methods typically use a fixed-length and sufficiently long trajectory of an agent as observations to predict its future trajectory. However, in real-world scenarios, we often lack the time to gather enough trajectory points before making predictions, e.g., when a car suddenly appears due to an obstruction, the system must make immediate predictions to prevent a collision. This poses a new challenge for trajectory prediction systems, requiring them to be capable of making accurate predictions based on observed trajectories of arbitrary lengths, leading to the failure of existing methods. In this paper, we propose a Length-agnostic Knowledge Distillation framework, named LaKD,  which can make accurate trajectory predictions, regardless of the length of observed data. Specifically, considering the fact that long trajectories, containing richer temporal information but potentially additional interference, may perform better or worse than short trajectories, we devise a dynamic length-agnostic knowledge distillation mechanism for exchanging information among trajectories of arbitrary lengths, dynamically determining the transfer direction based on prediction performance. In contrast to traditional knowledge distillation, LaKD employs a unique model that simultaneously serves as both the teacher and the student, potentially causing knowledge collision during the distillation process. Therefore, we design a dynamic soft-masking mechanism, where we first calculate the importance of neuron units and then apply soft-masking to them, so as to safeguard critical units from disruption during the knowledge distillation process. In essence, LaKD is a general and principled framework that can be naturally compatible with existing trajectory prediction models of different architectures. Extensive experiments on three benchmark datasets, Argoverse 1, nuScenes and Argoverse 2, demonstrate the effectiveness of our approach.',\n",
       "  903: 'Recent advancements in diffusion models and diffusion bridges primarily focus on finite-dimensional spaces, yet many real-world problems necessitate operations in infinite-dimensional function spaces for more natural and interpretable formulations. In this paper, we present a theory of stochastic optimal control (SOC) tailored to infinite-dimensional spaces, aiming to extend diffusion-based algorithms to function spaces. Specifically, we demonstrate how Doob’s $h$-transform, the fundamental tool for constructing diffusion bridges, can be derived from the SOC perspective and expanded to infinite dimensions. This expansion presents a challenge, as infinite-dimensional spaces typically lack closed-form densities. Leveraging our theory, we establish that solving the optimal control problem with a specific objective function choice is equivalent to learning diffusion-based generative models. We propose two applications: 1) learning bridges between two infinite-dimensional distributions and 2) generative models for sampling from an infinite-dimensional distribution. Our approach proves effective for diverse problems involving continuous function space representations, such as resolution-free images, time-series data, and probability density functions.',\n",
       "  904: 'Verbal and visual-spatial information processing are two critical subsystems that activate different brain regions and often collaborate together for cognitive reasoning. Despite the rapid advancement of LLM-based reasoning, the mainstream frameworks, such as Chain-of-Thought (CoT) and its variants, primarily focus on the verbal dimension, resulting in limitations in tackling reasoning problems with visual and spatial clues. To bridge the gap, we propose a novel dual-modality reasoning framework called Vision-Augmented Prompting (VAP). Upon receiving a textual problem description, VAP automatically synthesizes an image from the visual and spatial clues by utilizing external drawing tools. Subsequently, VAP formulates a chain of thought in both modalities and iteratively refines the synthesized image. Finally, a conclusive reasoning scheme based on self-alignment is proposed for final result generation. Extensive experiments are conducted across four versatile tasks, including solving geometry problems, Sudoku, time series prediction, and travelling salesman problem. The results validated the superiority of VAP over existing LLMs-based reasoning frameworks.',\n",
       "  905: 'The ability to compare objects, scenes, or situations is crucial for effective decision-making and problem-solving in everyday life. For instance, comparing the freshness of apples enables better choices during grocery shopping, while comparing sofa designs helps optimize the aesthetics of our living space. Despite its significance, the comparative capability is largely unexplored in artificial general intelligence (AGI). In this paper, we introduce MLLM-CompBench, a benchmark designed to evaluate the comparative reasoning capability of multimodal large language models (MLLMs). MLLM-CompBench mines and pairs images through visually oriented questions covering eight dimensions of relative comparison: visual attribute, existence, state, emotion, temporality, spatiality, quantity, and quality. We curate a collection of around 40K image pairs using metadata from diverse vision datasets and CLIP similarity scores. These image pairs span a broad array of visual domains, including animals, fashion, sports, and both outdoor and indoor scenes. The questions are carefully crafted to discern relative characteristics between two images and are labeled by human annotators for accuracy and relevance. We use MLLM-CompBench to evaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable shortcomings in their comparative abilities. We believe MLLM-CompBench not only sheds light on these limitations but also establishes a solid foundation for future enhancements in the comparative capability of MLLMs.',\n",
       "  906: 'Large multimodal models (LMMs) are processing increasingly longer and richer inputs. Albeit the progress, few public benchmark is available to measure such development. To mitigate this gap, we introduce LongVideoBench, a question-answering benchmark that features video-language interleaved inputs up to an hour long. Our benchmark includes 3,763 varying-length web-collected videos with their subtitles across diverse themes, designed to comprehensively evaluate LMMs on long-term multimodal understanding. To achieve this, we interpret the primary challenge as to accurately retrieve and reason over detailed multimodal information from long inputs. As such, we formulate a novel video question-answering task termed referring reasoning. Specifically, as part of the question, it contains a referring query that references related video contexts, called referred context. The model is then required to reason over relevant video details from the referred context. Following the paradigm of referring reasoning, we curate 6,678 human-annotated multiple-choice questions in 17 fine-grained categories, establishing one of the most comprehensive benchmarks for long-form video understanding. Evaluations suggest that the LongVideoBench presents significant challenges even for the most advanced proprietary models (e.g. GPT-4o, Gemini-1.5-Pro), while their open-source counterparts show an even larger performance gap. In addition, our results indicate that model performance on the benchmark improves only when they are capable of processing more frames, positioning LongVideoBench as a valuable benchmark for evaluating future-generation long-context LMMs.',\n",
       "  907: 'Users typically engage with LLMs interactively, yet most existing benchmarks evaluate them in a static, single-turn format, posing reliability concerns in interactive scenarios. We identify a key obstacle towards reliability: LLMs are trained to answer any question, even with incomplete context or insufficient knowledge. In this paper, we propose to change the static paradigm to an interactive one, develop systems that proactively ask questions to gather more information and respond reliably, and introduce an benchmark—MEDIQ—to evaluate question-asking ability in LLMs. MEDIQ simulates clinical interactions consisting of a Patient System and an adaptive Expert System; with potentially incomplete initial information, the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details via follow-up questions. We provide a pipeline to convert single-turn medical benchmarks into an interactive format. Our results show that directly prompting state-of-the-art LLMs to ask questions degrades performance, indicating that adapting LLMs to proactive information-seeking settings is nontrivial. We experiment with abstention strategies to better estimate model confidence and decide when to ask questions, improving diagnostic accuracy by 22.3%; however, performance still lags compared to an (unrealistic in practice) upper bound with complete information upfront. Further analyses show improved interactive performance with filtering irrelevant contexts and reformatting conversations. Overall, we introduce a novel problem towards LLM reliability, an interactive MEDIQ benchmark and a novel question-asking system, and highlight directions to extend LLMs’ information-seeking abilities in critical domains.',\n",
       "  908: \"Multimodal contrastive learning (MCL) has recently demonstrated significant success across various tasks. However, the existing MCL treats all negative samples equally and ignores the potential semantic association with positive samples, which limits the model's ability to achieve fine-grained alignment. In multi-view scenarios, MCL tends to prioritize shared information while neglecting modality-specific unique information across different views, leading to feature suppression and suboptimal performance in downstream tasks. To address these limitations, we propose a novel contrastive framework name QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization. In the QUEST framework, we propose quaternion contrastive objectives and orthogonal constraints to extract sufficient unique information. Meanwhile, a shared information-guided penalization is introduced to ensure that shared information does not excessively influence the optimization of unique information. Our method leverages quaternion vector spaces to simultaneously optimize shared and unique information. Experiments on multiple datasets show that our method achieves superior performance in multimodal contrastive learning benchmarks. On public benchmark, our approach achieves state-of-the-art performance, and on synthetic shortcut datasets, we outperform existing baseline methods by an average of 97.95\\\\% on the CLIP model.\",\n",
       "  909: 'Novel view acoustic synthesis (NVAS) aims to render binaural audio at any target viewpoint, given a mono audio emitted by a sound source at a 3D scene. Existing methods have proposed NeRF-based implicit models to exploit visual cues as a condition for synthesizing binaural audio. However, in addition to low efficiency originating from heavy NeRF rendering, these methods all have a limited ability of characterizing the entire scene environment such as room geometry, material properties, and the spatial relation between the listener and sound source. To address these issues, we propose a novel Audio-Visual Gaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-aware condition for audio synthesis, we learn an explicit point-based scene representation with audio-guidance parameters on locally initialized Gaussian points, taking into account the space relation from the listener and sound source. To make the visual scene model audio adaptive, we propose a point densification and pruning strategy to optimally distribute the Gaussian points, with the per-point contribution in sound propagation (e.g., more points needed for texture-less wall surfaces as they affect sound path diversion). Extensive experiments validate the superiority of our AV-GS over existing alternatives on the real-world RWAS and simulation-based SoundSpaces datasets. Project page: \\\\url{https://surrey-uplab.github.io/research/avgs/}',\n",
       "  910: 'The flexible and generalized nature of large language models has allowed for their application in a wide array of language-based domains.Much like their human contemporaries, these models are capable of engaging in discussions and debates as a means of improving answer quality.We first take a theoretical approach to analyzing debate and provide a framework through which debate can be mathematically examined.Building on this framework, we provide several theoretical results for multi-agent debate.In particular, we demonstrate that similar model capabilities, or similar model responses, can result in static debate dynamics where the debate procedure simply converges to the majority opinion. When this majority opinion is the result of a common misconception (ingrained in the models through shared training data) debate is likely to converge to answers associated with that common misconception.Using insights from our theoretical results we then propose three interventions which improve the efficacy of debate. For each intervention, we provide theoretical results demonstrating how debate is improved.We also demonstrate that these interventions result in better performance on four common benchmark tasks.',\n",
       "  911: \"Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled scalability in deep learning. SMoE has the potential to exponentially increase in parameter count while maintaining the efficiency of the model by only activating a small subset of these parameters for a given sample. However, it has been observed that SMoE suffers from unstable training and has difficulty adapting to new distributions, leading to the model's lack of robustness to data contamination. To overcome these limitations, we first establish a connection between the dynamics of the expert representations in SMoEs and gradient descent on a multi-objective optimization problem. Leveraging our framework, we then integrate momentum into SMoE and propose a new family of SMoEs, named MomentumSMoE. We theoretically prove and numerically validate that MomentumSMoE is more stable and robust than SMoE. In particular, we verify the advantages of MomentumSMoE over SMoE on a variety of practical tasks including ImageNet-1K object recognition and WikiText-103 language modeling. We demonstrate the applicability of MomentumSMoE to many types of SMoE models, including those in the Sparse MoE model for vision (V-MoE) and the Generalist Language Model (GLaM). We also show that other advanced momentum-based optimization methods, such as Adam, can be easily incorporated into the MomentumSMoE framework for designing new SMoE models with even better performance, almost negligible additional computation cost, and simple implementations.\",\n",
       "  912: 'Integrating multi-modal clinical data, such as electronic health records (EHR) and chest X-ray images (CXR), is particularly beneficial for clinical prediction tasks. However, in a temporal setting, multi-modal data are often inherently asynchronous. EHR can be continuously collected but CXR is generally taken with a much longer interval due to its high cost and radiation dose. When clinical prediction is needed, the last available CXR image might have been outdated, leading to suboptimal predictions. To address this challenge, we propose DDL-CXR, a method that dynamically generates an up-to-date latent representation of the individualized CXR images. Our approach leverages latent diffusion models for patient-specific generation strategically conditioned on a previous CXR image and EHR time series, providing information regarding anatomical structures and disease progressions, respectively. In this way, the interaction across modalities could be better captured by the latent CXR generation process, ultimately improving the prediction performance. Experiments using MIMIC datasets show that the proposed model could effectively address asynchronicity in multimodal fusion and consistently outperform existing methods.',\n",
       "  913: \"Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that ''Not all tokens in a corpus are equally important for language model training''. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring training tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both data efficiency and performance of the language model pre-training.\",\n",
       "  914: 'Offline reinforcement learning endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the out-of-distribution problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent advantage-weighted methods prioritize samples with high advantage values for agent training while inevitably ignoring the diversity of behavior policy. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a conditional variational auto-encoder to disentangle the action distributions of intertwined behavior policies by modeling the advantage values of all training data as conditional variables. Then the agent can follow such disentangled action distribution constraints to optimize the advantage-aware policy towards high advantage values. Extensive experiments conducted on both the single-quality and mixed-quality datasets of the D4RL benchmark demonstrate that A2PO yields results superior to the counterparts. Our code is available at https://github.com/Plankson/A2PO.',\n",
       "  915: \"This paper introduces a LArge-scale Video Interpolation Benchmark (LAVIB) for the low-level video task of Video Frame Interpolation (VFI). LAVIB comprises a large collection of high-resolution videos sourced from the web through an automated pipeline with minimal requirements for human verification. Metrics are computed for each video's motion magnitudes, luminance conditions, frame sharpness, and contrast. The collection of videos and the creation of quantitative challenges based on these metrics are under-explored by current low-level video task datasets. In total, LAVIB includes 283K clips from 17K ultra-HD videos, covering 77.6 hours. Benchmark train, val, and test sets maintain similar video metric distributions. Further splits are also created for out-of-distribution (OOD) challenges, with train and test splits including videos of dissimilar attributes.\",\n",
       "  916: \"Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting--a method of demonstrating solution procedures--with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem.This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples.We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes.Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.\",\n",
       "  917: 'Existing methods to generate aesthetic QR codes, such as image and style transfer techniques, tend to compromise either the visual appeal or the scannability of QR codes when they incorporate human face identity. Addressing these imperfections, we present Face2QR—a novel pipeline specifically designed for generating personalized QR codes that harmoniously blend aesthetics, face identity, and scannability. Our pipeline introduces three innovative components. First, the ID-refined QR integration (IDQR) seamlessly intertwines the background styling with face ID, utilizing a unified SD-based framework with control networks. Second, the ID-aware QR ReShuffle (IDRS) effectively rectifies the conflicts between face IDs and QR patterns, rearranging QR modules to maintain the integrity of facial features without compromising scannability. Lastly, the ID-preserved Scannability Enhancement (IDSE) markedly boosts scanning robustness through latent code optimization, striking a delicate balance between face ID, aesthetic quality and QR functionality. In comprehensive experiments, Face2QR demonstrates remarkable performance, outperforming existing approaches, particularly in preserving facial recognition features within custom QR code designs.',\n",
       "  918: 'Learning from AI feedback (LAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. LAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL) or direct preference optimization (DPO), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing LAIF pipelines. More generally, we find that the gains from LAIF vary substantially across base model families, test-time evaluation protocols, and critic models. Finally, we provide a mechanistic explanation for when SFT may outperform the full two-step LAIF pipeline as well as suggestions for making LAIF maximally useful in practice.',\n",
       "  919: 'Although Vision Transformers (ViTs) have recently advanced computer vision tasks significantly, an important real-world problem was overlooked: adapting to variable input resolutions. Typically, images are resized to a fixed resolution, such as 224x224, for efficiency during training and inference. However, uniform input size conflicts with real-world scenarios where images naturally vary in resolution. Modifying the preset resolution of a model may severely degrade the performance. In this work, we propose to enhance the model adaptability to resolution variation by optimizing the patch embedding. The proposed method, called Multi-Scale Patch Embedding (MSPE), substitutes the standard patch embedding with multiple variable-sized patch kernels and selects the best parameters for different resolutions, eliminating the need to resize the original image. Our method does not require high-cost training or modifications to other parts, making it easy to apply to most ViT models. Experiments in image classification, segmentation, and detection tasks demonstrate the effectiveness of MSPE, yielding superior performance on low-resolution inputs and performing comparably on high-resolution inputs with existing methods.',\n",
       "  920: 'Despite the widespread success of Transformers across various domains, their optimization guarantees in large-scale model settings are not well-understood. This paper rigorously analyzes the convergence properties of gradient flow in training Transformers with weight decay regularization. First, we construct the mean-field limit of large-scale Transformers, showing that as the model width and depth go to infinity, gradient flow converges to the Wasserstein gradient flow, which is represented by a partial differential equation. Then, we demonstrate that the gradient flow reaches a global minimum consistent with the PDE solution when the weight decay regularization parameter is sufficiently small. Our analysis is based on a series of novel mean-field techniques that adapt to Transformers. Compared with existing tools for deep networks (Lu et al., 2020) that demand homogeneity and global Lipschitz smoothness, we utilize a refined analysis assuming only $\\\\textit{partial homogeneity}$ and $\\\\textit{local Lipschitz smoothness}$. These new techniques may be of independent interest.',\n",
       "  921: 'This work considers the problem of sampling from a probability distribution known up to a normalization constant while satisfying a set of statistical constraints specified by the expected values of general nonlinear functions. This problem finds applications in, e.g., Bayesian inference, where it can constrain moments to evaluate counterfactual scenarios or enforce desiderata such as prediction fairness. Methods developed to handle support constraints, such as those based on mirror maps, barriers, and penalties, are not suited for this task. This work therefore relies on gradient descent-ascent dynamics in Wasserstein space to put forward a discrete-time primal-dual Langevin Monte Carlo algorithm (PD-LMC) that simultaneously constrains the target distribution and samples from it. We analyze the convergence of PD-LMC under standard assumptions on the target distribution and constraints, namely (strong) convexity and log-Sobolev inequalities. To do so, we bring classical optimization arguments for saddle-point algorithms to the geometry of Wasserstein space. We illustrate the relevance and effectiveness of PD-LMC in several applications.',\n",
       "  922: 'Recently, cutting-plane methods such as GCP-CROWN have been explored to enhance neural network verifiers and made significant advancements. However, GCP-CROWN currently relies on ${\\\\it generic}$ cutting planes (\"cuts\") generated from external mixed integer programming (MIP) solvers. Due to the poor scalability of MIP solvers, large neural networks cannot benefit from these cutting planes. In this paper, we exploit the structure of the neural network verification problem to generate efficient and scalable cutting planes ${\\\\it specific}$ to this problem setting. We propose a novel approach, Branch-and-bound Inferred Cuts with COnstraint Strengthening (BICCOS), that leverages the logical relationships of neurons within verified subproblems in the branch-and-bound search tree, and we introduce cuts that preclude these relationships in other subproblems. We develop a mechanism that assigns influence scores to neurons in each path to allow the strengthening of these cuts. Furthermore, we design a multi-tree search technique to identify more cuts, effectively narrowing the search space and accelerating the BaB algorithm. Our results demonstrate that BICCOS can generate hundreds of useful cuts during the branch-and-bound process and consistently increase the number of verifiable instances compared to other state-of-the-art neural network verifiers on a wide range of benchmarks, including large networks that previous cutting plane methods could not scale to.',\n",
       "  923: 'Text-to-image diffusion has attracted vast attention due to its impressive image-generation capabilities. However, when it comes to human-centric text-to-image generation, particularly in the context of faces and hands, the results often fall short of naturalness due to insufficient training priors. We alleviate the issue in this work from two perspectives. 1) From the data aspect, we carefully collect a human-centric dataset comprising over one million high-quality human-in-the-scene images and two specific sets of close-up images of faces and hands. These datasets collectively provide a rich prior knowledge base to enhance the human-centric image generation capabilities of the diffusion model. 2) On the methodological front, we propose a simple yet effective method called Mixture of Low-rank Experts (MoLE) by considering low-rank modules trained on close-up hand and face images respectively as experts. This concept draws inspiration from our observation of low-rank refinement, where a low-rank module trained by a customized close-up dataset has the potential to enhance the corresponding image part when applied at an appropriate scale. To validate the superiority of MoLE in the context of human-centric image generation compared to state-of-the-art, we construct two benchmarks and perform evaluations with diverse metrics and human studies. Datasets, model, and code are released at https://sites.google.com/view/mole4diffuser/.',\n",
       "  924: 'We propose a new comprehensive benchmark to revolutionize the current deepfake detection field to the next generation. Predominantly, existing works identify top-notch detection algorithms and models by adhering to the common practice: training detectors on one specific dataset (e.g., FF++) and testing them on other prevalent deepfake datasets. This protocol is often regarded as a \"golden compass\" for navigating SoTA detectors. But can these stand-out \"winners\" be truly applied to tackle the myriad of realistic and diverse deepfakes lurking in the real world? If not, what underlying factors contribute to this gap? In this work, we found the dataset (both train and test) can be the \"primary culprit\" due to the following: (1) forgery diversity: Deepfake techniques are commonly referred to as both face forgery (face-swapping and face-reenactment) and entire image synthesis (AIGC, especially face). Most existing datasets only contain partial types of them, with limited forgery methods implemented (e.g., 2 swapping and 2 reenactment methods in FF++); (2) forgery realism: The dominated training dataset, FF++, contains out-of-date forgery techniques from the past four years. \"Honing skills\" on these forgeries makes it difficult to guarantee effective detection generalization toward nowadays\\' SoTA deepfakes; (3) evaluation protocol: Most detection works perform evaluations on one type, e.g., face-swapping types only, which hinders the development of universal deepfake detectors.To address this dilemma, we construct a highly diverse and large-scale deepfake detection dataset called DF40,  which comprises 40 distinct deepfake techniques (10 times larger than FF++). We then conduct comprehensive evaluations using 4 standard evaluation protocols and 8 representative detection methods, resulting in over 2,000 evaluations. Through these evaluations, we provide an extensive analysis from various perspectives, leading to 7 new insightful findings contributing to the field. We also open up 4 valuable yet previously underexplored research questions to inspire future works. We release our dataset, code, and pre-trained weights at https://github.com/YZY-stack/DF40.',\n",
       "  925: 'Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and often requires advanced sampling techniques like Markov chain Monte Carlo (MCMC).A common approach is to write the model in a probabilistic programming language and then sample via Hamiltonian Monte Carlo (HMC).However, there are many ways a user can transform a model that make inference more or less efficient.In particular, marginalizing some variables can greatly improve inference but is difficult for users to do manually.We develop an algorithm to easily marginalize random effects in LMMs.A naive approach introduces cubic time operations within an inference algorithm like HMC, but we reduce the running time to linear using fast linear algebra techniques.We show that marginalization is always beneficial when applicable and highlight improvements in various models, especially ones from cognitive sciences.',\n",
       "  926: 'Generative Adversarial Imitation Learning (GAIL) provides a promising approach to training a generative policy to imitate a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from an adversarial discriminator. However, optimizing GAIL is difficult in practise, with the training loss oscillating during training, slowing convergence. This optimization instability can prevent GAIL from finding a good policy, harming its final performance. In this paper, we study GAIL’s optimization from a control-theoretic perspective. We show that GAIL cannot converge to the desired equilibrium. In response, we analyze the training dynamics of GAIL in function space and design a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a simplified “one-step” setting. Going from theory to practice, we propose Controlled-GAIL (C-GAIL), which adds a differentiable regularization term on the GAIL objective to stabilize training. Empirically, the C-GAIL regularizer improves the training of various existing GAIL methods, including the popular GAIL-DAC, by speeding up the convergence, reducing the range of oscillation, and matching the expert distribution more closely.',\n",
       "  927: 'Diffusion models have demonstrated great success in text-to-video (T2V) generation. However, existing methods may face challenges when handling complex (long) video generation scenarios that involve multiple objects or dynamic changes in object numbers. To address these limitations, we propose VideoTetris, a novel framework that enables compositional T2V generation. Specifically, we propose spatio-temporal compositional diffusion to precisely follow complex textual semantics by manipulating and composing the attention maps of denoising networks spatially and temporally. Moreover, we propose a new dynamic-aware data processing pipeline and a consistency regularization method to enhance the consistency of auto-regressive video generation. Extensive experiments demonstrate that our VideoTetris achieves impressive qualitative and quantitative results in compositional T2V generation. Code is available at: https://github.com/YangLing0818/VideoTetris',\n",
       "  928: 'Reliable prediction of protein variant effects is crucial for both protein optimization and for advancing biological understanding. For practical use in protein engineering, it is important that we can also provide reliable uncertainty estimates for our predictions, and while prediction accuracy has seen much progress in recent years, uncertainty metrics are rarely reported. We here provide a Gaussian process regression model, Kermut, with a novel composite kernel for modeling mutation similarity, which obtains state-of-the-art performance for supervised protein variant effect prediction while also offering estimates of uncertainty through its posterior. An analysis of the quality of the uncertainty estimates demonstrates that our model provides meaningful levels of overall calibration, but that instance-specific uncertainty calibration remains more challenging.',\n",
       "  929: 'We propose an unsupervised adaptation framework, Self-TAught Recognizer (STAR), which leverages unlabeled data to enhance the robustness of automatic speech recognition (ASR) systems in diverse target domains, such as noise and accents. STAR is developed for prevalent speech foundation models based on Transformer-related architecture with auto-regressive decoding (e.g., Whisper, Canary). Specifically, we propose a novel indicator that empirically integrates step-wise information during decoding to assess the token-level quality of pseudo labels without ground truth, thereby guiding model updates for effective unsupervised adaptation. Experimental results show that STAR achieves an average of 13.5% relative reduction in word error rate across 14 target domains, and it sometimes even approaches the upper-bound performance of supervised adaptation. Surprisingly, we also observe that STAR prevents the adapted model from the common catastrophic forgetting problem without recalling source-domain data. Furthermore, STAR exhibits high data efficiency that only requires less than one-hour unlabeled data, and seamless generality to alternative large speech models and speech translation tasks. Our code aims to open source to the research communities.',\n",
       "  930: 'Accurate and smooth shape matching is very hard to achieve. This is because for accuracy, one needs unique descriptors (signatures) on shapes that distinguish different vertices on a mesh accurately while at the same time being invariant to deformations. However, most existing unique shape descriptors are generally not smooth on the shape and are not noise-robust thus leading to non-smooth matches. On the other hand, for smoothness, one needs descriptors that are smooth and continuous on the shape. However, existing smooth descriptors are generally not unique and as such lose accuracy as they match neighborhoods (for smoothness) rather than exact vertices (for accuracy). In this work, we propose to use different k-hop neighborhoods of vertices as pairwise descriptors for shape matching. We use these descriptors in conjunction with local map distortion (LMD) to refine an initialized map for shape matching. We validate the effectiveness of our pipeline on benchmark datasets such as SCAPE, TOSCA, TOPKIDS, and others.',\n",
       "  931: 'Molecular representation learning has shown great success in advancing AI-based drug discovery. A key insight of many recent works is that the 3D geometric structure of molecules provides essential information about their physicochemical properties. Recently, denoising diffusion probabilistic models have achieved impressive performance in molecular 3D conformation generation.  However, most existing molecular diffusion models treat each atom as an independent entity, overlooking the dependency among atoms within the substructures. This paper introduces a novel approach that enhances molecular representation learning by incorporating substructural information in the diffusion model framework. We propose a novel diffusion model termed SubgDiff for involving the molecular subgraph information in diffusion. Specifically, SubgDiff adopts three vital techniques: i) subgraph prediction, ii) expectation state, and iii) k-step same subgraph diffusion, to enhance the perception of molecular substructure in the denoising network. Experiments on extensive downstream tasks, especially the molecular force predictions, demonstrate the superior performance of our approach.',\n",
       "  932: 'In *expand-and-sparsify* (EaS) representation, a data point in $\\\\mathcal{S}^{d-1}$ is first randomly mapped to higher dimension $\\\\mathbb{R}^m$, where $m>d$, followed by a sparsification operation where the informative $k \\\\ll m$ of the $m$ coordinates are set to one and the rest are set to zero. We propose two algorithms for non-parametric classification using such EaS representation. For our first algorithm, we use *winners-take-all* operation for the sparsification step and show that the proposed classifier admits the form of a locally weighted average classifier and establish its consistency via Stone\\'s Theorem. Further, assuming that the conditional probability function $P(y=1|x)=\\\\eta(x)$ is H\\\\\"{o}lder continuous and for optimal choice of $m$, we show that the convergence rate of this classifier is minimax-optimal. For our second algorithm, we use *empirical $k$-thresholding* operation for the sparsification step, and under the assumption that data lie on a low dimensional manifold of dimension $d_0\\\\ll d$, we show that the convergence rate of this classifier depends only on $d_0$ and is again minimax-optimal. Empirical evaluations performed on real-world datasets corroborate our theoretical results.',\n",
       "  933: 'Decision Transformer (DT), as one of the representative Reinforcement Learning via Supervised Learning (RvS) methods, has achieved strong performance in offline learning tasks by leveraging the powerful Transformer architecture for sequential decision-making. However, in adversarial environments, these methods can be non-robust, since the return is dependent on the strategies of both the decision-maker and adversary. Training a probabilistic model conditioned on observed return to predict action can fail to generalize, as the trajectories that achieve a return in the dataset might have done so due to a suboptimal behavior adversary. To address this, we propose a worst-case-aware RvS algorithm, the Adversarially Robust Decision Transformer (ARDT), which learns and conditions the policy on in-sample minimax returns-to-go. ARDT aligns the target return with the worst-case return learned through minimax expectile regression, thereby enhancing robustness against powerful test-time adversaries. In experiments conducted on sequential games with full data coverage, ARDT can generate a maximin (Nash Equilibrium) strategy, the solution with the largest adversarial robustness. In large-scale sequential games and continuous adversarial RL environments with partial data coverage, ARDT demonstrates significantly superior robustness to powerful test-time adversaries and attains higher worst-case returns compared to contemporary DT methods.',\n",
       "  934: 'The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content.Despite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts.To address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions.The IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor, preserving only essential information for the target LLMs to respond with the expected answer.Moreover, we further consider a situation where the gradient is not visible to be compatible with any LLM.Our empirical evaluations show that IBProtector outperforms current defense methods in mitigating jailbreak attempts, without overly affecting response quality or inference speed. Its effectiveness and adaptability across various attack methods and target LLMs underscore the potential of IBProtector as a novel, transferable defense that bolsters the security of LLMs without requiring modifications to the underlying models.',\n",
       "  935: 'We study a novel problem to automatically generate video background that tailors to foreground subject motion. It is an important problem for the movie industry and visual effects community, which traditionally requires tedious manual efforts to solve. To this end, we propose ActAnywhere, a video diffusion model that takes as input a sequence of foreground subject segmentation and an image of a novel background and generates a video of the subject interacting in this background. We train our model on a large-scale dataset of 2.4M videos of human-scene interactions. Through extensive evaluation, we show that our model produces videos with realistic foreground-background interaction while strictly following the guidance of the condition image. Our model generalizes to diverse scenarios including non-human subjects, gaming and animation clips, as well as videos with multiple moving subjects. Both quantitative and qualitative comparisons demonstrate that our model significantly outperforms existing methods, which fail to accomplish the studied task. Please visit our project webpage at https://actanywhere.github.io.',\n",
       "  936: 'Large Language Models (LLMs) are typically trained to predict in the forward direction of time. However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback. Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs. Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time. Further, to effectively infer in the response to query direction, we pre-train and fine-tune a language model (TRLM-Ba) in the reverse token order from scratch. We show empirically (and theoretically in a stylized setting) that time-reversed models can indeed complement forward model predictions when used to score the query given response for re-ranking multiple forward generations. We obtain up to 5\\\\% improvement on the widely used AlpacaEval Leaderboard over the competent baseline of best-of-N re-ranking using self log-perplexity scores. We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval. We next leverage the generative ability of TRLM to augment or provide unsupervised feedback to input safety filters of LLMs, demonstrating a drastic reduction in false negative rate with negligible impact on false positive rates against several attacks published on the popular JailbreakBench leaderboard.',\n",
       "  937: 'Analytics on structured data is a mature field with many successful methods.However, most real world data exists in unstructured form, such as images and conversations.We investigate the potential of Large Language Models (LLMs) to enable unstructured data analytics.In particular, we propose a new Universal Query Engine (UQE) that directly interrogates and draws insights from unstructured data collections.This engine accepts queries in a Universal Query Language (UQL), a dialect of SQL that provides full natural language flexibility in specifying conditions and operators.The new engine leverages the ability of LLMs to conduct analysis of unstructured data, while also allowing us to exploit advances in sampling and optimization techniques to achieve efficient and accurate query execution.In addition, we borrow techniques from classical compiler theory to better orchestrate the workflow between sampling methods and foundation model calls.We demonstrate the efficiency of UQE on data analytics across different modalities, including images, dialogs and reviews, across a range of useful query types, including conditional aggregation, semantic retrieval and abstraction aggregation.',\n",
       "  938: '3D generation methods have shown visually compelling results powered by diffusion image priors. However, they often fail to produce realistic geometric details, resulting in overly smooth surfaces or geometric details inaccurately baked in albedo maps. To address this, we introduce a new method that incorporates touch as an additional modality to improve the geometric details of generated 3D assets. We design a lightweight 3D texture field to synthesize visual and tactile textures, guided by 2D diffusion model priors on both visual and tactile domains. We condition the visual texture generation on high-resolution tactile normals and guide the patch-based tactile texture refinement with a customized TextureDreambooth. We further present a multi-part generation pipeline that enables us to synthesize different textures across various regions. To our knowledge, we are the first to leverage high-resolution tactile sensing to enhance geometric details for 3D generation tasks. We evaluate our method in both text-to-3D and image-to-3D settings. Our experiments demonstrate that our method provides customized and realistic fine geometric textures while maintaining accurate alignment between two modalities of vision and touch.',\n",
       "  939: \"Generative AI and large language models hold great promise in enhancing programming education by generating individualized feedback and hints for learners. Recent works have primarily focused on improving the quality of generated feedback to achieve human tutors' quality. While quality is an important performance criterion, it is not the only criterion to optimize for real-world educational deployments. In this paper, we benchmark language models for programming feedback generation across several performance criteria, including quality, cost, time, and data privacy. The key idea is to leverage recent advances in the new paradigm of in-browser inference that allow running these models directly in the browser, thereby providing direct benefits across cost and data privacy. To boost the feedback quality of small models compatible with in-browser inference engines, we develop a fine-tuning pipeline based on GPT-4 generated synthetic data. We showcase the efficacy of fine-tuned Llama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser inference engine on three different Python programming datasets. We will release the full implementation along with a web app and datasets to facilitate further research on in-browser language models.\",\n",
       "  940: 'Fine-tuning pre-trained models for downstream tasks is a widely adopted technique known for its adaptability and reliability across various domains. Despite its conceptual simplicity, fine-tuning entails several troublesome engineering choices, such as selecting hyperparameters and determining checkpoints from an optimization trajectory. To tackle the difficulty of choosing the best model, one effective solution is model fusion, which combines multiple models in a parameter space. However, we observe a large discrepancy between loss and metric landscapes during the fine-tuning of pre-trained language models. Building on this observation, we introduce a novel model fusion technique that optimizes both the desired metric and loss through multi-objective Bayesian optimization. In addition, to effectively select hyperparameters, we establish a two-stage procedure by integrating Bayesian optimization processes into our framework. Experiments across various downstream tasks show considerable performance improvements using our Bayesian optimization-guided method.',\n",
       "  941: 'Failures of fairness or robustness in machine learning predictive settings can be due to undesired dependencies between covariates, outcomes and auxiliary factors of variation. A common strategy to mitigate these failures is data balancing, which attempts to remove those undesired dependencies. In this work, we define conditions on the training distribution for data balancing to lead to fair or robust models. Our results display that in many cases, the balanced distribution does not correspond to selectively removing the undesired dependencies in a causal graph of the task, leading to multiple failure modes and even interference with other mitigation techniques such as regularization. Overall, our results highlight the importance of taking the causal graph into account before performing data balancing.',\n",
       "  942: 'Graph Neural Networks (GNNs) have become essential in interpreting relational data across various domains, yet, they often struggle to generalize to unseen graph data that differs markedly from training instances. In this paper, we introduce a novel framework called General Retrieval-Augmented Graph Learning (RAGraph), which brings external graph data into the general graph foundation model to improve model generalization on unseen scenarios. On the top of our framework is a toy graph vector library that we established, which captures key attributes, such as features and task-specific label information. During inference, the RAGraph adeptly retrieves similar toy graphs based on key similarities in downstream tasks, integrating the retrieved data to enrich the learning context via the message-passing prompting mechanism. Our extensive experimental evaluations demonstrate that RAGraph significantly outperforms state-of-the-art graph learning methods in multiple tasks such as node classification, link prediction, and graph classification across both dynamic and static datasets. Furthermore, extensive testing confirms that RAGraph consistently maintains high performance without the need for task-specific fine-tuning, highlighting its adaptability, robustness, and broad applicability.',\n",
       "  943: 'Efficient fine-tuning of large language models for task-specific applications is imperative, yet the vast number of parameters in these models makes their training increasingly challenging.Despite numerous proposals for effective methods, a substantial memory overhead remains for gradient computations during updates. \\\\thm{Can we fine-tune a series of task-specific small models and transfer their knowledge directly to a much larger model without additional training?} In this paper, we explore weak-to-strong specialization using logit arithmetic, facilitating a direct answer to this question.Existing weak-to-strong methods often employ a static knowledge transfer ratio and a single small model for transferring complex knowledge, which leads to suboptimal performance. To surmount these limitations,we propose a dynamic logit fusion approach that works with a series of task-specific small models, each specialized in a different task. This method adaptively allocates weights among these models at each decoding step,learning the weights through Kullback-Leibler divergence constrained optimization problems. We conduct extensive experiments across various benchmarks in both single-task and multi-task settings, achieving leading results.By transferring expertise from the 7B model to the 13B model, our method closes the performance gap by 96.4\\\\% in single-task scenarios and by 86.3\\\\% in multi-task scenarios compared to full fine-tuning of the 13B model. Notably, we achieve surpassing performance on unseen tasks. Moreover, we further demonstrate that our method can effortlessly integrate in-context learning for single tasks and task arithmetic for multi-task scenarios.',\n",
       "  944: 'Recent research has shown that representation learning models may accidentally memorize their training data. For example, the déjà vu method shows that for certain representation learning models and training images, it is sometimes possible to correctly predict the foreground label given only the representation of he background – better than through dataset-level correlations. However, their measurement method requires training two models – one to estimate dataset-level correlations and the other to estimate memorization. This multiple model setup becomes infeasible for large open-source models. In this work, we propose alter native simple methods to estimate dataset-level correlations, and show that these can be used to approximate an off-the-shelf model’s memorization ability without any retraining. This enables, for the first time, the measurement of memorization in pre-trained open-source image representation and vision-language models. Our results show that different ways of measuring memorization yield very similar aggregate results. We also find that open-source models typically have lower aggregate memorization than similar models trained on a subset of the data. The code is available both for vision (https://github.com/facebookresearch/DejaVuOSS) and vision language (https://github.com/facebookresearch/VLMDejaVu) models.',\n",
       "  945: 'We address the challenges of Byzantine-robust training in asynchronous distributed machine learning systems, aiming to enhance efficiency amid massive parallelization and heterogeneous compute resources. Asynchronous systems, marked by independently operating workers and intermittent updates, uniquely struggle with maintaining integrity against Byzantine failures, which encompass malicious or erroneous actions that disrupt learning. The inherent delays in such settings not only introduce additional bias to the system but also obscure the disruptions caused by Byzantine faults. To tackle these issues, we adapt the Byzantine framework to asynchronous dynamics by introducing a novel weighted robust aggregation framework. This allows for the extension of robust aggregators and a recent meta-aggregator to their weighted versions, mitigating the effects of delayed updates. By further incorporating a recent variance-reduction technique, we achieve an optimal convergence rate for the first time in an asynchronous Byzantine environment. Our methodology is rigorously validated through empirical and theoretical analysis, demonstrating its effectiveness in enhancing fault tolerance and optimizing performance in asynchronous ML systems.',\n",
       "  946: 'This paper proposes the RePAIR dataset that represents a challenging benchmark to test modern computational and data driven methods for puzzle-solving and reassembly tasks. Our dataset has unique properties that are uncommon to current benchmarks for 2D and 3D puzzle solving. The fragments and fractures are realistic, caused by a collapse of a fresco during a World War II bombing at the Pompeii archaeological park. The fragments are also eroded and have missing pieces with irregular shapes and different dimensions, challenging further the reassembly algorithms. The dataset is multi-modal providing  high resolution images with characteristic pictorial elements, detailed 3D scans of the fragments and meta-data annotated by the archaeologists. Ground truth has been generated through several years of unceasing fieldwork, including the excavation and cleaning of each fragment, followed by manual puzzle solving by archaeologists of a subset of approx. 1000 pieces among the 16000 available. After digitizing all the fragments in 3D, a benchmark was prepared to challenge current reassembly and puzzle-solving methods that often solve more simplistic synthetic scenarios. The tested baselines show that there clearly exists a gap to fill in solving this computationally complex problem.',\n",
       "  947: 'Adam has been shown to outperform gradient descent on large language models by a larger margin than on other tasks, but it is unclear why. We show that a key factor in this performance gap is the heavy-tailed class imbalance found in language tasks. When trained with gradient descent, the loss of infrequent words decreases more slowly than the loss of frequent ones. This leads to a slow decrease on the average loss as most samples come from infrequent words. On the other hand, Adam and sign-based methods are less sensitive to this problem. To establish that this behavior is caused by class imbalance, we show empirically that it can be reproduced across architectures and data types, on language transformers, vision CNNs, and linear models. On a linear model with cross-entropy loss, we show that class imbalance leads to imbalanced, correlated gradients and Hessians that have been hypothesized to benefit Adam. We also prove that, in continuous time, gradient descent converges slowly on low-frequency classes while sign descent does not.',\n",
       "  948: 'Node centralities play a pivotal role in network science, social network analysis, and recommender systems.In temporal data, static path-based centralities like closeness or betweenness can give misleading results about the true importance of nodes in a temporal graph. To address this issue, temporal generalizations of betweenness and closeness have been defined that are based on the shortest time-respecting paths between pairs of nodes. However, a major issue of those generalizations is that the calculation of such paths is computationally expensive.Addressing this issue, we study the application of De Bruijn Graph Neural Networks (DBGNN), a time-aware graph neural network architecture, to predict temporal path-based centralities in time series data. We experimentally evaluate our approach in 13 temporal graphs from biological and social systems and show that it considerably improves the prediction of betweenness and closeness centrality compared to (i) a static Graph Convolutional Neural Network, (ii) an efficient sampling-based approximation technique for temporal betweenness, and (iii) two state-of-the-art time-aware graph learning techniques for dynamic graphs.',\n",
       "  949: 'Diffusion models have recently advanced Combinatorial Optimization (CO) as a powerful backbone for neural solvers. However, their iterative sampling process requiring denoising across multiple noise levels incurs substantial overhead. We propose to learn direct mappings from different noise levels to the optimal solution for a given instance, facilitating high-quality generation with minimal shots. This is achieved through an optimization consistency training protocol, which, for a given instance, minimizes the difference among samples originating from varying generative trajectories and time steps relative to the optimal solution. The proposed model enables fast single-step solution generation while retaining the option of multi-step sampling to trade for sampling quality, which offers a more effective and efficient alternative backbone for neural solvers. In addition, within the training-to-testing (T2T) framework, to bridge the gap between training on historical instances and solving new instances, we introduce a novel consistency-based gradient search scheme during the test stage, enabling more effective exploration of the solution space learned during training. It is achieved by updating the latent solution probabilities under objective gradient guidance during the alternation of noise injection and denoising steps. We refer to this model as Fast T2T. Extensive experiments on two popular tasks, the Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS), demonstrate the superiority of Fast T2T regarding both solution quality and efficiency, even outperforming LKH given limited time budgets. Notably, Fast T2T with merely one-step generation and one-step gradient search can mostly outperform the SOTA diffusion-based counterparts that require hundreds of steps, while achieving tens of times speedup.',\n",
       "  950: 'We measure the out-of-domain uncertainty in the prediction of Neural Networks using a statistical notion called \"Lens Depth\\'\\' (LD) combined with Fermat Distance, which is able to capture precisely the \"depth\\'\\' of a point with respect to a distribution in feature space, without any distributional assumption. Our method also has no trainable parameter. The method is applied directly in the feature space at test time and does not intervene in training process. As such, it does not impact the performance of the original model. The proposed method gives excellent qualitative results on toy datasets and can give competitive or better uncertainty estimation on standard deep learning datasets compared to strong baseline methods.',\n",
       "  951: 'How can we build AI systems that can learn any set of individual human values both quickly and safely, avoiding causing harm or violating societal standards for acceptable behavior during the learning process? We explore the effects of representational alignment between humans and AI agents on learning human values. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We demonstrate that this kind of representational alignment can also support safely learning and exploring human values in the context of personalization. We begin with a theoretical prediction, show that it applies to learning human morality judgments, then show that our results generalize to ten different aspects of human values -- including ethics, honesty, and fairness -- training AI agents on each set of values in a multi-armed bandit setting, where rewards reflect human value judgments over the chosen action. Using a set of textual action descriptions, we collect value judgments from humans, as well as similarity judgments from both humans and multiple language models, and demonstrate that representational alignment enables both safe exploration and improved generalization when learning human values.',\n",
       "  952: 'We study the implicit regularization effects induced by (observation) weighting of pretrained features.For weight and feature matrices of bounded operator norms that are infinitesimally free with respect to (normalized) trace functionals, we derive equivalence paths connecting different weighting matrices and ridge regularization levels.Specifically, we show that ridge estimators trained on weighted features along the same path are asymptotically equivalent when evaluated against test vectors of bounded norms.These paths can be interpreted as matching the effective degrees of freedom of ridge estimators fitted with weighted features.For the special case of subsampling without replacement, our results apply to independently sampled random features and kernel features and confirm recent conjectures (Conjectures 7 and 8) of the authors on the existence of such paths in Patil and Du (2023).We also present an additive risk decomposition for ensembles of weighted estimators and show that the risks are equivalent along the paths when the ensemble size goes to infinity.As a practical consequence of the path equivalences, we develop an efficient cross-validation method for tuning and apply it to subsampled pretrained representations across several models (e.g., ResNet-50) and datasets (e.g., CIFAR-100).',\n",
       "  953: \"Diffusion models have obtained substantial progress in image-to-video generation. However, in this paper, we find that these models tend to generate videos with less motion than expected. We attribute this to the issue called conditional image leakage, where the image-to-video diffusion models (I2V-DMs) tend to over-rely on the conditional image at large time steps. We further address this challenge from both inference and training aspects. First, we propose to start the generation process from an earlier time step to avoid the unreliable large-time steps of I2V-DMs, as well as an initial noise distribution with optimal analytic expressions (Analytic-Init) by minimizing the KL divergence between it and the actual marginal distribution to bridge the training-inference gap. Second, we design a time-dependent noise distribution (TimeNoise) for the conditional image during training, applying higher noise levels at larger time steps to disrupt it and reduce the model's dependency on it. We validate these general strategies on various I2V-DMs on our collected open-domain image benchmark and the UCF101 dataset. Extensive results show that our methods outperform baselines by producing higher motion scores with lower errors while maintaining image alignment and temporal consistency, thereby yielding superior overall performance and enabling more accurate motion control. The project page: \\\\url{https://cond-image-leak.github.io/}.\",\n",
       "  954: 'We present MV2Cyl, a novel method for reconstructing 3D from 2D multi-view images, not merely as a field or raw geometry but as a sketch-extrude CAD. Extracting extrusion cylinders from raw 3D geometry has been extensively researched in computer vision, while the processing of 3D data through neural networks has remained a bottleneck. Since 3D scans are generally accompanied by multi-view images, leveraging 2D convolutional neural networks allows these images to be exploited as a rich source for extracting extrusion cylinder information. However, we observe that extracting only the surface information of the extrudes and utilizing it results in suboptimal outcomes due to the challenges in the occlusion and surface segmentation. By synergizing with the extracted base curve information, we achieve the optimal reconstruction result with the best accuracy in 2D sketch and extrude parameter estimation. Our experiments, comparing our method with previous work that takes a raw 3D point cloud as input, demonstrate the effectiveness of our approach by taking advantage of multi-view images.',\n",
       "  955: 'Understanding treatment effect heterogeneity is vital for scientific and policy research. However, identifying and evaluating heterogeneous treatment effects pose significant challenges due to the typically unknown subgroup structure. Recently, a novel approach, causal k-means clustering, has emerged to assess heterogeneity of treatment effect by applying the k-means algorithm to unknown counterfactual regression functions. In this paper, we expand upon this framework by integrating hierarchical and density-based clustering algorithms. We propose plug-in estimators which are simple and readily implementable using off-the-shelf algorithms. Unlike k-means clustering, which requires the margin condition, our proposed estimators do not rely on strong structural assumptions on the outcome process. We go on to study their rate of convergence, and show that under the minimal regularity conditions, the additional cost of causal clustering is essentially the estimation error of the outcome regression functions. Our findings significantly extend the capabilities of the causal clustering framework, thereby contributing to the progression of methodologies for identifying homogeneous subgroups in treatment response, consequently facilitating more nuanced and targeted interventions. The proposed methods also open up new avenues for clustering with generic pseudo-outcomes. We explore finite sample properties via simulation, and illustrate the proposed methods in voting and employment projection datasets.',\n",
       "  956: 'Novel view synthesis under sparse views has been a long-term important challenge in 3D reconstruction. Existing works mainly rely on introducing external semantic or depth priors to supervise the optimization of 3D representations. However, the diffusion model, as an external prior that can directly provide visual supervision, has always underperformed in sparse-view 3D reconstruction using Score Distillation Sampling (SDS) due to the low information entropy of sparse views compared to text, leading to optimization challenges caused by mode deviation. To this end, we present a thorough analysis of SDS from the mode-seeking perspective and propose Inline Prior Guided Score Matching (IPSM), which leverages visual inline priors provided by pose relationships between viewpoints to rectify the rendered image distribution and decomposes the original optimization objective of SDS, thereby offering effective diffusion visual guidance without any fine-tuning or pre-training. Furthermore, we propose the IPSM-Gaussian pipeline, which adopts 3D Gaussian Splatting as the backbone and supplements depth and geometry consistency regularization based on IPSM to further improve inline priors and rectified distribution. Experimental results on different public datasets show that our method achieves state-of-the-art reconstruction quality. The code is released at https://github.com/iCVTEAM/IPSM.',\n",
       "  957: 'Training and inference with large machine learning models that far exceed the memory capacity of individual devices necessitates the design of distributed architectures, forcing one to contend with communication constraints.  We present a framework for distributed computation over a quantum network in which data is encoded into specialized quantum states. We prove that for models within this framework, inference and training using gradient descent can be performed with exponentially less communication compared to their classical analogs, and with relatively modest overhead relative to standard gradient-based methods. We show that certain graph neural networks are particularly amenable to implementation within this framework, and moreover present empirical evidence that they perform well on standard benchmarks.To our knowledge, this is the first example of exponential quantum advantage for a generic class of machine learning problems that hold regardless of the data encoding cost. Moreover, we show that models in this class can encode highly nonlinear features of their inputs, and their expressivity increases exponentially with model depth.We also delineate the space of models for which exponential communication advantages hold by showing that they cannot hold for linear classification. Communication of quantum states that potentially limit the amount of information that can be extracted from them about the data and model parameters may also lead to improved privacy guarantees for distributed computation. Taken as a whole, these findings form a promising foundation for distributed machine learning over quantum networks.',\n",
       "  958: \"We address the problem of stochastic combinatorial semi-bandits, where a player selects among $P$ actions from the power set of a set containing $d$ base items. Adaptivity to the problem's structure is essential in order to obtain optimal regret upper bounds. As estimating the coefficients of a covariance matrix can be manageable in practice, leveraging them should improve the regret. We design ``optimistic''  covariance-adaptive algorithms relying on online estimations of the covariance structure, called OLS-UCB-C and COS-V (only the variances for the latter). They both yields improved gap-free regret. Although COS-V can be slightly suboptimal, it improves on computational complexity by taking inspiration from Thompson Sampling approaches. It is the first sampling-based algorithm satisfying a $\\\\sqrt{T}$ gap-free regret (up to poly-logs). We also show that in some cases, our approach efficiently leverages the semi-bandit feedback and outperforms bandit feedback approaches, not only in exponential regimes where $P\\\\gg d$ but also when $P\\\\leq d$, which is not covered by existing analyses.\",\n",
       "  959: 'Generating realistic images from arbitrary views based on a single source image remains a significant challenge in computer vision, with broad applications ranging from e-commerce to immersive virtual experiences. Recent advancements in diffusion models, particularly the Zero-1-to-3 model, have been widely adopted for generating plausible views, videos, and 3D models. However, these models still struggle with inconsistencies and implausibility in new views generation, especially for challenging changes in viewpoint. In this work, we propose Zero-to-Hero, a novel test-time approach that enhances view synthesis by manipulating attention maps during the denoising process of Zero-1-to-3. By drawing an analogy between the denoising process and stochastic gradient descent (SGD), we implement a filtering mechanism that aggregates attention maps, enhancing generation reliability and authenticity. This process improves geometric consistency without requiring retraining or significant computational resources. Additionally, we modify the self-attention mechanism to integrate information from the source view, reducing shape distortions. These processes are further supported by a specialized sampling schedule. Experimental results demonstrate substantial improvements in fidelity and consistency, validated on a diverse set of out-of-distribution objects. Additionally, we demonstrate the general applicability and effectiveness of Zero-to-Hero in multi-view, and image generation conditioned on semantic maps and pose.',\n",
       "  960: 'In this paper, we make the first attempt to align diffusion models for image inpainting with human aesthetic standards via a reinforcement learning framework, significantly improving the quality and visual appeal of inpainted images. Specifically, instead of directly measuring the divergence with paired images, we train a reward model with the dataset we construct, consisting of nearly 51,000 images annotated with human preferences. Then, we adopt a reinforcement learning process to fine-tune the distribution of a pre-trained diffusion model for image inpainting in the direction of higher reward. Moreover, we theoretically deduce the upper bound on the error of the reward model, which illustrates the potential confidence of reward estimation throughout the reinforcement alignment process, thereby facilitating accurate regularization.Extensive experiments on inpainting comparison and downstream tasks, such as image extension and 3D reconstruction, demonstrate the effectiveness of our approach, showing significant improvements in the alignment of inpainted images with human preference compared with state-of-the-art methods. This research not only advances the field of image inpainting but also provides a framework for incorporating human preference into the iterative refinement of generative models based on modeling reward accuracy, with broad implications for the design of visually driven AI applications. Our code and dataset are publicly available at \\\\url{https://prefpaint.github.io}.',\n",
       "  961: 'Integrating pretrained vision-language foundation models like CLIP into federated learning has attracted significant attention for enhancing generalization across diverse tasks. Typically, federated learning of vision-language models employs prompt learning to reduce communication and computational costs, i.e., prompt-based federated learning. However, there is limited theoretical analysis to understand the performance of prompt-based federated learning. In this work, we construct a theoretical analysis framework for prompt-based federated learning via feature learning theory. Specifically, we monitor the evolution of signal learning and noise memorization in prompt-based federated learning, demonstrating that performance can be assessed by the ratio of task-relevant to task-irrelevant coefficients. Furthermore, we draw an analogy between income and risk in portfolio optimization and the task-relevant and task-irrelevant terms in feature learning. Leveraging inspiration from portfolio optimization that combining two independent assets will maintain the income while reducing the risk, we introduce two prompts: global prompt and local prompt to construct a prompt portfolio to balance the generalization and personalization. Consequently, we showed the performance advantage of the prompt portfolio and derived the optimal mixing coefficient. These theoretical claims have been further supported by empirical experiments.',\n",
       "  962: \"Since language models (LMs) now outperform average humans on many challenging tasks, it is becoming increasingly difficult to develop challenging, high-quality, and realistic evaluations. We address this by examining LM capabilities to generate code for solving real scientific research problems. Incorporating input from scientists and AI researchers in 16 diverse natural science sub-fields, including mathematics, physics, chemistry, biology, and materials science, we create a scientist-curated coding benchmark, SciCode. The problems naturally factorize into multiple subproblems, each involving knowledge recall, reasoning, and code synthesis. In total, SciCode contains 338 subproblems decomposed from 80 challenging main problems, and it offers optional descriptions specifying useful scientific background information and scientist-annotated gold-standard solutions and test cases for evaluation. OpenAI o1-preview, the best-performing model among those tested, can solve only 7.7\\\\% of the problems in the most realistic setting. We believe that SciCode demonstrates both contemporary LMs' progress towards realizing helpful scientific assistants and sheds light on the building and evaluation of scientific AI in the future.\",\n",
       "  963: 'Advanced diffusion models (DMs) perform impressively in image super-resolution (SR), but the high memory and computational costs hinder their deployment. Binarization, an ultra-compression algorithm, offers the potential for effectively accelerating DMs. Nonetheless, due to the model structure and the multi-step iterative attribute of DMs, existing binarization methods result in significant performance degradation. In this paper, we introduce a novel binarized diffusion model, BI-DiffSR, for image SR. First, for the model structure, we design a UNet architecture optimized for binarization. We propose the consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up) to maintain dimension consistent and facilitate the full-precision information transfer. Meanwhile, we design the channel-shuffle-fusion (CS-Fusion) to enhance feature fusion in skip connection. Second, for the activation difference across timestep, we design the timestep-aware redistribution (TaR) and activation function (TaA). The TaR and TaA dynamically adjust the distribution of activations based on different timesteps, improving the flexibility and representation alability of the binarized module. Comprehensive experiments demonstrate that our BI-DiffSR outperforms existing binarization methods. Code is released at: https://github.com/zhengchen1999/BI-DiffSR.',\n",
       "  964: 'Recent advances in microscopy have enabled the rapid generation of terabytes of image data in cell biology and biomedical research. Vision-language models (VLMs) offer a promising solution for large-scale biological image analysis, enhancing researchers’ efficiency, identifying new image biomarkers, and accelerating hypothesis generation and scientific discovery. However, there is a lack of standardized, diverse, and large-scale vision-language benchmarks to evaluate VLMs’ perception and cognition capabilities in biological image understanding. To address this gap, we introduce Micro-Bench, an expert-curated benchmark encompassing 24 biomedical tasks across various scientific disciplines (biology, pathology), microscopy modalities (electron, fluorescence, light), scales (subcellular, cellular, tissue), and organisms in both normal and abnormal states. We evaluate state-of-the-art biomedical, pathology, and general VLMs on Micro-Bench and find that: i) current models struggle on all categories, even for basic tasks such as distinguishing microscopy modalities; ii) current specialist models fine-tuned on biomedical data often perform worse than generalist models; iii) fine-tuning in specific microscopy domains can cause catastrophic forgetting, eroding prior biomedical knowledge encoded in their base model. iv) weight interpolation between fine-tuned and pre-trained models offers one solution to forgetting and improves general performance across biomedical tasks. We release Micro-Bench under a permissive license to accelerate the research and development of microscopy foundation models.',\n",
       "  965: 'While large language models (LLMs) showcase unprecedented capabilities, they also exhibit certain inherent limitations when facing seemingly trivial tasks. A prime example is the recently debated \"reversal curse\", which surfaces when models, having been trained on the fact \"A is B\", struggle to generalize this knowledge to infer that \"B is A\".In this paper, we examine the manifestation of the reversal curse across various tasks and delve into both the generalization abilities and the problem-solving mechanisms of LLMs. This investigation leads to a series of significant insights:(1) LLMs are able to generalize to \"B is A\" when both A and B are presented in the context as in the case of a multiple-choice question.(2) This generalization ability is highly correlated to the structure of the fact \"A is B\" in the training documents. For example, this generalization only applies to biographies structured in \"[Name] is [Description]\" but not to \"[Description] is [Name]\".(3) We propose and verify the hypothesis that LLMs possess an inherent bias in fact recalling during knowledge application, which explains and underscores the importance of the document structure to successful learning.(4) The negative impact of this bias on the downstream performance of LLMs can hardly be mitigated through training alone.Based on these intriguing findings, our work not only presents a novel perspective for interpreting LLMs\\' generalization abilities from their intrinsic working mechanism but also provides new insights for the development of more effective learning methods for LLMs.',\n",
       "  966: 'Lip reading aims at transforming the videos of continuous lip movement into textual contents, and has achieved significant progress over the past decade. It serves as a critical yet practical assistance for speech-impaired individuals, with more practicability than speech recognition in noisy environments. With the increasing interpersonal communications in social media owing to globalization, the existing monolingual datasets for lip reading may not be sufficient to meet the exponential proliferation of bilingual and even multilingual users. However, to our best knowledge, research on code-switching is only explored in speech recognition, while the attempts in lip reading are seriously neglected. To bridge this gap, we have collected a bilingual code-switching lip reading benchmark composed of Chinese and English, dubbed CSLR. As the pioneering work, we recruited 62 speakers with proficient foundations in bothspoken Chinese and English to express sentences containing both involved languages. Through rigorous criteria in data selection, CSLR benchmark has accumulated 85,560 video samples with a resolution of 1080x1920, totaling over 71.3 hours of high-quality code-switching lip movement data. To systematically evaluate the technical challenges in CSLR, we implement commonly-used lip reading backbones, as well as competitive solutions in code-switching speech for benchmark testing. Experiments show CSLR to be a challenging and under-explored lip reading task. We hope our proposed benchmark will extend the applicability of code-switching lip reading, and further contribute to the communities of cross-lingual communication and collaboration. Our dataset and benchmark are accessible at https://github.com/cslr-lipreading/CSLR.',\n",
       "  967: 'Linear causal models are important tools for modeling causal dependencies and yet in practice, only a subset of the variables can be observed. In this paper,  we examine the parameter identifiability of these models by investigating whether the edge coefficients can be recovered given the causal structure and partially observed data. Our setting is more general than that of prior research—we allow all variables, including both observed and latent ones, to be flexibly related, and we consider the coefficients of all edges, whereas most existing works focus only on the edges between observed variables. Theoretically, we identify three types of indeterminacy for the parameters in partially observed linear causal models. We then provide graphical conditions that are sufficient for all parameters to be identifiable and show that some of them are provably necessary. Methodologically, we propose a novel likelihood-based parameter estimation method that addresses the variance indeterminacy of latent variables in a specific way and can asymptotically recover the underlying parameters up to trivial indeterminacy. Empirical studies on both synthetic and real-world datasets validate our identifiability theory and the effectiveness of the proposed method in the finite-sample regime.',\n",
       "  968: 'Increasing the throughput of the Transformer architecture, a foundational component used in numerous state-of-the-art models for vision and language tasks (e.g., GPT, LLaVa), is an important problem in machine learning. One recent and effective strategy is to merge token representations within Transformer models, aiming to reduce computational and memory requirements while maintaining accuracy. Prior work has proposed algorithms based on Bipartite Soft Matching (BSM), which divides tokens into distinct sets and merges the top $k$ similar tokens. However, these methods have significant drawbacks, such as sensitivity to token-splitting strategies and damage to informative tokens in later layers. This paper presents a novel paradigm called PiToMe, which prioritizes the preservation of informative tokens using an additional metric termed the \\\\textit{energy score}. This score identifies large clusters of similar tokens as high-energy, indicating potential candidates for merging, while smaller (unique and isolated) clusters are considered as low-energy and preserved. Experimental findings demonstrate that PiToMe saved from 40-60\\\\% FLOPs of the base models while exhibiting superior off-the-shelf performance on image classification (0.5\\\\% average performance drop of ViT-MAEH compared to 2.6\\\\% as baselines), image-text retrieval (0.3\\\\% average performance drop of Clip on Flick30k compared to 4.5\\\\% as others), and analogously in visual questions answering with LLaVa-7B. Furthermore, PiToMe is theoretically shown to preserve intrinsic spectral properties to the original token space under mild conditions.',\n",
       "  969: 'The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.',\n",
       "  970: 'Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality,  with many methods excelling across these criteria. Some recent works disclose the pitfalls of these editing methods such as knowledge distortion or conflict. However, the general abilities of post-edited language models remain unexplored. In this paper, we perform a comprehensive evaluation on various editing methods and different language models, and have following findings.(1) Existing editing methods lead to inevitable performance deterioration on general benchmarks, indicating that existing editing methods maintain the general abilities of the model within only a few dozen edits.When the number of edits is slightly large, the intrinsic knowledge structure of the model is disrupted or even completely damaged. (2) Instruction-tuned models are more robust to editing, showing less performance drop on general knowledge after editing. (3) Language model with large scale is more resistant to editing compared to small model.(4) The safety of the edited model, is significantly weakened, even for those safety-aligned models.Our findings indicate that current editing methods are only suitable for small-scale knowledge updates within language models, which motivates further research on more practical and reliable editing methods.',\n",
       "  971: 'We consider the problem of learning an $\\\\varepsilon$-optimal policy in controlled dynamical systems with low-rank latent structure. For this problem, we present LoRa-PI (Low-Rank Policy Iteration), a model-free learning algorithm alternating between policy improvement and policy evaluation steps. In the latter, the algorithm estimates the low-rank matrix corresponding to the (state, action) value function of the current policy using the following two-phase procedure. The entries of the matrix are first sampled uniformly at random to estimate, via a spectral method, the *leverage scores* of its rows and columns. These scores are then used to extract a few important rows and columns whose entries are further sampled. The algorithm exploits these new samples to  complete the matrix estimation using a CUR-like method. For this leveraged matrix estimation procedure, we establish entry-wise guarantees that remarkably, do not depend on the coherence of the matrix but only on its spikiness. These guarantees imply that LoRa-PI learns an $\\\\varepsilon$-optimal policy using $\\\\tilde{\\\\cal O}({(S+A)\\\\over \\\\mathrm{poly}(1-\\\\gamma)\\\\varepsilon^2})$ samples where $S$ (resp. $A$) denotes the number of states (resp. actions) and $\\\\gamma$ the discount factor. Our algorithm achieves this order-optimal (in $S$, $A$ and $\\\\varepsilon$) sample complexity under milder conditions than those assumed in previously proposed approaches.',\n",
       "  972: 'Existing image-text modality alignment in Vision Language Models (VLMs) treats each text token equally in an autoregressive manner. Despite being simple and effective, this method results in sub-optimal cross-modal alignment by over-emphasizing the text tokens that are less correlated with or even contradictory with the input images. In this paper, we advocate for distinct contributions for each text token based on its visual correlation. Specifically, we present by contrasting image inputs, the difference in prediction logits on each text token provides strong guidance of visual correlation. We therefore introduce Contrastive Alignment (CAL), a simple yet effective re-weighting strategy that prioritizes training visually correlated tokens. Our experimental results demonstrate that CAL consistently improves different types of VLMs across different resolutions and model sizes on various benchmark datasets. Importantly, our method incurs minimal additional computational overhead, rendering it highly efficient compared to alternative data scaling strategies.',\n",
       "  973: 'Current reinforcement-learning methods are unable to directly learn policies that solve the minimum cost reach-avoid problem to minimize cumulative costs subject to the constraints of reaching the goal and avoiding unsafe states, as the structure of this new optimization problem is incompatible with current methods. Instead, a surrogate problem is solved where all objectives are combined with a weighted sum. However, this surrogate objective results in suboptimal policies that do not directly minimize the cumulative cost. In this work, we propose RC-PPO, a reinforcement-learning-based method for solving the minimum-cost reach-avoid problem by using connections to Hamilton-Jacobi reachability. Empirical results demonstrate that RC-PPO learns policies with comparable goal-reaching rates to while achieving up to 57% lower cumulative costs compared to existing methods on a suite of minimum-cost reach-avoid benchmarks on the Mujoco simulator. The project page can be found at https://oswinso.xyz/rcppo.',\n",
       "  974: 'Previous deep learning approaches for survival analysis have primarily relied on ranking losses to improve discrimination performance, which often comes at the expense of calibration performance. To address such an issue, we propose a novel contrastive learning approach specifically designed to enhance discrimination without sacrificing calibration. Our method employs weighted sampling within a contrastive learning framework, assigning lower penalties to samples with similar survival outcomes. This aligns well with the assumption that patients with similar event times share similar clinical statuses. Consequently, when augmented with the commonly used negative log-likelihood loss, our approach significantly improves discrimination performance without directly manipulating the model outputs, thereby achieving better calibration.Experiments on multiple real-world clinical datasets demonstrate that our method outperforms state-of-the-art deep survival models in both discrimination and calibration. Through comprehensive ablation studies, we further validate the effectiveness of our approach through quantitative and qualitative analyses.',\n",
       "  975: 'Modern image generation (IG) models have been shown to capture rich semantics valuable for image understanding (IU) tasks. However, the potential of IU models to improve IG performance remains uncharted. We address this issue using a token-based IG framework, which relies on effective tokenizers to project images into token sequences. Currently, **pixel reconstruction** (e.g., VQGAN) dominates the training objective for image tokenizers. In contrast, our approach adopts the **feature reconstruction** objective, where tokenizers are trained by distilling knowledge from pretrained IU encoders. Comprehensive comparisons indicate that tokenizers with strong IU capabilities achieve superior IG performance across a variety of metrics, datasets, tasks, and proposal networks. Notably, VQ-KD CLIP achieves $4.10$ FID on ImageNet-1k (IN-1k). Visualization suggests that the superiority of VQ-KD can be partly attributed to the rich semantics within the VQ-KD codebook. We further introduce a straightforward pipeline to directly transform IU encoders into tokenizers, demonstrating exceptional effectiveness for IG tasks. These discoveries may energize further exploration into image tokenizer research and inspire the community to reassess the relationship between IU and IG. The code is released at https://github.com/magic-research/vector_quantization.',\n",
       "  976: \"Kernel methods are widely utilized in machine learning field to learn, from training data, a latent function in a reproducing kernel Hilbert space. It is well known that the approximator thus obtained usually achieves a linear representation, which brings various computational benefits, while maintaining great representation power (i.e., universal approximation). However, when non-negativity constraints are imposed on the function's outputs, the literature usually takes the kernel method-based approximators as offering linear representations at the expense of limited model flexibility or good representation power by allowing for their nonlinear forms. The main contribution of this paper is to derive a sufficient condition for a positive definite kernel so that it may construct flexible and linear approximators of non-negative functions. We call a kernel function that offers these attributes an inverse M-kernel; it is reminiscent of the inverse M-matrix. Furthermore, we show that for a one-dimensional input space, universal exponential/Abel kernels are inverse M-kernels and construct linear universal approximators of non-negative functions. To the best of our knowledge, it is the first time that the existence of linear universal approximators of non-negative functions has been elucidated. We confirm the effectiveness of our results by experiments on the problems of non-negativity-constrained regression, density estimation, and intensity estimation. Finally, we discuss issues and perspectives on multi-dimensional input settings.\",\n",
       "  977: 'Fine-tuning is a crucial process for adapting large language models (LLMs) to diverse applications. In certain scenarios, such as multi-tenant serving, deploying multiple LLMs becomes necessary to meet complex demands. Recent studies suggest decomposing a fine-tuned LLM into a base model and corresponding delta weights, which are then compressed using low-rank or low-bit approaches to reduce costs. In this work, we observe that existing low-rank and low-bit compression methods can significantly harm the model performance for task-specific fine-tuned LLMs (e.g., WizardMath for math problems). Motivated by the long-tail distribution of singular values in the delta weights, we propose a delta quantization approach using mixed-precision. This method employs higher-bit representation for singular vectors corresponding to larger singular values. We evaluate our approach on various fine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs. Experimental results demonstrate that our approach performs comparably to full fine-tuned LLMs, surpassing both low-rank and low-bit baselines by a considerable margin. Additionally, we show that our method is compatible with various backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its generalizability.',\n",
       "  978: \"Domain generalization methods aim to learn transferable knowledge from source domains that can generalize well to unseen target domains. Recent studies show that neural networks frequently suffer from a simplicity-biased learning behavior which leads to over-reliance on specific frequency sets, namely as frequency shortcuts, instead of semantic information, resulting in poor generalization performance. Despite previous data augmentation techniques successfully enhancing generalization performances, they intend to apply more frequency shortcuts, thereby causing hallucinations of generalization improvement.In this paper, we aim to prevent such learning behavior of applying frequency shortcuts from a data-driven perspective. Given the theoretical justification of models' biased learning behavior on different spatial frequency components, which is based on the dataset frequency properties, we argue that the learning behavior on various frequency components could be manipulated by changing the dataset statistical structure in the Fourier domain. Intuitively, as frequency shortcuts are hidden in the dominant and highly dependent frequencies of dataset structure, dynamically perturbating the over-reliance frequency components could prevent the application of frequency shortcuts.To this end, we propose two effective data augmentation modules designed to collaboratively and adaptively adjust the frequency characteristic of the dataset, aiming to dynamically influence the learning behavior of the model and ultimately serving as a strategy to mitigate shortcut learning. Our code will be made publicly available.\",\n",
       "  979: 'Few-shot 3D point cloud semantic segmentation aims to segment query point clouds with only a few annotated support point clouds. Existing prototype-based methods learn prototypes from the 3D support set to guide the segmentation of query point clouds. However, they encounter the challenge of low prototype quality due to constrained semantic information in the 3D support set and class information bias between support and query sets. To address these issues, in this paper, we propose a novel framework called Generated and Pseudo Content guided Prototype Refinement (GPCPR), which explicitly leverages LLM-generated content and reliable query context to enhance prototype quality. GPCPR achieves prototype refinement through two core components: LLM-driven Generated Content-guided Prototype Refinement (GCPR) and Pseudo Query Context-guided Prototype Refinement (PCPR). Specifically, GCPR integrates diverse and differentiated class descriptions generated by large language models to enrich prototypes with comprehensive semantic knowledge. PCPR further aggregates reliable class-specific pseudo-query context to mitigate class information bias and generate more suitable query-specific prototypes. Furthermore, we introduce a dual-distillation regularization term, enabling knowledge transfer between early-stage entities (prototypes or pseudo predictions) and their deeper counterparts to enhance refinement. Extensive experiments demonstrate the superiority of our method, surpassing the state-of-the-art methods by up to 12.10% and 13.75% mIoU on S3DIS and ScanNet, respectively.',\n",
       "  980: 'Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. However, it remains a challenge due to the domain gap between humans and robots. Moreover, it is difficult to extract useful information representing the dynamic world from human videos, because of its noisy and multimodal data structure. In this paper, we introduce a novel framework to tackle these challenges, which leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning with a limited set of robot data. Experiments demonstrate that our method generates high-fidelity future videos for planning and enhances the fine-tuned policies compared to previous state-of-the-art approaches with superior performance.',\n",
       "  981: 'This paper presents a method for estimating the hallucination rate for in-context learning (ICL) with generative AI. In ICL, a conditional generative model (CGM) is prompted with a dataset and a prediction question and asked to generate a response. One interpretation of ICL assumes that the CGM computes the posterior predictive of an unknown Bayesian model, which implicitly defines a joint distribution over observable datasets and latent mechanisms. This joint distribution factorizes into two components: the model prior over mechanisms and the model likelihood of datasets given a mechanism. With this perspective, we define a \\\\textit{hallucination} as a generated response to the prediction question with low model likelihood given the mechanism. We develop a new method that takes an ICL problem and estimates the probability that a CGM will generate a hallucination. Our method only requires generating prediction questions and responses from the CGM and evaluating its response log probability. We empirically evaluate our method using large language models for synthetic regression and natural language ICL tasks.',\n",
       "  982: 'We study how well machine learning models trained on causal features generalize across domains. We consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics. Each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another. For each prediction task, we select features that have a causal influence on the target of prediction. Our goal is to test the hypothesis that models trained on causal features generalize better across domains. Without exception, we find that predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features. Moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features.  In addition, we show that recent causal machine learning methods for domain generalization do not perform better in our evaluation than standard predictors trained on the set of causal features. Likewise, causal discovery algorithms either fail to run or select causal variables that perform no better than our selection. Extensive robustness checks confirm that our findings are stable under variable misclassification.',\n",
       "  983: 'Model selection in Gaussian processes scales prohibitively with the size of the training dataset, both in time and memory.While many approximations exist, all incur inevitable approximation error.Recent work accounts for this error in the form of computational uncertainty, which enables---at the cost of quadratic complexity---an explicit tradeoff between computational efficiency and precision.Here we extend this development to model selection, which requires significant enhancements to the existing approach, including linear-time scaling in the size of the dataset.We propose a novel training loss for hyperparameter optimization and demonstrate empirically that the resulting method can outperform SGPR, CGGP and SVGP, state-of-the-art methods for GP model selection, on medium to large-scale datasets.Our experiments show that model selection for computation-aware GPs trained on 1.8 million data points can be done within a few hours on a single GPU.As a result of this work, Gaussian processes can be trained on large-scale datasets without significantly compromising their ability to quantify uncertainty---a fundamental prerequisite for optimal decision-making.',\n",
       "  984: 'Time series forecasting typically needs to address non-stationary data with evolving trend and seasonal patterns. To address the non-stationarity, reversible instance normalization has been recently proposed to alleviate impacts from the trend with certain statistical measures, e.g., mean and variance. Although they  demonstrate improved predictive accuracy, they are limited to expressing basic trends and are incapable of handling seasonal patterns. To address this limitation, this paper proposes a new instance normalization solution, called frequency adaptive normalization (FAN), which extends instance normalization in handling both dynamic trend and seasonal patterns. Specifically, we employ the Fourier transform to identify instance-wise predominant frequent components that cover most non-stationary factors. Furthermore,  the discrepancy of those frequency components between inputs and outputs is explicitly modeled as a prediction task with a simple MLP model. FAN is a model-agnostic method that can be applied to arbitrary predictive backbones.  We instantiate FAN on four widely used forecasting models as the backbone and evaluate their prediction performance improvements on eight benchmark datasets. FAN demonstrates significant performance advancement, achieving 7.76\\\\%$\\\\sim$37.90\\\\%  average  improvements in MSE. Our code is publicly available at http://github.com/icannotnamemyself/FAN.',\n",
       "  985: 'Decision support systems based on prediction sets have proven to be effective at helping human experts solve classification tasks. Rather than providing single-label predictions, these systems provide sets of label predictions constructed using conformal prediction, namely prediction sets, and ask human experts to predict label values from these sets. In this paper, we first show that the prediction sets constructed using conformal prediction are, in general, suboptimal in terms of average accuracy. Then, we show that the problem of finding the optimal prediction sets under which the human experts achieve the highest average accuracy is NP-hard. More strongly, unless P = NP, we show that the problem is hard to approximate to any factor less than the size of the label set. However, we introduce a simple and efficient greedy algorithm that, for a large class of expert models and non-conformity scores, is guaranteed to find prediction sets that provably offer equal or greater performance than those constructed using conformal prediction. Further, using a simulation study with both synthetic and real expert predictions, we demonstrate that, in practice, our greedy algorithm finds near-optimal prediction sets offering greater performance than conformal prediction.',\n",
       "  986: 'Do neural networks learn to implement algorithms such as look-ahead or search \"in the wild\"? Or do they rely purely on collections of simple heuristics? We present evidence of learned look-ahead in the policy and value network of Leela Chess Zero, the currently strongest deep neural chess engine. We find that Leela internally represents future optimal moves and that these representations are crucial for its final output in certain board states. Concretely, we exploit the fact that Leela is a transformer that treats every chessboard square like a token in language models, and give three lines of evidence: (1) activations on certain squares of future moves are unusually important causally; (2) we find attention heads that move important information \"forward and backward in time,\" e.g., from squares of future moves to squares of earlier ones; and (3) we train a simple probe that can predict the optimal move 2 turns ahead with 92% accuracy (in board states where Leela finds a single best line). These findings are clear evidence of learned look-ahead in neural networks and might be a step towards a better understanding of their capabilities.',\n",
       "  987: 'To address the uncertainty in function types, recent progress in online convex optimization (OCO) has spurred the development of universal algorithms that simultaneously attain minimax rates for multiple types of convex functions. However, for a $T$-round online problem, state-of-the-art methods typically conduct $O(\\\\log T)$ projections onto the domain in each round, a process potentially time-consuming with complicated feasible sets. In this paper, inspired by the black-box reduction of Cutkosky and Orabona [2018], we employ a surrogate loss defined over simpler domains to develop universal OCO algorithms that only require $1$ projection. Embracing the framework of prediction with expert advice, we maintain a set of experts for each type of functions and aggregate their predictions via a meta-algorithm. The crux of our approach lies in a uniquely designed expert-loss for strongly convex functions, stemming from an innovative decomposition of the regret into the meta-regret and the expert-regret. Our analysis sheds new light on the surrogate loss, facilitating a rigorous examination of the discrepancy between the regret of the original loss and that of the surrogate loss, and carefully controlling meta-regret under the strong convexity condition. With only $1$ projection per round, we establish optimal regret bounds for general convex, exponentially concave, and strongly convex functions simultaneously. Furthermore, we enhance the expert-loss to exploit the smoothness property, and demonstrate that our algorithm can attain small-loss regret for multiple types of convex and smooth functions.',\n",
       "  988: 'Implicit Neural Representations (INRs) have peaked interest in recent years due to their ability to encode natural signals using neural networks. While INRs allow for useful applications such as interpolating new coordinates and signal compression, their black-box nature makes it difficult to modify them post-training. In this paper we explore the idea of editable INRs, and specifically focus on the widely used cropping operation. To this end, we present Local-Global SIRENs - a novel INR architecture that supports cropping by design. Local-Global SIRENs are based on combining local and global feature extraction for signal encoding. What makes their design unique is the ability to effortlessly remove specific portions of an encoded signal, with a proportional weight decrease. This is achieved by eliminating the corresponding weights from the network, without the need for retraining. We further show how this architecture can be used to support the straightforward extension of previously encoded signals. Beyond signal editing, we examine how the Local-Global approach can accelerate training, enhance encoding of various signals, improve downstream performance, and be applied to modern INRs such as INCODE, highlighting its potential and flexibility. Code is available at https://github.com/maorash/Local-Global-INRs.',\n",
       "  989: 'Large language models (LLMs) have demonstrated remarkable in-context learning capabilities across diverse applications. In this work, we explore the effectiveness of LLMs for generating realistic synthetic tabular data, identifying key prompt design elements to optimize performance. We introduce EPIC, a novel approach that leverages balanced, grouped data samples and consistent formatting with unique variable mapping to guide LLMs in generating accurate synthetic data across all classes, even for imbalanced datasets. Evaluations on real-world datasets show that EPIC achieves state-of-the-art machine learning classification performance, significantly improving generation efficiency. These findings highlight the effectiveness of EPIC for synthetic tabular data generation, particularly in addressing class imbalance.',\n",
       "  990: 'Influential and popular benchmarks in AI are largely irrelevant to developing NLP tools for low-resource, Indigenous languages. With the primary goal of measuring the performance of general-purpose AI systems, these benchmarks fail to give due consideration and care to individual language communities, especially low-resource languages. The datasets contain numerous grammatical and orthographic errors, poor pronunciation, limited vocabulary, and the content lacks cultural relevance to the language community. To overcome the issues with these benchmarks, we have created a dataset for te reo Māori (the Indigenous language of Aotearoa/New Zealand) to pursue NLP tools that are ‘fit-for-our-purpose’. This paper demonstrates how low-resourced, Indigenous languages can develop tailored, high-quality benchmarks that; i. Consider the impact of colonisation on their language; ii. Reflect the diversity of speakers in the language community; iii. Support the aspirations for the tools they are developing and their language revitalisation efforts.',\n",
       "  991: 'We investigate the statistical and computational limits of latent Diffusion Transformers (DiTs) under the low-dimensional linear latent space assumption. Statistically, we study the universal approximation and sample complexity of the DiTs score function, as well as the distribution recovery property of the initial data. Specifically, under mild data assumptions, we derive an approximation error bound for the score network of latent DiTs, which is sub-linear in the latent space dimension. Additionally, we derive the corresponding sample complexity bound and show that the data distribution generated from the estimated score function converges toward a proximate area of the original one.Computationally, we characterize the hardness of both forward inference and backward computation of latent DiTs, assuming the Strong Exponential Time Hypothesis (SETH). For forward inference, we identify efficient criteria for all possible latent DiTs inference algorithms and showcase our theory by pushing the efficiency toward almost-linear time inference. For backward computation, we leverage the low-rank structure within the gradient computation of DiTs training for possible algorithmic speedup. Specifically, we show that such speedup achieves almost-linear time latent DiTs training by casting the DiTs gradient as a series of chained low-rank approximations with bounded error.Under the low-dimensional assumption, we show that the statistical rates and the computational efficiency are all dominated by the dimension of the subspace, suggesting that latent DiTs have the potential to bypass the challenges associated with the high dimensionality of initial data.',\n",
       "  992: \"Unsupervised Multiplex Graph Learning (UMGL) aims to learn node representations on various edge types without manual labeling. However, existing research overlooks a key factor: the reliability of the graph structure. Real-world data often exhibit a complex nature and contain abundant task-irrelevant noise, severely compromising UMGL's performance. Moreover, existing methods primarily rely on contrastive learning to maximize mutual information across different graphs, limiting them to multiplex graph redundant scenarios and failing to capture view-unique task-relevant information. In this paper, we focus on a more realistic and challenging task: to unsupervisedly learn a fused graph from multiple graphs that preserve sufficient task-relevant information while removing task-irrelevant noise. Specifically, our proposed Information-aware Unsupervised Multiplex Graph Fusion framework (InfoMGF) uses graph structure refinement to eliminate irrelevant noise and simultaneously maximizes view-shared and view-unique task-relevant information, thereby tackling the frontier of non-redundant multiplex graph. Theoretical analyses further guarantee the effectiveness of InfoMGF. Comprehensive experiments against various baselines on different downstream tasks demonstrate its superior performance and robustness. Surprisingly, our unsupervised method even beats the sophisticated supervised approaches. The source code and datasets are available at https://github.com/zxlearningdeep/InfoMGF.\",\n",
       "  993: 'Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain.  In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous two-photon calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest.',\n",
       "  994: \"At a cocktail party, humans exhibit an impressive ability to direct their attention. The auditory attention detection (AAD) approach seeks to identify the attended speaker by analyzing brain signals, such as EEG signals. However, current AAD algorithms overlook the spatial distribution information within EEG signals and lack the ability to capture long-range latent dependencies, limiting the model's ability to decode brain activity.To address these issues, this paper proposes a dual attention refinement network with spatiotemporal construction for AAD, named DARNet, which consists of the spatiotemporal construction module, dual attention refinement module, and feature fusion \\\\& classifier module. Specifically, the spatiotemporal construction module aims to construct more expressive spatiotemporal feature representations, by capturing the spatial distribution characteristics of EEG signals. The dual attention refinement module aims to extract different levels of temporal patterns in EEG signals and enhance the model's ability to capture long-range latent dependencies. The feature fusion \\\\& classifier module aims to aggregate temporal patterns and dependencies from different levels and obtain the final classification results.The experimental results indicate that DARNet achieved excellent classification performance, particularly under short decision windows. While maintaining excellent classification performance, DARNet significantly reduces the number of required parameters. Compared to the state-of-the-art models, DARNet reduces the parameter count by 91\\\\%. Code is available at: https://github.com/fchest/DARNet.git.\",\n",
       "  995: 'Adversarial training can achieve robustness against adversarial perturbations and has been widely used in machine-learning models. This paper delivers a non-asymptotic consistency analysis of the adversarial training procedure under $\\\\ell_\\\\infty$-perturbation in high-dimensional linear regression. It will be shown that, under the restricted eigenvalue condition, the associated convergence rate of prediction error can achieve the minimax rate up to a logarithmic factor in the high-dimensional linear regression on the class of sparse parameters. Additionally, the group adversarial training procedure is analyzed. Compared with classic adversarial training, it will be proved that the group adversarial training procedure enjoys a better prediction error upper bound under certain group-sparsity patterns.',\n",
       "  996: 'Adversarial attacks against graph neural networks (GNNs) through perturbations of the graph structure are increasingly common in social network tasks like rumor detection. Social media platforms capture diverse attack sequence samples through both machine and manual screening processes. Investigating effective ways to leverage these adversarial samples to enhance robustness is imperative. We improve the maximum entropy inverse reinforcement learning (IRL) method with the mixture-of-experts approach to address multi-source graph adversarial attacks. This method reconstructs the attack policy, integrating various attack models and providing feature-level explanations, subsequently generating additional adversarial samples to fortify the robustness of detection models. We develop precise sample guidance and a bidirectional update mechanism to reduce the deviation caused by imprecise feature representation and negative sampling within the large action space of social graphs, while also accelerating policy learning. We take rumor detector as an example targeted GNN model on real-world rumor datasets. By utilizing a small subset of samples generated by various graph adversarial attack methods, we reconstruct the attack policy, closely approximating the performance of the original attack method. We validate that samples generated by the learned policy enhance model robustness through adversarial training and data augmentation.',\n",
       "  997: 'Even though neural networks have been long deployed in applications involving tabular data, still existing neural architectures are not explainable by design. In this paper, we propose a new class of interpretable neural networks for tabular data that are both deep and linear at the same time (i.e. mesomorphic). We optimize deep hypernetworks to generate explainable linear models on a per-instance basis. As a result, our models retain the accuracy of black-box deep networks while offering free-lunch explainability for tabular data by design. Through extensive experiments, we demonstrate that our explainable deep networks have comparable performance to state-of-the-art classifiers on tabular data and outperform current existing methods that are explainable by design.',\n",
       "  998: 'Transformer architectures have become a dominant paradigm for domains like language modeling but suffer in many inference settings due to their quadratic-time self-attention. Recently proposed subquadratic architectures, such as Mamba, have shown promise, but have been pretrained with substantially less computational resources than the strongest Transformer models. In this work, we present a method that is able to distill a pretrained Transformer architecture into alternative architectures such as state space models (SSMs). The key idea to our approach is that we can view both Transformers and SSMs as applying different forms of mixing matrices over the token sequences. We can thus progressively distill the Transformer architecture by matching different degrees of granularity in the SSM: first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens. Despite using less than 1% of the training data typically used to train models from scratch, Phi-Mamba boasts substantially stronger performance compared to all past open-source non-Transformer models. MOHAWK allows models like SSMs to leverage computational resources invested in training Transformer-based architectures, highlighting a new avenue for building such models.',\n",
       "  999: 'Large language models (LLMs) have recently demonstrated great success in generating and understanding natural language.  While they have also shown potential beyond the domain of natural language, it remains an open question as to what extent and in which way these LLMs can plan. We investigate their planning capabilities by proposing \\\\texttt{GameTraversalBenchmark (GTB)}, a benchmark consisting of diverse 2D grid-based game maps. An LLM succeeds if it can traverse through given objectives, with a minimum number of steps and a minimum number of generation errors. We evaluate a number of LLMs on \\\\texttt{GTB} and found that GPT-4-Turbo achieved the highest score of $44.97\\\\%$ on \\\\texttt{GTB\\\\_Score} (GTBS), a composite score that combines the three above criteria. Furthermore, we preliminarily test large reasoning models, namely o1, which scores $67.84\\\\%$ on GTBS, indicating that the benchmark remains challenging for current models. Code, data, and documentation are available at \\\\url{https://github.com/umair-nasir14/Game-Traversal-Benchmark}.',\n",
       "  ...},\n",
       " 'proceeding': {0: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  1: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  2: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  3: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  4: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  5: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  6: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  7: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  8: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  9: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  10: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  11: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  12: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  13: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  14: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  15: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  16: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  17: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  18: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  19: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  20: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  21: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  22: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  23: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  24: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  25: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  26: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  27: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  28: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  29: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  30: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  31: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  32: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  33: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  34: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  35: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  36: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  37: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  38: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  39: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  40: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  41: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  42: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  43: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  44: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  45: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  46: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  47: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  48: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  49: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  50: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  51: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  52: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  53: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  54: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  55: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  56: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  57: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  58: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  59: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  60: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  61: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  62: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  63: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  64: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  65: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  66: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  67: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  68: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  69: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  70: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  71: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  72: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  73: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  74: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  75: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  76: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  77: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  78: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  79: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  80: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  81: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  82: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  83: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  84: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  85: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  86: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  87: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  88: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  89: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  90: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  91: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  92: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  93: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  94: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  95: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  96: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  97: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  98: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  99: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  100: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  101: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  102: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  103: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  104: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  105: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  106: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  107: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  108: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  109: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  110: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  111: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  112: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  113: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  114: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  115: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  116: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  117: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  118: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  119: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  120: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  121: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  122: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  123: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  124: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  125: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  126: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  127: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  128: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  129: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  130: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  131: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  132: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  133: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  134: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  135: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  136: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  137: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  138: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  139: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  140: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  141: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  142: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  143: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  144: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  145: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  146: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  147: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  148: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  149: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  150: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  151: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  152: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  153: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  154: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  155: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  156: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  157: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  158: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  159: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  160: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  161: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  162: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  163: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  164: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  165: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  166: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  167: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  168: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  169: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  170: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  171: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  172: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  173: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  174: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  175: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  176: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  177: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  178: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  179: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  180: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  181: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  182: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  183: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  184: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  185: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  186: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  187: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  188: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  189: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  190: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  191: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  192: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  193: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  194: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  195: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  196: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  197: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  198: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  199: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  200: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  201: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  202: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  203: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  204: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  205: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  206: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  207: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  208: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  209: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  210: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  211: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  212: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  213: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  214: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  215: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  216: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  217: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  218: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  219: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  220: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  221: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  222: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  223: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  224: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  225: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  226: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  227: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  228: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  229: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  230: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  231: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  232: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  233: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  234: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  235: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  236: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  237: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  238: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  239: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  240: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  241: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  242: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  243: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  244: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  245: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  246: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  247: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  248: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  249: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  250: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  251: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  252: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  253: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  254: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  255: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  256: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  257: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  258: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  259: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  260: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  261: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  262: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  263: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  264: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  265: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  266: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  267: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  268: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  269: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  270: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  271: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  272: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  273: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  274: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  275: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  276: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  277: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  278: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  279: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  280: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  281: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  282: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  283: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  284: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  285: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  286: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  287: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  288: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  289: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  290: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  291: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  292: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  293: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  294: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  295: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  296: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  297: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  298: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  299: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  300: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  301: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  302: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  303: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  304: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  305: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  306: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  307: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  308: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  309: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  310: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  311: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  312: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  313: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  314: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  315: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  316: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  317: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  318: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  319: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  320: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  321: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  322: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  323: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  324: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  325: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  326: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  327: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  328: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  329: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  330: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  331: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  332: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  333: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  334: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  335: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  336: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  337: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  338: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  339: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  340: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  341: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  342: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  343: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  344: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  345: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  346: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  347: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  348: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  349: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  350: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  351: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  352: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  353: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  354: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  355: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  356: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  357: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  358: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  359: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  360: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  361: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  362: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  363: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  364: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  365: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  366: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  367: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  368: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  369: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  370: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  371: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  372: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  373: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  374: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  375: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  376: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  377: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  378: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  379: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  380: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  381: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  382: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  383: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  384: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  385: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  386: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  387: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  388: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  389: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  390: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  391: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  392: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  393: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  394: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  395: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  396: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  397: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  398: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  399: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  400: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  401: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  402: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  403: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  404: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  405: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  406: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  407: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  408: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  409: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  410: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  411: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  412: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  413: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  414: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  415: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  416: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  417: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  418: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  419: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  420: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  421: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  422: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  423: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  424: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  425: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  426: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  427: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  428: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  429: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  430: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  431: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  432: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  433: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  434: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  435: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  436: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  437: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  438: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  439: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  440: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  441: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  442: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  443: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  444: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  445: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  446: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  447: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  448: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  449: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  450: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  451: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  452: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  453: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  454: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  455: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  456: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  457: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  458: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  459: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  460: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  461: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  462: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  463: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  464: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  465: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  466: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  467: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  468: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  469: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  470: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  471: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  472: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  473: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  474: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  475: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  476: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  477: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  478: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  479: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  480: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  481: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  482: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  483: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  484: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  485: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  486: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  487: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  488: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  489: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  490: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  491: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  492: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  493: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  494: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  495: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  496: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  497: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  498: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  499: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  500: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  501: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  502: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  503: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  504: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  505: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  506: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  507: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  508: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  509: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  510: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  511: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  512: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  513: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  514: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  515: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  516: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  517: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  518: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  519: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  520: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  521: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  522: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  523: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  524: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  525: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  526: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  527: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  528: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  529: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  530: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  531: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  532: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  533: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  534: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  535: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  536: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  537: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  538: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  539: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  540: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  541: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  542: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  543: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  544: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  545: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  546: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  547: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  548: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  549: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  550: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  551: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  552: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  553: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  554: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  555: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  556: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  557: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  558: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  559: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  560: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  561: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  562: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  563: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  564: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  565: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  566: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  567: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  568: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  569: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  570: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  571: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  572: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  573: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  574: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  575: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  576: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  577: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  578: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  579: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  580: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  581: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  582: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  583: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  584: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  585: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  586: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  587: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  588: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  589: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  590: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  591: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  592: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  593: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  594: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  595: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  596: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  597: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  598: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  599: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  600: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  601: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  602: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  603: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  604: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  605: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  606: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  607: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  608: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  609: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  610: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  611: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  612: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  613: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  614: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  615: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  616: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  617: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  618: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  619: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  620: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  621: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  622: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  623: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  624: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  625: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  626: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  627: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  628: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  629: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  630: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  631: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  632: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  633: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  634: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  635: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  636: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  637: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  638: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  639: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  640: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  641: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  642: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  643: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  644: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  645: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  646: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  647: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  648: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  649: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  650: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  651: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  652: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  653: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  654: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  655: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  656: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  657: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  658: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  659: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  660: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  661: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  662: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  663: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  664: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  665: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  666: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  667: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  668: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  669: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  670: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  671: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  672: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  673: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  674: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  675: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  676: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  677: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  678: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  679: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  680: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  681: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  682: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  683: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  684: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  685: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  686: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  687: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  688: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  689: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  690: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  691: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  692: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  693: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  694: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  695: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  696: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  697: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  698: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  699: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  700: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  701: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  702: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  703: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  704: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  705: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  706: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  707: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  708: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  709: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  710: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  711: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  712: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  713: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  714: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  715: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  716: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  717: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  718: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  719: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  720: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  721: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  722: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  723: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  724: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  725: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  726: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  727: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  728: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  729: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  730: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  731: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  732: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  733: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  734: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  735: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  736: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  737: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  738: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  739: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  740: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  741: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  742: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  743: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  744: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  745: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  746: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  747: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  748: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  749: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  750: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  751: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  752: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  753: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  754: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  755: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  756: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  757: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  758: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  759: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  760: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  761: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  762: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  763: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  764: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  765: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  766: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  767: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  768: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  769: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  770: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  771: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  772: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  773: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  774: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  775: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  776: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  777: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  778: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  779: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  780: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  781: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  782: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  783: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  784: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  785: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  786: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  787: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  788: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  789: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  790: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  791: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  792: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  793: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  794: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  795: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  796: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  797: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  798: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  799: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  800: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  801: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  802: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  803: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  804: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  805: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  806: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  807: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  808: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  809: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  810: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  811: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  812: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  813: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  814: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  815: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  816: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  817: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  818: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  819: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  820: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  821: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  822: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  823: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  824: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  825: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  826: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  827: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  828: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  829: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  830: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  831: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  832: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  833: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  834: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  835: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  836: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  837: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  838: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  839: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  840: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  841: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  842: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  843: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  844: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  845: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  846: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  847: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  848: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  849: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  850: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  851: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  852: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  853: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  854: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  855: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  856: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  857: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  858: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  859: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  860: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  861: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  862: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  863: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  864: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  865: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  866: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  867: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  868: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  869: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  870: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  871: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  872: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  873: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  874: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  875: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  876: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  877: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  878: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  879: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  880: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  881: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  882: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  883: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  884: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  885: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  886: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  887: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  888: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  889: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  890: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  891: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  892: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  893: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  894: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  895: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  896: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  897: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  898: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  899: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  900: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  901: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  902: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  903: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  904: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  905: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  906: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  907: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  908: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  909: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  910: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  911: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  912: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  913: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  914: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  915: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  916: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  917: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  918: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  919: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  920: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  921: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  922: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  923: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  924: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  925: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  926: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  927: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  928: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  929: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  930: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  931: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  932: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  933: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  934: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  935: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  936: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  937: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  938: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  939: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  940: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  941: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  942: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  943: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  944: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  945: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  946: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  947: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  948: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  949: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  950: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  951: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  952: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  953: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  954: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  955: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  956: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  957: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  958: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  959: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  960: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  961: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  962: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  963: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  964: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  965: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  966: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  967: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  968: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  969: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  970: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  971: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  972: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  973: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  974: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  975: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  976: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  977: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  978: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  979: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  980: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  981: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  982: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  983: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  984: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  985: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  986: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  987: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  988: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  989: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  990: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  991: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  992: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  993: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  994: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  995: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  996: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  997: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  998: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  999: 'https://papers.nips.cc//paper_files/paper/2024',\n",
       "  ...},\n",
       " 'url': {0: 'https://papers.nips.cc//paper_files/paper/2024/hash/000f947dcaff8fbffcc3f53a1314f358-Abstract-Conference.html',\n",
       "  1: 'https://papers.nips.cc//paper_files/paper/2024/hash/00295cede6e1600d344b5cd6d9fd4640-Abstract-Conference.html',\n",
       "  2: 'https://papers.nips.cc//paper_files/paper/2024/hash/00532321a253959cedc4f971b5524131-Abstract-Conference.html',\n",
       "  3: 'https://papers.nips.cc//paper_files/paper/2024/hash/005413e90d003d13886019607b037f52-Abstract-Conference.html',\n",
       "  4: 'https://papers.nips.cc//paper_files/paper/2024/hash/00616a2d48f5716f3d6f783491149364-Abstract-Conference.html',\n",
       "  5: 'https://papers.nips.cc//paper_files/paper/2024/hash/008a16ead32f932b711788c276890456-Abstract-Conference.html',\n",
       "  6: 'https://papers.nips.cc//paper_files/paper/2024/hash/009729d26288b9a8826023692a876107-Abstract-Conference.html',\n",
       "  7: 'https://papers.nips.cc//paper_files/paper/2024/hash/0098a92f5f4e2d96c6db471e0c5507a8-Abstract-Conference.html',\n",
       "  8: 'https://papers.nips.cc//paper_files/paper/2024/hash/00a0db6d26a58c996ff915bd765eb969-Abstract-Conference.html',\n",
       "  9: 'https://papers.nips.cc//paper_files/paper/2024/hash/00d1f03b87a401b1c7957e0cc785d0bc-Abstract-Conference.html',\n",
       "  10: 'https://papers.nips.cc//paper_files/paper/2024/hash/00d80722b756de0166523a87805dd00f-Abstract-Conference.html',\n",
       "  11: 'https://papers.nips.cc//paper_files/paper/2024/hash/01025a4e79355bb37a10ba39605944b5-Abstract-Conference.html',\n",
       "  12: 'https://papers.nips.cc//paper_files/paper/2024/hash/010c5ba0cafc743fece8be02e7adb8dd-Abstract-Conference.html',\n",
       "  13: 'https://papers.nips.cc//paper_files/paper/2024/hash/010c855df402b443e0c16e5b7434e74c-Abstract-Conference.html',\n",
       "  14: 'https://papers.nips.cc//paper_files/paper/2024/hash/0113ef4642264adc2e6924a3cbbdf532-Abstract-Conference.html',\n",
       "  15: 'https://papers.nips.cc//paper_files/paper/2024/hash/013cf29a9e68e4411d0593040a8a1eb3-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  16: 'https://papers.nips.cc//paper_files/paper/2024/hash/013d743db3c684957305d32017f13339-Abstract-Conference.html',\n",
       "  17: 'https://papers.nips.cc//paper_files/paper/2024/hash/013f9cd52b38e3e53475605d2b8e7c23-Abstract-Conference.html',\n",
       "  18: 'https://papers.nips.cc//paper_files/paper/2024/hash/0142921fad7ef9192bd87229cdafa9d4-Abstract-Conference.html',\n",
       "  19: 'https://papers.nips.cc//paper_files/paper/2024/hash/0147d967a5db3b8dde08d2a327b24568-Abstract-Conference.html',\n",
       "  20: 'https://papers.nips.cc//paper_files/paper/2024/hash/014fe398da515cd552fa6e1f33e0565e-Abstract-Conference.html',\n",
       "  21: 'https://papers.nips.cc//paper_files/paper/2024/hash/015a8c69bedcb0a7b2ed2e1678f34399-Abstract-Conference.html',\n",
       "  22: 'https://papers.nips.cc//paper_files/paper/2024/hash/0164f863e66a4272b680ecc4e561bf9d-Abstract-Conference.html',\n",
       "  23: 'https://papers.nips.cc//paper_files/paper/2024/hash/01772a8b0420baec00c4d59fe2fbace6-Abstract-Conference.html',\n",
       "  24: 'https://papers.nips.cc//paper_files/paper/2024/hash/017761f94a1cd66d01c041aff85492c4-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  25: 'https://papers.nips.cc//paper_files/paper/2024/hash/019ef89617d539b15ed610ce8d1b76e1-Abstract-Conference.html',\n",
       "  26: 'https://papers.nips.cc//paper_files/paper/2024/hash/01a8d63f9cb6dcbaa3092ccddd2075ac-Abstract-Conference.html',\n",
       "  27: 'https://papers.nips.cc//paper_files/paper/2024/hash/01b3dea1871f7cea1e0e6be1f2f085bc-Abstract-Conference.html',\n",
       "  28: 'https://papers.nips.cc//paper_files/paper/2024/hash/01ce1ae7f94d139e4917f9e4425a4f38-Abstract-Conference.html',\n",
       "  29: 'https://papers.nips.cc//paper_files/paper/2024/hash/01ecd39ca49ddecc5729ca996304781b-Abstract-Conference.html',\n",
       "  30: 'https://papers.nips.cc//paper_files/paper/2024/hash/01fb6de3360f9e32862665580e2c5853-Abstract-Conference.html',\n",
       "  31: 'https://papers.nips.cc//paper_files/paper/2024/hash/0232cafe8d1909a01019abe8af32f3e1-Abstract-Conference.html',\n",
       "  32: 'https://papers.nips.cc//paper_files/paper/2024/hash/0235cbb8cd6425d0b55daefce388fc0b-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  33: 'https://papers.nips.cc//paper_files/paper/2024/hash/023d2c1a17cf35b11a0cbb43a0677c91-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  34: 'https://papers.nips.cc//paper_files/paper/2024/hash/0252a434b18962c94910c07cd9a7fecc-Abstract-Conference.html',\n",
       "  35: 'https://papers.nips.cc//paper_files/paper/2024/hash/026211b600a8d54760a87d78b16d9153-Abstract-Conference.html',\n",
       "  36: 'https://papers.nips.cc//paper_files/paper/2024/hash/0267925e3c276e79189251585b4100bf-Abstract-Conference.html',\n",
       "  37: 'https://papers.nips.cc//paper_files/paper/2024/hash/02802e3df178cce7b13e8f63dd29ad9f-Abstract-Conference.html',\n",
       "  38: 'https://papers.nips.cc//paper_files/paper/2024/hash/028ef7e68a5ea25fc26cd6abf3a5c147-Abstract-Conference.html',\n",
       "  39: 'https://papers.nips.cc//paper_files/paper/2024/hash/028fcbcf85435d39a40c4d61b42c99a4-Abstract-Conference.html',\n",
       "  40: 'https://papers.nips.cc//paper_files/paper/2024/hash/02c1d1d33dbfbaf03b3971bb542e72e2-Abstract-Conference.html',\n",
       "  41: 'https://papers.nips.cc//paper_files/paper/2024/hash/02cef2ae63853724eb99e70721d3bc65-Abstract-Conference.html',\n",
       "  42: 'https://papers.nips.cc//paper_files/paper/2024/hash/02cf868040aa0f0a1e1121cf255fdcfb-Abstract-Conference.html',\n",
       "  43: 'https://papers.nips.cc//paper_files/paper/2024/hash/02d425a464e48bda5e810f8f4914b77e-Abstract-Conference.html',\n",
       "  44: 'https://papers.nips.cc//paper_files/paper/2024/hash/02e978a2cc9a1d0d4376a7deb01db612-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  45: 'https://papers.nips.cc//paper_files/paper/2024/hash/02ee6b7295f720407b56c457b34c54d5-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  46: 'https://papers.nips.cc//paper_files/paper/2024/hash/02fd91a387a6a5a5751e81b58a75af90-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  47: 'https://papers.nips.cc//paper_files/paper/2024/hash/0302fb83c62991efbccf0a003e4f5a92-Abstract-Conference.html',\n",
       "  48: 'https://papers.nips.cc//paper_files/paper/2024/hash/030cf55d506515f39c042e63ba0376dd-Abstract-Conference.html',\n",
       "  49: 'https://papers.nips.cc//paper_files/paper/2024/hash/0318de478e18308a5f64297f618299d3-Abstract-Conference.html',\n",
       "  50: 'https://papers.nips.cc//paper_files/paper/2024/hash/031b5fd7d847f69ed33378a9a1117b4b-Abstract-Conference.html',\n",
       "  51: 'https://papers.nips.cc//paper_files/paper/2024/hash/03261886741f1f21f52f2a2d570616a2-Abstract-Conference.html',\n",
       "  52: 'https://papers.nips.cc//paper_files/paper/2024/hash/03341fe3c3f848f699b201ce72c13e11-Abstract-Conference.html',\n",
       "  53: 'https://papers.nips.cc//paper_files/paper/2024/hash/0337b41b4e8b2eb5d7ab161ffd42cf3b-Abstract-Conference.html',\n",
       "  54: 'https://papers.nips.cc//paper_files/paper/2024/hash/03469b1a66e351b18272be23baf3b809-Abstract-Conference.html',\n",
       "  55: 'https://papers.nips.cc//paper_files/paper/2024/hash/0346c8a510dd15971566a97a241c5e6a-Abstract-Conference.html',\n",
       "  56: 'https://papers.nips.cc//paper_files/paper/2024/hash/034b4a2860d4d170ce663584bc78cb32-Abstract-Conference.html',\n",
       "  57: 'https://papers.nips.cc//paper_files/paper/2024/hash/034cd49870f1cc253fc08686049ae7eb-Abstract-Conference.html',\n",
       "  58: 'https://papers.nips.cc//paper_files/paper/2024/hash/0356216f73660e15670510f5e42b5fa6-Abstract-Conference.html',\n",
       "  59: 'https://papers.nips.cc//paper_files/paper/2024/hash/03738e5f26967582eeb3b57eef82f1f0-Abstract-Conference.html',\n",
       "  60: 'https://papers.nips.cc//paper_files/paper/2024/hash/0382cb76309820f71c6eacd47b36ce71-Abstract-Conference.html',\n",
       "  61: 'https://papers.nips.cc//paper_files/paper/2024/hash/03bec5d9f651c1fb89be07a4120238a0-Abstract-Conference.html',\n",
       "  62: 'https://papers.nips.cc//paper_files/paper/2024/hash/03cd3cf3f74d4f9ce5958de269960884-Abstract-Conference.html',\n",
       "  63: 'https://papers.nips.cc//paper_files/paper/2024/hash/03cdf8e212ba92a3f36bffe1391928bd-Abstract-Conference.html',\n",
       "  64: 'https://papers.nips.cc//paper_files/paper/2024/hash/03ce71224741e6596059b8680a62978b-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  65: 'https://papers.nips.cc//paper_files/paper/2024/hash/03d113a060c0ac93a5859517a0f07271-Abstract-Conference.html',\n",
       "  66: 'https://papers.nips.cc//paper_files/paper/2024/hash/03dbc11a22e79cd38bea53cf518c2371-Abstract-Conference.html',\n",
       "  67: 'https://papers.nips.cc//paper_files/paper/2024/hash/03e7eaa586f0990c633f8a8e57e08ca6-Abstract-Conference.html',\n",
       "  68: 'https://papers.nips.cc//paper_files/paper/2024/hash/03e9a69e5b686c316a07d73f0cf5e225-Abstract-Conference.html',\n",
       "  69: 'https://papers.nips.cc//paper_files/paper/2024/hash/040ace837dd270a87055bb10dd7c0392-Abstract-Conference.html',\n",
       "  70: 'https://papers.nips.cc//paper_files/paper/2024/hash/040c816286b3844fd78f2124eec75f2e-Abstract-Conference.html',\n",
       "  71: 'https://papers.nips.cc//paper_files/paper/2024/hash/040d3b6af368bf71f952c18da5713b48-Abstract-Conference.html',\n",
       "  72: 'https://papers.nips.cc//paper_files/paper/2024/hash/0433292a3eb101df8f4d72a63f5410d4-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  73: 'https://papers.nips.cc//paper_files/paper/2024/hash/047682108c3b053c61ad2da5a6057b4e-Abstract-Conference.html',\n",
       "  74: 'https://papers.nips.cc//paper_files/paper/2024/hash/047bf3f8aa5a050351de38df589cc6af-Abstract-Conference.html',\n",
       "  75: 'https://papers.nips.cc//paper_files/paper/2024/hash/047c84ec50bd8ea29349b996fc64af4b-Abstract-Conference.html',\n",
       "  76: 'https://papers.nips.cc//paper_files/paper/2024/hash/04a80267ad46fc730011f8760f265054-Abstract-Conference.html',\n",
       "  77: 'https://papers.nips.cc//paper_files/paper/2024/hash/04ad66d02234541aac5143de7876e880-Abstract-Conference.html',\n",
       "  78: 'https://papers.nips.cc//paper_files/paper/2024/hash/04b98fd38bd42810d0764cb6c46d10d8-Abstract-Conference.html',\n",
       "  79: 'https://papers.nips.cc//paper_files/paper/2024/hash/04badd3b048315c8c3a0ca17eff723d7-Abstract-Conference.html',\n",
       "  80: 'https://papers.nips.cc//paper_files/paper/2024/hash/04c0c541d936f7cbfbb21085116236cd-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  81: 'https://papers.nips.cc//paper_files/paper/2024/hash/04cc90ec6868b97b7423dc38ced1e35c-Abstract-Conference.html',\n",
       "  82: 'https://papers.nips.cc//paper_files/paper/2024/hash/04d212c4eeeb710f170d47f8d5b9b88a-Abstract-Conference.html',\n",
       "  83: 'https://papers.nips.cc//paper_files/paper/2024/hash/04ea184dfb5f1babb78c093e850a83f9-Abstract-Conference.html',\n",
       "  84: 'https://papers.nips.cc//paper_files/paper/2024/hash/0506ad3d1bcc8398a920db9340f27fe4-Abstract-Conference.html',\n",
       "  85: 'https://papers.nips.cc//paper_files/paper/2024/hash/050f8591be3874b52fdac4e1060eeb29-Abstract-Conference.html',\n",
       "  86: 'https://papers.nips.cc//paper_files/paper/2024/hash/051f3997af1dd65da8e14397b6a72f8e-Abstract-Conference.html',\n",
       "  87: 'https://papers.nips.cc//paper_files/paper/2024/hash/0520537ba799d375b8ff5523295c337a-Abstract-Conference.html',\n",
       "  88: 'https://papers.nips.cc//paper_files/paper/2024/hash/0525fa17a8dbea687359116d01732e12-Abstract-Conference.html',\n",
       "  89: 'https://papers.nips.cc//paper_files/paper/2024/hash/0534abc9e6db91683d82186ef0d68202-Abstract-Conference.html',\n",
       "  90: 'https://papers.nips.cc//paper_files/paper/2024/hash/053ee34c0971568bfa5c773015c10502-Abstract-Conference.html',\n",
       "  91: 'https://papers.nips.cc//paper_files/paper/2024/hash/054e9f9a286671ababa3213d6e59c1c2-Abstract-Conference.html',\n",
       "  92: 'https://papers.nips.cc//paper_files/paper/2024/hash/054f771d614df12fe8def8ecdbe4e8e1-Abstract-Conference.html',\n",
       "  93: 'https://papers.nips.cc//paper_files/paper/2024/hash/056521a35eacd9d2127b66a7d3c499c5-Abstract-Conference.html',\n",
       "  94: 'https://papers.nips.cc//paper_files/paper/2024/hash/058373e239c542eae6cb796e4dc521da-Abstract-Conference.html',\n",
       "  95: 'https://papers.nips.cc//paper_files/paper/2024/hash/059445c2d5b3ef918079851628fef1d6-Abstract-Conference.html',\n",
       "  96: 'https://papers.nips.cc//paper_files/paper/2024/hash/059d2b9188cdb7ae00f4d78cc9469704-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  97: 'https://papers.nips.cc//paper_files/paper/2024/hash/05a0a9fc9566b7dbf471f2f6497c74b7-Abstract-Conference.html',\n",
       "  98: 'https://papers.nips.cc//paper_files/paper/2024/hash/05a2d9ef0ae6f249737c1e4cce724a0c-Abstract-Conference.html',\n",
       "  99: 'https://papers.nips.cc//paper_files/paper/2024/hash/05a30a0fc9e6bacdd3abd4ca8508a9e6-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  100: 'https://papers.nips.cc//paper_files/paper/2024/hash/05a7ad45d75a3082d7a3a70de8743140-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  101: 'https://papers.nips.cc//paper_files/paper/2024/hash/05aedcaf4bc6e78a5e22b4cf9114c5e8-Abstract-Conference.html',\n",
       "  102: 'https://papers.nips.cc//paper_files/paper/2024/hash/05b12f103c9e613efc4c85674cdc9066-Abstract-Conference.html',\n",
       "  103: 'https://papers.nips.cc//paper_files/paper/2024/hash/05b2ea5d57ed04c1dc105658e87b137e-Abstract-Conference.html',\n",
       "  104: 'https://papers.nips.cc//paper_files/paper/2024/hash/05b69cc4c8ff6e24c5de1ecd27223d37-Abstract-Conference.html',\n",
       "  105: 'https://papers.nips.cc//paper_files/paper/2024/hash/05b7f821234f66b78f99e7803fffa78a-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  106: 'https://papers.nips.cc//paper_files/paper/2024/hash/05cdc7feee41e3572a9a3f4acb773891-Abstract-Conference.html',\n",
       "  107: 'https://papers.nips.cc//paper_files/paper/2024/hash/05d42b7bd130ecbc57fdf02cbdcd370e-Abstract-Conference.html',\n",
       "  108: 'https://papers.nips.cc//paper_files/paper/2024/hash/05d6b5b6901fb57d2c287e1d3ce6d63c-Abstract-Conference.html',\n",
       "  109: 'https://papers.nips.cc//paper_files/paper/2024/hash/05fbf28602c5c2c994499db18363fbbb-Abstract-Conference.html',\n",
       "  110: 'https://papers.nips.cc//paper_files/paper/2024/hash/060f64f690417a5cc6a882479478fd96-Abstract-Conference.html',\n",
       "  111: 'https://papers.nips.cc//paper_files/paper/2024/hash/061d5d1b7d97117764f205d4e038f9eb-Abstract-Conference.html',\n",
       "  112: 'https://papers.nips.cc//paper_files/paper/2024/hash/0626822954674a06ccd9c234e3f0d572-Abstract-Conference.html',\n",
       "  113: 'https://papers.nips.cc//paper_files/paper/2024/hash/06297213eca57fa6eadf8d87caa21b3c-Abstract-Conference.html',\n",
       "  114: 'https://papers.nips.cc//paper_files/paper/2024/hash/06477eb61ea6b85c6608d42a222462df-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  115: 'https://papers.nips.cc//paper_files/paper/2024/hash/064ae24cdbb3eaacc801ee7f4fe0e4f2-Abstract-Conference.html',\n",
       "  116: 'https://papers.nips.cc//paper_files/paper/2024/hash/064f6bcd7d3c72fb187bfca35ba2bfd4-Abstract-Conference.html',\n",
       "  117: 'https://papers.nips.cc//paper_files/paper/2024/hash/067437c6d5d0369b6d09200bef89715b-Abstract-Conference.html',\n",
       "  118: 'https://papers.nips.cc//paper_files/paper/2024/hash/06a52a54c8ee03cd86771136bc91eb1f-Abstract-Conference.html',\n",
       "  119: 'https://papers.nips.cc//paper_files/paper/2024/hash/06cbd2e81dfbd3bb4cb0abce95b32584-Abstract-Conference.html',\n",
       "  120: 'https://papers.nips.cc//paper_files/paper/2024/hash/06cf4bae7ccb6ea37b968a394edc2e33-Abstract-Conference.html',\n",
       "  121: 'https://papers.nips.cc//paper_files/paper/2024/hash/06dc3d10e257563416d838f377d31b86-Abstract-Conference.html',\n",
       "  122: 'https://papers.nips.cc//paper_files/paper/2024/hash/06e2dd57e90a736a5a1fd3bb2bf95c6c-Abstract-Conference.html',\n",
       "  123: 'https://papers.nips.cc//paper_files/paper/2024/hash/06e9029d3d4d6cee71c5d9b8502f891b-Abstract-Conference.html',\n",
       "  124: 'https://papers.nips.cc//paper_files/paper/2024/hash/072769405a3c1b60171d09c0ade96ebf-Abstract-Conference.html',\n",
       "  125: 'https://papers.nips.cc//paper_files/paper/2024/hash/0729c38b421cd66f2313ab397d099e37-Abstract-Conference.html',\n",
       "  126: 'https://papers.nips.cc//paper_files/paper/2024/hash/0731f0e65559059eb9cd9d6f44ce2dd8-Abstract-Conference.html',\n",
       "  127: 'https://papers.nips.cc//paper_files/paper/2024/hash/0735ab358bf9bf0f30af6c871b666ec8-Abstract-Conference.html',\n",
       "  128: 'https://papers.nips.cc//paper_files/paper/2024/hash/073c8584ef86bee26fe9d639ec648e28-Abstract-Conference.html',\n",
       "  129: 'https://papers.nips.cc//paper_files/paper/2024/hash/074f42212be2c8ee651db00f17965ec4-Abstract-Conference.html',\n",
       "  130: 'https://papers.nips.cc//paper_files/paper/2024/hash/075b7d4bd7fc32d9cf468a7b67c38d15-Abstract-Conference.html',\n",
       "  131: 'https://papers.nips.cc//paper_files/paper/2024/hash/076c1fa639a7190e216e734f0a1b3e7b-Abstract-Conference.html',\n",
       "  132: 'https://papers.nips.cc//paper_files/paper/2024/hash/076c3e48fa502c660902105965fdd9f6-Abstract-Conference.html',\n",
       "  133: 'https://papers.nips.cc//paper_files/paper/2024/hash/07a363fd2263091c2063998e0034999c-Abstract-Conference.html',\n",
       "  134: 'https://papers.nips.cc//paper_files/paper/2024/hash/07c256a163a7559186ec1c71e95b9ec9-Abstract-Conference.html',\n",
       "  135: 'https://papers.nips.cc//paper_files/paper/2024/hash/07e278a120830b10aae20cc600a8c07b-Abstract-Conference.html',\n",
       "  136: 'https://papers.nips.cc//paper_files/paper/2024/hash/07e5845260f6f1e22c9b4e1471493056-Abstract-Conference.html',\n",
       "  137: 'https://papers.nips.cc//paper_files/paper/2024/hash/07fbde96bee50f4e09303fd4f877c2f3-Abstract-Conference.html',\n",
       "  138: 'https://papers.nips.cc//paper_files/paper/2024/hash/080be5eb7e887319ff30c792c2cbc28c-Abstract-Conference.html',\n",
       "  139: 'https://papers.nips.cc//paper_files/paper/2024/hash/0814a342597d65e0832fc7ec9b42c317-Abstract-Conference.html',\n",
       "  140: 'https://papers.nips.cc//paper_files/paper/2024/hash/081b08068e4733ae3e7ad019fe8d172f-Abstract-Conference.html',\n",
       "  141: 'https://papers.nips.cc//paper_files/paper/2024/hash/08342dc6ab69f23167b4123086ad4d38-Abstract-Conference.html',\n",
       "  142: 'https://papers.nips.cc//paper_files/paper/2024/hash/084252ec5f05f13bf565843c1873686d-Abstract-Conference.html',\n",
       "  143: 'https://papers.nips.cc//paper_files/paper/2024/hash/0845e3f7447ff9e36a26033bb7334674-Abstract-Conference.html',\n",
       "  144: 'https://papers.nips.cc//paper_files/paper/2024/hash/084727e8abf90a8365b940036329cb6f-Abstract-Conference.html',\n",
       "  145: 'https://papers.nips.cc//paper_files/paper/2024/hash/084a67fb91826028f555e288f3adc9a4-Abstract-Conference.html',\n",
       "  146: 'https://papers.nips.cc//paper_files/paper/2024/hash/084cf2b3d73abafa1705336a0e9ebb1c-Abstract-Conference.html',\n",
       "  147: 'https://papers.nips.cc//paper_files/paper/2024/hash/085185ea97db31ae6dcac7497616fd3e-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  148: 'https://papers.nips.cc//paper_files/paper/2024/hash/0852b88e96d973bd4e21b673f51621d0-Abstract-Conference.html',\n",
       "  149: 'https://papers.nips.cc//paper_files/paper/2024/hash/0856bc553d3e3b9827e5140d0ad3bf8d-Abstract-Conference.html',\n",
       "  150: 'https://papers.nips.cc//paper_files/paper/2024/hash/0857833a490eff6b49ce43eba1d01e8e-Abstract-Conference.html',\n",
       "  151: 'https://papers.nips.cc//paper_files/paper/2024/hash/085b4b5d1f81ad9e057ad2b3de922ad4-Abstract-Conference.html',\n",
       "  152: 'https://papers.nips.cc//paper_files/paper/2024/hash/0877af85978e9e630b77f6221db47876-Abstract-Conference.html',\n",
       "  153: 'https://papers.nips.cc//paper_files/paper/2024/hash/0885cd8bf11e9ca58302992ddcfd3652-Abstract-Conference.html',\n",
       "  154: 'https://papers.nips.cc//paper_files/paper/2024/hash/0886e50806c3faf55e557bd63ba3e70c-Abstract-Conference.html',\n",
       "  155: 'https://papers.nips.cc//paper_files/paper/2024/hash/088d99765bc121c6df215da7d45bc4e9-Abstract-Conference.html',\n",
       "  156: 'https://papers.nips.cc//paper_files/paper/2024/hash/0895e2538a68e786475410c6acb1c30f-Abstract-Conference.html',\n",
       "  157: 'https://papers.nips.cc//paper_files/paper/2024/hash/0898f05f6c1d247be3eab8da93d33da1-Abstract-Conference.html',\n",
       "  158: 'https://papers.nips.cc//paper_files/paper/2024/hash/08a362bd4ae1934e099ce025f06039fe-Abstract-Conference.html',\n",
       "  159: 'https://papers.nips.cc//paper_files/paper/2024/hash/08a4e10cb5b950971a64cf2155871f51-Abstract-Conference.html',\n",
       "  160: 'https://papers.nips.cc//paper_files/paper/2024/hash/08a7229eaba3b35cd8f933ac678f2096-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  161: 'https://papers.nips.cc//paper_files/paper/2024/hash/08a9e28c96d016dd63903ab51cd085b0-Abstract-Conference.html',\n",
       "  162: 'https://papers.nips.cc//paper_files/paper/2024/hash/08bd07d567d77d6dd8d82a4474706a5e-Abstract-Conference.html',\n",
       "  163: 'https://papers.nips.cc//paper_files/paper/2024/hash/08f0b541b6eb47bdafebea3a09ad79e5-Abstract-Conference.html',\n",
       "  164: 'https://papers.nips.cc//paper_files/paper/2024/hash/091166620a04a289c555f411d8899049-Abstract-Conference.html',\n",
       "  165: 'https://papers.nips.cc//paper_files/paper/2024/hash/09236f27bad623511341362f26ffcabb-Abstract-Conference.html',\n",
       "  166: 'https://papers.nips.cc//paper_files/paper/2024/hash/09265e2568cf7a6ff47b506acbc2c6eb-Abstract-Conference.html',\n",
       "  167: 'https://papers.nips.cc//paper_files/paper/2024/hash/0939f13ffce3ff487509d902ddba4571-Abstract-Conference.html',\n",
       "  168: 'https://papers.nips.cc//paper_files/paper/2024/hash/094324f386c836c75d4a26f3499d2ede-Abstract-Conference.html',\n",
       "  169: 'https://papers.nips.cc//paper_files/paper/2024/hash/097c514162ea7126d40671d23e12f51b-Abstract-Conference.html',\n",
       "  170: 'https://papers.nips.cc//paper_files/paper/2024/hash/098491b37deebbe6c007e69815729e09-Abstract-Conference.html',\n",
       "  171: 'https://papers.nips.cc//paper_files/paper/2024/hash/09887fac6cb071922e870090ce32aeff-Abstract-Conference.html',\n",
       "  172: 'https://papers.nips.cc//paper_files/paper/2024/hash/098d1bd3eb6156a4c2f834563cdcf617-Abstract-Conference.html',\n",
       "  173: 'https://papers.nips.cc//paper_files/paper/2024/hash/09b47a77997b7dd7d2b26bd8ff769392-Abstract-Conference.html',\n",
       "  174: 'https://papers.nips.cc//paper_files/paper/2024/hash/09bf6a87e80d099cf17c6347301c6120-Abstract-Conference.html',\n",
       "  175: 'https://papers.nips.cc//paper_files/paper/2024/hash/09d320a5b92d74bbde3d0c4f52e680a9-Abstract-Conference.html',\n",
       "  176: 'https://papers.nips.cc//paper_files/paper/2024/hash/09e1944b7f2372f9f81866470c59b663-Abstract-Conference.html',\n",
       "  177: 'https://papers.nips.cc//paper_files/paper/2024/hash/0a02c2bc2e2148b803c4ade1d71e1d25-Abstract-Conference.html',\n",
       "  178: 'https://papers.nips.cc//paper_files/paper/2024/hash/0a0e2c6a487314f821346bdc04869e36-Abstract-Conference.html',\n",
       "  179: 'https://papers.nips.cc//paper_files/paper/2024/hash/0a0eba34ab2ff40ca2d2843324dcc4ab-Abstract-Conference.html',\n",
       "  180: 'https://papers.nips.cc//paper_files/paper/2024/hash/0a38bff5278f4554313b6ebe87ab3cd8-Abstract-Conference.html',\n",
       "  181: 'https://papers.nips.cc//paper_files/paper/2024/hash/0a4e065fb5ee13441c90bb4b4d6072d0-Abstract-Conference.html',\n",
       "  182: 'https://papers.nips.cc//paper_files/paper/2024/hash/0a85f2414e354f9d61ffea5705a8bbf4-Abstract-Conference.html',\n",
       "  183: 'https://papers.nips.cc//paper_files/paper/2024/hash/0ad6ebd11593822b8a6d5873ca9c5b0b-Abstract-Conference.html',\n",
       "  184: 'https://papers.nips.cc//paper_files/paper/2024/hash/0ae94013da7cd459402fd77874e09ee3-Abstract-Conference.html',\n",
       "  185: 'https://papers.nips.cc//paper_files/paper/2024/hash/0aee38a6fe9fffc8b658cfb1d872c1d5-Abstract-Conference.html',\n",
       "  186: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b081a44ed0b8c0c4aa6bd886a60bea4-Abstract-Conference.html',\n",
       "  187: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b135d408253205ba501d55c6539bfc7-Abstract-Conference.html',\n",
       "  188: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b2b199fdd52089b31d3a0120e400b2a-Abstract-Conference.html',\n",
       "  189: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b2de71212384ffcaf80ad9fd1a21fe3-Abstract-Conference.html',\n",
       "  190: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b43289db08ed60edc6451cb2132e203-Abstract-Conference.html',\n",
       "  191: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b5669c3b07bb8429af19a7919376ff5-Abstract-Conference.html',\n",
       "  192: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b77d3a82b59e9d9899370b378087faf-Abstract-Conference.html',\n",
       "  193: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b7f639ef28a9035a71f7e0c04c1d681-Abstract-Conference.html',\n",
       "  194: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b82662b6c32e887bb252a74d8cb2d5e-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  195: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b8705a611ed1ce19cdb759031078705-Abstract-Conference.html',\n",
       "  196: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b8e4c8468273ee3bafb288229c0acbc-Abstract-Conference.html',\n",
       "  197: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b9536e186a77feff516893a5f393f7a-Abstract-Conference.html',\n",
       "  198: 'https://papers.nips.cc//paper_files/paper/2024/hash/0b99315234cc95e6ef281f9155b68832-Abstract-Conference.html',\n",
       "  199: 'https://papers.nips.cc//paper_files/paper/2024/hash/0ba385c3ea3bb417ac6d6a33e24411bc-Abstract-Conference.html',\n",
       "  200: 'https://papers.nips.cc//paper_files/paper/2024/hash/0bac492172db3311c7e116098cfcf521-Abstract-Conference.html',\n",
       "  201: 'https://papers.nips.cc//paper_files/paper/2024/hash/0bd32794b26cfc99214b89313764da8e-Abstract-Conference.html',\n",
       "  202: 'https://papers.nips.cc//paper_files/paper/2024/hash/0bd7c1a579459520e4d731a14b7bda7d-Abstract-Conference.html',\n",
       "  203: 'https://papers.nips.cc//paper_files/paper/2024/hash/0be40478ab6ee0006ee3b38b158bbc8f-Abstract-Conference.html',\n",
       "  204: 'https://papers.nips.cc//paper_files/paper/2024/hash/0c1124bd3be769dacf491d92d499c7d8-Abstract-Conference.html',\n",
       "  205: 'https://papers.nips.cc//paper_files/paper/2024/hash/0c38f54740062529aa4117a04b583f3c-Abstract-Conference.html',\n",
       "  206: 'https://papers.nips.cc//paper_files/paper/2024/hash/0c433d14179ee4d6598f0e3ed85d6bc2-Abstract-Conference.html',\n",
       "  207: 'https://papers.nips.cc//paper_files/paper/2024/hash/0c4bc137edaf0eb7f66a87275a8be706-Abstract-Conference.html',\n",
       "  208: 'https://papers.nips.cc//paper_files/paper/2024/hash/0c7ca207a051228f978971447a56464a-Abstract-Conference.html',\n",
       "  209: 'https://papers.nips.cc//paper_files/paper/2024/hash/0c8bbdf2657b58fa0a620f650fbdd457-Abstract-Conference.html',\n",
       "  210: 'https://papers.nips.cc//paper_files/paper/2024/hash/0c946accd3ccc88c09dfae7e1cd40ffe-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  211: 'https://papers.nips.cc//paper_files/paper/2024/hash/0caf026b344e6c455efc12fe3d254e9f-Abstract-Conference.html',\n",
       "  212: 'https://papers.nips.cc//paper_files/paper/2024/hash/0cb35e10bf7bb73d10c12414edbd63fd-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  213: 'https://papers.nips.cc//paper_files/paper/2024/hash/0cbbdfb0a4098af8dc7a497a5e59aff7-Abstract-Conference.html',\n",
       "  214: 'https://papers.nips.cc//paper_files/paper/2024/hash/0ccd06ff26fd6a7829293ce90e0e7f7d-Abstract-Conference.html',\n",
       "  215: 'https://papers.nips.cc//paper_files/paper/2024/hash/0ccd800d59e94679246ec79d4b19587e-Abstract-Conference.html',\n",
       "  216: 'https://papers.nips.cc//paper_files/paper/2024/hash/0cdbb43e66ea08c718bc1377ce629128-Abstract-Conference.html',\n",
       "  217: 'https://papers.nips.cc//paper_files/paper/2024/hash/0ce1eb87dbb03fdfa872a93d15cfe333-Abstract-Conference.html',\n",
       "  218: 'https://papers.nips.cc//paper_files/paper/2024/hash/0cf3e7eefb9d643e93e16ff1d94090a7-Abstract-Conference.html',\n",
       "  219: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d0dac08f4199f0c348dd2feace0305a-Abstract-Conference.html',\n",
       "  220: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d11b4035d54e449320fc36b0f64ebb7-Abstract-Conference.html',\n",
       "  221: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d18ab3b5fabfa6fe47c62e711af02f0-Abstract-Conference.html',\n",
       "  222: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d288dbe7a757672151aa5fb5423ef2e-Abstract-Conference.html',\n",
       "  223: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d3090c00d6e877c6e7267efe2734e1b-Abstract-Conference.html',\n",
       "  224: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d3496dd0cec77a999c98d35003203ca-Abstract-Conference.html',\n",
       "  225: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d67ec04032cccf4a21d04c0ae4ab268-Abstract-Conference.html',\n",
       "  226: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d70af566e69f1dfb687791ecf955e28-Abstract-Conference.html',\n",
       "  227: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d724619af0c483f9effa8ec23707953-Abstract-Conference.html',\n",
       "  228: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d89cf183391e12063cb63ff0d75ed95-Abstract-Conference.html',\n",
       "  229: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d97fe65d7a1dc12a05642d9fa4cd578-Abstract-Conference.html',\n",
       "  230: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d99a8c048befb6dd6e17d7684adacac-Abstract-Conference.html',\n",
       "  231: 'https://papers.nips.cc//paper_files/paper/2024/hash/0d9dcd4ebef57f1839d871fe7d891e91-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  232: 'https://papers.nips.cc//paper_files/paper/2024/hash/0db7f135f6991e8cec5e516ecc66bfba-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  233: 'https://papers.nips.cc//paper_files/paper/2024/hash/0dbc204928f1e111aff6a8cb5b148151-Abstract-Conference.html',\n",
       "  234: 'https://papers.nips.cc//paper_files/paper/2024/hash/0df38cd13520747e1e64e5b123a78ef8-Abstract-Conference.html',\n",
       "  235: 'https://papers.nips.cc//paper_files/paper/2024/hash/0dfa4b8a2eda4521992e2e1672001e2c-Abstract-Conference.html',\n",
       "  236: 'https://papers.nips.cc//paper_files/paper/2024/hash/0dfe31d6e703e138d46a7d2fced38b7c-Abstract-Conference.html',\n",
       "  237: 'https://papers.nips.cc//paper_files/paper/2024/hash/0e0a6539d1270ae8cd3a722eaff247ab-Abstract-Conference.html',\n",
       "  238: 'https://papers.nips.cc//paper_files/paper/2024/hash/0e0b39c69663e9c073739adf547ed778-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  239: 'https://papers.nips.cc//paper_files/paper/2024/hash/0e21697f3b9385f6c4100aad94c26bed-Abstract-Conference.html',\n",
       "  240: 'https://papers.nips.cc//paper_files/paper/2024/hash/0e4d695de4c606494ba9b0f3dac3b57a-Abstract-Conference.html',\n",
       "  241: 'https://papers.nips.cc//paper_files/paper/2024/hash/0e5b96f97c1813bb75f6c28532c2ecc7-Abstract-Conference.html',\n",
       "  242: 'https://papers.nips.cc//paper_files/paper/2024/hash/0e5cce15e1bfc6b3d7b71f24cc5da821-Abstract-Conference.html',\n",
       "  243: 'https://papers.nips.cc//paper_files/paper/2024/hash/0e705ac30e573d1526f81a0fd071a151-Abstract-Conference.html',\n",
       "  244: 'https://papers.nips.cc//paper_files/paper/2024/hash/0e797d5139ad94fc2dc2080c09119f29-Abstract-Conference.html',\n",
       "  245: 'https://papers.nips.cc//paper_files/paper/2024/hash/0e7e2af2e5ba822c9ad35a37b31b5dd4-Abstract-Conference.html',\n",
       "  246: 'https://papers.nips.cc//paper_files/paper/2024/hash/0e8909cae8248c98279f6cd82074aa6d-Abstract-Conference.html',\n",
       "  247: 'https://papers.nips.cc//paper_files/paper/2024/hash/0e9a05f5ce62284c91e4a33498899124-Abstract-Conference.html',\n",
       "  248: 'https://papers.nips.cc//paper_files/paper/2024/hash/0eb1ac7551ddbae575415aa5183a88be-Abstract-Conference.html',\n",
       "  249: 'https://papers.nips.cc//paper_files/paper/2024/hash/0ed52d7f6f641f228405d48a611e0684-Abstract-Conference.html',\n",
       "  250: 'https://papers.nips.cc//paper_files/paper/2024/hash/0ef1afa0daa888d695dcd5e9513bafa3-Abstract-Conference.html',\n",
       "  251: 'https://papers.nips.cc//paper_files/paper/2024/hash/0ef47f7b768e1a012e3d995ac8d8fac7-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  252: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f06be0008bc568c88d76206aa17954f-Abstract-Conference.html',\n",
       "  253: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f25eb6e9dc26c933a5d7516abf1eb8c-Abstract-Conference.html',\n",
       "  254: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f293260c9e3e9527c06920316326114-Abstract-Conference.html',\n",
       "  255: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f4d1fc085b7504c140e66bb26ed8842-Abstract-Conference.html',\n",
       "  256: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f53ecc0d36a5d5d3d3e94d42c4b23ca-Abstract-Conference.html',\n",
       "  257: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f588c9d9fa34762362697c1dd463294-Abstract-Conference.html',\n",
       "  258: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f5cb62a8e3331b253c232e229cd551e-Abstract-Conference.html',\n",
       "  259: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f6931a9e339a012a9909306d7c758b4-Abstract-Conference.html',\n",
       "  260: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f69b4b96a46f284b726fbd70f74fb3b-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  261: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f934dd2030f5740cde0aa2697a105a9-Abstract-Conference.html',\n",
       "  262: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f93c3e9b557980d93016671acd94bd2-Abstract-Conference.html',\n",
       "  263: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f94c552e5fe82bc152494985e34bd48-Abstract-Conference.html',\n",
       "  264: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f98645119923217a245735c2c4d23f4-Abstract-Conference.html',\n",
       "  265: 'https://papers.nips.cc//paper_files/paper/2024/hash/0f9e0309d8a947ca44463a9b7e8b6a3f-Abstract-Conference.html',\n",
       "  266: 'https://papers.nips.cc//paper_files/paper/2024/hash/0faa0019b0a8fcab8e6476bc43078e2e-Abstract-Conference.html',\n",
       "  267: 'https://papers.nips.cc//paper_files/paper/2024/hash/0fbbc5129cafcee8530223b8565561ac-Abstract-Conference.html',\n",
       "  268: 'https://papers.nips.cc//paper_files/paper/2024/hash/0fd17409385ab9304e5019c6a6eb327a-Abstract-Conference.html',\n",
       "  269: 'https://papers.nips.cc//paper_files/paper/2024/hash/0fd3d8093b3ba73d19b393a1326fdba7-Abstract-Conference.html',\n",
       "  270: 'https://papers.nips.cc//paper_files/paper/2024/hash/0fd489e5e393f61b355be86ed4c24a54-Abstract-Conference.html',\n",
       "  271: 'https://papers.nips.cc//paper_files/paper/2024/hash/0fd4ce94d29be88a5a262a2c77a18f47-Abstract-Conference.html',\n",
       "  272: 'https://papers.nips.cc//paper_files/paper/2024/hash/0fd5675f49141c79ad22d7a533c89b12-Abstract-Conference.html',\n",
       "  273: 'https://papers.nips.cc//paper_files/paper/2024/hash/0fe6a18be9491139fb759e2f645374b1-Abstract-Conference.html',\n",
       "  274: 'https://papers.nips.cc//paper_files/paper/2024/hash/0ff38d72a2e0aa6dbe42de83a17b2223-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  275: 'https://papers.nips.cc//paper_files/paper/2024/hash/10272bfd0371ef960ec557ed6c866058-Abstract-Conference.html',\n",
       "  276: 'https://papers.nips.cc//paper_files/paper/2024/hash/1055c730c7098c04579beb526c8cd4ba-Abstract-Conference.html',\n",
       "  277: 'https://papers.nips.cc//paper_files/paper/2024/hash/1057053100de064a44286239724f7865-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  278: 'https://papers.nips.cc//paper_files/paper/2024/hash/10826a1a80f816ea98d559d7c7a97973-Abstract-Conference.html',\n",
       "  279: 'https://papers.nips.cc//paper_files/paper/2024/hash/10a3b1c30b8cceb507b9e8ddcc9a1a6a-Abstract-Conference.html',\n",
       "  280: 'https://papers.nips.cc//paper_files/paper/2024/hash/10a6bdcabbd5a3d36b760daa295f63c1-Abstract-Conference.html',\n",
       "  281: 'https://papers.nips.cc//paper_files/paper/2024/hash/10bf96894abaf4c293b205709a98fc74-Abstract-Conference.html',\n",
       "  282: 'https://papers.nips.cc//paper_files/paper/2024/hash/10c456d2160517581a234dfde15a7505-Abstract-Conference.html',\n",
       "  283: 'https://papers.nips.cc//paper_files/paper/2024/hash/10d52f5d2ef0f69ac10da7c962fb6db9-Abstract-Conference.html',\n",
       "  284: 'https://papers.nips.cc//paper_files/paper/2024/hash/10e6dfea9a673bef4a7b1cb9234891bc-Abstract-Conference.html',\n",
       "  285: 'https://papers.nips.cc//paper_files/paper/2024/hash/10e9204f14c4daa08041343455435308-Abstract-Conference.html',\n",
       "  286: 'https://papers.nips.cc//paper_files/paper/2024/hash/10f1737aa6347ccc555ac068e1b45523-Abstract-Conference.html',\n",
       "  287: 'https://papers.nips.cc//paper_files/paper/2024/hash/10f34ee79b62627b7ebf6279d35ea480-Abstract-Conference.html',\n",
       "  288: 'https://papers.nips.cc//paper_files/paper/2024/hash/10fc83943b4540a9524af6fc67a23fef-Abstract-Conference.html',\n",
       "  289: 'https://papers.nips.cc//paper_files/paper/2024/hash/1119587863e78451f080da2a768c4935-Abstract-Conference.html',\n",
       "  290: 'https://papers.nips.cc//paper_files/paper/2024/hash/113e6f1d94af5df4f306fbcb3f82339f-Abstract-Conference.html',\n",
       "  291: 'https://papers.nips.cc//paper_files/paper/2024/hash/116a952bc0a8cb03113408c1a215be7c-Abstract-Conference.html',\n",
       "  292: 'https://papers.nips.cc//paper_files/paper/2024/hash/11715d433f6f8b9106baae0df023deb3-Abstract-Conference.html',\n",
       "  293: 'https://papers.nips.cc//paper_files/paper/2024/hash/1198b53fa686831d5f0c0860d7ec4f34-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  294: 'https://papers.nips.cc//paper_files/paper/2024/hash/119b45b5c2020d6bc9bca1e42826a2b3-Abstract-Conference.html',\n",
       "  295: 'https://papers.nips.cc//paper_files/paper/2024/hash/11c6625b0481a7d5625831369f6b7c82-Abstract-Conference.html',\n",
       "  296: 'https://papers.nips.cc//paper_files/paper/2024/hash/11c892a9fcc430cc0f4c7d457e5d60ea-Abstract-Conference.html',\n",
       "  297: 'https://papers.nips.cc//paper_files/paper/2024/hash/11f5520daf9132775e8604e89f53925a-Abstract-Conference.html',\n",
       "  298: 'https://papers.nips.cc//paper_files/paper/2024/hash/11faf17bf7e5412d9cded369f97db23d-Abstract-Conference.html',\n",
       "  299: 'https://papers.nips.cc//paper_files/paper/2024/hash/12143893d9d37c3569dda800b95cabd9-Abstract-Conference.html',\n",
       "  300: 'https://papers.nips.cc//paper_files/paper/2024/hash/122ea6470232ee5e79a2649243348005-Abstract-Conference.html',\n",
       "  301: 'https://papers.nips.cc//paper_files/paper/2024/hash/123a18dfd821c8b440f42a00a27648d6-Abstract-Conference.html',\n",
       "  302: 'https://papers.nips.cc//paper_files/paper/2024/hash/123cfe7d8b7702ac97aaf4468fc05fa5-Abstract-Conference.html',\n",
       "  303: 'https://papers.nips.cc//paper_files/paper/2024/hash/123fd8a56501194823c8e0dca00733df-Abstract-Conference.html',\n",
       "  304: 'https://papers.nips.cc//paper_files/paper/2024/hash/124256ed80af5d4bf4c4de17b66c4298-Abstract-Conference.html',\n",
       "  305: 'https://papers.nips.cc//paper_files/paper/2024/hash/126784e4d5a92afff92d13aee155554b-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  306: 'https://papers.nips.cc//paper_files/paper/2024/hash/128911cc894d57bcae78074a9551c132-Abstract-Conference.html',\n",
       "  307: 'https://papers.nips.cc//paper_files/paper/2024/hash/12b18a15dcd73e1991e9959a94375fab-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  308: 'https://papers.nips.cc//paper_files/paper/2024/hash/12ba5de27afcff1a5c796de4a6392154-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  309: 'https://papers.nips.cc//paper_files/paper/2024/hash/12c118ef87fde56a10bd858842781b34-Abstract-Conference.html',\n",
       "  310: 'https://papers.nips.cc//paper_files/paper/2024/hash/12d286282e1be5431ea05262a21f415c-Abstract-Conference.html',\n",
       "  311: 'https://papers.nips.cc//paper_files/paper/2024/hash/12d3e63be5574088f7c1bbc9162060bf-Abstract-Conference.html',\n",
       "  312: 'https://papers.nips.cc//paper_files/paper/2024/hash/12d7ba753894ed348904df1bf0ce02ec-Abstract-Conference.html',\n",
       "  313: 'https://papers.nips.cc//paper_files/paper/2024/hash/12ffe4499085e9a51beb02441212e26b-Abstract-Conference.html',\n",
       "  314: 'https://papers.nips.cc//paper_files/paper/2024/hash/13183a224208671a6fc33ba1aa661ec4-Abstract-Conference.html',\n",
       "  315: 'https://papers.nips.cc//paper_files/paper/2024/hash/13250eb13871b3c2c0a0667b54bad165-Abstract-Conference.html',\n",
       "  316: 'https://papers.nips.cc//paper_files/paper/2024/hash/1326b385e39647c733e4e81edc1d6670-Abstract-Conference.html',\n",
       "  317: 'https://papers.nips.cc//paper_files/paper/2024/hash/1343e23bc2d34c054040e73ad86582cf-Abstract-Conference.html',\n",
       "  318: 'https://papers.nips.cc//paper_files/paper/2024/hash/134ed7a477770f227f12450ef0cbb8f4-Abstract-Conference.html',\n",
       "  319: 'https://papers.nips.cc//paper_files/paper/2024/hash/136b9a13861308c8948cd308ccd02658-Abstract-Conference.html',\n",
       "  320: 'https://papers.nips.cc//paper_files/paper/2024/hash/13707aad517ddd6c09ea02e0f55e1e7a-Abstract-Conference.html',\n",
       "  321: 'https://papers.nips.cc//paper_files/paper/2024/hash/137101016144540ed3191dc2b02f09a5-Abstract-Conference.html',\n",
       "  322: 'https://papers.nips.cc//paper_files/paper/2024/hash/137cc5c0b8a3c932805e3c14812070ca-Abstract-Conference.html',\n",
       "  323: 'https://papers.nips.cc//paper_files/paper/2024/hash/13836f251823945316ae067350a5c366-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  324: 'https://papers.nips.cc//paper_files/paper/2024/hash/13848b5893119ff772b69812c95914fa-Abstract-Conference.html',\n",
       "  325: 'https://papers.nips.cc//paper_files/paper/2024/hash/13cd22c32c1330decd69c13cf8cadc0a-Abstract-Conference.html',\n",
       "  326: 'https://papers.nips.cc//paper_files/paper/2024/hash/13d7f172259b11b230cc5da8768abc5f-Abstract-Conference.html',\n",
       "  327: 'https://papers.nips.cc//paper_files/paper/2024/hash/13e8be77982beb73d7ed0bbf122f9f3c-Abstract-Conference.html',\n",
       "  328: 'https://papers.nips.cc//paper_files/paper/2024/hash/13f972adf12bdf886583d48cd528002f-Abstract-Conference.html',\n",
       "  329: 'https://papers.nips.cc//paper_files/paper/2024/hash/1403ab1a427050538ec59c7f570aec8b-Abstract-Conference.html',\n",
       "  330: 'https://papers.nips.cc//paper_files/paper/2024/hash/140aac600566125915df7e74ff538f66-Abstract-Conference.html',\n",
       "  331: 'https://papers.nips.cc//paper_files/paper/2024/hash/140edeced4facf41b14ca4f71a2322b7-Abstract-Conference.html',\n",
       "  332: 'https://papers.nips.cc//paper_files/paper/2024/hash/1419d8554191a65ea4f2d8e1057973e4-Abstract-Conference.html',\n",
       "  333: 'https://papers.nips.cc//paper_files/paper/2024/hash/142bff4f4c01dd55c4309860ff3a59f1-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  334: 'https://papers.nips.cc//paper_files/paper/2024/hash/1435d2d0fca85a84d83ddcb754f58c29-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  335: 'https://papers.nips.cc//paper_files/paper/2024/hash/143ea4a156ef64f32d4d905206cf32e1-Abstract-Conference.html',\n",
       "  336: 'https://papers.nips.cc//paper_files/paper/2024/hash/1456560769bbc38e4f8c5055048ea712-Abstract-Conference.html',\n",
       "  337: 'https://papers.nips.cc//paper_files/paper/2024/hash/1468ecc3d7e9dc2fbf336eed9bb292e0-Abstract-Conference.html',\n",
       "  338: 'https://papers.nips.cc//paper_files/paper/2024/hash/14730e0dd6ac1c4a5765310909fd51b1-Abstract-Conference.html',\n",
       "  339: 'https://papers.nips.cc//paper_files/paper/2024/hash/1486bbd863ba396398ff4227c5b3dccd-Abstract-Conference.html',\n",
       "  340: 'https://papers.nips.cc//paper_files/paper/2024/hash/149578235c90954e4f10e40fa181918f-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  341: 'https://papers.nips.cc//paper_files/paper/2024/hash/1496b12ccd6c8d2b4fd98f24a12bd438-Abstract-Conference.html',\n",
       "  342: 'https://papers.nips.cc//paper_files/paper/2024/hash/149ad6e32c08b73a3ecc3d11977fcc47-Abstract-Conference.html',\n",
       "  343: 'https://papers.nips.cc//paper_files/paper/2024/hash/14a1f12a530a934dc034f4c1e2d97aa8-Abstract-Conference.html',\n",
       "  344: 'https://papers.nips.cc//paper_files/paper/2024/hash/14ad9256c430e6c8977e470d8e268320-Abstract-Conference.html',\n",
       "  345: 'https://papers.nips.cc//paper_files/paper/2024/hash/14bb27f680bee45d83bc769738e7f9b5-Abstract-Conference.html',\n",
       "  346: 'https://papers.nips.cc//paper_files/paper/2024/hash/14bc8528848d15d2d096127d0f64c1f9-Abstract-Conference.html',\n",
       "  347: 'https://papers.nips.cc//paper_files/paper/2024/hash/14c00f4bc19a5498982b16647998e894-Abstract-Conference.html',\n",
       "  348: 'https://papers.nips.cc//paper_files/paper/2024/hash/14c018d2e72c521605b0567029ef0efb-Abstract-Conference.html',\n",
       "  349: 'https://papers.nips.cc//paper_files/paper/2024/hash/14cdc9013d80338bf81483a7736ea05c-Abstract-Conference.html',\n",
       "  350: 'https://papers.nips.cc//paper_files/paper/2024/hash/14d2d59c5e2302bfa6ce6cae59156e33-Abstract-Conference.html',\n",
       "  351: 'https://papers.nips.cc//paper_files/paper/2024/hash/14da7aea05debb963b3d8d46449d51a0-Abstract-Conference.html',\n",
       "  352: 'https://papers.nips.cc//paper_files/paper/2024/hash/14fc4a68da97a3d31eb11c642b0b10fc-Abstract-Conference.html',\n",
       "  353: 'https://papers.nips.cc//paper_files/paper/2024/hash/14fef58f09f2ebe69306e0a322e3be2b-Abstract-Conference.html',\n",
       "  354: 'https://papers.nips.cc//paper_files/paper/2024/hash/150f35763dc51bfc269690d36a5a7c88-Abstract-Conference.html',\n",
       "  355: 'https://papers.nips.cc//paper_files/paper/2024/hash/150fe800c9e6ed36d16f6a157eedb6e1-Abstract-Conference.html',\n",
       "  356: 'https://papers.nips.cc//paper_files/paper/2024/hash/152035ddc7b4f35cf7ede4125c39ea4a-Abstract-Conference.html',\n",
       "  357: 'https://papers.nips.cc//paper_files/paper/2024/hash/1522f1d7ee96fafd08244f0def8a1c05-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  358: 'https://papers.nips.cc//paper_files/paper/2024/hash/1529a05fdff470f4e7c239c80d85e28e-Abstract-Conference.html',\n",
       "  359: 'https://papers.nips.cc//paper_files/paper/2024/hash/1533b2d13d0e0078fd193ec78ac3f8a5-Abstract-Conference.html',\n",
       "  360: 'https://papers.nips.cc//paper_files/paper/2024/hash/154926e0b66e2b2a8c1120852f31a12d-Abstract-Conference.html',\n",
       "  361: 'https://papers.nips.cc//paper_files/paper/2024/hash/154bfcc104885ad8e0bfeb829a1839e8-Abstract-Conference.html',\n",
       "  362: 'https://papers.nips.cc//paper_files/paper/2024/hash/1551c01d7a3d0bf21e2518331e9f7074-Abstract-Conference.html',\n",
       "  363: 'https://papers.nips.cc//paper_files/paper/2024/hash/155a94c71f0a2a3cb7eacbf733b5c64b-Abstract-Conference.html',\n",
       "  364: 'https://papers.nips.cc//paper_files/paper/2024/hash/155ace40b55215479b6ad4665d1f6c06-Abstract-Conference.html',\n",
       "  365: 'https://papers.nips.cc//paper_files/paper/2024/hash/1568882ba1a50316e87852542523739c-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  366: 'https://papers.nips.cc//paper_files/paper/2024/hash/15807b6e09d691fe5e96cdecde6d7b80-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  367: 'https://papers.nips.cc//paper_files/paper/2024/hash/1582eaf9e0cf349e1e5a6ee453100aa1-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  368: 'https://papers.nips.cc//paper_files/paper/2024/hash/1588dc2b2ef339d6e4c47d212e36f991-Abstract-Conference.html',\n",
       "  369: 'https://papers.nips.cc//paper_files/paper/2024/hash/158ac5698e36a01ee5ca9e6732685b34-Abstract-Conference.html',\n",
       "  370: 'https://papers.nips.cc//paper_files/paper/2024/hash/158f036baa5b80a4fe2af094de8f7539-Abstract-Conference.html',\n",
       "  371: 'https://papers.nips.cc//paper_files/paper/2024/hash/15aaa9224a35527d76188b4d40e02308-Abstract-Conference.html',\n",
       "  372: 'https://papers.nips.cc//paper_files/paper/2024/hash/15add6732964d5b1f0954058bf3ccc88-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  373: 'https://papers.nips.cc//paper_files/paper/2024/hash/15b780350b302a1bf9a3bd273f5c15a4-Abstract-Conference.html',\n",
       "  374: 'https://papers.nips.cc//paper_files/paper/2024/hash/15ba84c1e19b0eb75f96922f5da0a021-Abstract-Conference.html',\n",
       "  375: 'https://papers.nips.cc//paper_files/paper/2024/hash/15cc8e4a46565dab0c1a1220884bd503-Abstract-Conference.html',\n",
       "  376: 'https://papers.nips.cc//paper_files/paper/2024/hash/15e5fccdfeb10bb54f8e74944de1c8bf-Abstract-Conference.html',\n",
       "  377: 'https://papers.nips.cc//paper_files/paper/2024/hash/15f4cefb0e143c7ad9d40e879b0a9d0c-Abstract-Conference.html',\n",
       "  378: 'https://papers.nips.cc//paper_files/paper/2024/hash/15f80ec0fed53885d2ca6272edb96ede-Abstract-Conference.html',\n",
       "  379: 'https://papers.nips.cc//paper_files/paper/2024/hash/16009ce3d8a6872d79f056c75618911d-Abstract-Conference.html',\n",
       "  380: 'https://papers.nips.cc//paper_files/paper/2024/hash/16336d94a5ffca8de019087ab7fe403f-Abstract-Conference.html',\n",
       "  381: 'https://papers.nips.cc//paper_files/paper/2024/hash/163b048741e1deea2b3d9a46c2c88af3-Abstract-Conference.html',\n",
       "  382: 'https://papers.nips.cc//paper_files/paper/2024/hash/1646e34971facbcda3727d1dc28ab635-Abstract-Conference.html',\n",
       "  383: 'https://papers.nips.cc//paper_files/paper/2024/hash/1651f1ea5ee6213e301a89f222f3fec7-Abstract-Conference.html',\n",
       "  384: 'https://papers.nips.cc//paper_files/paper/2024/hash/16659e412de3965fa195ddb9f2c4b356-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  385: 'https://papers.nips.cc//paper_files/paper/2024/hash/167bcf2af2cd08fcf75b932022db0311-Abstract-Conference.html',\n",
       "  386: 'https://papers.nips.cc//paper_files/paper/2024/hash/167cb476aab527e23741e314be80a4ea-Abstract-Conference.html',\n",
       "  387: 'https://papers.nips.cc//paper_files/paper/2024/hash/1697e3fb412da11dc9488249f9e7bbc9-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  388: 'https://papers.nips.cc//paper_files/paper/2024/hash/16986b69068fbe6acf64eb6566519c74-Abstract-Conference.html',\n",
       "  389: 'https://papers.nips.cc//paper_files/paper/2024/hash/16bce4070c4e23434451b180348e3814-Abstract-Conference.html',\n",
       "  390: 'https://papers.nips.cc//paper_files/paper/2024/hash/16c5b4102a6b6eb061e502ce6736ad8a-Abstract-Conference.html',\n",
       "  391: 'https://papers.nips.cc//paper_files/paper/2024/hash/16c628ab12dc4caca8e7712affa6c767-Abstract-Conference.html',\n",
       "  392: 'https://papers.nips.cc//paper_files/paper/2024/hash/16ce2c09a2ec6a162bee47f09659e366-Abstract-Conference.html',\n",
       "  393: 'https://papers.nips.cc//paper_files/paper/2024/hash/16e18fa3b3add076c30f2a2598f03031-Abstract-Conference.html',\n",
       "  394: 'https://papers.nips.cc//paper_files/paper/2024/hash/16f8a0852b31bc9dc791ecf313247a57-Abstract-Conference.html',\n",
       "  395: 'https://papers.nips.cc//paper_files/paper/2024/hash/1700ad4e6252e8f2955909f96367b34d-Abstract-Conference.html',\n",
       "  396: 'https://papers.nips.cc//paper_files/paper/2024/hash/1704ddd0bb89f159dfe609b32c889995-Abstract-Conference.html',\n",
       "  397: 'https://papers.nips.cc//paper_files/paper/2024/hash/1704fe7aaff33a54802b83a016050ab8-Abstract-Conference.html',\n",
       "  398: 'https://papers.nips.cc//paper_files/paper/2024/hash/170dc3e41f2d03e327e04dbab0fccbfb-Abstract-Conference.html',\n",
       "  399: 'https://papers.nips.cc//paper_files/paper/2024/hash/171291d8fed723c6dfc76330aa827ff8-Abstract-Conference.html',\n",
       "  400: 'https://papers.nips.cc//paper_files/paper/2024/hash/1716d022edeac750e57a2986a7135e13-Abstract-Conference.html',\n",
       "  401: 'https://papers.nips.cc//paper_files/paper/2024/hash/172be8b0b88fc2b4aee74237d43f8c04-Abstract-Conference.html',\n",
       "  402: 'https://papers.nips.cc//paper_files/paper/2024/hash/173e4732a89fab9fb225203f35996677-Abstract-Conference.html',\n",
       "  403: 'https://papers.nips.cc//paper_files/paper/2024/hash/1741917e3df34daa1a4c564e2980bb59-Abstract-Conference.html',\n",
       "  404: 'https://papers.nips.cc//paper_files/paper/2024/hash/175fa4fc8f275f877ec85340131c5d7a-Abstract-Conference.html',\n",
       "  405: 'https://papers.nips.cc//paper_files/paper/2024/hash/17790fc9b4828206f99a0dea35da3657-Abstract-Conference.html',\n",
       "  406: 'https://papers.nips.cc//paper_files/paper/2024/hash/178022c409938a9d634b88ce924c4b14-Abstract-Conference.html',\n",
       "  407: 'https://papers.nips.cc//paper_files/paper/2024/hash/1780db9272f81961459f4906036ebc6f-Abstract-Conference.html',\n",
       "  408: 'https://papers.nips.cc//paper_files/paper/2024/hash/1787533e171dcc8549cc2eb5a4840eec-Abstract-Conference.html',\n",
       "  409: 'https://papers.nips.cc//paper_files/paper/2024/hash/178ae4ba29022eb7bf509c2e27bc8ab8-Abstract-Conference.html',\n",
       "  410: 'https://papers.nips.cc//paper_files/paper/2024/hash/178b306c7ee66a66db2171646e17da36-Abstract-Conference.html',\n",
       "  411: 'https://papers.nips.cc//paper_files/paper/2024/hash/178b6cd141e003fa5ff808c7d7d8e2cc-Abstract-Conference.html',\n",
       "  412: 'https://papers.nips.cc//paper_files/paper/2024/hash/178de34a090423c359a0ca2d51a25706-Abstract-Conference.html',\n",
       "  413: 'https://papers.nips.cc//paper_files/paper/2024/hash/178f4666a84ecdd61e3b85145ed56484-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  414: 'https://papers.nips.cc//paper_files/paper/2024/hash/179a2b276b954a1369ec9119c4badabb-Abstract-Conference.html',\n",
       "  415: 'https://papers.nips.cc//paper_files/paper/2024/hash/179f5dcdeedc149443ebd3ba70811dbd-Abstract-Conference.html',\n",
       "  416: 'https://papers.nips.cc//paper_files/paper/2024/hash/17a1a1439421f1837e10cd612bf92861-Abstract-Conference.html',\n",
       "  417: 'https://papers.nips.cc//paper_files/paper/2024/hash/17af43527227c5c96db0f8d4c6aadc4e-Abstract-Conference.html',\n",
       "  418: 'https://papers.nips.cc//paper_files/paper/2024/hash/17b08a9de93e2accf13429643e7eafdc-Abstract-Conference.html',\n",
       "  419: 'https://papers.nips.cc//paper_files/paper/2024/hash/17bae482066287ce54814514cf12f248-Abstract-Conference.html',\n",
       "  420: 'https://papers.nips.cc//paper_files/paper/2024/hash/17d25665bf6f46b7b3d32bd5cad3cbb2-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  421: 'https://papers.nips.cc//paper_files/paper/2024/hash/17d60fef592086d1a5cb136f1946df59-Abstract-Conference.html',\n",
       "  422: 'https://papers.nips.cc//paper_files/paper/2024/hash/17f21a0a111c9e3a4cd96708d30064f2-Abstract-Conference.html',\n",
       "  423: 'https://papers.nips.cc//paper_files/paper/2024/hash/17fc467c11997914127c001fdc801bea-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  424: 'https://papers.nips.cc//paper_files/paper/2024/hash/18023809c155d6bbed27e443043cdebf-Abstract-Conference.html',\n",
       "  425: 'https://papers.nips.cc//paper_files/paper/2024/hash/180a476f22a52b8ef14f42b2b927afcc-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  426: 'https://papers.nips.cc//paper_files/paper/2024/hash/180d4373aca26bd86bf45fc50d1a709f-Abstract-Conference.html',\n",
       "  427: 'https://papers.nips.cc//paper_files/paper/2024/hash/180f6184a3458fa19c28c5483bc61877-Abstract-Conference.html',\n",
       "  428: 'https://papers.nips.cc//paper_files/paper/2024/hash/1812042b83f20707a898ff6f8af7db84-Abstract-Conference.html',\n",
       "  429: 'https://papers.nips.cc//paper_files/paper/2024/hash/18595bc3e802a3b11035927fd928eb9c-Abstract-Conference.html',\n",
       "  430: 'https://papers.nips.cc//paper_files/paper/2024/hash/185a120a3f709187e68bd092e6098851-Abstract-Conference.html',\n",
       "  431: 'https://papers.nips.cc//paper_files/paper/2024/hash/1862ea85f21ca819eed2b75af39bba60-Abstract-Conference.html',\n",
       "  432: 'https://papers.nips.cc//paper_files/paper/2024/hash/1867748a011e1425b924ec72a4066b62-Abstract-Conference.html',\n",
       "  433: 'https://papers.nips.cc//paper_files/paper/2024/hash/187d94b3c93343f0e925b5cf729eadd5-Abstract-Conference.html',\n",
       "  434: 'https://papers.nips.cc//paper_files/paper/2024/hash/189699197ead7a012c7fa4cdc4cc413d-Abstract-Conference.html',\n",
       "  435: 'https://papers.nips.cc//paper_files/paper/2024/hash/18aee41e1bb41bbb8fee53cfff8138b7-Abstract-Conference.html',\n",
       "  436: 'https://papers.nips.cc//paper_files/paper/2024/hash/18b0b4c788c8f2cf6c2943b989ad18c8-Abstract-Conference.html',\n",
       "  437: 'https://papers.nips.cc//paper_files/paper/2024/hash/18c0102cb7f1a02c14f0929089b2e576-Abstract-Conference.html',\n",
       "  438: 'https://papers.nips.cc//paper_files/paper/2024/hash/18c669b80d1a8f589713b768bc8fe9a4-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  439: 'https://papers.nips.cc//paper_files/paper/2024/hash/18d3a2f3068d6c669dcae19ceca1bc24-Abstract-Conference.html',\n",
       "  440: 'https://papers.nips.cc//paper_files/paper/2024/hash/192956d4857000578f626c5193b34419-Abstract-Conference.html',\n",
       "  441: 'https://papers.nips.cc//paper_files/paper/2024/hash/19305d2dbcc81c44d4a0120e7569856e-Abstract-Conference.html',\n",
       "  442: 'https://papers.nips.cc//paper_files/paper/2024/hash/1943190d92efee1fccfc8d6579b0f8d9-Abstract-Conference.html',\n",
       "  443: 'https://papers.nips.cc//paper_files/paper/2024/hash/194fa4536bf36f35a4505d20cd5dd6fc-Abstract-Conference.html',\n",
       "  444: 'https://papers.nips.cc//paper_files/paper/2024/hash/195c4e2910bedabd813f9fe4e70c642c-Abstract-Conference.html',\n",
       "  445: 'https://papers.nips.cc//paper_files/paper/2024/hash/19774ce2d4b0d17a3a8aea26ad99fe8a-Abstract-Conference.html',\n",
       "  446: 'https://papers.nips.cc//paper_files/paper/2024/hash/19a42d5885e25e51852aca8144e5af0d-Abstract-Conference.html',\n",
       "  447: 'https://papers.nips.cc//paper_files/paper/2024/hash/19a94fdf9e1c5b387830e4c4fef6972a-Abstract-Conference.html',\n",
       "  448: 'https://papers.nips.cc//paper_files/paper/2024/hash/19ae2b95d3831c14373271112f189a22-Abstract-Conference.html',\n",
       "  449: 'https://papers.nips.cc//paper_files/paper/2024/hash/19ba2b9448d5de25826f6eb408dab194-Abstract-Conference.html',\n",
       "  450: 'https://papers.nips.cc//paper_files/paper/2024/hash/19c145aaad40927c51f4d10eaa339c20-Abstract-Conference.html',\n",
       "  451: 'https://papers.nips.cc//paper_files/paper/2024/hash/19c9708f31ec44b5b1cbd67f91d05d95-Abstract-Conference.html',\n",
       "  452: 'https://papers.nips.cc//paper_files/paper/2024/hash/19cdab1dee61d55158cf106244ceab08-Abstract-Conference.html',\n",
       "  453: 'https://papers.nips.cc//paper_files/paper/2024/hash/19ded4cfc36a7feb7fce975393d378fd-Abstract-Conference.html',\n",
       "  454: 'https://papers.nips.cc//paper_files/paper/2024/hash/19e4ea30dded58259665db375885e412-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  455: 'https://papers.nips.cc//paper_files/paper/2024/hash/19e9a88d91917775b34fdad447ed8908-Abstract-Conference.html',\n",
       "  456: 'https://papers.nips.cc//paper_files/paper/2024/hash/19f10adb6749b0c9f1ff7610bd01d44d-Abstract-Conference.html',\n",
       "  457: 'https://papers.nips.cc//paper_files/paper/2024/hash/19f461132d8ed04764f58f88b7a8e842-Abstract-Conference.html',\n",
       "  458: 'https://papers.nips.cc//paper_files/paper/2024/hash/19f7f755908372efb25826d61959cdf9-Abstract-Conference.html',\n",
       "  459: 'https://papers.nips.cc//paper_files/paper/2024/hash/1a000ee0f122d0bbd3edb9bf55170ea3-Abstract-Conference.html',\n",
       "  460: 'https://papers.nips.cc//paper_files/paper/2024/hash/1a134b50202088aa8c595cc99b310e5a-Abstract-Conference.html',\n",
       "  461: 'https://papers.nips.cc//paper_files/paper/2024/hash/1a22b912945fb7c0bdd079e792b31b6f-Abstract-Conference.html',\n",
       "  462: 'https://papers.nips.cc//paper_files/paper/2024/hash/1a63a6a092a95bd45f0237766ac878ba-Abstract-Conference.html',\n",
       "  463: 'https://papers.nips.cc//paper_files/paper/2024/hash/1a675d804f50509b8e21d0d3ca709d03-Abstract-Conference.html',\n",
       "  464: 'https://papers.nips.cc//paper_files/paper/2024/hash/1a6d49c1a298ebb799d005b7b90ab31d-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  465: 'https://papers.nips.cc//paper_files/paper/2024/hash/1a8189929f3d7bd6183718f42c3f4309-Abstract-Conference.html',\n",
       "  466: 'https://papers.nips.cc//paper_files/paper/2024/hash/1a8d295871250443f9747d239925b89d-Abstract-Conference.html',\n",
       "  467: 'https://papers.nips.cc//paper_files/paper/2024/hash/1a958c6e813989a0d4b677e6a6f3339a-Abstract-Conference.html',\n",
       "  468: 'https://papers.nips.cc//paper_files/paper/2024/hash/1a970a3e62ac31c76ec3cea3a9f68fdf-Abstract-Conference.html',\n",
       "  469: 'https://papers.nips.cc//paper_files/paper/2024/hash/1abed6ee581b9ceb4e2ddf37822c7fcb-Abstract-Conference.html',\n",
       "  470: 'https://papers.nips.cc//paper_files/paper/2024/hash/1ac14e44228aeadabb3c934822c1212a-Abstract-Conference.html',\n",
       "  471: 'https://papers.nips.cc//paper_files/paper/2024/hash/1ac3030fc57850b0fb11dfe9d4880ad7-Abstract-Conference.html',\n",
       "  472: 'https://papers.nips.cc//paper_files/paper/2024/hash/1ac83203e88eb6cf6b30642f0239b932-Abstract-Conference.html',\n",
       "  473: 'https://papers.nips.cc//paper_files/paper/2024/hash/1adeeac24ce6168e20bcee85645720e9-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  474: 'https://papers.nips.cc//paper_files/paper/2024/hash/1ae3fa230adc0c9ac6a81f4b88dcc7ff-Abstract-Conference.html',\n",
       "  475: 'https://papers.nips.cc//paper_files/paper/2024/hash/1af3e0bf5905e33789979f666c31192d-Abstract-Conference.html',\n",
       "  476: 'https://papers.nips.cc//paper_files/paper/2024/hash/1b0da24d136f46bfaee78e8da907127e-Abstract-Conference.html',\n",
       "  477: 'https://papers.nips.cc//paper_files/paper/2024/hash/1b10264c77a2a1e0ef8abfbd68d36583-Abstract-Conference.html',\n",
       "  478: 'https://papers.nips.cc//paper_files/paper/2024/hash/1b2df10d5bc3ca563339c801fa2e14db-Abstract-Conference.html',\n",
       "  479: 'https://papers.nips.cc//paper_files/paper/2024/hash/1b33deb9e1e7c31f05300b1c2d4a4f7d-Abstract-Conference.html',\n",
       "  480: 'https://papers.nips.cc//paper_files/paper/2024/hash/1b3750390ca8b931fb9ca988647940cb-Abstract-Conference.html',\n",
       "  481: 'https://papers.nips.cc//paper_files/paper/2024/hash/1b57aaddf85ab01a2445a79c9edc1f4b-Abstract-Conference.html',\n",
       "  482: 'https://papers.nips.cc//paper_files/paper/2024/hash/1b60893887328a6a50dd00ae0d5ed51a-Abstract-Conference.html',\n",
       "  483: 'https://papers.nips.cc//paper_files/paper/2024/hash/1b6811b37b0d9fd49a8fefd288810a94-Abstract-Conference.html',\n",
       "  484: 'https://papers.nips.cc//paper_files/paper/2024/hash/1b8726b572e0dfa72793f9f6590664fd-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  485: 'https://papers.nips.cc//paper_files/paper/2024/hash/1b96f01343ff10150e6719eb163e1536-Abstract-Conference.html',\n",
       "  486: 'https://papers.nips.cc//paper_files/paper/2024/hash/1b99db17b54735d22dbed15c24f2dbdc-Abstract-Conference.html',\n",
       "  487: 'https://papers.nips.cc//paper_files/paper/2024/hash/1bbfea488a8968e2d3c6565639b08e5e-Abstract-Conference.html',\n",
       "  488: 'https://papers.nips.cc//paper_files/paper/2024/hash/1bd359b32ab8b2a6bbafa1ed2856cf40-Abstract-Conference.html',\n",
       "  489: 'https://papers.nips.cc//paper_files/paper/2024/hash/1bd8a2967508c5cf068ac204151b6ecd-Abstract-Conference.html',\n",
       "  490: 'https://papers.nips.cc//paper_files/paper/2024/hash/1bdcb065d40203a00bd39831153338bb-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  491: 'https://papers.nips.cc//paper_files/paper/2024/hash/1bf4cad47f5a54c98fbe7d10516ebf77-Abstract-Conference.html',\n",
       "  492: 'https://papers.nips.cc//paper_files/paper/2024/hash/1c0d54ebd0a6e58c4eca7d591e374b9d-Abstract-Conference.html',\n",
       "  493: 'https://papers.nips.cc//paper_files/paper/2024/hash/1c1b099a19621e4bd2753ac572e1dbd5-Abstract-Conference.html',\n",
       "  494: 'https://papers.nips.cc//paper_files/paper/2024/hash/1c2b1c8f7d317719a9ce32dd7386ba35-Abstract-Conference.html',\n",
       "  495: 'https://papers.nips.cc//paper_files/paper/2024/hash/1c2e08b3af7fee5cdc2755c654738c33-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  496: 'https://papers.nips.cc//paper_files/paper/2024/hash/1c32452f112719f7c1db6d983d060f78-Abstract-Conference.html',\n",
       "  497: 'https://papers.nips.cc//paper_files/paper/2024/hash/1c562586c1cb1da0547e92a9612879bc-Abstract-Conference.html',\n",
       "  498: 'https://papers.nips.cc//paper_files/paper/2024/hash/1c9c85bae6161d52182d0fe2f3640512-Abstract-Conference.html',\n",
       "  499: 'https://papers.nips.cc//paper_files/paper/2024/hash/1caf09c9f4e6b0150b06a07e77f2710c-Abstract-Conference.html',\n",
       "  500: 'https://papers.nips.cc//paper_files/paper/2024/hash/1cb57fcf7ff3f6d37eebae5becc9ea6d-Abstract-Conference.html',\n",
       "  501: 'https://papers.nips.cc//paper_files/paper/2024/hash/1cb5b3d64bdf3c6642c8d9a8fbecd019-Abstract-Conference.html',\n",
       "  502: 'https://papers.nips.cc//paper_files/paper/2024/hash/1cba8502063fab9df252a63968691768-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  503: 'https://papers.nips.cc//paper_files/paper/2024/hash/1cc12fb3d4033ad72d33a51f1d0ab5d0-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  504: 'https://papers.nips.cc//paper_files/paper/2024/hash/1cc8db5884a7474b4771762b6f0c8ee1-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  505: 'https://papers.nips.cc//paper_files/paper/2024/hash/1cded4f97cf5f01a284c574110b7e3b9-Abstract-Conference.html',\n",
       "  506: 'https://papers.nips.cc//paper_files/paper/2024/hash/1ce12df021f8fb43cf531c556efcebdc-Abstract-Conference.html',\n",
       "  507: 'https://papers.nips.cc//paper_files/paper/2024/hash/1cf760a547822e2b8276881ad45f0fe9-Abstract-Conference.html',\n",
       "  508: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d051fb631f104cb2a621451f37676b9-Abstract-Conference.html',\n",
       "  509: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d0815b056ee8bb243f936adb989e2aa-Abstract-Conference.html',\n",
       "  510: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d0bcb52067128f826c86db234280dce-Abstract-Conference.html',\n",
       "  511: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d0ed12c3fda52f2c241a0cebcf739a6-Abstract-Conference.html',\n",
       "  512: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d10fe211f5139de49f94c6f0c7cecbe-Abstract-Conference.html',\n",
       "  513: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d3591b6746204b332acb464b775d38d-Abstract-Conference.html',\n",
       "  514: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d35a777e932235b115645d5141e0342-Abstract-Conference.html',\n",
       "  515: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d35af80e775e342f4cd3792e4405837-Abstract-Conference.html',\n",
       "  516: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d3ea22480873b389a3365d711eb1e91-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  517: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d49235669869ab737c1da9d64b7c769-Abstract-Conference.html',\n",
       "  518: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d4c4047f5d82699cf01b19c991ec56d-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  519: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d774c112926348c3e25ea47d87c835b-Abstract-Conference.html',\n",
       "  520: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d8dc55c1f6cf124af840ce1d92d1896-Abstract-Conference.html',\n",
       "  521: 'https://papers.nips.cc//paper_files/paper/2024/hash/1d91d5689e251d27993a3c2182dddcf7-Abstract-Conference.html',\n",
       "  522: 'https://papers.nips.cc//paper_files/paper/2024/hash/1da38b872e19f1f4a3c2846720e8f64a-Abstract-Conference.html',\n",
       "  523: 'https://papers.nips.cc//paper_files/paper/2024/hash/1da9ca7e9cef4b1af63913f05d1630a4-Abstract-Conference.html',\n",
       "  524: 'https://papers.nips.cc//paper_files/paper/2024/hash/1dc9fbdb6b4d9955ad377cb983232c9f-Abstract-Conference.html',\n",
       "  525: 'https://papers.nips.cc//paper_files/paper/2024/hash/1dccfc3ee01871d05e33457c61037d59-Abstract-Conference.html',\n",
       "  526: 'https://papers.nips.cc//paper_files/paper/2024/hash/1dcee1cd6890ab7fcdf173ec10526da9-Abstract-Conference.html',\n",
       "  527: 'https://papers.nips.cc//paper_files/paper/2024/hash/1dd8ea647d80e38b3702cf01a0855bed-Abstract-Conference.html',\n",
       "  528: 'https://papers.nips.cc//paper_files/paper/2024/hash/1df1df43b58845650b8dada00fca9772-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  529: 'https://papers.nips.cc//paper_files/paper/2024/hash/1df493ec1c2530c038d94d7300b5b368-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  530: 'https://papers.nips.cc//paper_files/paper/2024/hash/1df5c96e327ea0cbd32e0d8bae835994-Abstract-Conference.html',\n",
       "  531: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e027da6bec9ceb2ec37951ceeccae93-Abstract-Conference.html',\n",
       "  532: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e0d38c676d5855bcfab7f6d29d20ad9-Abstract-Conference.html',\n",
       "  533: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e0ebde5c2152b03420d9258f7139a58-Abstract-Conference.html',\n",
       "  534: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e1cf05517b959c1ce5934734efc421b-Abstract-Conference.html',\n",
       "  535: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e269abc604816c35f600ae14b354efd-Abstract-Conference.html',\n",
       "  536: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e2dd2f1efbc6e65b68f17ce6e158b34-Abstract-Conference.html',\n",
       "  537: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e38b2a0b77541b14a3315c99697b835-Abstract-Conference.html',\n",
       "  538: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e5cff01121223de917a84a242de30a5-Abstract-Conference.html',\n",
       "  539: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e6057620ed314b0020b3a30284b0f83-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  540: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e616bde0438cb10cb6adf076ae7d336-Abstract-Conference.html',\n",
       "  541: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e69ff56d0ebff0752ff29caaddc25dd-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  542: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e6dcc16ffa7ced2228d1f2fdc8b5adf-Abstract-Conference.html',\n",
       "  543: 'https://papers.nips.cc//paper_files/paper/2024/hash/1e89c12621c0315373f20f0aeabe5dbe-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  544: 'https://papers.nips.cc//paper_files/paper/2024/hash/1eaa5146756be028ad6fff1efcc8e6bd-Abstract-Conference.html',\n",
       "  545: 'https://papers.nips.cc//paper_files/paper/2024/hash/1eb543faf7c69e8a7eb8b85f70be818f-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  546: 'https://papers.nips.cc//paper_files/paper/2024/hash/1ed4723f12853cbd02aecb8160f5e0c9-Abstract-Conference.html',\n",
       "  547: 'https://papers.nips.cc//paper_files/paper/2024/hash/1ef130b8249625e47ef96a7b27464845-Abstract-Conference.html',\n",
       "  548: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f0832859514e53a0e4f229fc9b3a4a2-Abstract-Conference.html',\n",
       "  549: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f1628d502c62ef3725fb3b0b8eb4219-Abstract-Conference.html',\n",
       "  550: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f28e9341ab99d8e5a5734f0a76601c7-Abstract-Conference.html',\n",
       "  551: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f471322127d6347e5ae09a14b1e5cf7-Abstract-Conference.html',\n",
       "  552: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f530eef1ae1d4d4f4e0f51437976395-Abstract-Conference.html',\n",
       "  553: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f59562caae05e6aae0ffd1145bea5da-Abstract-Conference.html',\n",
       "  554: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f5c76187c28e8d9d3b938d0c504436c-Abstract-Conference.html',\n",
       "  555: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f6591cc41be737e9ba4cc487ac8082d-Abstract-Conference.html',\n",
       "  556: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f69928210578f4cf5b538a8c8806798-Abstract-Conference.html',\n",
       "  557: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f6af963e891e7efa229c24a1607fa7f-Abstract-Conference.html',\n",
       "  558: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f6f0b6eec8a4ff0f6baa707ff91a442-Abstract-Conference.html',\n",
       "  559: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f7b3b0dd7710af02aac0db5be4cfc8d-Abstract-Conference.html',\n",
       "  560: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f7e17e9d60e7bc692b72f41d2178b95-Abstract-Conference.html',\n",
       "  561: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f84412e84da6440ca355d87184cb1b3-Abstract-Conference.html',\n",
       "  562: 'https://papers.nips.cc//paper_files/paper/2024/hash/1f96b24df4b06f5d68389845a9a13ed9-Abstract-Conference.html',\n",
       "  563: 'https://papers.nips.cc//paper_files/paper/2024/hash/1fa3d6ccbcd15f7285fee666b2bd57be-Abstract-Conference.html',\n",
       "  564: 'https://papers.nips.cc//paper_files/paper/2024/hash/1fac855f8225e0b9cdb904a1e0118fdc-Abstract-Conference.html',\n",
       "  565: 'https://papers.nips.cc//paper_files/paper/2024/hash/1fb0a4de9c14f5557eeea886e22569cd-Abstract-Conference.html',\n",
       "  566: 'https://papers.nips.cc//paper_files/paper/2024/hash/1fb6d0b52f5e41b11392841a66dbfe7d-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  567: 'https://papers.nips.cc//paper_files/paper/2024/hash/1fd2b71226c67013756d318d70c40eee-Abstract-Conference.html',\n",
       "  568: 'https://papers.nips.cc//paper_files/paper/2024/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html',\n",
       "  569: 'https://papers.nips.cc//paper_files/paper/2024/hash/1feddcfb42229ca84d3070c7d540daaa-Abstract-Conference.html',\n",
       "  570: 'https://papers.nips.cc//paper_files/paper/2024/hash/200661bf8f4993b7828a45a2a90f2ecf-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  571: 'https://papers.nips.cc//paper_files/paper/2024/hash/201408406e0c5cf7626c4baeae6eaadd-Abstract-Conference.html',\n",
       "  572: 'https://papers.nips.cc//paper_files/paper/2024/hash/20468143d610020710689e3368338ffc-Abstract-Conference.html',\n",
       "  573: 'https://papers.nips.cc//paper_files/paper/2024/hash/2049d75dd13db049897562bcf7d59da8-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  574: 'https://papers.nips.cc//paper_files/paper/2024/hash/20563b8508ba42e1b688d922e926ee26-Abstract-Conference.html',\n",
       "  575: 'https://papers.nips.cc//paper_files/paper/2024/hash/206018a258033def63607fbdf364bd2d-Abstract-Conference.html',\n",
       "  576: 'https://papers.nips.cc//paper_files/paper/2024/hash/2063a00c435aafbcc58c16ce1e522139-Abstract-Conference.html',\n",
       "  577: 'https://papers.nips.cc//paper_files/paper/2024/hash/2093ed77c549eda95bd6f7212b735b43-Abstract-Conference.html',\n",
       "  578: 'https://papers.nips.cc//paper_files/paper/2024/hash/209423f076b6479ab3a4f45886e30306-Abstract-Conference.html',\n",
       "  579: 'https://papers.nips.cc//paper_files/paper/2024/hash/2096bafd3073a2224f6f0adb594068df-Abstract-Conference.html',\n",
       "  580: 'https://papers.nips.cc//paper_files/paper/2024/hash/20b6b87ca17792337f414d948af7b0e8-Abstract-Conference.html',\n",
       "  581: 'https://papers.nips.cc//paper_files/paper/2024/hash/20c3775b73f9190b861f55ec0be9f53e-Abstract-Conference.html',\n",
       "  582: 'https://papers.nips.cc//paper_files/paper/2024/hash/20cea6c1b36ae5f69c48427a68b67fbc-Abstract-Conference.html',\n",
       "  583: 'https://papers.nips.cc//paper_files/paper/2024/hash/20ceffa1fcec0f882868e5b891e3e7fa-Abstract-Conference.html',\n",
       "  584: 'https://papers.nips.cc//paper_files/paper/2024/hash/20dcab0f14046a5c6b02b61da9f13229-Abstract-Conference.html',\n",
       "  585: 'https://papers.nips.cc//paper_files/paper/2024/hash/20e6b4dd2b1f82bc599c593882f67f75-Abstract-Conference.html',\n",
       "  586: 'https://papers.nips.cc//paper_files/paper/2024/hash/20fdaf67581e6d7157376d1ed584040a-Abstract-Conference.html',\n",
       "  587: 'https://papers.nips.cc//paper_files/paper/2024/hash/20ffc2b42c7de4a1960cfdadf305bbe2-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  588: 'https://papers.nips.cc//paper_files/paper/2024/hash/2122ee4d9f8933f71bc21f748a37e245-Abstract-Conference.html',\n",
       "  589: 'https://papers.nips.cc//paper_files/paper/2024/hash/21315abf210cef71317e887e9cda78b3-Abstract-Conference.html',\n",
       "  590: 'https://papers.nips.cc//paper_files/paper/2024/hash/215aeb07b5996c969c0123c3c6ee8f54-Abstract-Conference.html',\n",
       "  591: 'https://papers.nips.cc//paper_files/paper/2024/hash/216f4cd12cfd69d46770bb2b491ae24b-Abstract-Conference.html',\n",
       "  592: 'https://papers.nips.cc//paper_files/paper/2024/hash/217bb44ab14621754db8a392163e6b07-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  593: 'https://papers.nips.cc//paper_files/paper/2024/hash/21912f7057935149fa58408ee8cb460e-Abstract-Conference.html',\n",
       "  594: 'https://papers.nips.cc//paper_files/paper/2024/hash/21a33eba893ca3890e395651b38810df-Abstract-Conference.html',\n",
       "  595: 'https://papers.nips.cc//paper_files/paper/2024/hash/21a7b312c42af86b3cd17a26a8ec499e-Abstract-Conference.html',\n",
       "  596: 'https://papers.nips.cc//paper_files/paper/2024/hash/21b5883bc8fec922fdbbb06675388164-Abstract-Conference.html',\n",
       "  597: 'https://papers.nips.cc//paper_files/paper/2024/hash/21b7e46991301ad2126c3e39384df6ed-Abstract-Conference.html',\n",
       "  598: 'https://papers.nips.cc//paper_files/paper/2024/hash/21c86d5b10cdc28664ccdadf0a29065a-Abstract-Conference.html',\n",
       "  599: 'https://papers.nips.cc//paper_files/paper/2024/hash/21cf8411ed825614e00006a1d9aab7e4-Abstract-Conference.html',\n",
       "  600: 'https://papers.nips.cc//paper_files/paper/2024/hash/21eba560be81c3a1e1f3404493a92a6a-Abstract-Conference.html',\n",
       "  601: 'https://papers.nips.cc//paper_files/paper/2024/hash/21f6bdb10b7e019543daf35012e79210-Abstract-Conference.html',\n",
       "  602: 'https://papers.nips.cc//paper_files/paper/2024/hash/21f76686538a5f06dc431efea5f475f5-Abstract-Conference.html',\n",
       "  603: 'https://papers.nips.cc//paper_files/paper/2024/hash/21f7b745f73ce0d1f9bcea7f40b1388e-Abstract-Conference.html',\n",
       "  604: 'https://papers.nips.cc//paper_files/paper/2024/hash/220cbc7435d6a56205c87d73d15d9eda-Abstract-Conference.html',\n",
       "  605: 'https://papers.nips.cc//paper_files/paper/2024/hash/221ccaeaef4b9cc8f89b63d6fc98a271-Abstract-Conference.html',\n",
       "  606: 'https://papers.nips.cc//paper_files/paper/2024/hash/222d2eaf24cf8259a35d6c7130d31425-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  607: 'https://papers.nips.cc//paper_files/paper/2024/hash/223df9b793a8b384bf38b963e35e2cac-Abstract-Conference.html',\n",
       "  608: 'https://papers.nips.cc//paper_files/paper/2024/hash/223eb69a2f2fb97fde58eaa958babb7a-Abstract-Conference.html',\n",
       "  609: 'https://papers.nips.cc//paper_files/paper/2024/hash/227404a13d20898dec2018ebe368b202-Abstract-Conference.html',\n",
       "  610: 'https://papers.nips.cc//paper_files/paper/2024/hash/2277e2c5d7d20f847c2b7d5f9075ce30-Abstract-Conference.html',\n",
       "  611: 'https://papers.nips.cc//paper_files/paper/2024/hash/22862040c1781356c8c3df4d00e5811b-Abstract-Conference.html',\n",
       "  612: 'https://papers.nips.cc//paper_files/paper/2024/hash/22912080c32a244c20bfc101033a6ac7-Abstract-Conference.html',\n",
       "  613: 'https://papers.nips.cc//paper_files/paper/2024/hash/22a7476e4fd36818777c47e666f61a41-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  614: 'https://papers.nips.cc//paper_files/paper/2024/hash/22ae669a35bb9e70eb93ab77c1eff5b4-Abstract-Conference.html',\n",
       "  615: 'https://papers.nips.cc//paper_files/paper/2024/hash/22b111819c74453837899689166c4cf9-Abstract-Conference.html',\n",
       "  616: 'https://papers.nips.cc//paper_files/paper/2024/hash/22b2067b8f680812624032025864c5a1-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  617: 'https://papers.nips.cc//paper_files/paper/2024/hash/22b6bc18be9c2bfaa48adc1122f0a971-Abstract-Conference.html',\n",
       "  618: 'https://papers.nips.cc//paper_files/paper/2024/hash/22c799f287fd05e7174fd65a3ce134af-Abstract-Conference.html',\n",
       "  619: 'https://papers.nips.cc//paper_files/paper/2024/hash/22d258dfbdf840ccbf266bbc545dd95f-Abstract-Conference.html',\n",
       "  620: 'https://papers.nips.cc//paper_files/paper/2024/hash/22d72e9f55bc29cafcca6814a7feac8c-Abstract-Conference.html',\n",
       "  621: 'https://papers.nips.cc//paper_files/paper/2024/hash/2302f4e66752149be7f63015a548a84c-Abstract-Conference.html',\n",
       "  622: 'https://papers.nips.cc//paper_files/paper/2024/hash/2318d75a06437eaa257737a5cf3ab83c-Abstract-Conference.html',\n",
       "  623: 'https://papers.nips.cc//paper_files/paper/2024/hash/233ff375b9795cd890f78a1f64459b8d-Abstract-Conference.html',\n",
       "  624: 'https://papers.nips.cc//paper_files/paper/2024/hash/2341b14df6b4f684a30eb4e99807bea6-Abstract-Conference.html',\n",
       "  625: 'https://papers.nips.cc//paper_files/paper/2024/hash/2345275663a15ee92a06bc957be54a2c-Abstract-Conference.html',\n",
       "  626: 'https://papers.nips.cc//paper_files/paper/2024/hash/2360da01c2ed6592bb691326424de184-Abstract-Conference.html',\n",
       "  627: 'https://papers.nips.cc//paper_files/paper/2024/hash/2375085c9e5afdfcab8c564e42ec0019-Abstract-Conference.html',\n",
       "  628: 'https://papers.nips.cc//paper_files/paper/2024/hash/237ffa9a473eff1c66d085dba7f813ba-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  629: 'https://papers.nips.cc//paper_files/paper/2024/hash/23866f14c85e3dc534ad7202f94378ef-Abstract-Conference.html',\n",
       "  630: 'https://papers.nips.cc//paper_files/paper/2024/hash/238e6167319ad5da33c7b38594a1edb1-Abstract-Conference.html',\n",
       "  631: 'https://papers.nips.cc//paper_files/paper/2024/hash/23c32cb7ac397f612b7c16aaa2bf0340-Abstract-Conference.html',\n",
       "  632: 'https://papers.nips.cc//paper_files/paper/2024/hash/23c9c94227f937cfb50592a15e7fbb63-Abstract-Conference.html',\n",
       "  633: 'https://papers.nips.cc//paper_files/paper/2024/hash/23d64d26abb5a0e9f2014cfcc700f82a-Abstract-Conference.html',\n",
       "  634: 'https://papers.nips.cc//paper_files/paper/2024/hash/23f3a0f82d79d985b6076bc84d14f66b-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  635: 'https://papers.nips.cc//paper_files/paper/2024/hash/23fb7cd2350c3125db48a551ae28f4bf-Abstract-Conference.html',\n",
       "  636: 'https://papers.nips.cc//paper_files/paper/2024/hash/240225294cdd2c9b692c2519d3278a08-Abstract-Conference.html',\n",
       "  637: 'https://papers.nips.cc//paper_files/paper/2024/hash/2406694fd7bc7e7bf257446a14f9ea63-Abstract-Conference.html',\n",
       "  638: 'https://papers.nips.cc//paper_files/paper/2024/hash/240d297094fc76d1e7aa27b01f221b00-Abstract-Conference.html',\n",
       "  639: 'https://papers.nips.cc//paper_files/paper/2024/hash/24143e25a82f856aeed58b2f497d623b-Abstract-Conference.html',\n",
       "  640: 'https://papers.nips.cc//paper_files/paper/2024/hash/24258872c8566deedf9b56845e5ae074-Abstract-Conference.html',\n",
       "  641: 'https://papers.nips.cc//paper_files/paper/2024/hash/2428ff361a08bc6864fb240bc83fba42-Abstract-Conference.html',\n",
       "  642: 'https://papers.nips.cc//paper_files/paper/2024/hash/2432eb0ddcf3bb630b5bcf96ca7e592d-Abstract-Conference.html',\n",
       "  643: 'https://papers.nips.cc//paper_files/paper/2024/hash/243697ace81f57daef8737ff2c5cffd3-Abstract-Conference.html',\n",
       "  644: 'https://papers.nips.cc//paper_files/paper/2024/hash/244da015b91e64f2d9362703fa2a902b-Abstract-Conference.html',\n",
       "  645: 'https://papers.nips.cc//paper_files/paper/2024/hash/24662461d2194d1bc70a47b6b6771026-Abstract-Conference.html',\n",
       "  646: 'https://papers.nips.cc//paper_files/paper/2024/hash/2492288f6878e6f99124b362604e58f5-Abstract-Conference.html',\n",
       "  647: 'https://papers.nips.cc//paper_files/paper/2024/hash/249b8d8f41970822651435629e68a6e1-Abstract-Conference.html',\n",
       "  648: 'https://papers.nips.cc//paper_files/paper/2024/hash/24a8968affe71ffe4067d022b9d16566-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  649: 'https://papers.nips.cc//paper_files/paper/2024/hash/24c4d51f3ef48dd2dbab78243ecb26a1-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  650: 'https://papers.nips.cc//paper_files/paper/2024/hash/24c523085d10743633f9964e0623dbe0-Abstract-Conference.html',\n",
       "  651: 'https://papers.nips.cc//paper_files/paper/2024/hash/24c53bfa5b53fc2cf05644f5a7a26bb0-Abstract-Conference.html',\n",
       "  652: 'https://papers.nips.cc//paper_files/paper/2024/hash/24d2dd6dc9b79116f8ebc852ddb9dc94-Abstract-Conference.html',\n",
       "  653: 'https://papers.nips.cc//paper_files/paper/2024/hash/24e8b46430df965674221665816a4964-Abstract-Conference.html',\n",
       "  654: 'https://papers.nips.cc//paper_files/paper/2024/hash/24ef004f733548db6b3197d9f68dcb85-Abstract-Conference.html',\n",
       "  655: 'https://papers.nips.cc//paper_files/paper/2024/hash/24f3041067ab24157912330344dc3bbd-Abstract-Conference.html',\n",
       "  656: 'https://papers.nips.cc//paper_files/paper/2024/hash/24f7b98aef14fcd68acf3c941af1b59e-Abstract-Conference.html',\n",
       "  657: 'https://papers.nips.cc//paper_files/paper/2024/hash/24f8dd1b8f154f1ee0d7a59e368eccf3-Abstract-Conference.html',\n",
       "  658: 'https://papers.nips.cc//paper_files/paper/2024/hash/250190819ff1dda47cd23cecc0c5a69b-Abstract-Conference.html',\n",
       "  659: 'https://papers.nips.cc//paper_files/paper/2024/hash/25297252dcd3f4b11eec7ec7ab06bc80-Abstract-Conference.html',\n",
       "  660: 'https://papers.nips.cc//paper_files/paper/2024/hash/252cd9568ab41b3c10439ddb7cddf25e-Abstract-Conference.html',\n",
       "  661: 'https://papers.nips.cc//paper_files/paper/2024/hash/254404d551f6ce17bb7407b4d6b3c87b-Abstract-Conference.html',\n",
       "  662: 'https://papers.nips.cc//paper_files/paper/2024/hash/2548fbe155ed2405488b7d5373013a64-Abstract-Conference.html',\n",
       "  663: 'https://papers.nips.cc//paper_files/paper/2024/hash/2567c95fd41459a98a73ba893775d22a-Abstract-Conference.html',\n",
       "  664: 'https://papers.nips.cc//paper_files/paper/2024/hash/257b9a6a0e3856735d0e624e38fb6803-Abstract-Conference.html',\n",
       "  665: 'https://papers.nips.cc//paper_files/paper/2024/hash/257c140d25d1f7fc10f8ae5e17296299-Abstract-Conference.html',\n",
       "  666: 'https://papers.nips.cc//paper_files/paper/2024/hash/25869dbf7682272357bc2cbbf860e1c8-Abstract-Conference.html',\n",
       "  667: 'https://papers.nips.cc//paper_files/paper/2024/hash/259762417183b58aa5bb842c1e502076-Abstract-Conference.html',\n",
       "  668: 'https://papers.nips.cc//paper_files/paper/2024/hash/25b9960c8a5bd887eb5476c951260403-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  669: 'https://papers.nips.cc//paper_files/paper/2024/hash/25c0fe7b157821dd3140727dc07461da-Abstract-Conference.html',\n",
       "  670: 'https://papers.nips.cc//paper_files/paper/2024/hash/25cc3adf8c85f7c70989cb8a97a691a7-Abstract-Conference.html',\n",
       "  671: 'https://papers.nips.cc//paper_files/paper/2024/hash/25cd345233c65fac1fec0ce61d0f7836-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  672: 'https://papers.nips.cc//paper_files/paper/2024/hash/25ead0efeed514aec00109301d93bbbb-Abstract-Conference.html',\n",
       "  673: 'https://papers.nips.cc//paper_files/paper/2024/hash/25f7be9694d7b32d5cc670927b8091e1-Abstract-Conference.html',\n",
       "  674: 'https://papers.nips.cc//paper_files/paper/2024/hash/25fb771c98a4adf83da8a050ea21cef6-Abstract-Conference.html',\n",
       "  675: 'https://papers.nips.cc//paper_files/paper/2024/hash/26289c647c6828e862e271ca3c490486-Abstract-Conference.html',\n",
       "  676: 'https://papers.nips.cc//paper_files/paper/2024/hash/2628d4d3b054c2d7ad33ab03435204f4-Abstract-Conference.html',\n",
       "  677: 'https://papers.nips.cc//paper_files/paper/2024/hash/26300457961c3e056ea61c9d3ebec2a4-Abstract-Conference.html',\n",
       "  678: 'https://papers.nips.cc//paper_files/paper/2024/hash/263c763d00c6126d37ba670a1fa10847-Abstract-Conference.html',\n",
       "  679: 'https://papers.nips.cc//paper_files/paper/2024/hash/264a9b3ce46abdf572dcfe0401141989-Abstract-Conference.html',\n",
       "  680: 'https://papers.nips.cc//paper_files/paper/2024/hash/265d0413fa2efa41084f58b52a4a2a7f-Abstract-Conference.html',\n",
       "  681: 'https://papers.nips.cc//paper_files/paper/2024/hash/2663c994c84a79b338bca613fe1ae223-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  682: 'https://papers.nips.cc//paper_files/paper/2024/hash/266c0f191b04cbbbe529016d0edc847e-Abstract-Conference.html',\n",
       "  683: 'https://papers.nips.cc//paper_files/paper/2024/hash/2676109d49d1eb26d6bc584a8f556305-Abstract-Conference.html',\n",
       "  684: 'https://papers.nips.cc//paper_files/paper/2024/hash/26889e8359e7ef8a7f5d77457364ca55-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  685: 'https://papers.nips.cc//paper_files/paper/2024/hash/26b8e3dc3a21fcd660d80c63b767f324-Abstract-Conference.html',\n",
       "  686: 'https://papers.nips.cc//paper_files/paper/2024/hash/26c05ef0790f2a8ad53700e1ef436643-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  687: 'https://papers.nips.cc//paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html',\n",
       "  688: 'https://papers.nips.cc//paper_files/paper/2024/hash/26d01e5ed42d8dcedd6aa0e3e99cffc4-Abstract-Conference.html',\n",
       "  689: 'https://papers.nips.cc//paper_files/paper/2024/hash/26f88f4fc9ee99578a066be7a0ede6dd-Abstract-Conference.html',\n",
       "  690: 'https://papers.nips.cc//paper_files/paper/2024/hash/2718a032d15e0b80cd164b240220df89-Abstract-Conference.html',\n",
       "  691: 'https://papers.nips.cc//paper_files/paper/2024/hash/27245589131d17368cccdfa990cbf16e-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  692: 'https://papers.nips.cc//paper_files/paper/2024/hash/2725d878e29131c755db73fdd1d387e1-Abstract-Conference.html',\n",
       "  693: 'https://papers.nips.cc//paper_files/paper/2024/hash/272efd3a6091ceefcbc79f1f3a6fdba4-Abstract-Conference.html',\n",
       "  694: 'https://papers.nips.cc//paper_files/paper/2024/hash/274d0146144643ee2459a602123c60ff-Abstract-Conference.html',\n",
       "  695: 'https://papers.nips.cc//paper_files/paper/2024/hash/274de7d60333c0848f42e18ae97f13e3-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  696: 'https://papers.nips.cc//paper_files/paper/2024/hash/27571b74d6cd650b8eb6cf1837953ae8-Abstract-Conference.html',\n",
       "  697: 'https://papers.nips.cc//paper_files/paper/2024/hash/27666e699d9a94540fac44eae955d8ed-Abstract-Conference.html',\n",
       "  698: 'https://papers.nips.cc//paper_files/paper/2024/hash/277628cff838927d869cd1f671328ce0-Abstract-Conference.html',\n",
       "  699: 'https://papers.nips.cc//paper_files/paper/2024/hash/2783192ea2696ee2ceb8746f5eea6681-Abstract-Conference.html',\n",
       "  700: 'https://papers.nips.cc//paper_files/paper/2024/hash/27aa3a0e6d63db269977bb2df5607cb8-Abstract-Conference.html',\n",
       "  701: 'https://papers.nips.cc//paper_files/paper/2024/hash/27c8b849acba6793f0b73f7ee7ea7397-Abstract-Conference.html',\n",
       "  702: 'https://papers.nips.cc//paper_files/paper/2024/hash/27e5626cabdbb6cd5c56ce4114ff93e4-Abstract-Conference.html',\n",
       "  703: 'https://papers.nips.cc//paper_files/paper/2024/hash/2818054fc6de6dacdda0f142a3475933-Abstract-Conference.html',\n",
       "  704: 'https://papers.nips.cc//paper_files/paper/2024/hash/281a6b49c66b94f81f3b1ae8f1a0dea4-Abstract-Conference.html',\n",
       "  705: 'https://papers.nips.cc//paper_files/paper/2024/hash/281e0b9142763f2b6c944fedb8550ba9-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  706: 'https://papers.nips.cc//paper_files/paper/2024/hash/28236482f64a72eec43706b6f3a6c511-Abstract-Conference.html',\n",
       "  707: 'https://papers.nips.cc//paper_files/paper/2024/hash/28312c9491d60ed0c77f7fff4ad86dd1-Abstract-Conference.html',\n",
       "  708: 'https://papers.nips.cc//paper_files/paper/2024/hash/2844b1bec82dd4be201f68715c03ed1b-Abstract-Conference.html',\n",
       "  709: 'https://papers.nips.cc//paper_files/paper/2024/hash/2847043899e1171183ceadf86bdbb280-Abstract-Conference.html',\n",
       "  710: 'https://papers.nips.cc//paper_files/paper/2024/hash/2847d43f17410c5beb25b2736c3ae778-Abstract-Conference.html',\n",
       "  711: 'https://papers.nips.cc//paper_files/paper/2024/hash/2858e880333b3cd64f8192f13ddcca2f-Abstract-Conference.html',\n",
       "  712: 'https://papers.nips.cc//paper_files/paper/2024/hash/285b06e0dd856f20591b0a5beb954151-Abstract-Conference.html',\n",
       "  713: 'https://papers.nips.cc//paper_files/paper/2024/hash/285cf10c8c6153d66b8cd6a3ab0d69ce-Abstract-Conference.html',\n",
       "  714: 'https://papers.nips.cc//paper_files/paper/2024/hash/286d67ff96f99c614f75dbcfb72a3e5f-Abstract-Conference.html',\n",
       "  715: 'https://papers.nips.cc//paper_files/paper/2024/hash/286e7ab0ce6a68282394c92361c27b57-Abstract-Conference.html',\n",
       "  716: 'https://papers.nips.cc//paper_files/paper/2024/hash/28795419a644f41ede3fa058b13fc622-Abstract-Conference.html',\n",
       "  717: 'https://papers.nips.cc//paper_files/paper/2024/hash/288b63aa98084366c4536ba0574a0f22-Abstract-Conference.html',\n",
       "  718: 'https://papers.nips.cc//paper_files/paper/2024/hash/2897c21877ff242e9831ec112c95f6bb-Abstract-Conference.html',\n",
       "  719: 'https://papers.nips.cc//paper_files/paper/2024/hash/28997f1826c70bbe4024b62aa28e838f-Abstract-Conference.html',\n",
       "  720: 'https://papers.nips.cc//paper_files/paper/2024/hash/28aad3b3b315d86910d7f4ee2867dfa4-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  721: 'https://papers.nips.cc//paper_files/paper/2024/hash/28ab418242603e0f7323e54185d19bde-Abstract-Conference.html',\n",
       "  722: 'https://papers.nips.cc//paper_files/paper/2024/hash/28d38c036365420f61ce03300418e44a-Abstract-Conference.html',\n",
       "  723: 'https://papers.nips.cc//paper_files/paper/2024/hash/28ef7ee7cd3e03093acc39e1272411b7-Abstract-Conference.html',\n",
       "  724: 'https://papers.nips.cc//paper_files/paper/2024/hash/290141d6bfd7ea4d3f4483d126609bf6-Abstract-Conference.html',\n",
       "  725: 'https://papers.nips.cc//paper_files/paper/2024/hash/29021b06afa4c648ee438584f7ef3e7e-Abstract-Conference.html',\n",
       "  726: 'https://papers.nips.cc//paper_files/paper/2024/hash/290848c892a85a6936b473d4daa6d0ad-Abstract-Conference.html',\n",
       "  727: 'https://papers.nips.cc//paper_files/paper/2024/hash/29219efdb96e8164c589b4a0124451b7-Abstract-Conference.html',\n",
       "  728: 'https://papers.nips.cc//paper_files/paper/2024/hash/29416b66c2149872b9d1415a3fd2c5e0-Abstract-Conference.html',\n",
       "  729: 'https://papers.nips.cc//paper_files/paper/2024/hash/29496c942ed6e08ecc469f4521ebfff0-Abstract-Conference.html',\n",
       "  730: 'https://papers.nips.cc//paper_files/paper/2024/hash/29571f8fda54fe93631c41aad4215abc-Abstract-Conference.html',\n",
       "  731: 'https://papers.nips.cc//paper_files/paper/2024/hash/2974844555dc383ea16c5f35833c7a57-Abstract-Conference.html',\n",
       "  732: 'https://papers.nips.cc//paper_files/paper/2024/hash/29753d93c5fc11167567e5df800308ae-Abstract-Conference.html',\n",
       "  733: 'https://papers.nips.cc//paper_files/paper/2024/hash/298c3e32d7d402189444be2ff5d19979-Abstract-Conference.html',\n",
       "  734: 'https://papers.nips.cc//paper_files/paper/2024/hash/29a0ea49a103a233b17c0705cdeccb66-Abstract-Conference.html',\n",
       "  735: 'https://papers.nips.cc//paper_files/paper/2024/hash/29c294ddc628c94cd2c636383ef106c1-Abstract-Conference.html',\n",
       "  736: 'https://papers.nips.cc//paper_files/paper/2024/hash/29c861b02308a57aec18990f0dfe3777-Abstract-Conference.html',\n",
       "  737: 'https://papers.nips.cc//paper_files/paper/2024/hash/29c8c615b3187ee995029284702d3f43-Abstract-Conference.html',\n",
       "  738: 'https://papers.nips.cc//paper_files/paper/2024/hash/29cd7f8331d13ede6dc6d6ef3dfacb70-Abstract-Conference.html',\n",
       "  739: 'https://papers.nips.cc//paper_files/paper/2024/hash/29d319f7c1513c9ecd81d3a6e9632a6e-Abstract-Conference.html',\n",
       "  740: 'https://papers.nips.cc//paper_files/paper/2024/hash/29d4e09f060a95118762296d240b5e63-Abstract-Conference.html',\n",
       "  741: 'https://papers.nips.cc//paper_files/paper/2024/hash/29ff36c8fbed10819b2e50267862a52a-Abstract-Conference.html',\n",
       "  742: 'https://papers.nips.cc//paper_files/paper/2024/hash/2a02b560822d564119fe3ac3be024ac6-Abstract-Conference.html',\n",
       "  743: 'https://papers.nips.cc//paper_files/paper/2024/hash/2a07348a6a7b2c208ab5cb1ee0e78ab5-Abstract-Conference.html',\n",
       "  744: 'https://papers.nips.cc//paper_files/paper/2024/hash/2a1e2162d17c4986934d7740255c0157-Abstract-Conference.html',\n",
       "  745: 'https://papers.nips.cc//paper_files/paper/2024/hash/2a54def490213ee10631b991c5acc6b5-Abstract-Conference.html',\n",
       "  746: 'https://papers.nips.cc//paper_files/paper/2024/hash/2a568a9a84577769d838793433c817d9-Abstract-Conference.html',\n",
       "  747: 'https://papers.nips.cc//paper_files/paper/2024/hash/2a5a41a536d3ada8fbf61a9d6fbf18d2-Abstract-Conference.html',\n",
       "  748: 'https://papers.nips.cc//paper_files/paper/2024/hash/2a617efee5815f12b405d40569dea0a5-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  749: 'https://papers.nips.cc//paper_files/paper/2024/hash/2a6fc6774932e7835349089b0287ed2c-Abstract-Conference.html',\n",
       "  750: 'https://papers.nips.cc//paper_files/paper/2024/hash/2a7157c84dcf263f77b37d6c11d7d149-Abstract-Conference.html',\n",
       "  751: 'https://papers.nips.cc//paper_files/paper/2024/hash/2a79b96b0fc217afb317fbdb1c082639-Abstract-Conference.html',\n",
       "  752: 'https://papers.nips.cc//paper_files/paper/2024/hash/2a7e91c6e4b68325d9884a7469804837-Abstract-Conference.html',\n",
       "  753: 'https://papers.nips.cc//paper_files/paper/2024/hash/2a952768bb85041f95ed06a5b60cf4d5-Abstract-Conference.html',\n",
       "  754: 'https://papers.nips.cc//paper_files/paper/2024/hash/2aba6ec20299931d46ddeefd5ddcb442-Abstract-Conference.html',\n",
       "  755: 'https://papers.nips.cc//paper_files/paper/2024/hash/2ac79356a03fe5e9250e5e77ebc76e6e-Abstract-Conference.html',\n",
       "  756: 'https://papers.nips.cc//paper_files/paper/2024/hash/2ad2dffba5079687651226ac8752df97-Abstract-Conference.html',\n",
       "  757: 'https://papers.nips.cc//paper_files/paper/2024/hash/2ae6b2bdf3a179e3e24129e2c54bd871-Abstract-Conference.html',\n",
       "  758: 'https://papers.nips.cc//paper_files/paper/2024/hash/2aee1c4159e48407d68fe16ae8e6e49e-Abstract-Conference.html',\n",
       "  759: 'https://papers.nips.cc//paper_files/paper/2024/hash/2af641762dc02035c31a9314b2d090b6-Abstract-Conference.html',\n",
       "  760: 'https://papers.nips.cc//paper_files/paper/2024/hash/2b09bb02b90584e2be94ff3ae09289bc-Abstract-Conference.html',\n",
       "  761: 'https://papers.nips.cc//paper_files/paper/2024/hash/2b0e14abd8128e6bf98b6b0bec1cfcbf-Abstract-Conference.html',\n",
       "  762: 'https://papers.nips.cc//paper_files/paper/2024/hash/2b22bacd7ad8677f4837b28a11fe496f-Abstract-Conference.html',\n",
       "  763: 'https://papers.nips.cc//paper_files/paper/2024/hash/2b23626015b6311369e95a70735cbb72-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  764: 'https://papers.nips.cc//paper_files/paper/2024/hash/2b2e142192092fd72a9ad2f4e10aa482-Abstract-Conference.html',\n",
       "  765: 'https://papers.nips.cc//paper_files/paper/2024/hash/2b47305e1c81890b1089a405686ad183-Abstract-Conference.html',\n",
       "  766: 'https://papers.nips.cc//paper_files/paper/2024/hash/2b567c9aa100a8edcd13607a91ed3022-Abstract-Conference.html',\n",
       "  767: 'https://papers.nips.cc//paper_files/paper/2024/hash/2b8f4db0464cc5b6e9d5e6bea4b9f308-Abstract-Conference.html',\n",
       "  768: 'https://papers.nips.cc//paper_files/paper/2024/hash/2b94c01ad2b79432f9499501f8754aee-Abstract-Conference.html',\n",
       "  769: 'https://papers.nips.cc//paper_files/paper/2024/hash/2ba538ff6a168e937d07e360045c0d6b-Abstract-Conference.html',\n",
       "  770: 'https://papers.nips.cc//paper_files/paper/2024/hash/2bd3ffba268a2699c212a233ed2907f1-Abstract-Conference.html',\n",
       "  771: 'https://papers.nips.cc//paper_files/paper/2024/hash/2bd6c9e37df10754a8f5286fca465a80-Abstract-Conference.html',\n",
       "  772: 'https://papers.nips.cc//paper_files/paper/2024/hash/2bda52aca6d214904eceffbce50f2e8c-Abstract-Conference.html',\n",
       "  773: 'https://papers.nips.cc//paper_files/paper/2024/hash/2bdc2267c3d7d01523e2e17ac0a754f3-Abstract-Conference.html',\n",
       "  774: 'https://papers.nips.cc//paper_files/paper/2024/hash/2bde8fef08f7ebe42b584266cbcfc909-Abstract-Conference.html',\n",
       "  775: 'https://papers.nips.cc//paper_files/paper/2024/hash/2bed6c14cd5ea97a9bc1e6094941bde7-Abstract-Conference.html',\n",
       "  776: 'https://papers.nips.cc//paper_files/paper/2024/hash/2bf9868e940198406713451d7656c981-Abstract-Conference.html',\n",
       "  777: 'https://papers.nips.cc//paper_files/paper/2024/hash/2bfb9435865f86ecaf7a43c49a00ba04-Abstract-Conference.html',\n",
       "  778: 'https://papers.nips.cc//paper_files/paper/2024/hash/2c15b0221da28bc6f4373a7e78b896dd-Abstract-Conference.html',\n",
       "  779: 'https://papers.nips.cc//paper_files/paper/2024/hash/2c23b3c72127e15fedc276722faee927-Abstract-Conference.html',\n",
       "  780: 'https://papers.nips.cc//paper_files/paper/2024/hash/2c2e95b75a10adbd2359f8ed5c0a38cd-Abstract-Conference.html',\n",
       "  781: 'https://papers.nips.cc//paper_files/paper/2024/hash/2c30a37c75f062e0bf79297c73db8c6c-Abstract-Conference.html',\n",
       "  782: 'https://papers.nips.cc//paper_files/paper/2024/hash/2c37c5bcef24b9541550261dcd63261b-Abstract-Conference.html',\n",
       "  783: 'https://papers.nips.cc//paper_files/paper/2024/hash/2c428bb07062012236519b589db63f34-Abstract-Conference.html',\n",
       "  784: 'https://papers.nips.cc//paper_files/paper/2024/hash/2c487f8a54cf24c0684c32abc77fed56-Abstract-Conference.html',\n",
       "  785: 'https://papers.nips.cc//paper_files/paper/2024/hash/2c570b0f9938c7a58a612e5b00af9cc0-Abstract-Conference.html',\n",
       "  786: 'https://papers.nips.cc//paper_files/paper/2024/hash/2c572cad9ae98c5cb6f3fca040b2bc54-Abstract-Conference.html',\n",
       "  787: 'https://papers.nips.cc//paper_files/paper/2024/hash/2c8047bf3ed8ef6905351608d641f02f-Abstract-Conference.html',\n",
       "  788: 'https://papers.nips.cc//paper_files/paper/2024/hash/2c9d78ed62ff5bf2377c3840188114c0-Abstract-Conference.html',\n",
       "  789: 'https://papers.nips.cc//paper_files/paper/2024/hash/2cb40fc022ca7bdc1a9a78b793661284-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  790: 'https://papers.nips.cc//paper_files/paper/2024/hash/2cc0b08447bf9668db268e6c86364a6e-Abstract-Conference.html',\n",
       "  791: 'https://papers.nips.cc//paper_files/paper/2024/hash/2cc8dc30e52798b27d37b795cc153310-Abstract-Conference.html',\n",
       "  792: 'https://papers.nips.cc//paper_files/paper/2024/hash/2cd36d327f33d47b372d4711edd08de0-Abstract-Conference.html',\n",
       "  793: 'https://papers.nips.cc//paper_files/paper/2024/hash/2cdf71a65e95bf1874212f6e604c64db-Abstract-Conference.html',\n",
       "  794: 'https://papers.nips.cc//paper_files/paper/2024/hash/2ce10f144bb93449767f355c01f24cc1-Abstract-Conference.html',\n",
       "  795: 'https://papers.nips.cc//paper_files/paper/2024/hash/2ce4f0b8e24c45318352068603153590-Abstract-Conference.html',\n",
       "  796: 'https://papers.nips.cc//paper_files/paper/2024/hash/2ceda49041816da6d5a34eb3b612607f-Abstract-Conference.html',\n",
       "  797: 'https://papers.nips.cc//paper_files/paper/2024/hash/2cf153951b5e9b39564fc4a0ef6adc1a-Abstract-Conference.html',\n",
       "  798: 'https://papers.nips.cc//paper_files/paper/2024/hash/2cfa9b0d9be8a5c01cf3eb7f21b4f2b8-Abstract-Conference.html',\n",
       "  799: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d0842550e6d92b0e27e7e810b1a4792-Abstract-Conference.html',\n",
       "  800: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d0be3cd5173c10b6ec075d1c393a13d-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  801: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d2cf241331d7e71a6f20e9ed798a06b-Abstract-Conference.html',\n",
       "  802: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d43f7a61b57f83619f82c971e4bddc0-Abstract-Conference.html',\n",
       "  803: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d4eaf042567f1c03c086103cc154c1f-Abstract-Conference.html',\n",
       "  804: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d52879ef2ba487445ca2e143b104c3b-Abstract-Conference.html',\n",
       "  805: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d5e30cc59c46aebe9d35a73ff41d32b-Abstract-Conference.html',\n",
       "  806: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d66a70c770de7835678f1c1e65fe5e1-Abstract-Conference.html',\n",
       "  807: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d69e771d9f274f7c624198ea74f5b98-Abstract-Conference.html',\n",
       "  808: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d779258dd899505b56f237de66ae470-Abstract-Conference.html',\n",
       "  809: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d880acd7b31e25d45097455c8e8257f-Abstract-Conference.html',\n",
       "  810: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d8f2351de4e9248d91ffa52dae2e6a2-Abstract-Conference.html',\n",
       "  811: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d950a2cfd8a75124c178a89545b97fd-Abstract-Conference.html',\n",
       "  812: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d9ba88c62949a9cef5c056ee77f2366-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  813: 'https://papers.nips.cc//paper_files/paper/2024/hash/2d9c6cdb4cfe93869c090fea7375044b-Abstract-Conference.html',\n",
       "  814: 'https://papers.nips.cc//paper_files/paper/2024/hash/2dab2f94544f9297d01a46a5453b93cd-Abstract-Conference.html',\n",
       "  815: 'https://papers.nips.cc//paper_files/paper/2024/hash/2db08b94565c0d582cc53de6cee5fd47-Abstract-Conference.html',\n",
       "  816: 'https://papers.nips.cc//paper_files/paper/2024/hash/2db8ce969b000fe0b3fb172490c33ce8-Abstract-Conference.html',\n",
       "  817: 'https://papers.nips.cc//paper_files/paper/2024/hash/2ded44d59f5094eed0d02132fe75b60d-Abstract-Conference.html',\n",
       "  818: 'https://papers.nips.cc//paper_files/paper/2024/hash/2dfc26ce9039f00eee4aba0c54931e46-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  819: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e098001625a8b8f22bf6c1aef09c4e5-Abstract-Conference.html',\n",
       "  820: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e102d937d094b7211c4d32ce1f1126c-Abstract-Conference.html',\n",
       "  821: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e163450c1ae3167832971e6da29f38d-Abstract-Conference.html',\n",
       "  822: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e3073cb65608aa887bb945c382e687f-Abstract-Conference.html',\n",
       "  823: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e32d3a10985fc94c7e11ee6ea165cca-Abstract-Conference.html',\n",
       "  824: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e43584b7d7b32fb6b2aa83b32dbbb20-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  825: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e4531a45b99f61947b23ccdd608303b-Abstract-Conference.html',\n",
       "  826: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e5060adc71166792bc6e5251240eba4-Abstract-Conference.html',\n",
       "  827: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e53c02ea028cbf603f4b6b47fef3d97-Abstract-Conference.html',\n",
       "  828: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e5a74735e5ada01d85afbb49caad36f-Abstract-Conference.html',\n",
       "  829: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e604995c727e3cd37c52abd6e48cb34-Abstract-Conference.html',\n",
       "  830: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e622ac74f66df03b686a12e2e0e4424-Abstract-Conference.html',\n",
       "  831: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e68b2367d2e0bc8dd6f0ff86e07c2eb-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  832: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e8eaf43f20948ad878e6b8902797d1e-Abstract-Conference.html',\n",
       "  833: 'https://papers.nips.cc//paper_files/paper/2024/hash/2e9d5bcfa9c32d360bae3d4e3d9cc032-Abstract-Conference.html',\n",
       "  834: 'https://papers.nips.cc//paper_files/paper/2024/hash/2ea07a4acbf7e38913062fd69a70805f-Abstract-Conference.html',\n",
       "  835: 'https://papers.nips.cc//paper_files/paper/2024/hash/2eb31e84089286505c27dd5d5ea7f683-Abstract-Conference.html',\n",
       "  836: 'https://papers.nips.cc//paper_files/paper/2024/hash/2ee1c87245956e3eaa71aaba5f5753eb-Abstract-Conference.html',\n",
       "  837: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f050fa9f0d898e3f265d515f50ae8f9-Abstract-Conference.html',\n",
       "  838: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f0728449cb3150189d765fc87afc913-Abstract-Conference.html',\n",
       "  839: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f0bb736ccc8551ef5bcc9165c2a4d9e-Abstract-Conference.html',\n",
       "  840: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f1232aa8c790447419d3aadbc9927b4-Abstract-Conference.html',\n",
       "  841: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f1486343c2c942a617e4f5bb0cc64c8-Abstract-Conference.html',\n",
       "  842: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f32be3e112e151707cb12528bdfa7d5-Abstract-Conference.html',\n",
       "  843: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f46ef5725a8eca24f7f24a17955ad1a-Abstract-Conference.html',\n",
       "  844: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f55a8b7b1c2c6312eb86557bb9a2bd5-Abstract-Conference.html',\n",
       "  845: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f6a6317bada76b26a4f61bb70a7db59-Abstract-Conference.html',\n",
       "  846: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f7001684feb23ca6569352eda963f39-Abstract-Conference.html',\n",
       "  847: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f75a57e9c71e8369da0150ea769d5a2-Abstract-Conference.html',\n",
       "  848: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f803abdcad9de35b45d5a656dade45c-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  849: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f8ee6a3d766b426d2618e555b5aeb39-Abstract-Conference.html',\n",
       "  850: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f8f01acb3df19fea82fa68b4a310058-Abstract-Conference.html',\n",
       "  851: 'https://papers.nips.cc//paper_files/paper/2024/hash/2f9ee101e35b890d9eae79ee27bcd69a-Abstract-Conference.html',\n",
       "  852: 'https://papers.nips.cc//paper_files/paper/2024/hash/2fb57276bfbaf1b832d7bfcba36bb41c-Abstract-Conference.html',\n",
       "  853: 'https://papers.nips.cc//paper_files/paper/2024/hash/2fbeed1dd7162f91804e7b9246e0c1a8-Abstract-Conference.html',\n",
       "  854: 'https://papers.nips.cc//paper_files/paper/2024/hash/2fcd27d2806a6d4f616cb4e6084d74f3-Abstract-Conference.html',\n",
       "  855: 'https://papers.nips.cc//paper_files/paper/2024/hash/2fd67447702c8eff5683dda507a1b0a2-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  856: 'https://papers.nips.cc//paper_files/paper/2024/hash/2feff80094b297bcfb42dbb01f34b875-Abstract-Conference.html',\n",
       "  857: 'https://papers.nips.cc//paper_files/paper/2024/hash/2ff26b12ade4282de80c2461e447c373-Abstract-Conference.html',\n",
       "  858: 'https://papers.nips.cc//paper_files/paper/2024/hash/3018804d037cc101b73624f381bed0cb-Abstract-Conference.html',\n",
       "  859: 'https://papers.nips.cc//paper_files/paper/2024/hash/3040bf93152e90d5719379818a8664b4-Abstract-Conference.html',\n",
       "  860: 'https://papers.nips.cc//paper_files/paper/2024/hash/305b2288122d46bf0641bdd86c9a7921-Abstract-Conference.html',\n",
       "  861: 'https://papers.nips.cc//paper_files/paper/2024/hash/30699996ff411d48903c9752b782a5c1-Abstract-Conference.html',\n",
       "  862: 'https://papers.nips.cc//paper_files/paper/2024/hash/30732ddb12d9faf7180f5d0e8b5b5da7-Abstract-Conference.html',\n",
       "  863: 'https://papers.nips.cc//paper_files/paper/2024/hash/30754e5f4cd69d64b5527cdd87d3cf62-Abstract-Conference.html',\n",
       "  864: 'https://papers.nips.cc//paper_files/paper/2024/hash/3076133f08b40607d00a8f48f6acd71c-Abstract-Conference.html',\n",
       "  865: 'https://papers.nips.cc//paper_files/paper/2024/hash/309cadc33589efca4018a490c07db263-Abstract-Conference.html',\n",
       "  866: 'https://papers.nips.cc//paper_files/paper/2024/hash/309fd617a4168d592e543690fbd094db-Abstract-Conference.html',\n",
       "  867: 'https://papers.nips.cc//paper_files/paper/2024/hash/30bdd694743e381f9ba679ec49dab866-Abstract-Conference.html',\n",
       "  868: 'https://papers.nips.cc//paper_files/paper/2024/hash/30df5ecdd245bd2f4b0c5ba48de9674a-Abstract-Conference.html',\n",
       "  869: 'https://papers.nips.cc//paper_files/paper/2024/hash/30dfe47a3ccbee68cffa0c19ccb1bc00-Abstract-Conference.html',\n",
       "  870: 'https://papers.nips.cc//paper_files/paper/2024/hash/3100d29d662360bb1a40a5ded8e100ae-Abstract-Conference.html',\n",
       "  871: 'https://papers.nips.cc//paper_files/paper/2024/hash/3103b25853719847502559bf67eb4037-Abstract-Conference.html',\n",
       "  872: 'https://papers.nips.cc//paper_files/paper/2024/hash/3106a1387b24c33bf66682e7ff393d56-Abstract-Conference.html',\n",
       "  873: 'https://papers.nips.cc//paper_files/paper/2024/hash/3107e4bdb658c79053d7ef59cbc804dd-Abstract-Conference.html',\n",
       "  874: 'https://papers.nips.cc//paper_files/paper/2024/hash/3122aaa22b2fe83f9cead1a696f65ceb-Abstract-Conference.html',\n",
       "  875: 'https://papers.nips.cc//paper_files/paper/2024/hash/313044ccf6f6b91c4903e6894969f1ba-Abstract-Conference.html',\n",
       "  876: 'https://papers.nips.cc//paper_files/paper/2024/hash/31421b112e5f7faf4fc577b74e45dab2-Abstract-Conference.html',\n",
       "  877: 'https://papers.nips.cc//paper_files/paper/2024/hash/314f72f80227e21cd95f402c73f0d360-Abstract-Conference.html',\n",
       "  878: 'https://papers.nips.cc//paper_files/paper/2024/hash/316648eb8b4ffb6010f531b07848c300-Abstract-Conference.html',\n",
       "  879: 'https://papers.nips.cc//paper_files/paper/2024/hash/3173c427cb4ed2d5eaab029c17f221ae-Abstract-Conference.html',\n",
       "  880: 'https://papers.nips.cc//paper_files/paper/2024/hash/317ccced29ed464df181c781cb436180-Abstract-Conference.html',\n",
       "  881: 'https://papers.nips.cc//paper_files/paper/2024/hash/3181db351fd3ced43cd589b0b572675d-Abstract-Conference.html',\n",
       "  882: 'https://papers.nips.cc//paper_files/paper/2024/hash/31888563b194f9bb33ce1aebc7e1551c-Abstract-Conference.html',\n",
       "  883: 'https://papers.nips.cc//paper_files/paper/2024/hash/318f3ae8be3c97cb7555e1c932f472a1-Abstract-Conference.html',\n",
       "  884: 'https://papers.nips.cc//paper_files/paper/2024/hash/3191170938b6102e5c203b036b7c16dd-Abstract-Conference.html',\n",
       "  885: 'https://papers.nips.cc//paper_files/paper/2024/hash/3192aa37a2227627d7f8cbf582c075f3-Abstract-Conference.html',\n",
       "  886: 'https://papers.nips.cc//paper_files/paper/2024/hash/31994923f58ae5b2d661b300bd439107-Abstract-Conference.html',\n",
       "  887: 'https://papers.nips.cc//paper_files/paper/2024/hash/31a57804448363bcab777f818f75f5b4-Abstract-Conference.html',\n",
       "  888: 'https://papers.nips.cc//paper_files/paper/2024/hash/31d1946b6bccf54bdd4a811bedd9626b-Abstract-Conference.html',\n",
       "  889: 'https://papers.nips.cc//paper_files/paper/2024/hash/31df6a082046111e605abfec26ef5ccc-Abstract-Conference.html',\n",
       "  890: 'https://papers.nips.cc//paper_files/paper/2024/hash/31e6e0c09325a3be16d93f84e40e0c7e-Abstract-Conference.html',\n",
       "  891: 'https://papers.nips.cc//paper_files/paper/2024/hash/31f119089f702e48ecfd138c1bc82c4a-Abstract-Conference.html',\n",
       "  892: 'https://papers.nips.cc//paper_files/paper/2024/hash/31fb284a0aaaad837d2930a610cd5e50-Abstract-Conference.html',\n",
       "  893: 'https://papers.nips.cc//paper_files/paper/2024/hash/3205b048f9cc54b9f7963db0b0f52d53-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  894: 'https://papers.nips.cc//paper_files/paper/2024/hash/3209cf3312b2cbb68e33644362ddc2bd-Abstract-Conference.html',\n",
       "  895: 'https://papers.nips.cc//paper_files/paper/2024/hash/32133a6a24d6554263d3584e3ac10faa-Abstract-Conference.html',\n",
       "  896: 'https://papers.nips.cc//paper_files/paper/2024/hash/321387ba926b8e58d3591c0aeb52ffc2-Abstract-Conference.html',\n",
       "  897: 'https://papers.nips.cc//paper_files/paper/2024/hash/322e4a595afd9442a89f0bfaa441871e-Abstract-Conference.html',\n",
       "  898: 'https://papers.nips.cc//paper_files/paper/2024/hash/325ce3291a509ddacc1e08f457b4d86c-Abstract-Conference.html',\n",
       "  899: 'https://papers.nips.cc//paper_files/paper/2024/hash/32683193e1d0e7a5795b073acecb3549-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  900: 'https://papers.nips.cc//paper_files/paper/2024/hash/3268f1e2474ef9d1af7f034401197a7f-Abstract-Conference.html',\n",
       "  901: 'https://papers.nips.cc//paper_files/paper/2024/hash/32768f7faf1995026ef9821c696f3404-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  902: 'https://papers.nips.cc//paper_files/paper/2024/hash/327b9b8d4e45c3f81568e11ffc505f77-Abstract-Conference.html',\n",
       "  903: 'https://papers.nips.cc//paper_files/paper/2024/hash/328b81881da145412f2bc56c998dfb6a-Abstract-Conference.html',\n",
       "  904: 'https://papers.nips.cc//paper_files/paper/2024/hash/328c922d068dd4ccb23cec5c64e6c7fc-Abstract-Conference.html',\n",
       "  905: 'https://papers.nips.cc//paper_files/paper/2024/hash/32923dff09f75cf1974c145764a523e2-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  906: 'https://papers.nips.cc//paper_files/paper/2024/hash/329ad516cf7a6ac306f29882e9c77558-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  907: 'https://papers.nips.cc//paper_files/paper/2024/hash/32b80425554e081204e5988ab1c97e9a-Abstract-Conference.html',\n",
       "  908: 'https://papers.nips.cc//paper_files/paper/2024/hash/32cc61322f1e2f56f989d29ccc7cfbb7-Abstract-Conference.html',\n",
       "  909: 'https://papers.nips.cc//paper_files/paper/2024/hash/32cf311edd3cad32dc6672b4f973366e-Abstract-Conference.html',\n",
       "  910: 'https://papers.nips.cc//paper_files/paper/2024/hash/32e07a110c6c6acf1afbf2bf82b614ad-Abstract-Conference.html',\n",
       "  911: 'https://papers.nips.cc//paper_files/paper/2024/hash/32eb183794ef5ef9a3ab1d40a3d2b303-Abstract-Conference.html',\n",
       "  912: 'https://papers.nips.cc//paper_files/paper/2024/hash/3310034c97fab48fdbcba18f90fd5364-Abstract-Conference.html',\n",
       "  913: 'https://papers.nips.cc//paper_files/paper/2024/hash/3322a9a72a1707de14badd5e552ff466-Abstract-Conference.html',\n",
       "  914: 'https://papers.nips.cc//paper_files/paper/2024/hash/333a7697dbb67f09249337f81c27d749-Abstract-Conference.html',\n",
       "  915: 'https://papers.nips.cc//paper_files/paper/2024/hash/333f81766b242b1837fa65c2172afb76-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  916: 'https://papers.nips.cc//paper_files/paper/2024/hash/3365d974ce309623bd8151082d78206c-Abstract-Conference.html',\n",
       "  917: 'https://papers.nips.cc//paper_files/paper/2024/hash/3379db43afd10e49be5cee62ad71cb31-Abstract-Conference.html',\n",
       "  918: 'https://papers.nips.cc//paper_files/paper/2024/hash/33870b3e099880cd8e705cd07173ac27-Abstract-Conference.html',\n",
       "  919: 'https://papers.nips.cc//paper_files/paper/2024/hash/3396657fe1a3c9a43ac7cd809c51a41e-Abstract-Conference.html',\n",
       "  920: 'https://papers.nips.cc//paper_files/paper/2024/hash/33b47b3d2441a17b95344cd635f3dd01-Abstract-Conference.html',\n",
       "  921: 'https://papers.nips.cc//paper_files/paper/2024/hash/33c674cb3ce9dae35021930d8d63308f-Abstract-Conference.html',\n",
       "  922: 'https://papers.nips.cc//paper_files/paper/2024/hash/33d93e4dc57453e7667b20f62e7c0681-Abstract-Conference.html',\n",
       "  923: 'https://papers.nips.cc//paper_files/paper/2024/hash/3415a8f8127d5b0ceb7fd321180b1954-Abstract-Conference.html',\n",
       "  924: 'https://papers.nips.cc//paper_files/paper/2024/hash/34239f60eca7ce9bee5280aaf81362d8-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  925: 'https://papers.nips.cc//paper_files/paper/2024/hash/34268a70fbb49ff67f2de39331fd27b9-Abstract-Conference.html',\n",
       "  926: 'https://papers.nips.cc//paper_files/paper/2024/hash/34293d684b1012ed45c3274b4a7edc00-Abstract-Conference.html',\n",
       "  927: 'https://papers.nips.cc//paper_files/paper/2024/hash/345208bdbbb6104616311dfc1d093fe7-Abstract-Conference.html',\n",
       "  928: 'https://papers.nips.cc//paper_files/paper/2024/hash/34547650b2ca69d91f3b3c3ae8b21962-Abstract-Conference.html',\n",
       "  929: 'https://papers.nips.cc//paper_files/paper/2024/hash/347110fb894281e5e937f6ccd998a6eb-Abstract-Conference.html',\n",
       "  930: 'https://papers.nips.cc//paper_files/paper/2024/hash/3472d9d6bd588315879efca259c35da6-Abstract-Conference.html',\n",
       "  931: 'https://papers.nips.cc//paper_files/paper/2024/hash/3477ca0ce484aa2fa42c1361ab601c25-Abstract-Conference.html',\n",
       "  932: 'https://papers.nips.cc//paper_files/paper/2024/hash/349956dee974cfdcbbb2d06afad5dd4a-Abstract-Conference.html',\n",
       "  933: 'https://papers.nips.cc//paper_files/paper/2024/hash/349a45f211fb1b3850da1ccd829e869e-Abstract-Conference.html',\n",
       "  934: 'https://papers.nips.cc//paper_files/paper/2024/hash/34a1fc7890141f1ada3d8bc6199cce07-Abstract-Conference.html',\n",
       "  935: 'https://papers.nips.cc//paper_files/paper/2024/hash/34a9582cd36c0b6eb94e5cf11bd6a008-Abstract-Conference.html',\n",
       "  936: 'https://papers.nips.cc//paper_files/paper/2024/hash/34aec5ab2f99a8f592e0cca4974013f2-Abstract-Conference.html',\n",
       "  937: 'https://papers.nips.cc//paper_files/paper/2024/hash/34b3a40ec9752c1ae48fe85fef8fe8dc-Abstract-Conference.html',\n",
       "  938: 'https://papers.nips.cc//paper_files/paper/2024/hash/34b70ece5f8d273fd670a17e2248d034-Abstract-Conference.html',\n",
       "  939: 'https://papers.nips.cc//paper_files/paper/2024/hash/34cc2ded6daba59357134c0b9fb06bfe-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  940: 'https://papers.nips.cc//paper_files/paper/2024/hash/34d3cf97696022b179171e5abda42c0b-Abstract-Conference.html',\n",
       "  941: 'https://papers.nips.cc//paper_files/paper/2024/hash/34d5143080c89a7ce10932c8c5e1907f-Abstract-Conference.html',\n",
       "  942: 'https://papers.nips.cc//paper_files/paper/2024/hash/34d6c7090bc5af0b96aeaf92fa074899-Abstract-Conference.html',\n",
       "  943: 'https://papers.nips.cc//paper_files/paper/2024/hash/34ec1286b2ccd4794c5ca4ad078b7150-Abstract-Conference.html',\n",
       "  944: 'https://papers.nips.cc//paper_files/paper/2024/hash/3501bea1ac61fedbaaff2f88e5fa9447-Abstract-Conference.html',\n",
       "  945: 'https://papers.nips.cc//paper_files/paper/2024/hash/3504a4fa45685d668ce92797fbbf1895-Abstract-Conference.html',\n",
       "  946: 'https://papers.nips.cc//paper_files/paper/2024/hash/3507ec8d7d6895eb9feb87a2098abe11-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  947: 'https://papers.nips.cc//paper_files/paper/2024/hash/350e718ff74062b4bac2c6ffd9e1ac66-Abstract-Conference.html',\n",
       "  948: 'https://papers.nips.cc//paper_files/paper/2024/hash/3514dbacaebf0f38b25adfe59ed81a8a-Abstract-Conference.html',\n",
       "  949: 'https://papers.nips.cc//paper_files/paper/2024/hash/352b13f01566ae34affacc60e98c16af-Abstract-Conference.html',\n",
       "  950: 'https://papers.nips.cc//paper_files/paper/2024/hash/356e52580f8d14514eb5e7d2fa1696a0-Abstract-Conference.html',\n",
       "  951: 'https://papers.nips.cc//paper_files/paper/2024/hash/3578fd44b2381db12bf16e28a667c934-Abstract-Conference.html',\n",
       "  952: 'https://papers.nips.cc//paper_files/paper/2024/hash/359ddb9caccb4c54cc915dceeacf4892-Abstract-Conference.html',\n",
       "  953: 'https://papers.nips.cc//paper_files/paper/2024/hash/35cb54b887e7aafe74829677cce6c5c6-Abstract-Conference.html',\n",
       "  954: 'https://papers.nips.cc//paper_files/paper/2024/hash/35d127a008e3ea420dd1775d1e3ed5b4-Abstract-Conference.html',\n",
       "  955: 'https://papers.nips.cc//paper_files/paper/2024/hash/35f4adf1bfca0a5c99d6c87967282e26-Abstract-Conference.html',\n",
       "  956: 'https://papers.nips.cc//paper_files/paper/2024/hash/362d683723e7d6c12a093961ec2e5051-Abstract-Conference.html',\n",
       "  957: 'https://papers.nips.cc//paper_files/paper/2024/hash/3639efedc51a522595372f76b91cbb25-Abstract-Conference.html',\n",
       "  958: 'https://papers.nips.cc//paper_files/paper/2024/hash/3640a1997a4c9571cea9db2c82e1fc35-Abstract-Conference.html',\n",
       "  959: 'https://papers.nips.cc//paper_files/paper/2024/hash/364d565b4b726c607aa40e1632045873-Abstract-Conference.html',\n",
       "  960: 'https://papers.nips.cc//paper_files/paper/2024/hash/3658e78b56268b7fd089e3165843086b-Abstract-Conference.html',\n",
       "  961: 'https://papers.nips.cc//paper_files/paper/2024/hash/36721d1209a059dcb7a090dd543f34c4-Abstract-Conference.html',\n",
       "  962: 'https://papers.nips.cc//paper_files/paper/2024/hash/36850592258c8c41cecdaa3dea5ff7de-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  963: 'https://papers.nips.cc//paper_files/paper/2024/hash/3685de48976169ca9fd68cb4c8e48b76-Abstract-Conference.html',\n",
       "  964: 'https://papers.nips.cc//paper_files/paper/2024/hash/36b31e1bb8ecd4f4081686448e9eff2d-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  965: 'https://papers.nips.cc//paper_files/paper/2024/hash/36b6180f3dab4025ba763e853b044814-Abstract-Conference.html',\n",
       "  966: 'https://papers.nips.cc//paper_files/paper/2024/hash/36d4b4c04d07974fe7d455d62783ac22-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  967: 'https://papers.nips.cc//paper_files/paper/2024/hash/36ecc1d1b883afc0e882876cbdd123ab-Abstract-Conference.html',\n",
       "  968: 'https://papers.nips.cc//paper_files/paper/2024/hash/37094fdc81632915a5738293cf9b7ad4-Abstract-Conference.html',\n",
       "  969: 'https://papers.nips.cc//paper_files/paper/2024/hash/370df50ccfdf8bde18f8f9c2d9151bda-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  970: 'https://papers.nips.cc//paper_files/paper/2024/hash/370fa2e691f57eb319bc263a07dad4a5-Abstract-Conference.html',\n",
       "  971: 'https://papers.nips.cc//paper_files/paper/2024/hash/371713c3e5314dff9483c62c5abb98a8-Abstract-Conference.html',\n",
       "  972: 'https://papers.nips.cc//paper_files/paper/2024/hash/37294f033582ac0064bf90fa557c2573-Abstract-Conference.html',\n",
       "  973: 'https://papers.nips.cc//paper_files/paper/2024/hash/3750e99b522bd36a099d2e8b9f0550c7-Abstract-Conference.html',\n",
       "  974: 'https://papers.nips.cc//paper_files/paper/2024/hash/375fca131243755f9e268d1a37ffcd85-Abstract-Conference.html',\n",
       "  975: 'https://papers.nips.cc//paper_files/paper/2024/hash/37611b0fc2b65cdbb60865af5f6cf453-Abstract-Conference.html',\n",
       "  976: 'https://papers.nips.cc//paper_files/paper/2024/hash/3763f6861da87de0d2b04ef26fd02443-Abstract-Conference.html',\n",
       "  977: 'https://papers.nips.cc//paper_files/paper/2024/hash/37664246a1e07e212ddacea6e5a523f2-Abstract-Conference.html',\n",
       "  978: 'https://papers.nips.cc//paper_files/paper/2024/hash/377235d5cee7b104501407c0e5066c92-Abstract-Conference.html',\n",
       "  979: 'https://papers.nips.cc//paper_files/paper/2024/hash/377d0752059d3d4686aa021b664a25dd-Abstract-Conference.html',\n",
       "  980: 'https://papers.nips.cc//paper_files/paper/2024/hash/378226e5df7eded3e401de5c9493143c-Abstract-Conference.html',\n",
       "  981: 'https://papers.nips.cc//paper_files/paper/2024/hash/3791f5fc0e8e43730466afd2bcdb7493-Abstract-Conference.html',\n",
       "  982: 'https://papers.nips.cc//paper_files/paper/2024/hash/3792ddbf94b68ff4369f510f7a3e1777-Abstract-Conference.html',\n",
       "  983: 'https://papers.nips.cc//paper_files/paper/2024/hash/379ea6eb0faad176b570c2e26d58ff2b-Abstract-Conference.html',\n",
       "  984: 'https://papers.nips.cc//paper_files/paper/2024/hash/37c6d0bc4d2917dcbea693b18504bd87-Abstract-Conference.html',\n",
       "  985: 'https://papers.nips.cc//paper_files/paper/2024/hash/37d4d4413b7c7558cc27a6d3d42ea998-Abstract-Conference.html',\n",
       "  986: 'https://papers.nips.cc//paper_files/paper/2024/hash/37d9f19150fce07bced2a81fc87d47a6-Abstract-Conference.html',\n",
       "  987: 'https://papers.nips.cc//paper_files/paper/2024/hash/37e90dcf2909b5068858b34b5239f187-Abstract-Conference.html',\n",
       "  988: 'https://papers.nips.cc//paper_files/paper/2024/hash/37eb3a53e5931a811c1a9498edce298a-Abstract-Conference.html',\n",
       "  989: 'https://papers.nips.cc//paper_files/paper/2024/hash/37f2f382b1e1f1e887d610e7ea047086-Abstract-Conference.html',\n",
       "  990: 'https://papers.nips.cc//paper_files/paper/2024/hash/37f4dd559db9cd0a42ce72987d27ab27-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  991: 'https://papers.nips.cc//paper_files/paper/2024/hash/37f6bed18b9b404f53dcaec4607c4fb7-Abstract-Conference.html',\n",
       "  992: 'https://papers.nips.cc//paper_files/paper/2024/hash/380a0b16a7e6f8c5010f798c9f2d3c61-Abstract-Conference.html',\n",
       "  993: 'https://papers.nips.cc//paper_files/paper/2024/hash/381efefc0d765e680451978c0392f637-Abstract-Conference.html',\n",
       "  994: 'https://papers.nips.cc//paper_files/paper/2024/hash/382066d1460144ddcb041f32d05de053-Abstract-Conference.html',\n",
       "  995: 'https://papers.nips.cc//paper_files/paper/2024/hash/382a8606a85ca6ec7c06185a1a95ce8b-Abstract-Conference.html',\n",
       "  996: 'https://papers.nips.cc//paper_files/paper/2024/hash/3838bf9070b80e888d571ec126d844c2-Abstract-Conference.html',\n",
       "  997: 'https://papers.nips.cc//paper_files/paper/2024/hash/3848856978da28639d2057094a1287a5-Abstract-Conference.html',\n",
       "  998: 'https://papers.nips.cc//paper_files/paper/2024/hash/3848fef259495bfd04d60cdc5c1b4db7-Abstract-Conference.html',\n",
       "  999: 'https://papers.nips.cc//paper_files/paper/2024/hash/3852c8254bc6d904c09db9921157f59b-Abstract-Datasets_and_Benchmarks_Track.html',\n",
       "  ...},\n",
       " 'year': {0: 2024,\n",
       "  1: 2024,\n",
       "  2: 2024,\n",
       "  3: 2024,\n",
       "  4: 2024,\n",
       "  5: 2024,\n",
       "  6: 2024,\n",
       "  7: 2024,\n",
       "  8: 2024,\n",
       "  9: 2024,\n",
       "  10: 2024,\n",
       "  11: 2024,\n",
       "  12: 2024,\n",
       "  13: 2024,\n",
       "  14: 2024,\n",
       "  15: 2024,\n",
       "  16: 2024,\n",
       "  17: 2024,\n",
       "  18: 2024,\n",
       "  19: 2024,\n",
       "  20: 2024,\n",
       "  21: 2024,\n",
       "  22: 2024,\n",
       "  23: 2024,\n",
       "  24: 2024,\n",
       "  25: 2024,\n",
       "  26: 2024,\n",
       "  27: 2024,\n",
       "  28: 2024,\n",
       "  29: 2024,\n",
       "  30: 2024,\n",
       "  31: 2024,\n",
       "  32: 2024,\n",
       "  33: 2024,\n",
       "  34: 2024,\n",
       "  35: 2024,\n",
       "  36: 2024,\n",
       "  37: 2024,\n",
       "  38: 2024,\n",
       "  39: 2024,\n",
       "  40: 2024,\n",
       "  41: 2024,\n",
       "  42: 2024,\n",
       "  43: 2024,\n",
       "  44: 2024,\n",
       "  45: 2024,\n",
       "  46: 2024,\n",
       "  47: 2024,\n",
       "  48: 2024,\n",
       "  49: 2024,\n",
       "  50: 2024,\n",
       "  51: 2024,\n",
       "  52: 2024,\n",
       "  53: 2024,\n",
       "  54: 2024,\n",
       "  55: 2024,\n",
       "  56: 2024,\n",
       "  57: 2024,\n",
       "  58: 2024,\n",
       "  59: 2024,\n",
       "  60: 2024,\n",
       "  61: 2024,\n",
       "  62: 2024,\n",
       "  63: 2024,\n",
       "  64: 2024,\n",
       "  65: 2024,\n",
       "  66: 2024,\n",
       "  67: 2024,\n",
       "  68: 2024,\n",
       "  69: 2024,\n",
       "  70: 2024,\n",
       "  71: 2024,\n",
       "  72: 2024,\n",
       "  73: 2024,\n",
       "  74: 2024,\n",
       "  75: 2024,\n",
       "  76: 2024,\n",
       "  77: 2024,\n",
       "  78: 2024,\n",
       "  79: 2024,\n",
       "  80: 2024,\n",
       "  81: 2024,\n",
       "  82: 2024,\n",
       "  83: 2024,\n",
       "  84: 2024,\n",
       "  85: 2024,\n",
       "  86: 2024,\n",
       "  87: 2024,\n",
       "  88: 2024,\n",
       "  89: 2024,\n",
       "  90: 2024,\n",
       "  91: 2024,\n",
       "  92: 2024,\n",
       "  93: 2024,\n",
       "  94: 2024,\n",
       "  95: 2024,\n",
       "  96: 2024,\n",
       "  97: 2024,\n",
       "  98: 2024,\n",
       "  99: 2024,\n",
       "  100: 2024,\n",
       "  101: 2024,\n",
       "  102: 2024,\n",
       "  103: 2024,\n",
       "  104: 2024,\n",
       "  105: 2024,\n",
       "  106: 2024,\n",
       "  107: 2024,\n",
       "  108: 2024,\n",
       "  109: 2024,\n",
       "  110: 2024,\n",
       "  111: 2024,\n",
       "  112: 2024,\n",
       "  113: 2024,\n",
       "  114: 2024,\n",
       "  115: 2024,\n",
       "  116: 2024,\n",
       "  117: 2024,\n",
       "  118: 2024,\n",
       "  119: 2024,\n",
       "  120: 2024,\n",
       "  121: 2024,\n",
       "  122: 2024,\n",
       "  123: 2024,\n",
       "  124: 2024,\n",
       "  125: 2024,\n",
       "  126: 2024,\n",
       "  127: 2024,\n",
       "  128: 2024,\n",
       "  129: 2024,\n",
       "  130: 2024,\n",
       "  131: 2024,\n",
       "  132: 2024,\n",
       "  133: 2024,\n",
       "  134: 2024,\n",
       "  135: 2024,\n",
       "  136: 2024,\n",
       "  137: 2024,\n",
       "  138: 2024,\n",
       "  139: 2024,\n",
       "  140: 2024,\n",
       "  141: 2024,\n",
       "  142: 2024,\n",
       "  143: 2024,\n",
       "  144: 2024,\n",
       "  145: 2024,\n",
       "  146: 2024,\n",
       "  147: 2024,\n",
       "  148: 2024,\n",
       "  149: 2024,\n",
       "  150: 2024,\n",
       "  151: 2024,\n",
       "  152: 2024,\n",
       "  153: 2024,\n",
       "  154: 2024,\n",
       "  155: 2024,\n",
       "  156: 2024,\n",
       "  157: 2024,\n",
       "  158: 2024,\n",
       "  159: 2024,\n",
       "  160: 2024,\n",
       "  161: 2024,\n",
       "  162: 2024,\n",
       "  163: 2024,\n",
       "  164: 2024,\n",
       "  165: 2024,\n",
       "  166: 2024,\n",
       "  167: 2024,\n",
       "  168: 2024,\n",
       "  169: 2024,\n",
       "  170: 2024,\n",
       "  171: 2024,\n",
       "  172: 2024,\n",
       "  173: 2024,\n",
       "  174: 2024,\n",
       "  175: 2024,\n",
       "  176: 2024,\n",
       "  177: 2024,\n",
       "  178: 2024,\n",
       "  179: 2024,\n",
       "  180: 2024,\n",
       "  181: 2024,\n",
       "  182: 2024,\n",
       "  183: 2024,\n",
       "  184: 2024,\n",
       "  185: 2024,\n",
       "  186: 2024,\n",
       "  187: 2024,\n",
       "  188: 2024,\n",
       "  189: 2024,\n",
       "  190: 2024,\n",
       "  191: 2024,\n",
       "  192: 2024,\n",
       "  193: 2024,\n",
       "  194: 2024,\n",
       "  195: 2024,\n",
       "  196: 2024,\n",
       "  197: 2024,\n",
       "  198: 2024,\n",
       "  199: 2024,\n",
       "  200: 2024,\n",
       "  201: 2024,\n",
       "  202: 2024,\n",
       "  203: 2024,\n",
       "  204: 2024,\n",
       "  205: 2024,\n",
       "  206: 2024,\n",
       "  207: 2024,\n",
       "  208: 2024,\n",
       "  209: 2024,\n",
       "  210: 2024,\n",
       "  211: 2024,\n",
       "  212: 2024,\n",
       "  213: 2024,\n",
       "  214: 2024,\n",
       "  215: 2024,\n",
       "  216: 2024,\n",
       "  217: 2024,\n",
       "  218: 2024,\n",
       "  219: 2024,\n",
       "  220: 2024,\n",
       "  221: 2024,\n",
       "  222: 2024,\n",
       "  223: 2024,\n",
       "  224: 2024,\n",
       "  225: 2024,\n",
       "  226: 2024,\n",
       "  227: 2024,\n",
       "  228: 2024,\n",
       "  229: 2024,\n",
       "  230: 2024,\n",
       "  231: 2024,\n",
       "  232: 2024,\n",
       "  233: 2024,\n",
       "  234: 2024,\n",
       "  235: 2024,\n",
       "  236: 2024,\n",
       "  237: 2024,\n",
       "  238: 2024,\n",
       "  239: 2024,\n",
       "  240: 2024,\n",
       "  241: 2024,\n",
       "  242: 2024,\n",
       "  243: 2024,\n",
       "  244: 2024,\n",
       "  245: 2024,\n",
       "  246: 2024,\n",
       "  247: 2024,\n",
       "  248: 2024,\n",
       "  249: 2024,\n",
       "  250: 2024,\n",
       "  251: 2024,\n",
       "  252: 2024,\n",
       "  253: 2024,\n",
       "  254: 2024,\n",
       "  255: 2024,\n",
       "  256: 2024,\n",
       "  257: 2024,\n",
       "  258: 2024,\n",
       "  259: 2024,\n",
       "  260: 2024,\n",
       "  261: 2024,\n",
       "  262: 2024,\n",
       "  263: 2024,\n",
       "  264: 2024,\n",
       "  265: 2024,\n",
       "  266: 2024,\n",
       "  267: 2024,\n",
       "  268: 2024,\n",
       "  269: 2024,\n",
       "  270: 2024,\n",
       "  271: 2024,\n",
       "  272: 2024,\n",
       "  273: 2024,\n",
       "  274: 2024,\n",
       "  275: 2024,\n",
       "  276: 2024,\n",
       "  277: 2024,\n",
       "  278: 2024,\n",
       "  279: 2024,\n",
       "  280: 2024,\n",
       "  281: 2024,\n",
       "  282: 2024,\n",
       "  283: 2024,\n",
       "  284: 2024,\n",
       "  285: 2024,\n",
       "  286: 2024,\n",
       "  287: 2024,\n",
       "  288: 2024,\n",
       "  289: 2024,\n",
       "  290: 2024,\n",
       "  291: 2024,\n",
       "  292: 2024,\n",
       "  293: 2024,\n",
       "  294: 2024,\n",
       "  295: 2024,\n",
       "  296: 2024,\n",
       "  297: 2024,\n",
       "  298: 2024,\n",
       "  299: 2024,\n",
       "  300: 2024,\n",
       "  301: 2024,\n",
       "  302: 2024,\n",
       "  303: 2024,\n",
       "  304: 2024,\n",
       "  305: 2024,\n",
       "  306: 2024,\n",
       "  307: 2024,\n",
       "  308: 2024,\n",
       "  309: 2024,\n",
       "  310: 2024,\n",
       "  311: 2024,\n",
       "  312: 2024,\n",
       "  313: 2024,\n",
       "  314: 2024,\n",
       "  315: 2024,\n",
       "  316: 2024,\n",
       "  317: 2024,\n",
       "  318: 2024,\n",
       "  319: 2024,\n",
       "  320: 2024,\n",
       "  321: 2024,\n",
       "  322: 2024,\n",
       "  323: 2024,\n",
       "  324: 2024,\n",
       "  325: 2024,\n",
       "  326: 2024,\n",
       "  327: 2024,\n",
       "  328: 2024,\n",
       "  329: 2024,\n",
       "  330: 2024,\n",
       "  331: 2024,\n",
       "  332: 2024,\n",
       "  333: 2024,\n",
       "  334: 2024,\n",
       "  335: 2024,\n",
       "  336: 2024,\n",
       "  337: 2024,\n",
       "  338: 2024,\n",
       "  339: 2024,\n",
       "  340: 2024,\n",
       "  341: 2024,\n",
       "  342: 2024,\n",
       "  343: 2024,\n",
       "  344: 2024,\n",
       "  345: 2024,\n",
       "  346: 2024,\n",
       "  347: 2024,\n",
       "  348: 2024,\n",
       "  349: 2024,\n",
       "  350: 2024,\n",
       "  351: 2024,\n",
       "  352: 2024,\n",
       "  353: 2024,\n",
       "  354: 2024,\n",
       "  355: 2024,\n",
       "  356: 2024,\n",
       "  357: 2024,\n",
       "  358: 2024,\n",
       "  359: 2024,\n",
       "  360: 2024,\n",
       "  361: 2024,\n",
       "  362: 2024,\n",
       "  363: 2024,\n",
       "  364: 2024,\n",
       "  365: 2024,\n",
       "  366: 2024,\n",
       "  367: 2024,\n",
       "  368: 2024,\n",
       "  369: 2024,\n",
       "  370: 2024,\n",
       "  371: 2024,\n",
       "  372: 2024,\n",
       "  373: 2024,\n",
       "  374: 2024,\n",
       "  375: 2024,\n",
       "  376: 2024,\n",
       "  377: 2024,\n",
       "  378: 2024,\n",
       "  379: 2024,\n",
       "  380: 2024,\n",
       "  381: 2024,\n",
       "  382: 2024,\n",
       "  383: 2024,\n",
       "  384: 2024,\n",
       "  385: 2024,\n",
       "  386: 2024,\n",
       "  387: 2024,\n",
       "  388: 2024,\n",
       "  389: 2024,\n",
       "  390: 2024,\n",
       "  391: 2024,\n",
       "  392: 2024,\n",
       "  393: 2024,\n",
       "  394: 2024,\n",
       "  395: 2024,\n",
       "  396: 2024,\n",
       "  397: 2024,\n",
       "  398: 2024,\n",
       "  399: 2024,\n",
       "  400: 2024,\n",
       "  401: 2024,\n",
       "  402: 2024,\n",
       "  403: 2024,\n",
       "  404: 2024,\n",
       "  405: 2024,\n",
       "  406: 2024,\n",
       "  407: 2024,\n",
       "  408: 2024,\n",
       "  409: 2024,\n",
       "  410: 2024,\n",
       "  411: 2024,\n",
       "  412: 2024,\n",
       "  413: 2024,\n",
       "  414: 2024,\n",
       "  415: 2024,\n",
       "  416: 2024,\n",
       "  417: 2024,\n",
       "  418: 2024,\n",
       "  419: 2024,\n",
       "  420: 2024,\n",
       "  421: 2024,\n",
       "  422: 2024,\n",
       "  423: 2024,\n",
       "  424: 2024,\n",
       "  425: 2024,\n",
       "  426: 2024,\n",
       "  427: 2024,\n",
       "  428: 2024,\n",
       "  429: 2024,\n",
       "  430: 2024,\n",
       "  431: 2024,\n",
       "  432: 2024,\n",
       "  433: 2024,\n",
       "  434: 2024,\n",
       "  435: 2024,\n",
       "  436: 2024,\n",
       "  437: 2024,\n",
       "  438: 2024,\n",
       "  439: 2024,\n",
       "  440: 2024,\n",
       "  441: 2024,\n",
       "  442: 2024,\n",
       "  443: 2024,\n",
       "  444: 2024,\n",
       "  445: 2024,\n",
       "  446: 2024,\n",
       "  447: 2024,\n",
       "  448: 2024,\n",
       "  449: 2024,\n",
       "  450: 2024,\n",
       "  451: 2024,\n",
       "  452: 2024,\n",
       "  453: 2024,\n",
       "  454: 2024,\n",
       "  455: 2024,\n",
       "  456: 2024,\n",
       "  457: 2024,\n",
       "  458: 2024,\n",
       "  459: 2024,\n",
       "  460: 2024,\n",
       "  461: 2024,\n",
       "  462: 2024,\n",
       "  463: 2024,\n",
       "  464: 2024,\n",
       "  465: 2024,\n",
       "  466: 2024,\n",
       "  467: 2024,\n",
       "  468: 2024,\n",
       "  469: 2024,\n",
       "  470: 2024,\n",
       "  471: 2024,\n",
       "  472: 2024,\n",
       "  473: 2024,\n",
       "  474: 2024,\n",
       "  475: 2024,\n",
       "  476: 2024,\n",
       "  477: 2024,\n",
       "  478: 2024,\n",
       "  479: 2024,\n",
       "  480: 2024,\n",
       "  481: 2024,\n",
       "  482: 2024,\n",
       "  483: 2024,\n",
       "  484: 2024,\n",
       "  485: 2024,\n",
       "  486: 2024,\n",
       "  487: 2024,\n",
       "  488: 2024,\n",
       "  489: 2024,\n",
       "  490: 2024,\n",
       "  491: 2024,\n",
       "  492: 2024,\n",
       "  493: 2024,\n",
       "  494: 2024,\n",
       "  495: 2024,\n",
       "  496: 2024,\n",
       "  497: 2024,\n",
       "  498: 2024,\n",
       "  499: 2024,\n",
       "  500: 2024,\n",
       "  501: 2024,\n",
       "  502: 2024,\n",
       "  503: 2024,\n",
       "  504: 2024,\n",
       "  505: 2024,\n",
       "  506: 2024,\n",
       "  507: 2024,\n",
       "  508: 2024,\n",
       "  509: 2024,\n",
       "  510: 2024,\n",
       "  511: 2024,\n",
       "  512: 2024,\n",
       "  513: 2024,\n",
       "  514: 2024,\n",
       "  515: 2024,\n",
       "  516: 2024,\n",
       "  517: 2024,\n",
       "  518: 2024,\n",
       "  519: 2024,\n",
       "  520: 2024,\n",
       "  521: 2024,\n",
       "  522: 2024,\n",
       "  523: 2024,\n",
       "  524: 2024,\n",
       "  525: 2024,\n",
       "  526: 2024,\n",
       "  527: 2024,\n",
       "  528: 2024,\n",
       "  529: 2024,\n",
       "  530: 2024,\n",
       "  531: 2024,\n",
       "  532: 2024,\n",
       "  533: 2024,\n",
       "  534: 2024,\n",
       "  535: 2024,\n",
       "  536: 2024,\n",
       "  537: 2024,\n",
       "  538: 2024,\n",
       "  539: 2024,\n",
       "  540: 2024,\n",
       "  541: 2024,\n",
       "  542: 2024,\n",
       "  543: 2024,\n",
       "  544: 2024,\n",
       "  545: 2024,\n",
       "  546: 2024,\n",
       "  547: 2024,\n",
       "  548: 2024,\n",
       "  549: 2024,\n",
       "  550: 2024,\n",
       "  551: 2024,\n",
       "  552: 2024,\n",
       "  553: 2024,\n",
       "  554: 2024,\n",
       "  555: 2024,\n",
       "  556: 2024,\n",
       "  557: 2024,\n",
       "  558: 2024,\n",
       "  559: 2024,\n",
       "  560: 2024,\n",
       "  561: 2024,\n",
       "  562: 2024,\n",
       "  563: 2024,\n",
       "  564: 2024,\n",
       "  565: 2024,\n",
       "  566: 2024,\n",
       "  567: 2024,\n",
       "  568: 2024,\n",
       "  569: 2024,\n",
       "  570: 2024,\n",
       "  571: 2024,\n",
       "  572: 2024,\n",
       "  573: 2024,\n",
       "  574: 2024,\n",
       "  575: 2024,\n",
       "  576: 2024,\n",
       "  577: 2024,\n",
       "  578: 2024,\n",
       "  579: 2024,\n",
       "  580: 2024,\n",
       "  581: 2024,\n",
       "  582: 2024,\n",
       "  583: 2024,\n",
       "  584: 2024,\n",
       "  585: 2024,\n",
       "  586: 2024,\n",
       "  587: 2024,\n",
       "  588: 2024,\n",
       "  589: 2024,\n",
       "  590: 2024,\n",
       "  591: 2024,\n",
       "  592: 2024,\n",
       "  593: 2024,\n",
       "  594: 2024,\n",
       "  595: 2024,\n",
       "  596: 2024,\n",
       "  597: 2024,\n",
       "  598: 2024,\n",
       "  599: 2024,\n",
       "  600: 2024,\n",
       "  601: 2024,\n",
       "  602: 2024,\n",
       "  603: 2024,\n",
       "  604: 2024,\n",
       "  605: 2024,\n",
       "  606: 2024,\n",
       "  607: 2024,\n",
       "  608: 2024,\n",
       "  609: 2024,\n",
       "  610: 2024,\n",
       "  611: 2024,\n",
       "  612: 2024,\n",
       "  613: 2024,\n",
       "  614: 2024,\n",
       "  615: 2024,\n",
       "  616: 2024,\n",
       "  617: 2024,\n",
       "  618: 2024,\n",
       "  619: 2024,\n",
       "  620: 2024,\n",
       "  621: 2024,\n",
       "  622: 2024,\n",
       "  623: 2024,\n",
       "  624: 2024,\n",
       "  625: 2024,\n",
       "  626: 2024,\n",
       "  627: 2024,\n",
       "  628: 2024,\n",
       "  629: 2024,\n",
       "  630: 2024,\n",
       "  631: 2024,\n",
       "  632: 2024,\n",
       "  633: 2024,\n",
       "  634: 2024,\n",
       "  635: 2024,\n",
       "  636: 2024,\n",
       "  637: 2024,\n",
       "  638: 2024,\n",
       "  639: 2024,\n",
       "  640: 2024,\n",
       "  641: 2024,\n",
       "  642: 2024,\n",
       "  643: 2024,\n",
       "  644: 2024,\n",
       "  645: 2024,\n",
       "  646: 2024,\n",
       "  647: 2024,\n",
       "  648: 2024,\n",
       "  649: 2024,\n",
       "  650: 2024,\n",
       "  651: 2024,\n",
       "  652: 2024,\n",
       "  653: 2024,\n",
       "  654: 2024,\n",
       "  655: 2024,\n",
       "  656: 2024,\n",
       "  657: 2024,\n",
       "  658: 2024,\n",
       "  659: 2024,\n",
       "  660: 2024,\n",
       "  661: 2024,\n",
       "  662: 2024,\n",
       "  663: 2024,\n",
       "  664: 2024,\n",
       "  665: 2024,\n",
       "  666: 2024,\n",
       "  667: 2024,\n",
       "  668: 2024,\n",
       "  669: 2024,\n",
       "  670: 2024,\n",
       "  671: 2024,\n",
       "  672: 2024,\n",
       "  673: 2024,\n",
       "  674: 2024,\n",
       "  675: 2024,\n",
       "  676: 2024,\n",
       "  677: 2024,\n",
       "  678: 2024,\n",
       "  679: 2024,\n",
       "  680: 2024,\n",
       "  681: 2024,\n",
       "  682: 2024,\n",
       "  683: 2024,\n",
       "  684: 2024,\n",
       "  685: 2024,\n",
       "  686: 2024,\n",
       "  687: 2024,\n",
       "  688: 2024,\n",
       "  689: 2024,\n",
       "  690: 2024,\n",
       "  691: 2024,\n",
       "  692: 2024,\n",
       "  693: 2024,\n",
       "  694: 2024,\n",
       "  695: 2024,\n",
       "  696: 2024,\n",
       "  697: 2024,\n",
       "  698: 2024,\n",
       "  699: 2024,\n",
       "  700: 2024,\n",
       "  701: 2024,\n",
       "  702: 2024,\n",
       "  703: 2024,\n",
       "  704: 2024,\n",
       "  705: 2024,\n",
       "  706: 2024,\n",
       "  707: 2024,\n",
       "  708: 2024,\n",
       "  709: 2024,\n",
       "  710: 2024,\n",
       "  711: 2024,\n",
       "  712: 2024,\n",
       "  713: 2024,\n",
       "  714: 2024,\n",
       "  715: 2024,\n",
       "  716: 2024,\n",
       "  717: 2024,\n",
       "  718: 2024,\n",
       "  719: 2024,\n",
       "  720: 2024,\n",
       "  721: 2024,\n",
       "  722: 2024,\n",
       "  723: 2024,\n",
       "  724: 2024,\n",
       "  725: 2024,\n",
       "  726: 2024,\n",
       "  727: 2024,\n",
       "  728: 2024,\n",
       "  729: 2024,\n",
       "  730: 2024,\n",
       "  731: 2024,\n",
       "  732: 2024,\n",
       "  733: 2024,\n",
       "  734: 2024,\n",
       "  735: 2024,\n",
       "  736: 2024,\n",
       "  737: 2024,\n",
       "  738: 2024,\n",
       "  739: 2024,\n",
       "  740: 2024,\n",
       "  741: 2024,\n",
       "  742: 2024,\n",
       "  743: 2024,\n",
       "  744: 2024,\n",
       "  745: 2024,\n",
       "  746: 2024,\n",
       "  747: 2024,\n",
       "  748: 2024,\n",
       "  749: 2024,\n",
       "  750: 2024,\n",
       "  751: 2024,\n",
       "  752: 2024,\n",
       "  753: 2024,\n",
       "  754: 2024,\n",
       "  755: 2024,\n",
       "  756: 2024,\n",
       "  757: 2024,\n",
       "  758: 2024,\n",
       "  759: 2024,\n",
       "  760: 2024,\n",
       "  761: 2024,\n",
       "  762: 2024,\n",
       "  763: 2024,\n",
       "  764: 2024,\n",
       "  765: 2024,\n",
       "  766: 2024,\n",
       "  767: 2024,\n",
       "  768: 2024,\n",
       "  769: 2024,\n",
       "  770: 2024,\n",
       "  771: 2024,\n",
       "  772: 2024,\n",
       "  773: 2024,\n",
       "  774: 2024,\n",
       "  775: 2024,\n",
       "  776: 2024,\n",
       "  777: 2024,\n",
       "  778: 2024,\n",
       "  779: 2024,\n",
       "  780: 2024,\n",
       "  781: 2024,\n",
       "  782: 2024,\n",
       "  783: 2024,\n",
       "  784: 2024,\n",
       "  785: 2024,\n",
       "  786: 2024,\n",
       "  787: 2024,\n",
       "  788: 2024,\n",
       "  789: 2024,\n",
       "  790: 2024,\n",
       "  791: 2024,\n",
       "  792: 2024,\n",
       "  793: 2024,\n",
       "  794: 2024,\n",
       "  795: 2024,\n",
       "  796: 2024,\n",
       "  797: 2024,\n",
       "  798: 2024,\n",
       "  799: 2024,\n",
       "  800: 2024,\n",
       "  801: 2024,\n",
       "  802: 2024,\n",
       "  803: 2024,\n",
       "  804: 2024,\n",
       "  805: 2024,\n",
       "  806: 2024,\n",
       "  807: 2024,\n",
       "  808: 2024,\n",
       "  809: 2024,\n",
       "  810: 2024,\n",
       "  811: 2024,\n",
       "  812: 2024,\n",
       "  813: 2024,\n",
       "  814: 2024,\n",
       "  815: 2024,\n",
       "  816: 2024,\n",
       "  817: 2024,\n",
       "  818: 2024,\n",
       "  819: 2024,\n",
       "  820: 2024,\n",
       "  821: 2024,\n",
       "  822: 2024,\n",
       "  823: 2024,\n",
       "  824: 2024,\n",
       "  825: 2024,\n",
       "  826: 2024,\n",
       "  827: 2024,\n",
       "  828: 2024,\n",
       "  829: 2024,\n",
       "  830: 2024,\n",
       "  831: 2024,\n",
       "  832: 2024,\n",
       "  833: 2024,\n",
       "  834: 2024,\n",
       "  835: 2024,\n",
       "  836: 2024,\n",
       "  837: 2024,\n",
       "  838: 2024,\n",
       "  839: 2024,\n",
       "  840: 2024,\n",
       "  841: 2024,\n",
       "  842: 2024,\n",
       "  843: 2024,\n",
       "  844: 2024,\n",
       "  845: 2024,\n",
       "  846: 2024,\n",
       "  847: 2024,\n",
       "  848: 2024,\n",
       "  849: 2024,\n",
       "  850: 2024,\n",
       "  851: 2024,\n",
       "  852: 2024,\n",
       "  853: 2024,\n",
       "  854: 2024,\n",
       "  855: 2024,\n",
       "  856: 2024,\n",
       "  857: 2024,\n",
       "  858: 2024,\n",
       "  859: 2024,\n",
       "  860: 2024,\n",
       "  861: 2024,\n",
       "  862: 2024,\n",
       "  863: 2024,\n",
       "  864: 2024,\n",
       "  865: 2024,\n",
       "  866: 2024,\n",
       "  867: 2024,\n",
       "  868: 2024,\n",
       "  869: 2024,\n",
       "  870: 2024,\n",
       "  871: 2024,\n",
       "  872: 2024,\n",
       "  873: 2024,\n",
       "  874: 2024,\n",
       "  875: 2024,\n",
       "  876: 2024,\n",
       "  877: 2024,\n",
       "  878: 2024,\n",
       "  879: 2024,\n",
       "  880: 2024,\n",
       "  881: 2024,\n",
       "  882: 2024,\n",
       "  883: 2024,\n",
       "  884: 2024,\n",
       "  885: 2024,\n",
       "  886: 2024,\n",
       "  887: 2024,\n",
       "  888: 2024,\n",
       "  889: 2024,\n",
       "  890: 2024,\n",
       "  891: 2024,\n",
       "  892: 2024,\n",
       "  893: 2024,\n",
       "  894: 2024,\n",
       "  895: 2024,\n",
       "  896: 2024,\n",
       "  897: 2024,\n",
       "  898: 2024,\n",
       "  899: 2024,\n",
       "  900: 2024,\n",
       "  901: 2024,\n",
       "  902: 2024,\n",
       "  903: 2024,\n",
       "  904: 2024,\n",
       "  905: 2024,\n",
       "  906: 2024,\n",
       "  907: 2024,\n",
       "  908: 2024,\n",
       "  909: 2024,\n",
       "  910: 2024,\n",
       "  911: 2024,\n",
       "  912: 2024,\n",
       "  913: 2024,\n",
       "  914: 2024,\n",
       "  915: 2024,\n",
       "  916: 2024,\n",
       "  917: 2024,\n",
       "  918: 2024,\n",
       "  919: 2024,\n",
       "  920: 2024,\n",
       "  921: 2024,\n",
       "  922: 2024,\n",
       "  923: 2024,\n",
       "  924: 2024,\n",
       "  925: 2024,\n",
       "  926: 2024,\n",
       "  927: 2024,\n",
       "  928: 2024,\n",
       "  929: 2024,\n",
       "  930: 2024,\n",
       "  931: 2024,\n",
       "  932: 2024,\n",
       "  933: 2024,\n",
       "  934: 2024,\n",
       "  935: 2024,\n",
       "  936: 2024,\n",
       "  937: 2024,\n",
       "  938: 2024,\n",
       "  939: 2024,\n",
       "  940: 2024,\n",
       "  941: 2024,\n",
       "  942: 2024,\n",
       "  943: 2024,\n",
       "  944: 2024,\n",
       "  945: 2024,\n",
       "  946: 2024,\n",
       "  947: 2024,\n",
       "  948: 2024,\n",
       "  949: 2024,\n",
       "  950: 2024,\n",
       "  951: 2024,\n",
       "  952: 2024,\n",
       "  953: 2024,\n",
       "  954: 2024,\n",
       "  955: 2024,\n",
       "  956: 2024,\n",
       "  957: 2024,\n",
       "  958: 2024,\n",
       "  959: 2024,\n",
       "  960: 2024,\n",
       "  961: 2024,\n",
       "  962: 2024,\n",
       "  963: 2024,\n",
       "  964: 2024,\n",
       "  965: 2024,\n",
       "  966: 2024,\n",
       "  967: 2024,\n",
       "  968: 2024,\n",
       "  969: 2024,\n",
       "  970: 2024,\n",
       "  971: 2024,\n",
       "  972: 2024,\n",
       "  973: 2024,\n",
       "  974: 2024,\n",
       "  975: 2024,\n",
       "  976: 2024,\n",
       "  977: 2024,\n",
       "  978: 2024,\n",
       "  979: 2024,\n",
       "  980: 2024,\n",
       "  981: 2024,\n",
       "  982: 2024,\n",
       "  983: 2024,\n",
       "  984: 2024,\n",
       "  985: 2024,\n",
       "  986: 2024,\n",
       "  987: 2024,\n",
       "  988: 2024,\n",
       "  989: 2024,\n",
       "  990: 2024,\n",
       "  991: 2024,\n",
       "  992: 2024,\n",
       "  993: 2024,\n",
       "  994: 2024,\n",
       "  995: 2024,\n",
       "  996: 2024,\n",
       "  997: 2024,\n",
       "  998: 2024,\n",
       "  999: 2024,\n",
       "  ...}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c70fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (4.66.2)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.7.1-cp311-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.16.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (0.21.4)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-11.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (4.10.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached torch-2.7.1-cp311-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached pillow-11.2.1-cp311-cp311-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached scikit_learn-1.7.0-cp311-cp311-macosx_12_0_arm64.whl (10.7 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.16.0-cp311-cp311-macosx_14_0_arm64.whl (20.8 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, regex, Pillow, networkx, MarkupSafe, joblib, hf-xet, scikit-learn, jinja2, huggingface-hub, torch, tokenizers, transformers, sentence-transformers\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.21.4━━━━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.21.4:0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scikit-learn]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.21.4━━━━━━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scikit-learn]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/18\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 Pillow-11.2.1 hf-xet-1.1.5 huggingface-hub-0.33.0 jinja2-3.1.6 joblib-1.5.1 mpmath-1.3.0 networkx-3.5 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.7.0 scipy-1.16.0 sentence-transformers-4.1.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.7.1 transformers-4.52.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e137aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embeddings = []\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "for abstract in data['abstract'].values():\n",
    "    embeddings.append(model.encode(abstract))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
