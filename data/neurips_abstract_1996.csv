year,proceeding_link,paper_link,title,authors,abstract
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/0188e8b8b014829e2fa0f430f0a95961-Abstract.html,Hebb Learning of Features based on their Information Content,"Ferdinand Peper, Hideki Noda","This  paper  investigates  the  stationary  points  of a  Hebb  learning  rule  with  a  sigmoid  nonlinearity  in  it.  We  show  mathematically  that  when  the  input  has  a  low  information  content,  as  measured  by  the  input's  variance,  this learning rule suppresses learning,  that is,  forces the weight  vector  to  converge  to  the  zero  vector.  When  the  information  content  exceeds  a  certain  value,  the  rule  will  automatically  begin  to  learn  a  feature in the input.  Our analysis suggests that under certain conditions  it  is  the  first  principal  component  that  is  learned.  The  weight  vector  length  remains  bounded,  provided  the  variance  of  the  input  is  finite.  Simulations confirm the theoretical results derived."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/018b59ce1fd616d874afad0f44ba338d-Abstract.html,A Variational Principle for Model-based Morphing,"Lawrence K. Saul, Michael I. Jordan","Given  a  multidimensional  data  set  and  a  model  of  its  density,  we  consider  how  to  define  the  optimal interpolation  between  two  points.  This is done by assigning a cost to each path through space,  based on two competing goals-one to interpolate through regions  of high  density,  the other  to minimize arc  length.  From this  path  functional,  we  derive  the  Euler-Lagrange  equations  for  extremal  motionj given two points, the desired  interpolation is found by solv(cid:173) ing a boundary value problem.  We show that this interpolation can  be done efficiently, in high dimensions, for Gaussian, Dirichlet, and  mixture models."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/01e9565cecc4e989123f9620c1d09c09-Abstract.html,ARTEX: A Self-organizing Architecture for Classifying Image Regions,"Stephen Grossberg, James R. Williamson","A self-organizing architecture is  developed for  image region  classi(cid:173) fication.  The system  consists  of a  preprocessor  that utilizes multi(cid:173) scale filtering, competition, cooperation, and diffusion to compute a  vector  of image boundary  and surface  properties,  notably  texture  and  brightness  properties.  This  vector  inputs  to  a  system  that  incrementally  learns  noisy  multidimensional  mappings  and  their  probabilities.  The  architecture  is  applied  to  difficult  real-world  image  classification  problems,  including  classification  of synthet(cid:173) ic  aperture  radar  and  natural  texture  images,  and  outperforms a  recent state-of-the-art system at classifying natural textures."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/0829424ffa0d3a2547b6c9622c77de03-Abstract.html,The CONDENSATION Algorithm - Conditional Density Propagation and Applications to Visual Tracking,"Andrew Blake, Michael Isard",The power of sampling methods in Bayesian reconstruction of noisy  signals is well known.  The extension of sampling to temporal prob(cid:173) lems  is  discussed.  Efficacy  of sampling over  time is  demonstrated  with visual tracking.
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/0ce2ffd21fc958d9ef0ee9ba5336e357-Abstract.html,A Silicon Model of Amplitude Modulation Detection in the Auditory Brainstem,"André van Schaik, Eric Fragnière, Eric A. Vittoz","Detectim of the periodicity of amplitude modulatim is a major step in  the  determinatim  of  the  pitch  of  a  SOODd.  In  this  article  we  will  present  a silicm model  that uses  synchrroicity of spiking neurms  to  extract  the  fundamental  frequency  of  a  SOODd.  It  is  based  m  the  observatim that the so called  'Choppers'  in the mammalian  Cochlear  Nucleus  synchrmize well  for  certain  rates  of amplitude  modulatim,  depending  m  the  cell's  intrinsic  chopping  frequency.  Our  silicm  model uses  three different circuits, i.e.,  an  artificial cochlea,  an  Inner  Hair Cell circuit, and a spiking neuron circuit"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/0e095e054ee94774d6a496099eb1cf6a-Abstract.html,Adaptive On-line Learning in Changing Environments,"Noboru Murata, Klaus-Robert Müller, Andreas Ziehe, Shun-ichi Amari","An adaptive on-line algorithm extending the learning of learning  idea is proposed and theoretically motivated. Relying only on gra(cid:173) dient flow information it can be applied to learning continuous  functions or distributions, even when no explicit loss function is gi(cid:173) ven and the Hessian is not available. Its efficiency is demonstrated  for a non-stationary blind separation task of acoustic signals."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/0e9fa1f3e9e66792401a6972d477dcc3-Abstract.html,Minimizing Statistical Bias with Queries,David A. Cohn,"I describe  a  querying criterion that attempts to minimize the error  of a  learner  by  minimizing its estimated squared  bias.  I  describe  experiments  with  locally-weighted  regression  on  two  simple prob(cid:173) lems,  and observe  that this  ""bias-only""  approach  outperforms the  more  common  ""variance-only""  exploration  approach,  even  in  the  presence  of noise."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/144a3f71a03ab7c4f46f9656608efdb2-Abstract.html,A Micropower Analog VLSI HMM State Decoder for Wordspotting,"John Lazzaro, John Wawrzynek, Richard P Lippmann","We  describe  the  implementation of a  hidden  Markov  model  state  decoding  system,  a component for  a  wordspotting speech  recogni(cid:173) tion system.  The key  specification for  this state decoder  design  is  microwatt power dissipation; this requirement led to a continuous(cid:173) time, analog circuit implementation.  We characterize the operation  of a  10-word (81  state)  state decoder  test chip."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/147702db07145348245dc5a2f2fe5683-Abstract.html,"Dual Kalman Filtering Methods for Nonlinear Prediction, Smoothing and Estimation","Eric A. Wan, Alex T. Nelson","Prediction,  estimation,  and smoothing  are  fundamental  to  signal  processing.  To perform  these  interrelated  tasks  given  noisy  data,  we  form  a  time  series  model  of  the  process  that  generates  the  data.  Taking noise in the system explicitly into account, maximum(cid:173) likelihood and Kalman frameworks are discussed  which involve the  dual process  of estimating both the model parameters and the un(cid:173) derlying state of the system.  We  review  several  established  meth(cid:173) ods in the linear case,  and propose severa! extensions utilizing dual  Kalman filters  (DKF)  and forward-backward  (FB)  filters  that  are  applicable  to  neural  networks.  Methods  are  compared on  several  simulations of noisy  time series.  We  also  include  an  example  of  nonlinear noise reduction in speech."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/160c88652d47d0be60bfbfed25111412-Abstract.html,Source Separation and Density Estimation by Faithful Equivariant SOM,"Juan K. Lin, Jack D. Cowan, David G. Grier",We  couple  the  tasks  of source  separation  and  density  estimation  by  extracting  the  local  geometrical  structure  of distributions  ob(cid:173) tained  from  mixtures  of statistically  independent  sources.  Our  modifications of the self-organizing map  (SOM)  algorithm  results  in  purely digital learning rules which perform non-parametric his(cid:173) togram density estimation.  The non-parametric nature of the sep(cid:173) aration  allows  for  source  separation  of non-linear  mixtures.  An  anisotropic coupling  is  introduced  into  our  SOM  with  the  role  of  aligning the network locally with the independent component con(cid:173) tours.  This  approach  provides  an  exact  verification  condition  for  source separation with no prior on  the source distributions.
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/17326d10d511828f6b34fa6d751739e2-Abstract.html,Continuous Sigmoidal Belief Networks Trained using Slice Sampling,Brendan J. Frey,"Real-valued  random hidden  variables  can  be  useful  for  modelling  latent  structure  that  explains  correlations  among  observed  vari(cid:173) ables.  I propose a simple unit that adds zero-mean Gaussian noise  to its input before passing it through  a  sigmoidal  squashing func(cid:173) tion.  Such units can produce a variety of useful  behaviors, ranging  from  deterministic to binary stochastic to continuous stochastic.  I  show  how  ""slice sampling""  can  be  used  for  inference  and learning  in  top-down  networks of these  units and demonstrate learning on  two  simple problems."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/17fafe5f6ce2f1904eb09d2e80a4cbf6-Abstract.html,"Compositionality, MDL Priors, and Object Recognition","Elie Bienenstock, Stuart Geman, Daniel Potter","Images  are ambiguous  at  each  of many  levels  of a  contextual  hi(cid:173) erarchy.  Nevertheless,  the high-level  interpretation of most  scenes  is  unambiguous,  as  evidenced  by the superior  performance of hu(cid:173) mans.  This observation argues for global vision models, such as de(cid:173) formable templates.  Unfortunately,  such  models are computation(cid:173) ally intractable for unconstrained problems.  We propose a composi(cid:173) tional model in which  primitives are recursively composed, subject  to syntactic restrictions, to form tree-structured objects and object  groupings.  Ambiguity  is  propagated up the hierarchy in  the form  of multiple interpretations, which are later resolved by a  Bayesian,  equivalently minimum-description-Iength,  cost functional. 
1  Bayesian decision theory and compositionaiity 
In  his  Essay  on  Probability,  Laplace  (1812)  devotes  a  short  chapter-his  ""Sixth  Principle"" -to what  we  call  today  the  Bayesian  decision  rule.  Laplace  observes  that  we  interpret  a  ""regular  combination,""  e.g.,  an  arrangement  of objects  that  displays some particular symmetry, as having resulted from a  ""regular cause""  rather  than arisen  by  chance.  It is  not,  he argues,  that a  symmetric configuration is  less  likely to happen by chance than another arrangement.  Rather, it is  that among all  possible  combinations,  which  are equally  favored  by chance,  there are very  few  of  the regular type:  ""On a table  we  see  letters  arranged in this  order,  Constantinople,  and  we  judge  that  this  arrangement  is  not  the  result  of chance,  not  because  it  is  less  possible  than  the  others,  for  if this  word  were  not  employed  in  any  language 
Compositionality, MDL Priors, and Object Recognition 
839 
we  should  not suspect it came from  any particular  cause,  but this word  being  in use  amongst  us,  it  is  incomparably  more probable  that  some  person  has  thus  arranged  the  aforesaid letters than that this  arrangement is  due  to  chance."" In this example,  regularity is not a mathematical symmetry.  Rather, it is a convention shared among  language users, whereby  Constantinople is  a word, whereas Jpctneolnosant,  a string  containing the same letters but arranged in a  random order,  is  not. 
Central  in  Laplace's argument is  the observation that the number of words in  the  language  is  smaller,  indeed  ""incomparably""  smaller,  than  the  number  of possible  arrangements of letters.  Indeed,  if the collection  of 14-letter words  in  a  language  made up, say, half of all 14-letter strings- a rich language indeed-we would, upon  seeing the string  Constantinople on the table, be far less inclined to deem it a word,  and far  more inclined  to accept it as  a  possible coincidence.  The sparseness of al(cid:173) lowed combinations can be observed at all linguistic articulations (phonetic-syllabic,  syllabic-lexical, lexical-syntactic, syntactic-pragmatic, to use broadly defined levels),  and may be viewed  as a form of redundancy-by analogy to error-correcting codes.  This redundancy was likely devised by evolution to ensure efficient  communication  in  spite  of the  ambiguity of elementary  speech  signals.  The hierarchical  composi(cid:173) tional  structure of natural  visual  scenes  can also  be thought  of as  redundant:  the  rules  that  govern  the  composition  of edge  elements  into  object  boundaries,  of in(cid:173) tensities into surfaces etc., all the way to the assembly of 2-D  projections of named  objects, amount to a  collection of drastic combinatorial restrictions.  Arguably,  this  is why in all but a few-generally hand-crafted-cases, natural images have a unique  high-level interpretation in spite of pervasive low-level ambiguity-this being amply  demonstrated by the performances of our brains. 
In sum, compositionality appears to be a fundamental aspect of cognition (see also  von  der  Malsburg 1981,  1987;  Fodor and Pylyshyn 1988;  Bienenstock,  1991,  1994,  1996;  Bienenstock and Geman 1995).  We  propose here to account for  mental com(cid:173) putation  in  general  and  scene  interpretation in  particular in  terms  of  elementary  composition  operations,  and  describe a  mathematical framework  that  we  have de(cid:173) veloped  to this  effect.  The present  description is  a  cursory one,  and some notions  are illustrated on two simple examples rather than formally defined-for a  detailed  account, see  Geman et al.  (1996),  Potter (1997).  The  binary-image example refers  to an  N  x  N  array of binary-valued pixels,  while the  Laplace-Table  example refers  to a  one-dimensional array of length N, where each position can be filled  with  one  of the 26 letters of the alphabet or remain blank. 
2  Labels and  composition rules 
The  objects  operated  upon  are  denoted  Wi, i  =  1,2, ... , k.  Each  composite object  W  carries  a  label,  I  =  L(w),  and  the  list  of  its  constituents,  (Wt,W2,·· .).  These  uniquely  determine  w,  so  we  write W  =  I (WI, W2, .•. ) .  A  scene S  is  a  collection  of  primitive  objects.  In  the  binary-image  case,  a  scene  S  consists  of a  collection  of  black pixels in the N  x N  array.  All these primitives carry the same label,  L(w)  =  p  (for  ""Point""), and a parameter 7r(w)  which is the position in the image.  In Laplace's  Table, a scene S  consists of an arrangement of characters on the table.  There are 26  primitive labels,  ""A"", ""B"" , ... , ""Z"" , and the parameter of a primitive W  is  its position  1 ~ 7r(w)  ~ N  (all primitives in such a  scene must have different  positions). 
An example of a composite W  in the binary-image case is an arrangement composed 
840 
E.  Bienenstock. S.  Geman and D.  Potter 
of a  black pixel at any position except on the rightmost column and another black  pixel  to  the  immediate  right  of the  first  one.  The  label  is  ""Horizontal  Linelet,""  denoted  L(w)  =  hl,  and there are  N(N - 1)  possible horizontallinelets.  Another  non-primitive  label,  ""Vertical  Linelet,""  or vl,  is  defined  analogously.  An  example  of a  composite  W  for  Laplace's  Table  is  an  arrangement  of 14  neighboring  primi(cid:173) tives  carrying the labels  ""G"", ""0"", ""N"", ""S"", ... , ""E""  in  that order, wherever  that  arrangement will  fit.  We  then have L(w) = Ganstantinople, and there are N  - 13  possible Constantinople objects. 
The  composition  rule  for  label  type  1 consists  of a  binding  junction,  B""  and a  set  of allowed  binding-function  values,  or  binding  support,  S,:  denoting  by  0  the set  of  all  objects  in  the  model,  we  have,  for  any  WI, ' .. ,Wk  E  0,  B, (WI. ... ,Wk)  E  s,  ¢:} l(WI""""  ,Wk)  E O.  In the binary-image example, Bhl(WI,W2)  =  Bv,(WI,W2)  =  (L(WI),L(W2),7I'(W2)  -7I'(WI)),  Sh'  =  {(P,p,(I,O))}  and Sv'  =  {(p,p,(O,I))}  define  the hl- and vl-composition rules, p+p -+  hl and p+p -+  vl.  In Laplace's Table, G +  0+· .. + E  -+ Ganstantinpole is an example of a 14-ary composition rule, where we  must check the label and position of each constituent.  One way to define the binding  function and support for  this rule is:  B(WI, ' ""  ,WI4)  =  (L(WI),' ""  ,L(WI4), 71'(W2)  - 71'(Wt} , 71'(W3)  - 71'(WI),""', 71'(W14)  - 71'(WI))  and S  =  (G,""', E, 1,2"""",13). 
We now introduce recursive labels and composition rules:  the label of the composite  object is  identical to the label of one or more of its constituents, and the rule may  be applied  an arbitrary number of times,  to yield objects of arbitrary complexity.  In  the binary-image case,  we  use a  recursive label  c,  for  Curve,  and an associated  binding function which creates objects of the form  hl + p -+ c, vl + p -+ c,  c + p -+ c,  p + hl  -+  c,  p + vl  -+  c,  p + c  -+  c,  and  c + c  -+  c.  The  reader  may  easily  fill  in  the details,  i.e.,  define  a  binding function  and  binding  support  which  result  in  ""c"" -objects  being  precisely  curves  in  the  image,  where  a  curve  is  of length  at  In  the  previous  examples,  primitives  were  least  3  and  may  be  self-intersecting.  composed  into  compositions;  here  compositions  are  further  composed  into  more  complex  compositions.  In general,  an object W  is  a  labeled  tree,  where each  vertex  carries  the  name  of  an  object,  and  each  leaf is  associated  with  a  primitive  (the  association is  not necessarily one-to-one, as in the case of a  self-intersecting curve). 
Let  M  be  a  model-Le.,  a  collection  of labels  with  their  binding  functions  and  binding  supports-and  0  the  set  of  all  objects  in  M .  We  say  that  object  W  E  o  covers  S  if S  is  precisely  the  set  of  primitives  that  make  up  w's  leaves.  An  interpretation  I  of S  is  any  finite  collection  of objects  in  0  such  that  the  union  of the  sets  of primitives  they  cover  is  S.  We  use  the convention  that,  for  all  M  and S, 10  denotes the  trivial interpretation, defined  as the collection of (unbound)  primitives in S.  In most cases of interest, a model M  will allow many interpretations  for  a  scene  S .  For  instance,  given  a  long  curve  in  the binary-image model,  there  will  be  many  ways  to recursively  construct  a  ""c""-labeled tree that covers  exactly  that curve. 
3  The MDL  formulation 
In  Laplace's  Table,  a  scene  consisting  of  the  string  Constantinople  admits,  in  addition  to  10 ,  the  interpretation  II  =  {WI},  where  WI  is  a  ""G anstantinople"" - object.  We  wish to define a probability distribution D  on interpretations such that  D(I1 )  »  D(Io), in order to realize Laplace's  ""incomparably more probable"".  Our 
Compositionality, MDL Priors, and Object Recognition 
841 
definition of D  will  be motivated by the following  use of the Minimum Description  Length  (MDL)  principle (Rissanen 1989).  Consider a  scene S and pretend we want  to transmit S  as  quickly as  possible through a  noiseless  channel,  hence  we  seek to  encode it as efficiently as possible, i.e., with the shortest possible binary code c.  We  can always use  the trivial interpretation 10:  the codeword  c(Io)  is a  mere list  of n  locations in S.  We  need not specify labels, since there is  only one primitive label in  this example.  The length, or cost,  of this code for  S  is  Ic(Io)1  =  nlog2 (N 2 ). 
Now  however  we  want  to  take  advantage of regularities,  in  the  sense  of Laplace,  that  we  expect to be present  in  S.  We  are specifically  interested in  compositional  regularities, where some arrangements that occur more frequently  than by chance  can  be  interpreted  advantageously  using  an  appropriate  compositional  model  M.  Interpretation I  is advantageous if Ic(I)1  < Ic(Io)l.  An example in the binary-image  case is a linelet scene S.  The trivial encoding of this scene costs us Ic(Io)1  =  2[log2 3+  log2(N2)]  bits,  whereas  the  cost  of the  compositional  interpretation  II  =  {wI}  is  Ic(Idl =  log2 3+log2 (N(N -1)), where WI  is an hI or vl object, as the case may be.  The first  log23  bits  encode the label  L(WI)  E  {p, hi, vi}, and  the rest  encodes  the  position in the image.  The compositional {p, hl, vl} model is therefore advantageous  for a  linelet scene,  since It affords us  a gain in encoding cost of about 2log2 N  bits.  In general, the gain realized by encoding {w}  =  {I (WI, W2)}  instead of {WI, W2}  may  be  viewed  as  a  binding  energy,  measuring  the  affinity  that WI  and  W2  exhibit  for  each  other as  they assemble into w.  This binding energy is c,  =  IC(WI)I  +  IC(W2)1  - I c( I (WI, W2) ) I,  and an efficient  M  is one that contains judiciously chosen cost-saving  composition  rules.  In  effect,  if,  say,  linelets  were  very  rare,  we  would  be  better  off  with  the  trivial  model.  The  inclusion  of non-primitive  labels  would  force  us  to add at least  one bit to the code  of every  object-to specify  its  label-and this  would increase the  average encoding cost, since the infrequent use of non-primitive  labels  would  not balance the extra small  cost  incurred  on primitives.  In practical  applications,  the  construction  of  a  sound  M  is  no  trivial  issue.  Note  however  the simple rationale for  including a  rule such  as  p +  p  --7  hl.  Giving  ourselves the  label hi renders redundant the independent encoding of the positions of horizontally  adjacent pixels.  In general, a good model should allow one to hierarchically compose  with each other frequently  occurring arrangements of objects. 
This use of MDL leads in a straightforward way to an equivalent Bayesian formula(cid:173) tion.  Setting P'(w)  =  2- lc(w)lj L:w'EO 2- lc(w')I  yields  a  probability distribution  P'  on n for  which c is approximately a Shannon code (Cover and Thomas 1991).  With  this definition,  the decision to include the label hl-or the label Con8tantinople(cid:173) would be viewed, in principle, as a statement about the prior probability of finding  horizontal linelets-or  Constantinople strings-in the scene to be interpreted. 
4  The observable-measure formulation 
The MDL formulation  however has a  number of shortcomings;  in  particular, com(cid:173) puting the  binding  energy for  composite objects  can  be  problematic.  We  outline  now  an alternative approach  (Geman et al.  1996,  Potter 1997),  where a  probabil(cid:173) ity distribution  P(w)  on n is  defined  through  a  family  of observable  measures Q,.  These measures assign probabilities to each possible binding-function value,  s E  S""  and also to the primitives.  We  require L:'EM L:sEsr Q,(8)  =  1,  where the notion of  binding function has been extended to primitives via Bprim (w)  =  7r(w)  for primitive 
842 
E.  Bienenstoc/c, S. Geman and D. Potter 
w.  The probabilities  induced  on  0  by  Q,  are  given  by  P(w)  =  Qprim(Bprim(w))  for  a  primitive w,  and P(w) =  Q,(B,(WI,W2))P2(WI,W2IB,(WI,W2))  for  a  composite  object w =  l(wI, W2).1  Here p 2  =  P  X  P  is  the product probability, i.e., the free,  or  not-bound, distribution for the pair (WI, W2)  E 0 2.  For instance, with C + ... + E  -?  Canstantinople,  p 14 (WI,W2,'""  ,w14IBcons ... (W1, ... ,W14)  =  (C, 0,···,13))  is  the  conditional  probability of observing  a  particular string  Constantinople,  under  the  free  distribution,  given  that  (WI, ... , W14)  constitutes such  a  string.  With the rea(cid:173) sonable  assumption  that,  under  Q,  primitives  are  uniformly  distributed  over  the  table,  this  conditional  probability  is  simply  the  inverse  of the  number of possible  Constantinople strings, Le.,  1/(N - 13). 
The  binding  energy,  defined,  by  analogy  to  the  MDL  approach,  as  [,  =  log2(P(w)/(P(wdP(w2))),  now  becomes  [,  =  log2(P  x  P(B'(Wl,W2)))'  Finally,  if I  is  the collection of all finite interpretations /  c 0, we  define the probability of /  E I  as D(/) = IIwElP(w)/Z, with Z  = L:I'EI IIwEl'P(w),  Thus, the probability of an interpretation containing several free objects is obtained  by assuming that these objects occurred in the scene independently of each  other.  Given a  scene S,  recognition is formulated  as the task of maximizing D  over all the  l's in I  that are interpretations of S. 
log2(Q,(B,(wI,w2)))  -
We  now illustrate the use of D  on our two examples.  In the binary-image example  with  model  M  =  {p, hi, vi}, we  use  a  parameter q, 0  ~ q  ~ 1,  to adjust  the prior  probability  of linelets.  Thus,  Qprim(Bprim(W))  =  (1  - q)/N2 for  primitives,  and  Qh'«P,p,O, 1)) = Qv'«P,p, 1,0)) =  q/2 for  linelets.  It is  easily seen that regardless  of  the  normalizing  constant  Z,  the  binding  energy  of two  adjacent  pixels  into  a  linelet  is  [h'  =  [v,  =  log2(q/2)  - log2[{lNf N(N - 1)].  Interestingly,  as  long  as  q =1=  0  and q  =1=  1,  the binding energy,  for  large N, is  approximately 2log2 N, which  is  independent of q.  Thus, the linelet interpretation is  ""incomparably""  more likely  than  the  independent  occurrence  of two  primitives  at  neighboring  positions.  We  leave  it  to  the  reader  to  construct  a  prior  P  for  the  model  {p, hl, vI, c},  e.g.  by  distributing the Q-mass evenly between all  composition rules.  Finally, in  Laplace's  Table,  if there are M  equally likely non-primitive labels-say city names-and q  is  their total mass, the binding energy for  Constantinople is [Cons ...  =  log2  M(r! -13)  - log2[~~.&]14, and the  ""regular""  cause is  again  ""incomparably""  more likely. 
There are several advantages to this reformulation from codewords into probabilities  using  the  Q-parameters.  First,  the Q-parameters can  in  principle  be  adjusted  to  better account for  a  particular world of images.  Second,  we  get an explicit formula  for  the  binding energy,  (namely  log2 (Q / P  x  P)).  Of course,  we  need  to evaluate  the  product  probability  P  x  P,  and this  can be highly  non-trivial-one approach  is  through  sampling,  as  demonstrated  in  Potter  (1997).  Fi~ally,  this  formulation  is  well-suited  for  parameter estimation:  the Q's,  which  are the parameters of the  distribution P, are indeed observables, Le.,  directly available empirically. 
5  Concluding remarks 
The  approach  described  here  was  applied  by  X.  Xing  to  the  recognition  of  ""on(cid:173) line""  handwritten characters, using a  binary-image-type model  as  above,  enriched 
IThis is actually an implicit definition.  Under reasonable conditions, it is well defined(cid:173)
see  Geman et al.  (1996). 
Compositionality, MDL Priors, and Object Recognition 
843 
with  higher-level  labels  including  curved  lines,  straight lines,  angles,  crossings,  T(cid:173) junctions,  L-junctions  {right  angles},  and  the  26  letters  of the alphabet.  In such  a  model,  the search for  an optimal  solution  cannot  be done  exhaustively.  We  ex(cid:173) perimented with a  number of strategies, including a  two-step algorithm which first  generates all possible objects in the scene, and then selects the  ""best""  objects, Le.,  the objects with highest total binding energy, using a greedy method, to yield a final  scene interpretation.  (The total binding energy of W  is  the sum of the binding ener(cid:173) gies  £,  over all  the composition rules  I  used  in the composition of w.  Equivalently,  the total binding energy is  the log-likelihood ratio log2{P{w}/IIi P{Wi)),  where  the  product is  taken over all the primitives Wi  covered by w.} 
The first  step  of the  algorithm  typically  results  in  high-level  objects  partly over(cid:173) lapping on the set of primitives they cover,  i.e.,  competing for  the interpretation of  shared  primitives.  Ambiguity  is  thus  propagated  in  a  ""bottom-up""  fashion.  The  ambiguity is  resolved  in  the second  ""top-down""  pass,  when high-level  composition  rules are used to select the best compositions, at all levels including the lower ones.  A  detailed  account  of our experiments  will  be given  elsewhere.  We  found  the  re(cid:173) sults quite encouraging, particularly in view of the potential scope of the approach.  In effect,  we  believe  that this  approach  is  in principle capable of addressing unre(cid:173) stricted vision problems, where images are typically very ambiguous at lower levels  for  a  variety of reasons-including occlusion and mutual overlap of objects-hence  purely bottom-up segmentation is  impractical. 
Turning now  to  biological implications,  note that dynamic binding in  the nervous  system has been a subject of intensive research and debate in the last decade.  Most  interesting in the present context is  the suggestion, first  clearly articulated by von  der  Malsburg {1981},  that composition may be performed  thanks to a  dual  mech(cid:173) anism  of accurate  synchronization  of spiking  activity-not  necessarily  relying  on  periodic  firing-and  fast  reversible  synaptic  plasticity.  If there is  some  neurobio(cid:173) logical  truth to the model  described  in the present  paper,  the binding mechanism  proposed  by  von  der  Malsburg would  appear to be an attractive implementation.  In effect, the use of fine temporal structure of neural activity opens up a large realm  of possible high-order codes in  networks of neurons. 
In the present  model,  constituents  always  bind  in  the  service of a  new  object,  an  operation one may refer to as  triangular  binding.  Composite objects can engage in  further composition, thus giving rise to arbitrarily deep  tree-structured constructs.  Physiological evidence of triangular binding in the visual system can be found in Sil(cid:173) lito et al.  {1994}; Damasio {1989} describes an approach derived from neuroanatom(cid:173) ical  data and lesion  studies that is  largely consistent with  the formalism  described  here. 
An  important requirement for  the neural representation of the tree-structured ob(cid:173) jects  used  in  our model  is  that the doing and undoing of links  operating on  some  constituents, say Wi  and W2,  while affecting in some  useful way the high-order pat(cid:173) terns that represent these objects, leaves these patterns, as representations of Wi  and  W2,  intact.  A family of biologically plausible patterns that would appear to satisfy  this  requirement  is  provided  by  synfire  patterns  {Abeles  1991}.  We  hypothesized  elsewhere  {Bienenstock 1991,  1994,  1996}  that synfire chains could be dynamically  bound via weak synaptic couplings; such couplings would synchronize the wave-like  activities  of two  synfire  chains,  in  much  the same  way  as  coupled  oscillators  lock 
844 
E.  Bienenstock, S.  Geman and D.  Potter 
their  phases.  Recursiveness  of compositionality could,  in  principle,  arise from  the  further  binding of these composite structures. 
Acknow ledgements 
Supported by the Army Research Office (DAAL03-92-G-0115), the National Science  Foundation (DMS-9217655), and the Office of Naval  Research (N00014-96-1-0647)."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/193002e668758ea9762904da1a22337c-Abstract.html,A Mean Field Algorithm for Bayes Learning in Large Feed-forward Neural Networks,"Manfred Opper, Ole Winther",We present an algorithm which is expected to realise Bayes optimal  predictions in large feed-forward networks.  It is based on mean field  methods developed  within  statistical  mechanics  of disordered  sys(cid:173) tems.  We give a derivation for  the single layer perceptron and show  that  the  algorithm  also  provides  a  leave-one-out  cross-validation  test of the  predictions.  Simulations show excellent  agreement with  theoretical  results  of statistical mechanics.
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/1c65cef3dfd1e00c0b03923a1c591db4-Abstract.html,Ordered Classes and Incomplete Examples in Classification,Mark Mathieson,"The classes in classification tasks often have a natural ordering, and the  training and testing examples are often incomplete.  We  propose a non(cid:173) linear ordinal  model for  classification into ordered classes.  Predictive,  simulation-based approaches are used to learn from past and classify fu(cid:173) ture  incomplete examples.  These techniques are  illustrated  by  making  prognoses for patients who have suffered severe head injuries."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html,Unification of Information Maximization and Minimization,Ryotaro Kamimura,"In  the  present  paper,  we  propose  a  method  to  unify  information  maximization and minimization  in  hidden  units.  The information  maximization and minimization are performed on two different lev(cid:173) els:  collective and individual level.  Thus, two kinds  of information:  collective and  individual  information  are  defined.  By  maximizing  collective  information  and  by  minimizing  individual  information,  simple  networks  can  be  generated  in  terms  of the  number  of con(cid:173) nections  and  the  number  of hidden  units.  Obtained  networks  are  expected  to give better generalization and improved interpretation  of internal representations.  This method  was applied  to the infer(cid:173) ence  of the  maximum onset  principle  of an artificial  language.  In  this  problem,  it  was  shown  that  the  individual  information  min(cid:173) imization  is  not  contradictory  to  the  collective  information  max(cid:173) imization.  In  addition,  experimental  results  confirmed  improved  generalization performance,  because over-training can significantly  be suppressed."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/1d72310edc006dadf2190caad5802983-Abstract.html,Multi-Task Learning for Stock Selection,"Joumana Ghosn, Yoshua Bengio","Artificial  Neural  Networks  can  be  used  to  predict  future  returns  of stocks  in order to  take financial  decisions .  Should  one  build  a  separate network for  each stock or share the same network for  all  the  stocks?  In  this  paper  we  also  explore  other  alternatives,  in  which  some  layers  are  shared  and  others  are  not  shared.  When  the  prediction  of future  returns  for  different  stocks  are viewed  as  different  tasks,  sharing  some  parameters  across  stocks  is  a  form  of multi-task  learning.  In  a  series  of experiments  with  Canadian  stocks,  we obtain yearly returns that are more than 14% above  various benchmarks. 
1"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/1ee3dfcd8a0645a25a35977997223d22-Abstract.html,Cholinergic Modulation Preserves Spike Timing Under Physiologically Realistic Fluctuating Input,"Akaysha C. Tang, Andreas M. Bartels, Terrence J. Sejnowski","Neuromodulation  can  change  not  only  the  mean  firing  rate  of a  neuron,  but  also  its  pattern  of firing .  Therefore,  a  reliable  neu(cid:173) ral  coding  scheme,  whether  a  rate  coding  or  a  spike  time  based  coding,  must  be  robust  in  a  dynamic  neuromodulatory  environ(cid:173) ment.  The common observation that cholinergic modulation leads  to  a  reduction  in  spike  frequency  adaptation  implies  a  modifica(cid:173) tion  of spike  timing,  which  would  make  a  neural  code  based  on  precise spike timing difficult to maintain.  In this paper, the effects  of cholinergic modulation were  studied to test  the hypothesis that  precise  spike timing can serve  as a  reliable neural code.  Using  the  whole  cell  patch-clamp technique  in rat neocortical  slice  prepara(cid:173) tion and compartmental modeling techniques,  we show that cholin(cid:173) ergic  modulation, surprisingly,  preserved  spike  timing in response  to a fluctuating  inputs that resembles  in  vivo conditions.  This re(cid:173) sult suggests that in vivo spike timing may be much more resistant  to changes in neuromodulator concentrations than previous physi(cid:173) ological studies have  implied. 
112 
A. C.  Tang, A.  M.  Bartels and T.  J. Sejnowski"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/242c100dc94f871b6d7215b868a875f8-Abstract.html,Analytical Mean Squared Error Curves in Temporal Difference Learning,"Satinder P. Singh, Peter Dayan","We  have  calculated  analytical  expressions  for  how  the  bias  and  variance of the estimators provided by various temporal difference  value estimation algorithms change with offline updates over trials  in absorbing Markov chains using lookup table representations.  We  illustrate  classes  of learning curve  behavior in various  chains,  and  show the manner in which TD is sensitive to the choice of its step(cid:173) size  and eligibility trace parameters."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/25df35de87aa441b88f22a6c2a830a17-Abstract.html,Dynamics of Training,"Siegfried Bös, Manfred Opper","A new method to calculate the full training process of a neural net(cid:173) work is  introduced.  No sophisticated methods like the replica trick  are  used.  The results  are directly  related  to  the actual number of  training  steps.  Some  results  are  presented  here,  like  the  maximal  learning rate, an exact description of early stopping, and the neces(cid:173) sary number of training steps.  Further problems can be addressed  with  this approach."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/26588e932c7ccfa1df309280702fe1b5-Abstract.html,A Constructive RBF Network for Writer Adaptation,"John C. Platt, Nada Matic","This  paper  discusses  a  fairly  general  adaptation  algorithm  which  augments a standard neural network  to increase  its recognition  ac(cid:173) curacy  for  a  specific  user.  The  basis for  the  algorithm is  that  the  output of a neural network is characteristic of the input, even when  the  output  is  incorrect.  We  exploit  this  characteristic  output  by  using  an  Output  Adaptation  Module  (OAM)  which  maps this out(cid:173) put  into  the  correct  user-dependent  confidence  vector.  The  OAM  is  a  simplified  Resource  Allocating  Network  which  constructs  ra(cid:173) dial  basis  functions  on-line.  We  applied  the  OAM  to  construct  a  writer-adaptive  character  recognition  system  for  on-line  hand(cid:173) printed  characters.  The  OAM  decreases  the  word  error  rate  on  a  test  set  by  an  average  of 45%,  while  creating  only  3  to  25  basis  functions for  each  writer  in  the  test  set."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/2715518c875999308842e3455eda2fe3-Abstract.html,MLP Can Provably Generalize Much Better than VC-bounds Indicate,"Adam Kowalczyk, Herman L. Ferrá","Results  of a  study of the  worst  case  learning  curves  for  a  partic(cid:173) ular class of probability  distribution on input  space  to MLP  with  hard  threshold  hidden  units are  presented.  It is  shown in  partic(cid:173) ular,  that  in  the  thermodynamic  limit  for  scaling  by  the  number  of connections to the first  hidden layer, although the true learning  curve behaves as  ~ a-I for  a  ~ 1, its VC-dimension  based bound  is trivial  (= 1)  and  its VC-entropy bound is trivial for  a  ::;  6.2.  It  is also shown that bounds following the true learning curve can be  derived  from  a  formalism  based on the density of error patterns."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/2812e5cf6d8f21d69c91dddeefb792a7-Abstract.html,3D Object Recognition: A Model of View-Tuned Neurons,"Emanuela Bricolo, Tomaso Poggio, Nikos K. Logothetis","In  1990  Poggio and  Edelman proposed  a  view-based  model of ob(cid:173) ject recognition that accounts for several psychophysical properties  of certain  recognition  tasks.  The model predicted  the existence  of  view-tuned  and view-invariant units,  that were  later found  by  Lo(cid:173) gothetis  et  al.  (Logothetis  et  al.,  1995)  in  IT  cortex  of monkeys  trained  with views  of specific  paperclip  objects.  The model,  how(cid:173) ever,  does  not specify  the inputs to the view-tuned  units and their  internal  organization.  In  this  paper  we  propose  a  model of these  view-tuned  units  that  is  consistent  with  physiological  data from  single cell  responses."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/285ab9448d2751ee57ece7f762c39095-Abstract.html,Learning Bayesian Belief Networks with Neural Network Estimators,"Stefano Monti, Gregory F. Cooper","In  this  paper  we  propose  a  method  for  learning  Bayesian  belief  networks  from  data.  The  method  uses  artificial  neural  networks  as  probability estimators, thus  avoiding the need for  making prior  assumptions on the nature of the probability distributions govern(cid:173) ing  the relationships  among the  participating variables.  This new  method has  the potential for  being  applied to domains containing  both discrete  and continuous variables  arbitrarily distributed.  We  compare  the  learning  performance  of this  new  method  with  the  performance  of the  method  proposed  by  Cooper  and  Herskovits  in  [7].  The experimental results  show  that,  although  the  learning  scheme based on the use  of ANN  estimators is  slower, the learning  accuracy  of the two methods is  comparable.  Category:  Algorithms and Architectures."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/286674e3082feb7e5afb92777e48821f-Abstract.html,Approximate Solutions to Optimal Stopping Problems,"John N. Tsitsiklis, Benjamin Van Roy","We  propose and analyze an algorithm that approximates solutions  to the problem of optimal stopping in a discounted irreducible ape(cid:173) riodic  Markov  chain.  The  scheme  involves  the  use  of linear  com(cid:173) binations  of  fixed  basis  functions  to  approximate  a  Q-function.  The weights  of the  linear  combination  are incrementally  updated  through  an iterative process similar  to  Q-Iearning,  involving sim(cid:173) ulation of the underlying  Markov chain.  Due to space limitations,  we  only provide an overview of a  proof of convergence  (with prob(cid:173) ability 1)  and bounds on the approximation error.  This is  the first  theoretical result  that establishes the soundness  of a  Q-Iearning(cid:173) like  algorithm  when  combined  with  arbitrary  linear  function  ap(cid:173) proximators  to  solve  a  sequential  decision  problem.  Though  this  paper focuses  on  the case of finite  state spaces, the results extend  naturally to continuous and unbounded state spaces, which are ad(cid:173) dressed in a forthcoming full-length  paper."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/2a50e9c2d6b89b95bcb416d6857f8b45-Abstract.html,An Orientation Selective Neural Network for Pattern Identification in Particle Detectors,"Halina Abramowicz, David Horn, Ury Naftaly, Carmit Sahar-Pikielny","We  present  an  algorithm for  identifying linear  patterns  on  a  two(cid:173) dimensional lattice based on the concept of an orientation selective  cell,  a  concept  borrowed  from  neurobiology  of vision.  Construct(cid:173) ing  a  multi-layered  neural  network  with  fixed  architecture  which  implements orientation selectivity,  we  define  output elements cor(cid:173) responding  to different  orientations,  which  allow  us  to make  a  se(cid:173) lection  decision.  The  algorithm takes into account  the granularity  of the lattice as well  as the presence of noise and inefficiencies.  The  method  is  applied  to  a  sample  of data  collected  with  the  ZEUS  detector  at  HERA  in  order  to  identify  cosmic  muons  that  leave  a  linear  pattern  of signals  in  the  segmented  calorimeter.  A  two  dimensional  representation  of the  relevant  part  of the  detector  is  used.  The  algorithm performs  very  well.  Given  its  architecture,  this system  becomes a  good candidate for fast  pattern recognition  in  parallel processing  devices."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/2ac2406e835bd49c70469acae337d292-Abstract.html,Early Brain Damage,"Volker Tresp, Ralph Neuneier, Hans-Georg Zimmermann","Optimal Brain Damage  (OBD)  is  a  method for  reducing  the num(cid:173) ber of weights in a  neural network.  OBD estimates the increase  in  cost  function  if weights  are  pruned  and  is  a  valid  approximation  if the learning algorithm has converged  into a  local minimum.  On  the  other hand it is  often desirable  to terminate the learning pro(cid:173) cess  before  a  local  minimum is  reached  (early  stopping).  In  this  paper  we  show  that  OBD  estimates  the  increase  in cost  function  incorrectly  if the network is  not in a local minimum.  We also show  how  OBD  can  be  extended  such  that  it  can  be  used  in  connec(cid:173) tion  with  early  stopping.  We  call  this  new  approach  Early  Brain  Damage,  EBD.  EBD also allows to revive  already pruned weights.  We  demonstrate  the  improvements achieved  by  EBD  using  three  publicly available data sets."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/2b6d65b9a9445c4271ab9076ead5605a-Abstract.html,Text-Based Information Retrieval Using Exponentiated Gradient Descent,"Ron Papka, James P. Callan, Andrew G. Barto",The following  investigates  the  use  of single-neuron  learning  algo(cid:173) rithms  to improve  the  performance of text-retrieval  systems  that  accept  natural-language  queries.  A  retrieval  process  is  explained  that transforms the natural-language query into the query syntax  of a real retrieval system:  the initial query is expanded using statis(cid:173) tical and learning techniques and is then used for document ranking  and binary classification.  The results  of experiments suggest that  Kivinen and Warmuth's Exponentiated Gradient Descent learning  algorithm works significantly better than previous approaches.
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/2ba8698b79439589fdd2b0f7218d8b07-Abstract.html,An Analog Implementation of the Constant Average Statistics Constraint For Sensor Calibration,"John G. Harris, Yu-Ming Chiang",We  use  the  constant  statistics  constraint  to calibrate an  array  of  sensors that contains gain and offset variations.  This algorithm has  been mapped to analog hardware and designed and fabricated with  a 2um CMOS technology.  Measured results from the chip show that  the system achieves invariance to gain and offset  variations of the  input signal.
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/2c89109d42178de8a367c0228f169bf8-Abstract.html,A Neural Model of Visual Contour Integration,Zhaoping Li,"We  introduce  a  neurobiologically plausible model  of contour inte(cid:173) gration from visual inputs of individual oriented edges.  The model  is  composed  of interacting  excitatory  neurons  and  inhibitory  in(cid:173) terneurons, receives visual inputs via oriented receptive fields  (RFs)  like those in VI.  The RF centers are distributed in space.  At each  location,  a  finite  number  of cells  tuned  to  orientations  spanning  1800  compose a  model hypercolumn.  Cortical interactions modify  neural activities  produced by  visual  inputs,  selectively amplifying  activities for edge elements belonging to smooth input contours.  El(cid:173) ements within one contour produce synchronized neural activities.  We  show  analytically  and  empirically  that  contour  enhancement  and  neural  synchrony  increase  with  contour  length,  smoothness  and closure, as  observed experimentally.  This model gives testable  predictions, and in  addition, introduces a  feedback  mechanism  al(cid:173) lowing  higher  visual  centers  to  enhance,  suppress,  and  segment  contours."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html,Unsupervised Learning by Convex and Conic Coding,"Daniel D. Lee, H. Sebastian Seung","Unsupervised  learning algorithms  based  on  convex  and  conic  en(cid:173) coders are proposed.  The encoders find  the closest convex or conic  combination of basis vectors to the input.  The learning algorithms  produce basis vectors that minimize the reconstruction error of the  encoders.  The convex  algorithm develops  locally  linear models  of  the  input,  while  the  conic  algorithm  discovers  features.  Both  al(cid:173) gorithms are used to model handwritten digits and compared with  vector quantization and principal component analysis.  The neural  network implementations involve feedback connections that project  a  reconstruction back to the input layer."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/2df45244f09369e16ea3f9117ca45157-Abstract.html,Reconstructing Stimulus Velocity from Neuronal Responses in Area MT,"Wyeth Bair, James R. Cavanaugh, J. Anthony Movshon","We  employed  a  white-noise velocity  signal to study the dynamics  of the response of single neurons in the cortical area MT to visual  motion.  Responses were quantified using reverse correlation, opti(cid:173) mal linear reconstruction filters,  and reconstruction signal-to-noise  ratio  (SNR).  The  SNR and  lower  bound estimates  of information  rate were  lower  than we  expected.  Ninety percent of the informa(cid:173) tion was transmitted below  18 Hz, and the highest lower bound on  bit rate was  12  bits/so  A  simulated opponent motion energy sub(cid:173) unit with Poisson spike statistics was  able to out-perform the MT  neurons.  The temporal integration window, measured from  the re(cid:173) verse  correlation half-width,  ranged  from  30-90 ms.  The window  was  narrower  when  a  stimulus  moved  faster,  but  did  not  change  when temporal frequency  was  held constant."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/310ce61c90f3a46e340ee8257bc70e93-Abstract.html,Multidimensional Triangulation and Interpolation for Reinforcement Learning,Scott Davies,"Dynamic  Programming,  Q-Iearning  and  other  discrete  Markov  Decision  Process  solvers  can be -applied to continuous d-dimensional state-spaces  by  quantizing the state space into an array of boxes.  This is often problematic  above  two dimensions:  a  coarse  quantization can lead  to poor policies, and  fine  quantization is too expensive.  Possible solutions are variable-resolution  discretization,  or function  approximation  by  neural  nets.  A  third option,  which  has  been  little  studied  in  the  reinforcement  learning  literature,  is  interpolation on  a  coarse  grid.  In  this  paper we  study  interpolation tech(cid:173) niques  that  can  result  in  vast  improvements in  the  online  behavior of the  resulting  control  systems:  multilinear  interpolation,  and  an  interpolation  algorithm  based  on  an  interesting  regular  triangulation  of d-dimensional  space.  We  adapt  these  interpolators  under  three  reinforcement  learning  paradigms:  (i)  offline value iteration with  a  known  model,  (ii)  Q-Iearning,  and  (iii)  online  value  iteration  with  a  previously  unknown  model  learned  from data.  We  describe empirical results,  and the resulting implications for  practical learning of continuous non-linear dynamic control. 
1  GRID-BASED INTERPOLATION TECHNIQUES  Reinforcement  learning algorithms generate functions  that map states to  ""cost-t
•  Fine grids may be used  in one or two dimensions.  Above  two dimensions,  fine  grids  are  too expensive.  Value  functions  can  be  discontinuous,  which  (as we will see) can lead to su boptimalities even with very fine  discretization  in two dimensions . 
•  Neural  nets have  been  used  in conjunction  with TD  [Sutton,  1988]  and  Q-Iearning [Watkins,  1989]  in very  high dimensional spaces [Tesauro,  1991,  Crites  and  Barto,  1996].  While promising, it  is  not  always  clear  that they  produce  the  accurate  value  functions  that  might  be  needed  for  fine  near(cid:173) optimal control of dynamic systems, and the most commonly used  methods  of applying value  iteration or policy iteration with a  neural-net  value func(cid:173) tion are often unstable.  [Boyan  and  Moore,  1995]."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/3210ddbeaa16948a702b6049b8d9a202-Abstract.html,Viewpoint Invariant Face Recognition using Independent Component Analysis and Attractor Networks,"Marian Stewart Bartlett, Terrence J. Sejnowski","We  have  explored  two  approaches  to  recogmzmg  faces  across  changes in pose.  First, we developed a representation of face images  based on independent  component analysis  (ICA)  and compared it  to  a  principal  component  analysis  (PCA)  representation for  face  recognition.  The  ICA  basis  vectors  for  this  data set  were  more  spatially local than the PCA basis vectors and the ICA representa(cid:173) tion had greater invariance to changes in pose.  Second, we present  a  model  for  the  development  of  viewpoint  invariant  responses  to  faces  from  visual experience  in a  biological system.  The temporal  continuity  of natural  visual  experience  was  incorporated  into  an  attractor network model  by  Hebbian learning following  a  lowpass  temporal  filter  on  unit  activities.  When  combined  with  the  tem(cid:173) poral filter,  a  basic  Hebbian  update rule  became  a  generalization  of Griniasty  et  al.  (1993),  which  associates  temporally  proximal  input patterns into basins of attraction.  The system acquired rep(cid:173) resentations of faces  that were largely independent of pose. 
1 
Independent  component representations of faces 
Important  advances  in face  recognition  have  employed  forms  of principal  compo(cid:173) nent analysis, which  considers only second-order moments of the input  (Cottrell &  Metcalfe,  1991;  Turk  &  Pentland  1991).  Independent  component  analysis  (ICA)  is  a  generalization of principal component  analysis  (PCA),  which  decorrelates the  higher-order moments of the input  (Comon,  1994).  In a task such  as face  recogni(cid:173) tion,  much of the important information is contained in the high-order statistics of  the images.  A representational basis in which the high-order statistics are decorre(cid:173) lated may  be more powerful for face recognition than one in which  only the second  order statistics are decorrelated, as in PCA representations.  We compared an ICA(cid:173) based  representation  to  a  PCA-based  representation  for  recognizing  faces  across  changes in pose. 
818 
M. S.  Bartlett and T.  J.  Sejnowski"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/33ceb07bf4eeb3da587e268d663aba1a-Abstract.html,On the Effect of Analog Noise in Discrete-Time Analog Computations,"Wolfgang Maass, Pekka Orponen","We  introduce a  model  for  noise-robust  analog  computations  with  discrete  time  that is  flexible  enough  to cover  the  most  important  concrete  cases,  such  as  computations  in  noisy  analog  neural  nets  and networks of noisy spiking neurons.  We  show that the presence  of arbitrarily small  amounts of analog noise  reduces the  power  of  analog  computational  models  to  that  of  finite  automata,  and  we  also  prove  a  new  type  of  upper  bound  for  the  VC-dimension  of  computational models with  analog noise."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/3546ab441e56fa333f8b44b610d95691-Abstract.html,Combinations of Weak Classifiers,"Chuanyi Ji, Sheng Ma","To obtain classification systems with both good generalization per(cid:173) formance  and  efficiency  in  space  and  time,  we  propose  a  learning  method based on combinations of weak classifiers,  where weak clas(cid:173) sifiers are linear classifiers (perceptrons) which can do a little better  than making random guesses.  A randomized algorithm is  proposed  to find  the weak classifiers.  They· are then combined through a  ma(cid:173) jority vote.  As  demonstrated through systematic experiments,  the  method developed is able to obtain combinations of weak classifiers  with  good  generalization  performance  and  a  fast  training  time on  a  variety of test  problems and real  applications."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/3948ead63a9f2944218de038d8934305-Abstract.html,Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems,"Satinder P. Singh, Dimitri P. Bertsekas","In cellular telephone systems, an important problem is to dynami(cid:173) cally allocate the communication resource  (channels) so as to max(cid:173) imize  service  in  a  stochastic  caller  environment.  This problem  is  naturally formulated  as  a  dynamic programming problem and  we  use a reinforcement learning (RL) method to find  dynamic channel  allocation policies that are better than previous heuristic solutions.  The policies obtained perform well for  a  broad variety of call traf(cid:173) fic  patterns.  We  present  results  on  a  large  cellular  system  with  approximately 4949  states. 
In  cellular  communication systems,  an  important problem  is  to  allocate the com(cid:173) munication resource  (bandwidth) so as to maximize the service provided to a set of  mobile callers whose  demand for  service  changes stochastically.  A given  geograph(cid:173) ical  area is  divided  into mutually disjoint  cells,  and each  cell  serves  the  calls that  are  within  its  boundaries  (see  Figure  1a).  The total system  bandwidth is  divided  into channels, with each channel centered  around a frequency.  Each channel can be  used simultaneously at different  cells,  provided these  cells are sufficiently separated  spatially, so that there  is  no interference  between  them.  The minimum separation  distance between simultaneous reuse of the same channel is called the  channel reuse  constraint. 
When a call requests service in a given cell  either a free  channel  (one that does not  violate the channel  reuse  constraint)  may be assigned  to the  call, or else  the call  is  blocked  from  the system;  this  will  happen  if no free  channel  can  be found.  Also,  when a mobile caller crosses from one cell to another, the call is  ""handed off""  to the  cell of entry;  that is,  a  new  free  channel is provided to the call at the new  cell.  If no  such  channel  is available,  the call  must be dropped/disconnected  from  the system. 
RLfor Dynamic Channel Allocation 
975 
One  objective  of a  channel  allocation  policy  is  to  allocate  the  available  channels  to calls  so  that the  number of blocked  calls is  minimized.  An  additional objective  is  to  minimize the  number of calls  that  are  dropped  when  they  are  handed off to  a  busy  cell.  These  two objectives  must  be  weighted  appropriately  to reflect  their  relative importance, since dropping existing calls is generally more undesirable than  blocking new  calls.  To  illustrate  the  qualitative  nature  of the  channel  assignment  decisions,  suppose  that  there  are  only  two  channels  and  three  cells  arranged  in  a  line.  Assume  a  channel  reuse  constraint  of 2,  i.e.,  a  channel  may  be  used  simultaneously  in  cells  1 and  3,  but may not  be  used  in  channel  2  if it is  already  used  in  cell  1 or  in  cell  3.  Suppose  that  the  system  is  serving  one  call  in  cell  1 and  another  call  in  cell  3.  Then  serving  both  calls  on  the  same  channel  results  in  a  better  channel  usage  pattern than serving them on different  channels,  since  in  the former case the other  channel  is  free  to  be  used  in  cell  2.  The  purpose  of the  channel  assignment  and  channel rearrangement strategy is,  roughly speaking, to create such favorable usage  patterns that minimize the likelihood of calls  being blocked. 
We formulate the channel assignment problem as a dynamic programming problem,  which,  however,  is too complex to be solved exactly.  We introduce approximations  based on the methodology of reinforcement learning (RL) (e.g.,  Barto,  Bradtke and  Singh,  1995, or the recent textbook by  Bertsekas and Tsitsiklis, 1996).  Our method  learns channel allocation policies that outperform not only the most commonly used  policy  in  cellular  systems,  but  also  the  best  heuristic  policy  we  could  find  in  the  literature. 
1  CHANNEL ASSIGNMENT POLICIES 
Many cellular systems are based on a fixed  assignment  (FA) channel allocation; that  is,  the set  of channels  is  partitioned,  and  the  partitions are  permanently  assigned  to cells  so  that  all  cells  can  use  all  the  channels  assigned  to them  simultaneously  without  interference  (see  Figure  1a).  When  a  call  arrives  in  a  cell,  if any  pre(cid:173) assigned channel is unused; it is assigned, else the call is blocked.  No rearrangement  is done when a call terminates.  Such a policy is static and cannot take advantage of  temporary stochastic  variations in  demand for  service.  More  efficient  are  dynamic  channel  allocation  policies,  which  assign  channels  to different  cells,  so  that  every  channel  is  available to every  cell  on  a  need  basis,  unless  the  channel  is  used  in  a  nearby  cell  and the reuse  constraint is  violated. 
The  best  existing  dynamic  channel  allocation  policy  we  found  in  the  literature  is  Borrowing with  Directional Channel  Locking  (BDCL)  of Zhang  &  Yum  (1989).  It  numbers the  channels  from  1 to N,  partitions and  assigns  them to cells  as  in  FA.  The  channels  assigned  to  a  cell  are  its  nominal  channels.  If a  nominal  channel  is  available  when  a  call  arrives  in  a  cell,  the  smallest  numbered  such  channel  is  assigned  to the call.  If no  nominal channel  is  available, then  the  largest  numbered  free  channel  is  borrowed from  the neighbour  with the most free  channels.  When a  channel  is  borrowed,  careful  accounting  of the  directional  effect  of which  cells  can  no  longer  use  that  channel  because  of interference  is  done.  The  call  is  blocked  if  there  are  no free  channels  at  all.  When a  call  terminates  in  a  cell  and the channel  so  freed  is  a  nominal channel,  say  numbered  i,  of that  cell,  then  if there  is  a  call  in  that  cell  on  a  borrowed  channel,  the  call  on  the  smallest  numbered  borrowed  channel  is  reassigned  to i  and the borrowed  channel  is  returned  to the appropriate  cell.  If there  is no  call  on  a  borrowed  channel,  then  if there  is  a  call  on  a  nominal  channel numbered  larger than i, the call on the highest numbered nominal channel  is  reassigned  to i.  If the call just terminated was  itself on  a  borrowed channel,  the 
976 
S.  Singh and D.  Bertsekas 
call on the smallest numbered borrowed channel is reassigned to it and that channel  is  returned  to the  cell  from  which  it  was  borrowed.  Notice  that  when  a  borrowed  channel  is  returned  to its original cell,  a  nominal channel  becomes free  in that cell  and triggers  a  reassignment.  Thus,  in the  worst  case  a  call  termination in one  cell  can sequentially cause reassignments in arbitrarily far  away cells - making BDCL  somewhat impractical. 
BOCL is quite sophisticated and combines the notions of channel-ordering, nominal  channels,  and  channel  borrowing.  Zhang  and  Yum  (1989)  show  that  BOCL  is  superior  to  its  competitors,  including  FA.  Generally,  BOCL  has  continued  to  be  highly  regarded  in  the  literature  as  a  powerful  heuristic  (Enrico  et.al.,  1996) .  In  this  paper,  we  compare  the  performance  of dynamic  channel  allocation  policies  learned  by  RL  with  both  FA  and  BOCL. 
1.1  DYNAMIC  PROGRAMMING FORMULATION 
We  can formulate the dynamic channel allocation problem using dynamic program(cid:173) ming (e.g.,  Bertsekas,  1995).  State transitions occur when channels become free due  to call departures,  or  when  a  call  arrives  at a  given  cell  and  wishes  to be  assigned  a  channel,  or when  there  is  a  handoff,  which  can  be  viewed  as  a  simultaneous call  departure  from  one  cell  and  a  call  arrival  at  another  cell.  The  state  at  each  time  consists of two components: 
(1)  The  list  of occupied  and  unoccupied  channels  at  each  cell.  We  call  this  the  configuration of the cellular system.  It is  exponential in the number of cells.  (2) The event that causes the state transition (arrival, departure, or handoff).  This 
component of the state is uncontrollable. 
The  decision/control  applied  at  the  time of a  call  departure  is  the  rearrangement  of the  channels  in  use  with  the  aim of creating  a  more favorable  channel  packing  pattern  among  the  cells  (one  that  will  leave  more  channels  free  for  future  assign(cid:173) ments) .  Unlike  BDCL,  our  RL  solution  will restrict  this rearrangement  to the  cell  with  the  current  call departure.  The  control exercised  at the time of a  call  arrival  is  the  assignment of a  free  channel,  or the blocking of the call if no free  channel  is  currently  available.  In  general,  it may also  be  useful  to  do  admission  control,  i.e.,  to  allow  the  possibility  of not  accepting  a  new  call  even  when  there  exists  a  free  channel to minimize the dropping of ongoing calls during handoff in the future.  We  address  admission control in a separate paper  and here  restrict  ourselves  to always  accepting a call if a free  channel is available.  The objective is to learn a  policy that  assigns  decisions  (assignment or  rearrangement  depending  on  event)  to each  state  so  as  to  maximize 
J  =  E {lCO  e- f3t e(t)dt} , 
where  E{-} is  the expectation operator,  e(t) is  the number of ongoing calls at time  t,  and j3  is  a discount factor that makes immediate profit more valuable than future  profit.  Maximizing J  is equivalent to minimizing the expected  (discounted)  number  of blocked  calls over  an infinite horizon. 
2  REINFORCEMENT LEARNING SOLUTION 
RL  methods solve optimal control (or dynamic programming) problems by  learning  good  approximations  to  the  optimal  value  function,  J*,  given  by  the  solution  to 
RLfor Dynamic Channel Allocation 
977 
the  Bellman optimality equation  which  takes  the  following form  for  the  dynamic  channel allocation problem:"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/39e4973ba3321b80f37d9b55f63ed8b8-Abstract.html,Neural Learning in Structured Parameter Spaces - Natural Riemannian Gradient,Shun-ichi Amari,"The  parameter  space  of neural  networks  has  a  Riemannian  met(cid:173) ric  structure.  The  natural  Riemannian  gradient  should  be  used  instead of the  conventional gradient, since  the former  denotes  the  true steepest descent direction of a loss function in the Riemannian  space.  The behavior of the  stochastic gradient learning  algorithm  is  much more effective if the natural gradient is  used.  The present  paper studies the information-geometrical structure of perceptrons  and  other  networks,  and  prove  that  the  on-line  learning  method  based  on  the  natural  gradient  is  asymptotically  as  efficient  as  the  optimal  batch  algorithm.  Adaptive  modification  of the  learning  constant is proposed and analyzed in terms of the Riemannian mea(cid:173) sure  and  is  shown  to  be  efficient.  The  natural  gradient  is  finally  applied to blind separation of mixtured independent signal sources. 
1"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/3a029f04d76d32e79367c4b3255dda4d-Abstract.html,Extraction of Temporal Features in the Electrosensory System of Weakly Electric Fish,"Fabrizio Gabbiani, Walter Metzner, Ralf Wessel, Christof Koch","The encoding of random time-varying stimuli in single spike trains  of electrosensory neurons in the weakly electric fish Eigenmannia  was investigated using methods of statistical signal processing. At  the first stage of the electrosensory system, spike trains were found  to encode faithfully the detailed time-course of random stimuli,  while at the second stage neurons responded specifically to features  in the temporal waveform of the stimulus. Therefore stimulus infor(cid:173) mation is processed at the second stage of the electrosensory system  by extracting temporal features from the faithfully preserved image  of the environment sampled at the first stage."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/3bf55bbad370a8fcad1d09b005e278c2-Abstract.html,A Model of Recurrent Interactions in Primary Visual Cortex,"Emanuel Todorov, Athanassios Siapas, David Somers","A  general  feature  of the  cerebral  cortex  is  its  massive  intercon(cid:173) nectivity  - it  has  been  estimated  anatomically  [19]  that  cortical  neurons  receive  upwards  of 5,000 synapses,  the  majority of which  originate  from  other  nearby  cortical  neurons.  Numerous  experi(cid:173) ments in primary visual cortex (VI)  have revealed strongly nonlin(cid:173) ear interactions between stimulus elements which activate classical  and  non-classical  receptive  field  regions.  Recurrent  cortical  con(cid:173) nections  likely  contribute  substantially  to  these  effects.  However,  most  theories  of visual  processing  have  either  assumed  a  feedfor(cid:173) ward  processing  scheme  [7],  or have used  recurrent  interactions  to  account  for  isolated  effects  only  [1,  16,  18].  Since  nonlinear  sys(cid:173) tems  cannot  in  general  be  taken  apart  and  analyzed  in  pieces,  it  is  not  clear  what  one  learns  by  building  a  recurrent  model  that  only  accounts  for  one,  or  very  few  phenomena.  Here  we  develop  a  relatively simple model of recurrent  interactions  in  VI,  that  re(cid:173) flects  major anatomical and  physiological features  of intracortical  connectivity,  and  simultaneously accounts for  a  wide range of phe(cid:173) nomena observed  physiologically.  All  phenomena we  address  are  strongly nonlinear,  and cannot be  explained by  linear feedforward  models."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/3f67fd97162d20e6fe27748b5b372509-Abstract.html,On a Modification to the Mean Field EM Algorithm in Factorial Learning,"A. P. Dunmur, D. M. Titterington","A  modification  is  described  to  the  use  of mean  field  approxima(cid:173) tions in the E step of EM algorithms for  analysing data from latent  structure models, as described by Ghahramani (1995), among oth(cid:173) ers.  The modification involves second-order Taylor approximations  to expectations computed in the E step.  The potential benefits of  the method are illustrated using very simple latent profile models."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/4079016d940210b4ae9ae7d41c4a2065-Abstract.html,VLSI Implementation of Cortical Visual Motion Detection Using an Analog Neural Computer,"Ralph Etienne-Cummings, Jan Van der Spiegel, Naomi Takahashi, Alyssa Apsel, Paul Mueller","Two dimensional image motion detection neural networks have been  implemented using a general purpose analog neural computer.  The  neural circuits perform spatiotemporal feature extraction based on the  cortical motion detection model of Adelson and Bergen. The neural  computer provides the neurons, synapses and synaptic time-constants  required to realize the model in VLSI hardware. Results show that  visual motion estimation can be implemented with simple sum-and(cid:173) threshold neural hardware with temporal computational capabilities.  The neural circuits compute general 20 visual motion in real-time."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/4122cb13c7a474c1976c9706ae36521d-Abstract.html,Local Bandit Approximation for Optimal Learning Problems,"Michael O. Duff, Andrew G. Barto","In  general,  procedures  for  determining  Bayes-optimal  adaptive  controls  for  Markov  decision  processes  (MDP's)  require  a  pro(cid:173) hibitive  amount  of  computation-the  optimal  learning  problem  is  intractable.  This  paper  proposes  an  approximate  approach  in  which bandit processes are used to model, in a certain ""local"" sense,  a given MDP. Bandit processes constitute an important subclass of  MDP's,  and  have  optimal learning  strategies  (defined  in  terms  of  Gittins indices)  that can be  computed relatively  efficiently.  Thus,  one  scheme  for  achieving  approximately-optimal learning  for  gen(cid:173) eral MDP's proceeds by taking actions suggested by strategies that  are  optimal with respect  to local  bandit  models."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/43cca4b3de2097b9558efefd0ecc3588-Abstract.html,Learning Appearance Based Models: Mixtures of Second Moment Experts,"Christoph Bregler, Jitendra Malik","This paper describes a new technique for object recognition based on learning  appearance  models.  The  image is  decomposed  into  local  regions  which  are  described  by  a  new  texture  representation  called  ""Generalized  Second  Mo(cid:173) ments""  that are  derived  from the  output of multiscale,  multiorientation filter  banks.  Class-characteristic local texture features and their global composition  is learned by a hierarchical mixture of experts architecture (Jordan &  Jacobs).  The  technique  is  applied  to  a  vehicle  database  consisting  of 5  general  car  categories  (Sedan,  Van  with back-doors, Van  without back-doors, old Sedan,  and Volkswagen Bug).  This  is  a difficult problem with considerable in-class  variation.  The new  technique has  a 6.5% misclassification rate, compared to  eigen-images which give  17.4% misclassification rate,  and nearest neighbors  which give  15 .7% misclassification rate."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/4476b929e30dd0c4e8bdbcc82c6ba23a-Abstract.html,Interpreting Images by Propagating Bayesian Beliefs,Yair Weiss,"A central theme of computational vision research  has been  the  re(cid:173) alization that reliable estimation of local scene  properties requires  propagating  measurements  across  the  image.  Many  authors  have  therefore  suggested  solving  vision  problems using  architectures  of  locally  connected  units updating their  activity in  parallel.  Unfor(cid:173) tunately, the convergence of traditional relaxation methods on such  architectures  has  proven  to  be  excruciatingly  slow  and  in  general  they  do  not  guarantee that the stable  point will  be  a  global mini(cid:173) mum.  In this paper we  show  that an  architecture  in  which  Bayesian  Be(cid:173) liefs  about  image  properties  are  propagated  between  neighboring  units  yields  convergence  times  which  are  several  orders  of magni(cid:173) tude faster  than traditional methods and  avoids  local  minima.  In  particular our architecture is  non-iterative in  the sense of Marr [5]:  at every  time step,  the  local estimates  at  a  given  location are  op(cid:173) timal given the information which  has already been  propagated to  that  location.  We  illustrate  the  algorithm's  performance  on  real  images and  compare it to several existing methods."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/459a4ddcb586f24efd9395aa7662bc7c-Abstract.html,Why did TD-Gammon Work?,"Jordan B. Pollack, Alan D. Blair","Although TD-Gammon is one of the major successes in machine learn(cid:173) ing,  it has  not led to  similar impressive breakthroughs  in  temporal dif(cid:173) ference  learning  for  other applications  or even  other games.  We  were  able  to  replicate  some  of the  success  of TD-Gammon,  developing  a  competitive evaluation function on a 4000 parameter feed-forward neu(cid:173) ral network, without using back-propagation, reinforcement or temporal  difference learning methods. Instead we apply simple hill-climbing in  a  relative fitness  environment.  These results and further  analysis  suggest  that the surprising success of Tesauro's program had more to do with the  co-evolutionary  structure of the  learning task and  the  dynamics  of the  backgammon game itself."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/4671aeaf49c792689533b00664a5c3ef-Abstract.html,NeuroScale: Novel Topographic Feature Extraction using RBF Networks,"David Lowe, Michael E. Tipping","Dimension-reducing feature  extraction  neural  network  techniques  which also preserve neighbourhood  relationships in data have tra(cid:173) ditionally  been  the  exclusive  domain  of  Kohonen  self organising  maps.  Recently, we  introduced a  novel dimension-reducing feature  extraction process, which is  also topographic, based upon a  Radial  Basis Function  architecture.  It has been observed that the gener(cid:173) alisation performance of the system is  broadly insensitive to model  order  complexity  and  other  smoothing factors  such  as  the  kernel  widths,  contrary to  intuition  derived  from  supervised  neural  net(cid:173) work models.  In this paper we  provide an effective demonstration  of this property and give a theoretical justification for the apparent  'self-regularising' behaviour of the 'NEUROSCALE'  architecture. 
1 
'NeuroScale':  A  Feed-forward Neural Network  Topographic Transformation 
Recently an important class of topographic neural network based feature extraction  approaches, which  can be related to the traditional statistical methods of Sammon  Mappings  (Sammon,  1969)  and  Multidimensional  Scaling  (Kruskal,  1964),  have  been introduced (Mao and Jain, 1995; Lowe,  1993; Webb,  1995; Lowe and Tipping,  1996).  These novel alternatives to Kohonen-like approaches for topographic feature  extraction  possess  several  interesting properties.  For  instance,  the  NEuROSCALE  architecture  has  the empirically observed  property  that the generalisation perfor-
544 
D.  Lowe and M.  E.  Tipping 
mance does  not  seem  to  depend  critically on  model order  complexity,  contrary to  intuition based upon knowledge of its supervised counterparts.  This paper presents  evidence for their 'self-regularising' behaviour and provides an explanation in terms  of the curvature of the trained models. 
We  now  provide a  brief introduction to the  NEUROSCALE  philosophy of nonlinear  topographic feature extraction.  Further details may be found  in  (Lowe,  1993;  Lowe  and  Tipping,  1996).  We  seek  a  dimension-reducing,  topographic  transformation of  data for the purposes of visualisation and analysis.  By 'topographic', we imply that  the geometric structure of the data be optimally preserved  in  the transformation,  and the embodiment of this constraint is that the inter-point distances in the feature  space should correspond as closely as  possible to those distances in  the data space.  The implementation of this principle by  a  neural network is  very simple.  A  Radial  Basis  Function  (RBF)  neural  network  is  utilised  to predict the coordinates of the  data point in the transformed feature space.  The locations of the feature points are  indirectly determined by adjusting the weights of the network.  The transformation  is  determined by optimising the network parameters in order to minimise a suitable  error measure that embodies the topographic principle. 
The  specific  details  of  this  alternative  approach  are  as  follows.  Given  an  m(cid:173) dimensional  input  space  of N  data  points  x q ,  an  n-dimensional  feature  space  of  points  Yq  is  generated  such  that the  relative  positions of the feature  space points  minimise the error, or  'STRESS',  term: 
N 
E  = 2: 2:(d~p - dqp)2,"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/491442df5f88c6aa018e86dac21d3606-Abstract.html,Time Series Prediction using Mixtures of Experts,"Assaf J. Zeevi, Ron Meir, Robert J. Adler","We  consider  the  problem  of  prediction  of stationary  time  series,  using the architecture known as  mixtures of experts  (MEM).  Here  we  suggest  a  mixture  which  blends  several autoregressive models.  This study focuses  on  some  theoretical foundations  of the  predic(cid:173) tion  problem  in  this  context.  More  precisely,  it  is  demonstrated  that this model is  a  universal approximator,  with  respect to learn(cid:173) ing the  unknown  prediction function .  This statement is  strength(cid:173) ened  as  upper  bounds  on the  mean  squared error are  established.  Based on these results it is  possible to compare the MEM to other  families of models  (e.g.,  neural networks and state dependent mod(cid:173) els).  It is  shown  that  a  degenerate  version  of the  MEM  is  in  fact  equivalent to a  neural  network, and  the  number  of experts  in  the  architecture  plays  a  similar role  to the  number  of hidden  units  in  the latter model."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/4c22bd444899d3b6047a10b20a2f26db-Abstract.html,MIMIC: Finding Optima by Estimating Probability Densities,"Jeremy S. De Bonet, Charles Lee Isbell Jr., Paul A. Viola","In  many optimization problems,  the structure of solutions reflects  complex relationships between  the different  input parameters.  For  example, experience may tell us  that certain parameters are closely  related  and  should  not  be  explored  independently.  Similarly, ex(cid:173) perience  may establish  that  a  subset  of parameters must  take  on  particular  values.  Any  search  of the  cost  landscape  should  take  advantage of these relationships.  We  present  MIMIC,  a framework  in which  we  analyze the global structure of the optimization land(cid:173) scape.  A  novel  and  efficient  algorithm for  the  estimation of this  structure is  derived.  We  use  knowledge of this structure to guide a  randomized search  through the solution space  and,  in  turn,  to re(cid:173) fine our estimate ofthe structure.  Our technique obtains significant  speed  gains over  other randomized optimization procedures."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/4d2e7bd33c475784381a64e43e50922f-Abstract.html,Neural Network Models of Chemotaxis in the Nematode Caenorhabditis Elegans,"Thomas C. Ferrée, Ben A. Marcotte, Shawn R. Lockery","We  train recurrent networks  to control chemotaxis in a  computer  model of the nematode  C.  elegans.  The model  presented  is  based  closely on the body mechanics, behavioral analyses, neuroanatomy  and neurophysiology of  C.  elegans,  each imposing constraints rel(cid:173) evant  for  information  processing.  Simulated  worms  moving  au(cid:173) tonomously in simulated chemical environments display  a  variety  of chemotaxis strategies similar to those of biological worms."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/4e4e53aa080247bc31d0eb4e7aeb07a0-Abstract.html,GTM: A Principled Alternative to the Self-Organizing Map,"Christopher M. Bishop, Markus Svensén, Christopher K. I. Williams","The  Self-Organizing Map  (SOM)  algorithm  has  been  extensively  studied and has  been  applied with considerable success to a  wide  variety of problems.  However, the algorithm is derived from heuris(cid:173) tic  ideas  and  this  leads  to a  number of significant  limitations.  In  this  paper,  we  consider  the  problem  of  modelling  the  probabil(cid:173) ity  density  of  data  in  a  space  of several  dimensions  in  terms  of  a  smaller number of latent,  or hidden,  variables.  We  introduce  a  novel form  of latent variable model,  which  we  call the GTM algo(cid:173) rithm (for  Generative  Topographic  Mapping),  which allows general  non-linear  transformations  from  latent  space  to  data  space,  and  which  is  trained  using  the  EM  (expectation-maximization)  algo(cid:173) rithm.  Our approach overcomes the limitations of the SOM, while  introducing no significant disadvantages.  We demonstrate the per(cid:173) formance of the GTM algorithm on simulated data from flow  diag(cid:173) nostics for  a  multi-phase oil pipeline."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/4f284803bd0966cc24fa8683a34afc6e-Abstract.html,"Support Vector Method for Function Approximation, Regression Estimation and Signal Processing","Vladimir Vapnik, Steven E. Golowich, Alex J. Smola","The  Support  Vector  (SV)  method  was  recently  proposed  for  es(cid:173) timating  regressions,  constructing  multidimensional  splines,  and  solving linear operator equations  [Vapnik,  1995].  In  this  presenta(cid:173) tion we report results of applying the SV method to these problems."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/50905d7b2216bfeccb5b41016357176b-Abstract.html,Smoothing Regularizers for Projective Basis Function Networks,"John E. Moody, Thorsteinn S. Rögnvaldsson","Smoothing regularizers for radial basis functions have been studied extensively,  but no general smoothing regularizers for projective basis junctions (PBFs), such  as  the  widely-used  sigmoidal  PBFs,  have  heretofore  been  proposed.  We  de(cid:173) rive  new  classes of algebraically-simple  mH'-order smoothing regularizers  for  networks of the form  f(W, x)  =  L7=1 Ujg [x T  Vj + Vjol  + uo,  with general  projective basis functions g[.].  These regularizers are: 
Ra(W,m)  =  LU;lIvjIl2m-1  GlobalForm"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/52292e0c763fd027c6eba6b8f494d2eb-Abstract.html,"Bangs, Clicks, Snaps, Thuds and Whacks: An Architecture for Acoustic Transient Processing","Fernando J. Pineda, Gert Cauwenberghs, R. Timothy Edwards","We propose  a  neuromorphic  architecture  for  real-time  processing  of 
acoustic transients in analog VLSI.  We show how judicious normalization 
of a time-frequency signal allows an elegant and robust implementation 
of a correlation algorithm. The algorithm uses binary multiplexing instead 
of analog-analog  multiplication.  This  removes  the  need  for  analog 
storage and analog-multiplication.  Simulations  show that the resulting 
algorithm has the same out-of-sample classification performance (-93% 
correct) as a baseline template-matching algorithm."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/535ab76633d94208236a2e829ea6d888-Abstract.html,Salient Contour Extraction by Temporal Binding in a Cortically-based Network,"Shih-Cheng Yen, Leif H. Finkel","It has been suggested that long-range intrinsic  connections  in  striate cortex  may  play  a  role  in  contour  extraction  (Gilbert  et  aI.,  1996).  A  number  of  recent  physiological  and  psychophysical  studies  have  examined  the  possible  role  of  long range connections in the modulation  of contrast detection  thresholds  (Polat  and Sagi,  1993,1994; Kapadia et aI.,  1995; Kovacs and Julesz,  1994) and various  pre-attentive  detection  tasks  (Kovacs  and  Julesz,  1993;  Field  et aI.,  1993).  We  have  developed  a  network  architecture based  on  the  anatomical  connectivity  of  striate cortex,  as  well  as  the  temporal  dynamics  of neuronal  processing,  that  is  able to reproduce the observed experimental results. The  network  has  been  tested  on  real  images  and  has  applications  in  terms  of identifying  salient  contours  in  automatic image processing systems."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html,Learning Decision Theoretic Utilities through Reinforcement Learning,"Magnus Stensmo, Terrence J. Sejnowski","Probability models can be used to predict outcomes and compensate for  missing data, but even a perfect model cannot be used to make decisions  unless the utility of the outcomes, or preferences between them, are also  provided.  This arises in many real-world problems, such as medical di(cid:173) agnosis,  where the cost of the test as  well as  the expected improvement  in the outcome must be considered. Relatively little work has been done  on learning the utilities of outcomes for optimal decision making. In this  paper, we show how temporal-difference reinforcement learning (TO(A»  can be used to determine decision theoretic utilities within the context of  a mixture model and apply this new approach to a problem in medical di(cid:173) agnosis.  TO( A) learning of utilities reduces the number of tests that have  to be done to achieve the same level of performance compared with the  probability model alone, which results in significant cost savings and in(cid:173) creased efficiency."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/5caf41d62364d5b41a893adc1a9dd5d4-Abstract.html,Spatial Decorrelation in Orientation Tuned Cortical Cells,"Alexander Dimitrov, Jack D. Cowan","In  this  paper  we  propose  a  model  for  the  lateral  connectivity  of  orientation-selective cells in the visual cortex based on information(cid:173) theoretic  considerations.  We study the properties of the input sig(cid:173) nal  to  the  visual  cortex  and  find  new  statistical  structures  which  have not been processed in the retino-geniculate pathway.  Applying  the idea that the system optimizes the  representation  of incoming  signals,  we  derive  the  lateral connectivity that will  achieve  this for  a  set  of local orientation-selective patches,  as  well  as  the  complete  spatial structure of a layer of such patches.  We compare the results  with various  physiological measurements."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/5eac43aceba42c8757b54003a58277b5-Abstract.html,Sequential Tracking in Pricing Financial Options using Model Based and Neural Network Approaches,Mahesan Niranjan,This paper shows how the prices of option contracts traded in finan(cid:173) cial markets can be tracked sequentially by means of the Extended  Kalman Filter algorithm. I consider call and put option pairs with  identical strike price and time of maturity as a two output nonlin(cid:173) ear system. The Black-Scholes approach popular in Finance liter(cid:173) ature and the Radial Basis Functions neural network are used in  modelling the nonlinear system generating these observations. I  show how both these systems may be identified recursively using  the EKF algorithm. I present results of simulations on some FTSE  100 Index options data and discuss the implications of viewing the  pricing problem in this sequential manner.
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/68a83eeb494a308fe5295da69428a507-Abstract.html,Are Hopfield Networks Faster than Conventional Computers?,"Ian Parberry, Hung-Li Tseng","It is shown that conventional computers can be exponentiallx faster  than planar Hopfield  networks:  although there  are planar Hopfield  networks that take exponential time to converge,  a stable state of an  arbitrary  planar Hopfield  network  can be found  by  a  conventional  computer  in  polynomial  time.  The  theory  of 'P.cS-completeness  gives  strong evidence  that such  a separation is  unlikely for  nonpla(cid:173) nar  Hopfield  networks,  and it is  demonstrated  that  this is  also  the  case  for  several  restricted  classes  of nonplanar  Hopfield  networks,  including  those  who  interconnection  graphs are  the class  of bipar(cid:173) tite graphs,  graphs of degree  3, the dual of the knight's graph,  the  8-neighbor mesh,  the hypercube ,  the butterfly,  the cube-connected  cycles,  and the shuffle-exchange  graph."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/68d13cf26c4b4f4f932e3eff990093ba-Abstract.html,Learning from Demonstration,Stefan Schaal,"By  now  it is  widely  accepted  that learning  a  task  from  scratch,  i.e.,  without  any prior knowledge,  is a daunting  undertaking. Humans,  however,  rarely at(cid:173) tempt  to  learn  from  scratch.  They  extract  initial  biases  as  well  as  strategies  how  to  approach a learning problem from  instructions and/or demonstrations  of other  humans.  For  learning  control,  this  paper  investigates  how  learning  from  demonstration  can  be  applied  in  the  context of reinforcement  learning.  We  consider priming  the  Q-function,  the  value  function,  the  policy,  and  the  model of the task dynamics as possible areas where demonstrations can speed  up  learning.  In  general  nonlinear learning  problems,  only  model-based  rein(cid:173) forcement learning shows significant speed-up after a demonstration,  while in  the  special  case  of linear  quadratic  regulator  (LQR)  problems,  all  methods  profit  from  the  demonstration.  In  an  implementation  of pole  balancing  on  a  complex  anthropomorphic  robot  arm,  we  demonstrate  that,  when  facing  the  complexities  of real  signal  processing,  model-based  reinforcement  learning  offers  the  most robustness for LQR problems. Using  the suggested methods,  the  robot  learns  pole  balancing  in  just a  single trial  after  a  30  second  long  demonstration of the human instructor."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/6a61d423d02a1c56250dc23ae7ff12f3-Abstract.html,Clustering Sequences with Hidden Markov Models,Padhraic Smyth,"This paper discusses  a probabilistic model-based approach to clus(cid:173) tering sequences,  using hidden Markov models (HMMs) .  The prob(cid:173) lem  can  be  framed  as  a  generalization  of  the  standard  mixture  model approach to clustering in feature space.  Two primary issues  are  addressed.  First,  a  novel  parameter initialization procedure  is  proposed,  and  second,  the  more  difficult  problem  of determining  the  number of clusters  K,  from  the data,  is  investigated.  Experi(cid:173) mental results  indicate that the proposed  techniques  are useful  for  revealing hidden  cluster structure in  data sets of sequences."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/6c14da109e294d1e8155be8aa4b1ce8e-Abstract.html,Promoting Poor Features to Supervisors: Some Inputs Work Better as Outputs,"Rich Caruana, Virginia R. de Sa","In supervised  learning there is  usually  a  clear  distinction  between  inputs  and  outputs - inputs are  what  you  will  measure,  outputs  are  what  you  will  predict  from  those  measurements.  This  paper  shows  that the distinction between  inputs  and  outputs is  not this  simple.  Some  features  are  more  useful  as  extra  outputs  than  as  inputs.  By using a feature  as  an output we  get  more than just the  case  values but can. learn a  mapping from  the other inputs to that  feature.  For many features  this mapping may be more useful  than  the  feature  value  itself.  We  present  two  regression  problems  and  one  classification problem  where  performance improves  if features  that  could  have  been  used  as  inputs  are  used  as  extra  outputs  instead.  This result is  surprising since  a feature used  as  an output  is  not  used  during testing."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/6c8dba7d0df1c4a79dd07646be9a26c8-Abstract.html,Hidden Markov Decision Trees,"Michael I. Jordan, Zoubin Ghahramani, Lawrence K. Saul","We  study  a  time  series  model  that  can  be  viewed  as  a  decision  tree  with Markov  temporal structure.  The model is  intractable for  exact  calculations,  thus we  utilize  variational approximations.  We  consider  three different  distributions for  the approximation: one in  which the Markov calculations are performed exactly and the layers  of the  decision  tree  are  decoupled,  one  in  which  the  decision  tree  calculations are performed exactly and the time steps of the Markov  chain are decoupled,  and one in which  a  Viterbi-like assumption is  made to  pick out  a  single  most  likely  state sequence.  We  present  simulation results for  artificial data and the Bach chorales."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/6eb6e75fddec0218351dc5c0c8464104-Abstract.html,Representing Face Images for Emotion Classification,"Curtis Padgett, Garrison W. Cottrell",We  compare  the  generalization  performance of three  distinct  rep(cid:173) resentation schemes for facial emotions using a  single classification  strategy  (neural  network).  The face  images presented  to the  clas(cid:173) sifiers  are  represented  as:  full  face  projections  of the  dataset  onto  their  eigenvectors  (eigenfaces);  a  similar projection  constrained  to  eye  and  mouth  areas  (eigenfeatures);  and  finally  a  projection  of  the eye and mouth areas onto the eigenvectors obtained from 32x32  random image patches from the dataset.  The latter system achieves  86%  generalization on novel face  images  (individuals the networks  were  not trained  on)  drawn from  a  database in  which  human sub(cid:173) jects  consistently  identify a single emotion for  the face .
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/70222949cc0db89ab32c9969754d4758-Abstract.html,Separating Style and Content,"Joshua B. Tenenbaum, William T. Freeman","We  seek to analyze and manipulate two factors,  which we  call style  and content,  underlying a set of observations.  We fit  training data  with bilinear models which explicitly represent the two-factor struc(cid:173) ture.  These models can adapt easily during testing to new  styles or  content,  allowing us  to solve three general tasks:  extrapolation of a  new style to unobserved content;  classification of content observed  in  a  new  style;  and  translation of new  content  observed  in  a  new  style.  For classification, we embed bilinear models in a probabilistic  framework,  Separable  Mixture  Models  (SMMsj,  which  generalizes  earlier  work  on  factorial  mixture  models  [7,  3].  Significant  per(cid:173) formance  improvement  on  a  benchmark  speech  dataset  shows  the  benefits  of our approach."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html,Contour Organisation with the EM Algorithm,"José A. F. Leite, Edwin R. Hancock",This paper describes how the early visual process of contour organ(cid:173) isation  can  be  realised  using  the  EM  algorithm.  The  underlying  computational representation is  based on fine  spline coverings.  Ac(cid:173) cording  to our  EM  approach  the  adjustment  of spline  parameters  draws  on  an  iterative  weighted  least-squares  fitting  process.  The  expectation  step  of our  EM  procedure  computes  the  likelihood  of  the data using a mixture model defined over the set of spline cover(cid:173) ings.  These splines  are limited in  their  spatial extent  using  Gaus(cid:173) sian windowing functions.  The maximisation of the likelihood leads  to a set of linear equations in the spline parameters which solve the  weighted least squares problem.  We  evaluate the technique  on  the  localisation of road structures in  aerial infra-red images.
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/7501e5d4da87ac39d782741cd794002d-Abstract.html,Combining Neural Network Regression Estimates with Regularized Linear Weights,"Christopher J. Merz, Michael J. Pazzani","When  combining a  set  of learned  models  to form  an  improved es(cid:173) timator, the  issue  of redundancy  or  multicollinearity in the set  of  models  must  be  addressed.  A  progression  of existing  approaches  and  their  limitations with  respect  to the  redundancy  is  discussed.  A  new  approach,  PCR ,  based  on  principal  components  regres(cid:173) sion  is  proposed to address  these  limitations.  An evaluation of the  new  approach  on  a  collection  of domains  reveals  that:  1)  PCR  was the most robust combination method as the redundancy of the  learned models increased,  2)  redundancy could be handled without  eliminating any of the learned models, and 3) the principal compo(cid:173) nents of the learned models provided a  continuum of ""regularized""  weights from  which  PCR * could choose."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/76cf99d3614e23eabab16fb27e944bf9-Abstract.html,Triangulation by Continuous Embedding,"Marina Meila, Michael I. Jordan","When  triangulating a  belief network  we  aim  to  obtain  a junction  tree of minimum state space.  According to (Rose,  1970), searching  for  the  optimal triangulation can  be  cast  as  a  search  over  all  the  permutations  of the  graph's  vertices.  Our  approach  is  to  embed  the discrete set of permutations in  a  convex continuous domain D.  By  suitably  extending  the  cost  function  over  D  and  solving  the  continous  nonlinear  optimization  task  we  hope  to  obtain  a  good  triangulation  with respect  to  the  aformentioned  cost.  This  paper  presents  two  ways  of embedding  the  triangulation  problem  into  continuous  domain and shows  that they perform well  compared to  the  best  known  heuristic. 
1 
INTRODUCTION.  WHAT IS  TRIANGULATION? 
Belief networks  are graphical representations of probability distributions over a  set  of  variables.  In  what  follows  it  will  be  always  assumed  that  the  variables  take  values  in  a  finite  set  and  that  they  correspond  to  the  vertices  of  a  graph.  The  graph's arcs will represent the dependencies among variables.  There are two kinds of  representations  that have gained wide use:  one is the directed  acyclic graph model,  also  called  a  Bayes  net,  which  represents  the joint distribution as  a  product of the  probabilities of each vertex conditioned on the values of its parents;  the other is the  undirected  graph model,  also called  a  Markov  field,  where  the joint distribution is  factorized  over  the  cliques!  of an  undirected  graph.  This  factorization  is  called  a  junction tree and optimizing it is the subject of the present paper.  The power of both  models lies in their ability to display and exploit existent  marginal and conditional  independencies  among subsets  of variables.  Emphasizing independencies  is  useful 
1 A  clique is  a  fully  connected  set  of  vertices  and  a  maximal  clique  is  a  clique  that  is 
not contained  in  any other clique. 
558 
M. Meilii and M. /.  Jordan 
from both a qualitative point of view  (it reveals something about the domain under  study)  and a  quantitative one  (it makes computations tractable).  The two models  differ  in  the  kinds  of independencies  they  are  able  to  represent  and  often  times  in  their  naturalness  in  particular  tasks.  Directed  graphs  are  more  convenient  for  learning a  model from  data;  on  the  other  hand,  the clique structure  of undirected  graphs  organizes  the  information in  a  way  that  makes  it immediately available to  inference  algorithms.  Therefore  it is  a  standard  procedure  to  construct  the  model  of a domain as a  Bayes net and then to convert  it to a  Markov field  for  the purpose  of querying it. 
This  process  is  known  as  decomposition  and  it  consists  of  the  following  stages:  first,  the  directed  graph  is  transformed  into  an undirected  graph  by  an  operation  called  moralization.  Second,  the moralized graph is  triangulated.  A  graph is called  triangulated if any  cycle  of length>  3  has  a  chord  (i.e.  an  edge  connecting  two  nonconsecutive  vertices).  If a  graph is  not triangulated it is always possible  to add  new  edges  so  that the resulting graph is  triangulated.  We  shall  call  this procedure  triangulation  and  the  added  edges  the  fill-in.  In  the  final  stage,  the junction  tree  (Kjrerulff,  1991)  is  constructed from the maximal cliques of the triangulated graph.  We define  the state space of a  clique to be the cartesian product of the state spaces  of the  variables  associated  to  the  vertices  in  the  clique  and  we  call  weight  of the  clique the size  of this state space.  The  weight  of the  junction  tree is  the sum of the  weights of its component cliques.  All  further  exact  inference in  the net takes  place  in  the junction  tree  representation.  The  number  of computations  required  by  an  inference operation is  proportional to the  weight of the tree. 
For  each  graph  there  are  several  and  usually  a  large  number  of possible  triangu(cid:173) lations,  with  widely  varying state space  sizes.  Moreover,  triangulation is  the  only  stage where  the cost  of inference  can  be  influenced.  It is  therefore  critical that the  triangulation procedure  produces  a  graph that is optimal or at least  ""good""  in this  respect. 
Unfortunately, this is a hard problem.  No optimal triangulation algorithm is known  to date.  However,  a number of heuristic algorithms like maximum  cardinality search  (Tarjan  and  Yannakakis,  1984),  lexicographic  search  (Rose  et  al.,  1976)  and  the  minimum  weight  heuristic  (MW)  (Kjrerulff,  1990)  are  known.  An  optimization  method  based  on  simulated  annealing  which  performs  better  than  the  heuristics  on  large  graphs has  been  proposed  in  (Kjrerulff,  1991)  and  recently  a  ""divide  and  conquer""  algorithm which bounds the maximum clique size of the triangulated graph  has been published  (Becker and Geiger,  1996).  All but the last algorithm are based  on Rose's (Rose,  1970)  elimination procedure:  choose a node v of the graph, connect  all  its  neighbors  to form  a  clique,  then eliminate v  and all  the  edges  incident  to it  and proceed  recursively.  The resulting filled-in  graph is  triangulated. 
It can be proven that the optimal triangulation can always be obtained by applying  Rose's elimination procedure  with an  appropriate ordering of the  nodes.  It follows  then that searching for  an optimal triangulation can be cast as a search in the space  of all  node  permutations.  The  idea  of the  present  work  is  the  following:  embed  the  discrete  search  space  of permutations of n  objects  (where  n  is  the  number of  vertices)  into a suitably chosen  continuous space.  Then extend the cost  to a smooth  function over  the  continuous domain and  thus  transform the discrete  optimization  problem  into  a  continuous  nonlinear  optimization  task.  This  allows  one  to  take  advantage of the thesaurus  of optimization methods that exist  for  continuous cost  functions. The rest of the paper will present this procedure in the following sequence:  the  next  section  introduces  and  discusses  the  objective  function;  section  3  states  the  continuous  version  of the  problem;  section  4  discusses  further  aspects  of the  optimization procedure  and  presents  experimental results  and  section  5  concludes 
Triangulation by Continuous Embedding"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/7810ccd41bf26faaa2c4e1f20db70a71-Abstract.html,Bayesian Model Comparison by Monte Carlo Chaining,"David Barber, Christopher M. Bishop","The techniques of Bayesian inference  have been  applied with great  success to many problems in neural computing including evaluation  of regression  functions,  determination of error bars on predictions,  and  the  treatment of hyper-parameters.  However,  the  problem of  model comparison is a much more challenging one for which current  techniques  have significant limitations.  In this paper we  show  how  an  extended  form  of Markov  chain  Monte  Carlo,  called  chaining,  is  able  to provide effective  estimates of the relative probabilities of  different  models.  We  present  results  from  the  robot  arm problem  and  compare  them  with  the  corresponding  results  obtained  using  the standard  Gaussian approximation framework. 
1  Bayesian Model  Comparison 
In a Bayesian treatment of statistical inference, our state of knowledge of the values  of the parameters w  in a model M  is described in terms of a probability distribution  function.  Initially this  is  chosen  to be  some prior distribution p(wIM), which  can  be  combined with a  likelihood function  p( Dlw, M) using Bayes'  theorem  to give  a  posterior  distribution p(wID, M) in  the form"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/7940ab47468396569a906f75ff3f20ef-Abstract.html,Practical Confidence and Prediction Intervals,Tom Heskes,"We  propose  a  new  method to compute prediction intervals.  Espe(cid:173) cially for small data sets the width of a prediction interval does not  only depend on the variance of the target distribution, but also on  the accuracy of our estimator of the mean of the target, i.e., on the  width  of the  confidence  interval.  The  confidence  interval  follows  from the variation in an ensemble of neural networks,  each of them  trained and stopped on bootstrap replicates of the original data set.  A second improvement is the use of the residuals on validation pat(cid:173) terns  instead of on training patterns for  estimation of the variance  of the  target  distribution.  As  illustrated  on  a  synthetic  example,  our method is  better than existing methods with regard to extrap(cid:173) olation and interpolation in data regimes with a limited amount of  data, and yields prediction intervals which  actual confidence  levels  are closer  to the desired  confidence  levels. 
1  STATISTICAL  INTERVALS 
In  this  paper  we  will  consider  feedforward  neural  networks  for  regression  tasks:  estimating an underlying mathematical function between input and output variables  based  on a finite  number of data points possibly corrupted  by noise.  We  are given  a set of Pdata  pairs  {ifJ, tfJ }  which  are assumed to be generated  according to 
(1)  where  e(i)  denotes  noise  with  zero  mean.  Straightforwardly  trained  on  such  a  regression  task,  the  output  of a  network  o(i)  given  a  new  input  vector  i  can  be 
t(i)  = f(i) + e(i) , 
RWCP:  Real  World  Computing Partnership;  SNN:  Foundation for  Neural  Networks. 
Practical Confidence and Prediction Intervals 
177 
interpreted  as  an  estimate  of the  regression  f(i) , i.e .,  of the  mean  of the  target  distribution  given  input  i.  Sometimes  this  is  all  we  are  interested  in:  a  reliable  estimate of the  regression  f(i).  In  many applications, however,  it  is  important to  quantify the accuracy of our statements.  For regression problems we can distinguish  two  different  aspects:  the  accuracy  of our estimate of the  true regression  and  the  accuracy of our estimate with respect  to the observed  output.  Confidence intervals  deal with the first  aspect, i.e. , consider the distribution of the quantity f(i) - o(i),  prediction intervals with the latter, i.e.,  treat the quantity t(i) - o(i).  We  see  from 
t(i) - o(i)  =  [f(i) - o(i)]  + ~(i) , 
(2) 
that a prediction interval necessarily encloses the corresponding confidence interval.  In  [7]  a method somewhat similar to ours is  introduced  to estimate both the mean  and the variance of the target probability distribution.  It is based on the assumption  that there is  a sufficiently large data set, i.e., that their is  no risk of overfitting and  that the neural network finds  the correct  regression.  In practical applications with  limited  data sets  such  assumptions  are  too  strict.  In  this  paper  we  will  propose  a  new  method  which  estimates the inaccuracy  of the  estimator through  bootstrap  resampling and corrects  for  the tendency  to overfit  by  considering the residuals on  validation patterns rather than those on  training patterns. 
2  BOOTSTRAPPING  AND EARLY STOPPING 
Bootstrapping  [3]  is  based  on  the  idea  that  the  available data set  is  nothing  but  a  particular realization of some unknown  probability distribution.  Instead  of sam(cid:173) pling  over  the  ""true""  probability  distribution,  which  is  obviously  impossible,  one  defines  an empirical distribution.  With so-called naive bootstrapping the empirical  distribution is a sum of delta peaks on the available data points, each with probabil(cid:173) ity content  l/Pdata.  A bootstrap sample is  a collection of Pdata patterns drawn with  replacement from  this empirical probability distribution.  This bootstrap sample is  nothing  but our training set  and  all patterns  that do  not occur in  the training set  are  by  definition  part of the  validation set .  For  large  Pdata,  the probability that  a  pattern becomes  part of the validation set  is  (1  -
l/Pdata)Pdata  ~ lie ~ 0.37. 
When  training a  neural network  on a  particular bootstrap sample, the  weights  are  adjusted  in order  to minimize the  error  on  the  training data.  Training is  stopped  when  the error  on  the  validation data starts to increase.  This so-called early stop(cid:173) ping procedure  is  a  popular strategy  to prevent  overfitting in  neural  networks  and  can  be  viewed  as  an alternative to regularization  techniques  such  as  weight  decay.  In this context bootstrapping is just a procedure to generate subdivisions in training  and  validation set similar to k-fold  cross-validation or subsampling. 
On each of the nrun  bootstrap replicates  we  train and stop a single neural network. 
The  output  of network  i  on  input  vector  i IJ  is  written  oi(ilJ )  ==  or.  As  ""the""  estimate of our  ensemble  of networks  for  the  regression  f(i)  we  take  the  average  output l"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/7bb060764a818184ebb1cc0d43d382aa-Abstract.html,"Consistent Classification, Firm and Soft",Yoram Baram,"A classifier is called  consistent with respect to a given set of class(cid:173) labeled points if it correctly  classifies the set.  We  consider  classi(cid:173) fiers  defined  by unions of local separators and propose  algorithms  for consistent classifier reduction.  The expected complexities of the  proposed algorithms are derived along with the expected classifier  sizes.  In  particular, the proposed approach yields  a  consistent re(cid:173) duction  of the  nearest  neighbor  classifier,  which  performs  ""firm""  classification,  assigning  each  new  object  to  a  class,  regardless  of  the  data  structure.  The  proposed  reduction  method  suggests  a  notion  of ""soft""  classification,  allowing for  indecision  with respect  to  objects  which  are  insufficiently  or  ambiguously  supported  by  the data.  The performances of the proposed classifiers in  predict(cid:173) ing  stock behavior  are  compared  to  that  achieved  by  the nearest  neighbor method."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/7bccfde7714a1ebadf06c5f4cea752c1-Abstract.html,Neural Models for Part-Whole Hierarchies,"Maximilian Riesenhuber, Peter Dayan","We present a connectionist method for  representing images that ex(cid:173) plicitly addresses their hierarchical nature.  It blends data from neu(cid:173) roscience about whole-object viewpoint sensitive cells in inferotem(cid:173) poral  cortex8  and  attentional  basis-field  modulation  in  V43  with  ideas  about  hierarchical  descriptions  based  on  microfeatures.5,11  The resulting model makes critical use of bottom-up and top-down  pathways for  analysis and synthesis.6  We  illustrate the model  with  a simple example of representing information about faces. 
1  Hierarchical Models 
Images  of objects  constitute an  important  paradigm case of a  representational  hi(cid:173) erarchy, in  which  'wholes', such as faces,  consist of 'parts', such as eyes,  noses  and  mouths.  The  representation and manipulation of part-whole hierarchical informa(cid:173) tion  in  fixed  hardware  is  a  heavy  millstone  around  connectionist  necks,  and  has  consequently been the inspiration for  many interesting proposals, such as  Pollack's  RAAM.l1 
We  turned to the  primate visual  system for  clues.  Anterior  inferotemporal  cortex  (IT) appears to construct representations of visually presented objects.  Mouths and  faces  are both objects, and so require fully  elaborated representations,  presumably  at  the level  of anterior IT,  probably using  different  (or  possibly  partially overlap(cid:173) ping) sets of cells.  The natural way to represent the part-whole relationship between  mouths and faces is to have a neuronal hierarchy, with connections bottom-up from  the mouth units to the face units so that information about the mouth can be used  to help  recognize  or  analyze  the  image  of a  face,  and  connections  top-down  from  the face  units to the mouth units expressing the generative or synthetic knowledge  that if there is a face in a scene, then there is  (usually) a  mouth too.  There is little 
We thank Larry Abbott, Geoff Hinton, Bruno Olshausen, Tomaso Poggio, Alex Pouget, 
Emilio Salinas and Pawan Sinha for  discussions  and comments. 
18 
M.  Riesenhuberand P.  Dayan 
empirical support for  or against such a  neuronal hierarchy,  but it seems extremely  unlikely  on the grounds that arranging for  one with  the correct set of levels for  all  classes of objects seems  to be impossible. 
There  is  recent  evidence  that  activities  of cells  in  intermediate  areas  in  the  visual  processing hierarchy  (such  as  V4)  are  influenced  by  the locus  of visual  attention. 3  This  suggests  an  alternative  strategy  for  representing  part-whole  information,  in  which  there  is  an  interaction,  subject  to  attentional  control,  between  top-down  generative  and  bottom-up  recognition  processing.  In  one  version  of our example,  activating units in IT that represent a  particular face  leads,  through the top-down  generative  model,  to a  pattern of activity  in  lower  areas  that  is  closely  related  to  the  pattern  of activity  that  would  be  seen  when  the  entire  face  is  viewed.  This  activation  in  the  lower  areas  in  turn  provides  bottom-up input  to the  recognition  system.  In the bottom-up direction, the attentional signal controls which aspects of  that activation are actually processed, for example, specifying that only the activity  reflecting  the lower  part of the face  should  be  recognized.  In this  case,  the mouth  units in IT can then recognize this restricted pattern of activity as being a particular  sort of mouth.  Therefore,  we  have provided a  way  by  which  the visual system can  represent the part-whole relationship between faces  and mouths. 
This describes just one of many possibilities.  For instance, attentional control could  be mainly active during the top-down phase instead.  Then it would create in VI  (or  indeed in  intermediate areas)  just the  activity  corresponding to  the  lower  portion  of the face  in the first  place.  Also the focus  of attention need  not be so ineluctably  spatial. 
The overall scheme is  based  on  an hierarchical top-down  synthesis  and  bottom-up  analysis model for  visual  processing, as in  the Helmholtz machine6  (note that  ""hi(cid:173) erarchy""  here refers  to a  processing hierarchy rather than the part-whole hierarchy  discussed above)  with a  synthetic model forming  the effective map: 
'object' 18)  'attentional eye-position'  -t 'image' 
(1) 
(shown  in  cartoon form  in  figure  1)  where  'image'  stands in  for  the  (probabilities  over the)  activities of units at various levels in the system that would be caused by  seeing the aspect of the 'object' selected by placing the focus  and scale of attention  appropriately.  We  use this  generative model during synthesis in  the way  described  above to traverse the hierarchical  description  of any particular image.  We  use  the  statistical inverse of the synthetic model as the way of analyzing images to determine  what  objects  they  depict.  This  inversion  process  is  clearly  also  sensitive  to  the  attentional  eye-position - it actually determines  not only  the nature of the object  in  the scene,  but  also the  way  that it is  depicted  (ie  its instantiation parameters)  as  reflected  in the attentional eye  position. 
In  particular,  the  bottom-up  analysis  model  exists  in  the  connections  leading  to  the  2D  viewpoint-selective  image  cells  in  IT reported  by  Logothetis  et  al8  which  form  population  codes  for  all  the  represented  images  (mouths,  noses,  etc).  The  top-down synthesis model exists in the connections leading in the reverse direction.  In generalizations of our scheme, it may,  of course, not be necessary to generate an  image all  the way down in VI. 
The map  (1)  specifies a  top-down  computational task very  like  the  bottom-up one  addressed  using  a  multiplicatively  controlled synaptic matrix in  the shifter  model 
Neural Modelsfor Part-Whole Hierarchies 
19 
attentional  eye  position e  =(c;, tyl t%)"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/7c82fab8c8f89124e2ce92984e04fb40-Abstract.html,Bayesian Unsupervised Learning of Higher Order Structure,"Michael S. Lewicki, Terrence J. Sejnowski","Multilayer architectures such  as  those used in Bayesian belief net(cid:173) works  and  Helmholtz  machines  provide  a  powerful framework  for  representing  and learning higher  order statistical relations  among  inputs.  Because  exact  probability  calculations  with  these  mod(cid:173) els  are often intractable, there is  much interest in finding  approxi(cid:173) mate algorithms.  We present an algorithm that efficiently discovers  higher  order structure using EM  and Gibbs  sampling.  The model  can be interpreted as a stochastic recurrent network in which ambi(cid:173) guity in lower-level states is resolved through feedback from higher  levels.  We demonstrate the performance of the algorithm on bench(cid:173) mark problems."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/7d771e0e8f3633ab54856925ecdefc5d-Abstract.html,An Architectural Mechanism for Direction-tuned Cortical Simple Cells: The Role of Mutual Inhibition,"Silvio P. Sabatini, Fabio Solari, Giacomo M. Bisio","A  linear  architectural  model  of cortical  simple cells  is  presented.  The  model  evidences  how  mutual  inhibition,  occurring  through  synaptic  coupling  functions  asymmetrically  distributed  in  space,  can be a possible basis for  a wide variety of spatio-temporal simple  cell  response properties, including direction selectivity and velocity  tuning.  While  spatial  asymmetries  are  included  explicitly  in  the  structure of the inhibitory interconnections, temporal asymmetries  originate  from  the  specific  mutual  inhibition  scheme  considered.  Extensive simulations supporting the model are reported."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/7e7e69ea3384874304911625ac34321c-Abstract.html,Complex-Cell Responses Derived from Center-Surround Inputs: The Surprising Power of Intradendritic Computation,"Bartlett W. Mel, Daniel L. Ruderman, Kevin A. Archie","Biophysical  modeling  studies  have  previously  shown  that  cortical  pyramidal  cells  driven  by  strong  NMDA-type  synaptic  currents  and/or containing dendritic voltage-dependent Ca++ or Na+  chan(cid:173) nels, respond more strongly when synapses are activated in several  spatially  clustered  groups  of  optimal  size-in  comparison  to  the  same  number  of synapses  activated  diffusely  about  the  dendritic  arbor  [8]- The  nonlinear  intradendritic  interactions  giving  rise  to  this  ""cluster sensitivity""  property are akin to a layer of virtual non(cid:173) linear ""hidden units"" in the dendrites, with implications for the cel(cid:173) lular basis of learning and memory  [7,  6],  and for  certain classes of  nonlinear sensory processing [8]- In the present study, we show that  a  single  neuron,  with  access  only  to excitatory inputs  from  unori(cid:173) ented ON- and OFF-center cells in the LGN, exhibits the principal  nonlinear response properties of a  ""complex""  cell in primary visual  cortex,  namely orientation tuning  coupled with  translation invari(cid:173) ance  and  contrast  insensitivity_  We  conjecture  that  this  type  of  intradendritic processing could explain how complex cell responses  can persist in the absence of oriented simple cell input  [13]-
84 
B.  W.  Mel,  D.  L.  Ruderman and K.  A. Archie"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/7eb3c8be3d411e8ebfab08eba5f49632-Abstract.html,Training Algorithms for Hidden Markov Models using Entropy Based Distance Functions,"Yoram Singer, Manfred K. Warmuth","We  present  new  algorithms  for  parameter  estimation  of HMMs.  By  adapting a framework used for supervised learning, we construct iterative  algorithms  that maximize  the  likelihood of the  observations while also  attempting to stay ""close"" to the current estimated parameters.  We use a  bound on the relative entropy between the two HMMs as a distance mea(cid:173) sure between them.  The result is new iterative training algorithms which  are similar to the EM (Baum-Welch) algorithm for training HMMs.  The  proposed algorithms are  composed  of a  step similar to the expectation  step of Baum-Welch and a new update of the parameters  which replaces  the maximization (re-estimation) step.  The algorithm takes only negligi(cid:173) bly more time per iteration and an  approximated version  uses  the same  expectation  step  as  Baum-Welch.  We  evaluate  experimentally  the  new  algorithms on synthetic and natural speech pronunciation data.  For sparse  models, i.e.  models with relatively small number of non-zero parameters,  the proposed algorithms require significantly fewer iterations. 
1  Preliminaries  We use the numbers from 0 to N  to name the states of an  HMM. State 0 is a special initial  state and state N  is a special final  state.  Any  state sequence, denoted by  s, starts with the  initial state but never returns to it and ends in the final  state.  Observations symbols are also  numbers  in  {I, ... , M}  and  observation sequences  are  denoted  by  x.  A discrete output  hidden Markov model (HMM) is parameterized by two matrices A and B. The first matrix  is  of dimension  [N, N]  and  ai,j  (0:5:  i  :5:  N  - 1,1  :5:  j  :5:  N) denotes the probability of  moving from state i to state j. The second matrix is of dimension [N + 1, M] and bi ,k  is the  probability of outputting symbol k at state i.  The set of parameters of an HMM is denoted  by 0 =  (A, B). (The initial state distribution vector is represented by the first row of A.)  An  HMM is  a probabilistic generator of sequences.  It starts in the initial state O.  It then  iteratively does the following until the final  state is reached.  If i  is the current state then a  next state j  is chosen according to the transition probabilities out of the current state (row i of  matrix A).  After arriving at state j  a symbol is output according to the output probabilities  of that state (row j  of matrix B).  Let P(x, slO) denote the probability (likelihood) that an  HMM 0  generates the observation sequence x  on the path s  starting at state 0 and ending  at  state  N:  P(x, sllsl  =  Ixl + 1, So  =  0, slSI  =  N, 0)  ~ I1~~ll as._t,s.bs.,x •.  For the  sake of brevity we  omit the conditions on  s  and  x.  Throughout the paper we  assume that  the  HMMs are absorbing, that is from  every  state there is a path to the final  state  with a 
642 
Y.  Singer and M.  K.  Warmuth 
non-zero probability.  Similar parameter estimation algorithms can  be derived for ergodic  HMMs.  Absorbing  HMMs  induce  a  probability  over  all  state-observation  sequences,  i.e.  Ex,s P(x, s18)  = 1.  The  likelihood of an  observation  sequence  x  is  obtained  by  summing  over all  possible hidden paths  (state sequences),  P(xI8)  =  Es P(x, sI8).  To  obtain the likelihood for a set X  of observations we simply mUltiply the likelihood values  for  the  individual sequences.  We  seek  an  HMM  8  that  maximizes  the  likelihood for  a  given  set  of observations  X,  or equivalently,  maximizes  the log-likelihood, LL(XI8)  =  r:h EXEX In P(xI8).  To  simplify  our  notation  we  denote  the  generic  parameter  in  8  by  Oi,  where  i  ranges  from  1 to  the  total  number  of parameters  in  A  and  B  (There  might be  less  if some  are  clamped to zero).  We denote the total number of parameters of 8  by I  and leave the (fixed)  correspondence  between  the Oi  and  the entries  of A  and  B  unspecified.  The indices are  naturally partitioned into classes corresponding to the rows of the matrices.  We denote by  [i]  the class of parameters to which Oi  belongs and by O[i)  the vector of all OJ  S.t.  j  E  [i].  If  j  E  [i]  then both Oi  and OJ  are parameters from  the  same row of one of the two matrices.  Whenever it is  clear from the context, we will use  [i]  to denote both a class of parameters  and the row number (i.e.  state) associated with the class.  We now can rewrite P(x, s18)  as  nf=l O~'(X,S), where ni(x, s) is the number of times parameter i  is used along the path s  with observation sequence x.  (Note that this value does not depend on the actual parameters  8.)  We next compute partial derivatives ofthe likelihood and the log-likelihood using this  notation."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/81dc9bdb52d04dc20036dbd8313ed055-Abstract.html,A Constructive Learning Algorithm for Discriminant Tangent Models,"Diego Sona, Alessandro Sperduti, Antonina Starita","(HSS)  developed  an  algo(cid:173)
To  reduce  the  computational complexity of classification systems  using  tangent  distance,  Hastie  et  al.  rithm  to  devise  rich  models  for  representing  large  subsets  of the  data  which  computes  automatically  the  ""best""  associated  tan(cid:173) gent subspace.  Schwenk  &  Milgram proposed  a  discriminant mod(cid:173) ular classification system (Diabolo)  based on several autoassociative  multilayer perceptrons  which  use  tangent  distance  as  error  recon(cid:173) struction  measure.  We  propose  a  gradient  based  constructive  learning  algorithm for  building  a  tangent  subspace  model  with  discriminant  capabilities  which  combines  several  of the  the  advantages  of both  HSS  and  Diabolo:  devised  tangent  models  hold  discriminant  capabilities,  space  requirements  are  improved  with  respect  to  HSS  since  our  algorithm is discriminant and thus it needs fewer prototype models,  dimension of the tangent subspace is  determined automatically by  the  constructive algorithm, and our  algorithm is  able to learn new  transformations."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/81e5f81db77c596492e6f1a5a792ed53-Abstract.html,Effective Training of a Neural Network Character Classifier for Word Recognition,"Larry S. Yaeger, Richard F. Lyon, Brandyn J. Webb","We  have  combined  an  artificial  neural  network  (ANN)  character  classifier with context-driven search over character segmentation, word  segmentation,  and  word  recognition  hypotheses  to  provide  robust  recognition  of hand-printed  English  text  in  new  models  of  Apple  Computer's Newton MessagePad.  We present some innovations in  the  training and use of ANNs al;  character classifiers for  word recognition,  including normalized output error, frequency balancing, error emphasis,  negative training, and stroke warping.  A recurring theme of reducing a  priori biases emerges and is discussed."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/82c2559140b95ccda9c6ca4a8b981f1e-Abstract.html,Second-order Learning Algorithm with Squared Penalty Term,"Kazumi Saito, Ryohei Nakano","This  paper compares three penalty terms  with respect to the effi(cid:173) ciency of supervised learning, by using first- and second-order learn(cid:173) ing algorithms.  Our experiments showed that for  a reasonably ade(cid:173) quate penalty factor,  the combination of the squared penalty term  and  the second-order  learning  algorithm  drastically improves  the  convergence  performance more than 20  times  over  the other  com(cid:173) binations, at the same time bringing about a better generalization  performance."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/838e8afb1ca34354ac209f53d90c3a43-Abstract.html,Multi-effect Decompositions for Financial Data Modeling,"Lizhong Wu, John E. Moody","High  frequency  foreign  exchange  data  can  be  decomposed  into  three  components:  the  inventory effect component,  the  surprise infonnation  (news) component and the regular infonnation component. The presence  of the inventory effect and news can make analysis of trends due to  the  diffusion of infonnation (regular information component) difficult.  We propose a neural-net-based, independent component analysis to sep(cid:173) arate high frequency foreign exchange data into these three components.  Our empirical results show that our proposed multi-effect decomposition  can reveal the intrinsic price behavior."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/83f97f4825290be4cb794ec6a234595f-Abstract.html,Microscopic Equations in Rough Energy Landscape for Neural Networks,K. Y. Michael Wong,"We  consider  the  microscopic  equations  for  learning  problems  in  neural  networks.  The  aligning  fields  of an  example  are  obtained  from  the  cavity  fields,  which  are  the  fields  if  that  example  were  absent  in  the  learning  process.  In  a  rough  energy  landscape,  we  assume  that  the  density  of the  local  minima obey  an  exponential  distribution, yielding macroscopic properties agreeing with the first  step replica symmetry breaking solution.  Iterating the microscopic  equations  provide  a  learning  algorithm,  which  results  in  a  higher  stability than conventional algorithms."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/84438b7aae55a0638073ef798e50b4ef-Abstract.html,Dynamic Features for Visual Speechreading: A Systematic Comparison,"Michael S. Gray, Javier R. Movellan, Terrence J. Sejnowski","Humans  use  visual as  well  as  auditory speech signals to recognize  spoken words.  A variety of systems have been investigated for per(cid:173) forming  this  task.  The main  purpose of this  research  was  to sys(cid:173) tematically compare the performance of a range of dynamic visual  features  on  a  speechreading  task.  We  have  found  that  normal(cid:173) ization  of images  to eliminate  variation  due  to  translation,  scale,  and planar rotation  yielded  substantial  improvements  in  general(cid:173) ization performance regardless of the visual representation used.  In  addition,  the dynamic  information  in  the difference  between  suc(cid:173) cessive  frames  yielded  better performance than optical-flow  based  approaches, and compression by local low-pass filtering worked sur(cid:173) prisingly better than global principal components analysis  (PCA).  These results are examined and possible explanations are explored."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/884ce4bb65d328ecb03c598409e2b168-Abstract.html,The Effect of Correlated Input Data on the Dynamics of Learning,"Søren Halkjær, Ole Winther",The convergence properties of the gradient descent  algorithm in the  case  of the  linear  perceptron  may  be  obtained  from  the  response  function.  We  derive  a general expression for  the  response  function  and  apply it to the  case  of data with simple input  correlations.  It  is  found  that  correlations severely  may slow  down  learning.  This  explains the success of PCA as a method for reducing training time.  Motivated by this finding  we  furthermore propose to transform the  input  data by  removing the mean across  input variables as  well  as  examples to decrease correlations.  Numerical findings for  a medical  classification  problem  are  in  fine  agreement  with  the  theoretical  results.
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/8e2cfdc275761edc592f73a076197c33-Abstract.html,Learning Temporally Persistent Hierarchical Representations,Suzanna Becker,"A biologically motivated model of cortical self-organization is  pro(cid:173) posed.  Context  is  combined  with  bottom-up  information  via  a  maximum  likelihood  cost  function.  Clusters of one  or more  units  are modulated by a common contextual gating Signal;  they thereby  organize themselves into mutually supportive predictors of abstract  contextual features.  The model was tested in its ability to discover  viewpoint-invariant classes on a  set of real image sequences of cen(cid:173) tered,  gradually  rotating faces.  It  performed  considerably  better  than  supervised  back-propagation  at  generalizing  to  novel  views  from  a  small number of training examples. 
1  THE ROLE  OF  CONTEXT 
The importance of context  effects l  in  perception  has  been  demonstrated  in  many  domains.  For  example,  letters  are  recognized  more  quickly  and  accurately  in  the  context  of words  (see  e.g.  McClelland  &  Rumelhart,  1981),  words  are  recognized  more efficiently  when  preceded  by  related  words  (see  e.g.  Neely,  1991),  individual  speech utterances are more intelligible in the context of continuous speech, etc.  Fur(cid:173) ther, there is  mounting evidence that neuronal responses are modulated by context.  For example,  even  at the level  of the  LGN  in  the thalamus,  the primary source of  visual input  to the  cortex,  Murphy &  Sillito  (1987)  have  reported cells  with  ""end(cid:173) stopped""  or  length-tuned  receptive  fields  which  depend  on  top-down  inputs  from  the  cortex.  The end-stopped  behavior disappears  when  the  top-down  connections  are removed, suggesting that the cortico-thalamic connections are providing contex(cid:173) tual modulation  to the LGN.  Moving a  bit  higher up  the visual hierarchy,  von  der  Heydt et al.  (1984) found  cells  which  respond to  ""illusory contours"", in  the absence  of  a  contoured  stimulus  within  the  cells'  classical  receptive  fields.  These  exam(cid:173) ples  demonstrate that  neuronal responses  can  be  modulated  by secondary sources  of information  in  complex  ways,  provided  the  information  is  consistent  with  their  expected or preferred input. 
1 We  use  the term context  rather loosely  here to  mean  any secondary  source  of input.  It could  be from  a  different  sensory  modality,  a  different  input  channel  within  the same  modality,  a  temporal history of the input, or top-down information. 
Learning Temporally Persistent Hierarchical Representations 
825 
Figure 1:  Two  sequences  of 48  by  48  pixel images  digitized  with  an  IndyCam  and prepro(cid:173) cessed  with  a  Sobel  edge  filter.  Eleven  views  of each  of four  to  ten faces  were  used  in  the  simulations  reported  here.  The  alternate  (odd)  views  of two  of the  faces  are  shown  above. 
Why would  contextual modulation  be such a  pervasive  phenomenon?  One obvious  reason is  that if context  can  influence  processing,  it  can  help  in  disambiguating or  cleaning  up  a  noisy  stimulus.  A  less  obvious  reason  may  be  that  if  context  can  influence  learning,  it may lead to more compact representations, and hence  a  more  powerful  processing  system.  To  illustrate,  consider  the  benefits  of incorporating  temporal history into an unsupervised classifier.  Given  a  continuous sensory signal  as  input,  the  classifier  must  try  to  discover  important  partitions  in  its  training  data.  If it can discover features  that are  temporally  persistent,  and thus insensitive  to transformations in  the input, it should be able to represent the signal compactly  with a  small set offeatures.  FUrther, these features are more likely to be associated  with  the identity of objects rather than lower-level attributes.  However,  most  classifiers  group  patterns  together on  the  basis  of spatial overlap.  This may be reasonable if there is very little shift or other form of distortion between  one  time  step and the  next,  but is  not a  reasonable assumption  about  the sensory  input to the cortex.  Pre-cortical stages of sensory processing, certainly in the visual  system (and probably in other modalities), tend to remove low-order correlations in  space and  time,  e.g.  with  centre-surround filters.  Consider the image  sequences  of  gradually rotating faces in Figure 1.  They have been preprocessed by a simple edge(cid:173) filter,  so that successive views of the same face have relatively little pixel overlap.  In  contrast, identical views  of different  faces  may  have  considerable overlap.  Thus,  a  classifier such as k-means, which groups patterns based on their Euclidean distance,  would  not  be  expected to do well  at classifying these  patterns.  So  how  are people  (and in fact very young children)  able to learn to classify a virtually infinite number  of objects based on relatively brief exposures?  It is argued here that the assumption  of temporal  persistence  is  a  powerful  constraining factor  for  achieving  this,  and  is  one which  may be used  to advantage in  artificial neural networks  as  well.  Not only  does it lead to the development of higher-order feature analyzers, but it can result in  more  compact  codes  which  are important for  applications  like  image  compression.  Further,  as  the  simulations  reported  here  show,  improved  generalization  may  be  achieved  by  allowing  high-level  expectations  (e.g.  of class  labels)  to  influence  the  development of lower-level feature  detectors. 
2  THE MODEL  Competitive  learning  (for  a  review,  see  Becker  &  Plumbley,  1996)  is  considered  by  many  to  be  a  reasonably  strong  candidate  model  of cortical  learning.  It can  be  implemented,  in  its  simplest  form,  by  a  Hebbian  learning  rule  in  a  network"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/93fb9d4b16aa750c7475b6d601c35c2c-Abstract.html,Exploiting Model Uncertainty Estimates for Safe Dynamic Control Learning,Jeff G. Schneider,"Model  learning combined  with  dynamic  programming has  been  shown  to  be effective  for  learning control  of continuous state dynamic systems.  The  simplest method assumes the learned model is correct  and applies dynamic  programming to it, but many approximators provide uncertainty estimates  on  the  fit.  How  can  they  be  exploited?  This  paper  addresses  the  case  where the system must be prevented from having catastrophic failures dur(cid:173) ing learning.  We  propose  a  new  algorithm adapted  from  the  dual  control  literature  and  use  Bayesian  locally  weighted  regression  models  with  dy(cid:173) namic programming.  A common reinforcement learning assumption is that  aggressive exploration should be encouraged.  This paper addresses the con(cid:173) verse  case  in which  the system  has  to  reign  in exploration.  The algorithm  is illustrated on a 4 dimensional simulated control  problem."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/944bdd9636749a0801c39b6e449dbedc-Abstract.html,Spectroscopic Detection of Cervical Pre-Cancer through Radial Basis Function Networks,"Kagan Tumer, Nirmala Ramanujam, Rebecca R. Richards-Kortum, Joydeep Ghosh","The  mortality  related  to cervical  cancer  can  be  substantially  re(cid:173) duced  through  early  detection  and  treatment.  However,  cur(cid:173) rent  detection  techniques,  such  as  Pap  smear  and  colposcopy,  fail  to  achieve  a  concurrently  high  sensitivity  and  specificity.  In  vivo  fluorescence  spectroscopy  is  a  technique  which  quickly,  non(cid:173) invasively and quantitatively probes the biochemical and morpho(cid:173) logical  changes that occur in pre-cancerous tissue.  RBF ensemble  algorithms based on such spectra provide automated, and near real(cid:173) time implementation of pre-cancer  detection in the  hands of non(cid:173) experts.  The  results  are  more  reliable,  direct  and  accurate  than  those achieved  by  either human experts or multivariate statistical  algorithms."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/97af4fb322bb5c8973ade16764156bed-Abstract.html,Radial Basis Function Networks and Complexity Regularization in Function Learning,"Adam Krzyzak, Tamás Linder","In this paper we apply the method of complexity regularization to de(cid:173) rive estimation bounds for nonlinear function estimation using a single hidden layer radial basis function network. Our approach differs from the previous complexity regularization neural network function learning schemes in that we operate with random covering numbers and 11 metric entropy, making it po~sibleto consider much broader families of activa(cid:173) tion functions, namely functions of bounded variation. Some constraints previously imposed on the network parameters are also eliminated this way. The network is trained by means of complexity regularization in(cid:173) volving empirical risk minimization. Bounds on the expected risk in tenns of the sample size are obtained for a large class of loss functions. Rates of convergence to the optimal loss are also derived."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/995665640dc319973d3173a74a03860c-Abstract.html,Adaptively Growing Hierarchical Mixtures of Experts,"Jürgen Fritsch, Michael Finke, Alex Waibel",We propose a novel approach to automatically growing and pruning  Hierarchical  Mixtures  of Experts.  The constructive  algorithm pro(cid:173) posed  here  enables  large  hierarchies  consisting  of several  hundred  experts  to be  trained effectively.  We  show  that HME's  trained  by  our  automatic  growing  procedure  yield  better  generalization  per(cid:173) formance  than  traditional  static  and  balanced  hierarchies.  Eval(cid:173) uation  of  the  algorithm  is  performed  (1)  on  vowel  classification  and  (2)  within  a  hybrid  version  of the  JANUS  r9]  speech  recog(cid:173) nition system  using a  subset  of the Switchboard large-vocabulary  speaker-independent  continuous speech  recognition database.
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/996009f2374006606f4c0b0fda878af1-Abstract.html,On-line Policy Improvement using Monte-Carlo Search,"Gerald Tesauro, Gregory R. Galperin","We present a Monte-Carlo simulation algorithm for  real-time policy  improvement  of an  adaptive  controller.  In  the  Monte-Carlo  sim(cid:173) ulation,  the  long-term expected  reward  of each  possible  action  is  statistically measured, using the initial policy to make decisions  in  each  step of the simulation.  The action maximizing the  measured  expected reward is then taken, resulting in an improved policy.  Our  algorithm is easily parallelizable and has been implemented on the  IBM  SP! and SP2 parallel-RISC supercomputers.  We  have  obtained  promising  initial  results  in  applying  this  algo(cid:173) rithm  to the  domain of backgammon.  Results  are  reported  for  a  wide  variety  of initial  policies,  ranging  from  a  random  policy  to  TD-Gammon, an extremely strong multi-layer neural  network.  In  each case,  the Monte-Carlo algorithm gives a substantial reduction,  by  as  much as  a  factor  of 5 or  more,  in  the error  rate  of the  base  players.  The  algorithm  is  also  potentially  useful  in  many  other  adaptive control applications in which it is possible to simulate the  environment."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/9996535e07258a7bbfd8b132435c5962-Abstract.html,Blind Separation of Delayed and Convolved Sources,"Te-Won Lee, Anthony J. Bell, Russell H. Lambert","We  address  the  difficult  problem  of separating multiple  speakers  with  multiple microphones  in  a  real room.  We  combine the work  of Torkkola and  Amari,  Cichocki  and Yang,  to give  Natural  Gra(cid:173) dient information maximisation rules for recurrent (IIR)  networks,  blindly  adjusting  delays,  separating  and  deconvolving  mixed  sig(cid:173) nals.  While  they  work  well  on  simulated  data,  these  rules  fail  in  real  rooms  which  usually  involve  non-minimum phase  transfer  functions,  not-invertible using stable IIR filters.  An approach that  sidesteps this problem is to perform infomax on a feedforward archi(cid:173) tecture in the frequency domain  (Lambert 1996).  We  demonstrate  real-room separation of two natural signals using this approach."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/9a3d458322d70046f63dfd8b0153ece4-Abstract.html,A New Approach to Hybrid HMM/ANN Speech Recognition using Mutual Information Neural Networks,"Gerhard Rigoll, Christoph Neukirchen","This paper presents a new approach to speech recognition with hybrid  HMM/ANN  technology.  While  the  standard  approach  to  hybrid  HMMI ANN  systems  is  based  on  the  use  of  neural  networks  as  posterior probability estimators, the new approach is based on the use  of mutual information neural  networks trained with a special learning  algorithm  in  order to  maximize the  mutual  information  between  the  input classes of the network and its resulting sequence of firing output  neurons  during  training.  It is  shown  in  this  paper that  such  a  neural  network  is  an  optimal  neural  vector quantizer for  a  discrete  hidden  Markov  model  system  trained  on  Maximum  Likelihood  principles.  One  of the  main  advantages  of this  approach  is  the  fact,  that  such  neural  networks  can  be  easily  combined  with  HMM's  of  any  complexity  with context-dependent capabilities.  It is  shown  that  the  resulting  hybrid  system  achieves  very  high  recognition  rates,  which  are  now  already  on  the  same  level  as  the  best  conventional  HMM  systems  with  continuous  parameters,  and  the  capabilities  of  the  mutual information neural networks are not yet entirely exploited."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/9adeb82fffb5444e81fa0ce8ad8afe7a-Abstract.html,Competition Among Networks Improves Committee Performance,"Paul W. Munro, Bambang Parmanto","The separation of generalization error into two types, bias and variance  (Geman,  Bienenstock,  Doursat,  1992),  leads  to  the  notion  of error  reduction  by  averaging  over  a  ""committee""  of classifiers  (Perrone,  1993).  Committee perfonnance decreases with both the average error of  the constituent classifiers and increases  with  the degree to  which the  misclassifications are correlated across the committee.  Here, a method  for  reducing  correlations  is  introduced,  that  uses  a  winner-take-all  procedure  similar  to  competitive  learning  to  drive  the  individual  networks  to  different  minima  in  weight  space  with  respect  to  the  training set, such that correlations in  generalization perfonnance will be  reduced, thereby reducing committee error."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/a01610228fe998f515a72dd730294d87-Abstract.html,Selective Integration: A Model for Disparity Estimation,"Michael S. Gray, Alexandre Pouget, Richard S. Zemel, Steven J. Nowlan, Terrence J. Sejnowski","Local disparity information is often sparse and noisy, which creates  two conflicting demands when estimating disparity in an image re(cid:173) gion:  the need to spatially average to get an accurate estimate, and  the problem of not averaging over discontinuities.  We  have devel(cid:173) oped a  network model of disparity estimation based on disparity(cid:173) selective neurons, such as those found in the early stages of process(cid:173) ing  in  visual cortex.  The model  can accurately estimate multiple  disparities in a region, which may be caused by transparency or oc(cid:173) clusion,  in real images and random-dot stereograms.  The use of a  selection mechanism to selectively integrate reliable local disparity  estimates  results  in  superior  performance  compared  to  standard  back-propagation  and  cross-correlation  approaches.  In  addition,  the representations learned with this selection mechanism are con(cid:173) sistent  with  recent  neurophysiological  results  of  von  der  Heydt,  Zhou, Friedman, and Poggio [8]  for cells in cortical visual area V2.  Combining multi-scale biologically-plausible image processing with  the power of the mixture-of-experts learning algorithm represents  a  promising approach that yields both high  performance and new  insights into visual system function. 
Selective Integration: A Model for Disparity Estimation"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/a0833c8a1817526ac555f8d67727caf6-Abstract.html,The Neurothermostat: Predictive Optimal Control of Residential Heating Systems,"Michael Mozer, Lucky Vidmar, Robert H. Dodier","The  Neurothermostat  is  an  adaptive  controller  that  regulates  in(cid:173) door  air  temperature  in  a  residence  by  switching  a  furnace  on  or  off.  The  task  is  framed  as  an  optimal control  problem  in  which  both  comfort  and  energy  costs  are  considered  as  part of the  con(cid:173) trol  objective.  Because  the  consequences  of control  decisions  are  delayed  in  time, the  N eurothermostat  must  anticipate heating de(cid:173) mands with predictive models of occupancy  patterns and the ther(cid:173) mal response of the house and furnace.  Occupancy  pattern predic(cid:173) tion  is  achieved  by  a  hybrid  neural  net  /  look-up table.  The  Neu(cid:173) rothermostat  searches,  at  each  discrete  time  step,  for  a  decision  sequence  that  minimizes  the  expected  cost  over  a  fixed  planning  horizon.  The first  decision  in this sequence  is  taken,  and  this  pro(cid:173) cess  repeats.  Simulations of the  Neurothermostat  were  conducted  using  artificial  occupancy  data  in  which  regularity  was  systemat(cid:173) ically  varied,  as  well  as  occupancy  data from  an  actual  residence.  The Neurothermostat is  compared against three  conventional poli(cid:173) cies,  and  achieves  reliably  lower  costs.  This result  is  robust  to the  relative  weighting of comfort  and  energy  costs  and  the  degree  of  variability in the occupancy  patterns. 
For over a  quarter  century,  the home automation industry has  promised  to revolu(cid:173) tionize our lifestyle with the so-called  Smart House@  in which appliances,  lighting,  stereo,  video,  and  security  systems  are  integrated  under  computer  control.  How(cid:173) ever,  home automation has yet  to make significant inroads, at least in  part because  software  must be tailored  to the  home occupants.  Instead  of expecting  the occupants  to program their  homes or  to hire  someone  to  do  so,  one  would  ideally like  the  home  to  essentially  program  itself  by  observing  the lifestyle of the occupants.  This is the goal of the  Neural  Network  House  (Mozer  et  al.,  1995),  an  actual  residence  that  has  been  outfitted  with  over  75  sensors(cid:173) including temperature,  light, sound,  motion-and actua.tors  to control air heating,  water  heating,  lighting,  and  ventilation.  In  this  paper,  we  describe  one  research 
954 
M.  C.  Mozer. L.  Vidmar and R.  H.  Dodier 
project  within  the  house,  the  Neurothermostat,  that  learns  to  regulate  the  indoor  air temperature automatically by observing and detecting patterns in the occupants'  schedules  and  comfort  preferences.  We  focus  on  the  problem  of air  heating  with  a  whole-house  furnace,  but  the  same  approach  can  be  taken  with  alternative  or  multiple heating devices,  and to the  problems of cooling and ventilation. 
1  TEMPERATURE REGULATION AS  AN OPTIMAL"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/a0872cc5b5ca4cc25076f3d868e1bdf8-Abstract.html,Softening Discrete Relaxation,"Andrew M. Finch, Richard C. Wilson, Edwin R. Hancock","This paper describes a  new framework for  relational graph match(cid:173) ing.  The starting point is  a recently reported Bayesian consistency  measure  which  gauges  structural  differences  using  Hamming  dis(cid:173) tance.  The main  contributions of the  work  are threefold.  Firstly,  we  demonstrate  how  the  discrete  components  of  the  cost  func(cid:173) tion  can  be  softened.  The  second  contribution  is  to  show  how  the  softened  cost  function  can  be  used  to  locate  matches  using  continuous non-linear optimisation.  Finally,  we  show how  the res(cid:173) ulting graph matching algorithm relates to the standard quadratic  assignment problem."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/a284df1155ec3e67286080500df36a9a-Abstract.html,A Comparison between Neural Networks and other Statistical Techniques for Modeling the Relationship between Tobacco and Alcohol and Cancer,"Tony Plate, Pierre Band, Joel Bert, John Grace","Epidemiological  data  is  traditionally  analyzed  with  very  simple  techniques.  Flexible  models,  such  as  neural  networks,  have  the  potential to discover unanticipated features in the data.  However,  to be useful,  flexible  models  must have effective control on overfit(cid:173) ting.  This paper reports on a  comparative study of the predictive  quality of neural networks and other flexible models applied to real  and artificial epidemiological data.  The results  suggest that there  are no major unanticipated complex features  in the real data, and  also  demonstrate  that  MacKay's  [1995]  Bayesian  neural  network  methodology  provides effective  control on overfitting while  retain(cid:173) ing the ability  to discover complex features  in the artificial data."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/a42a596fc71e17828440030074d15e74-Abstract.html,An Apobayesian Relative of Winnow,"Nick Littlestone, Chris Mesterharm","We  study  a  mistake-driven  variant  of an  on-line  Bayesian  learn(cid:173) ing  algorithm  (similar  to one  studied  by  Cesa-Bianchi,  Helmbold,  and Panizza [CHP96]).  This variant only updates its state (learns)  on trials in which it makes  a mistake.  The algorithm makes binary  classifications using a linear-threshold classifier and runs in time lin(cid:173) ear in  the  number of attributes seen by the learner.  We  have  been  able  to  show,  theoretically  and  in  simulations,  that  this  algorithm  performs well  under assumptions quite different from  those embod(cid:173) ied  in  the  prior of the  original  Bayesian algorithm.  It can handle  situations that  we  do  not  know  how  to handle  in  linear  time  with  Bayesian  algorithms.  We  expect  our  techniques  to  be  useful  in  deriving and analyzing other apobayesian algorithms."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/a4d2f0d23dcc84ce983ff9157f8b7f88-Abstract.html,LSTM can Solve Hard Long Time Lag Problems,"Sepp Hochreiter, Jürgen Schmidhuber","Standard recurrent nets cannot deal with long minimal time lags between relevant signals. Several recent NIPS papers propose alter(cid:173) native methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm, to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of.
1 TRIVIAL PREVIOUS LONG TIME LAG PROBLEMS
Traditional recurrent nets fail in case 'of long minimal time lags between input sig(cid:173) nals and corresponding error signals [7, 3]. Many recent papers propose alternative methods, e.g., [16, 12, 1,5,9]. For instance, Bengio et ale investigate methods such as simulated annealing, multi-grid random search, time-weighted pseudo-Newton optimization, and discrete error propagation [3]. They also propose. an EM ap(cid:173) proach [1]. Quite a few papers use variants of the ""2-sequence problem"" (and ('latch problem"" ) to show the proposed algorithm's superiority, e.g. [3, 1, 5, 9]. Some pa(cid:173) pers also use the ""parity problem"", e.g., [3, 1]. Some of Tomita's [18] grammars are also often used as benchmark problems for recurrent nets [2, 19, 14, 11].
Trivial versus non-trivial tasks. By our definition, a ""trivial"" task is one that can be solved quickly by random search (RS) in weight space. RS works as follows: REPEAT randomly initialize the weights and test the resulting net on a training set UNTIL solution found."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/a51fb975227d6640e4fe47854476d133-Abstract.html,488 Solutions to the XOR Problem,"Frans Coetzee, Virginia L. Stonick","A  globally convergent homotopy method is  defined  that is capable  of sequentially producing large numbers of stationary points of the  multi-layer perceptron  mean-squared error  surface.  Using  this  al(cid:173) gorithm large subsets of the stationary points of two test problems  are  found.  It is  shown  empirically that  the  MLP  neural  network  appears  to  have  an  extreme  ratio  of saddle  points  compared  to  local  minima,  and  that  even  small neural  network  problems  have  extremely large numbers of solutions."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/a58149d355f02887dfbe55ebb2b64ba3-Abstract.html,A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled Data,"David J. Miller, Hasan S. Uyar","We  address  statistical  classifier  design  given  a  mixed  training  set  con(cid:173) sisting  of  a  small  labelled  feature  set  and  a  (generally  larger)  set  of  unlabelled features.  This situation  arises,  e.g., for  medical images,  where  although  training  features  may  be  plentiful,  expensive  expertise  is  re(cid:173) quired  to  extract  their  class  labels.  We  propose  a  classifier  structure  and learning  algorithm  that make effective  use of unlabelled data to im(cid:173) prove performance.  The learning  is  based  on  maximization  of the  total  data  likelihood,  i.e.  over  both  the  labelled  and  unlabelled  data  sub(cid:173) sets.  Two distinct EM learning  algorithms  are proposed,  differing  in  the  EM  formalism  applied  for  unlabelled  data.  The classifier,  based  on  a  joint probability model for features  and labels,  is  a  ""mixture of experts""  structure that is  equivalent to the  radial  basis function  (RBF) classifier,  but unlike RBFs, is  amenable  to likelihood-based training.  The scope of  application  for  the  new  method  is  greatly  extended  by  the  observation  that test data, or any new data to classify, is in fact  additional,  unlabelled  data - thus,  a combined learning/classification operation - much akin  to  what  is  done  in  image  segmentation  - can  be  invoked  whenever  there  is  new  data to classify.  Experiments  with  data sets from  the  UC  Irvine  database  demonstrate  that  the  new  learning  algorithms  and  structure  achieve substantial performance  gains  over alternative  approaches."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/a7d8ae4569120b5bec12e7b6e9648b86-Abstract.html,Statistical Mechanics of the Mixture of Experts,"Kukjin Kang, Jong-Hoon Oh","We study generalization capability of the mixture of experts  learn(cid:173) ing  from  examples  generated  by  another  network  with  the  same  architecture.  When the number of examples is smaller than  a crit(cid:173) ical  value,  the  network  shows  a  symmetric  phase  where  the  role  of the experts  is  not specialized.  Upon  crossing  the critical  point,  the  system  undergoes  a  continuous  phase  transition  to  a  symme(cid:173) try  breaking  phase  where  the gating network  partitions  the  input  space effectively  and each expert is assigned  to an appropriate sub(cid:173) space.  We  also find  that the mixture of experts  with multiple level  of hierarchy shows  multiple phase transitions."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/a9078e8653368c9c291ae2f8b74012e7-Abstract.html,Predicting Lifetimes in Dynamically Allocated Memory,"David A. Cohn, Satinder P. Singh","Predictions oflifetimes of dynamically allocated objects can be used  to improve time and space efficiency  of dynamic memory manage(cid:173) ment in computer programs.  Barrett and Zorn  [1993]  used a simple  lifetime predictor and demonstrated this improvement on a variety  of computer programs.  In  this  paper,  we  use  decision  trees  to  do  lifetime  prediction  on  the  same  programs  and  show  significantly  better  prediction .  Our method also has the advantage that  during  training we  can use  a large number of features  and let the decision  tree  automatically choose  the relevant  subset. 
1 
INTELLIGENT MEMORY ALLOCATION 
Dynamic  memory  allocation  is  used  in  many  computer  applications.  The  appli(cid:173) cation  requests  blocks  of memory from  the  operating  system  or  from  a  memory  manager when  needed  and explicitly frees  them up after use.  Typically, all of these  requests  are handled  in  the same way,  without  any  regard  for  how  or for  how  long  the  requested  block  will  be  used.  Sometimes programmers use  runtime profiles  to  analyze  the  typical  behavior  of their  program  and  write  special  purpose  memory  management  routines  specifically  tuned  to  dominant  classes  of allocation  events.  Machine learning methods offer  the opportunity to automate the process  of tuning  memory management systems. 
In  a  recent  study,  Barrett  and  Zorn  [1993]  used  two  allocators:  a  special  allocator  for  objects  that  are  short-lived,  and  a  default  allocator  for  everything  else.  They  tried a simple prediction method on a number of public-domain , allocation-intensive  programs and  got  mixed results  on  the  lifetime prediction  problem.  Nevertheless,  they showed that for all the cases where they were able to predict well,  their strategy  of assigning objects predicted to be short-lived to the special allocator led to savings 
940 
D. A.  Cohn and S.  Singh 
in  program running  times.  Their results  imply that  if we  could  predict  well  in  all  cases  we  could  get  similar savings for  all programs.  We  concentrate on  the lifetime  prediction  task  in  this  paper  and  show  that using  axis-parallel  decision  trees  does  indeed lead to significantly better prediction on all the programs studied by Zorn and  Grunwald  and  some others  that we  included.  Another  advantage of our approach  is that we  can  use  a large number of features  about the allocation requests  and  let  the decision  tree decide  on their relevance.  There are  a number of advantages of using lifetime predictions for  intelligent mem(cid:173) ory  management.  It can  improve CPU  usage,  by  using special-purpose  allocators,  e.g., short-lived objects can be allocated  in small spaces by  incrementing a  pointer  and deallocated together when they  are  all dead.  It can decrease  memory fragmen(cid:173) tation, because the short-lived objects do not pollute the address space of long lived  objects.  Finally, it can  improve program locality, and thus program speed,  because  the short-lived objects  are  all allocated in a  small part of the  heap. 
The advantages of prediction must be weighed against the time required to examine  each  request  and  make  that  prediction  about  its  intended  use.  It is  frequently  argued  that,  as  computers  and  memory  become  faster  and  cheaper,  we  need  to  be  less  concerned  about  the  speed  and  efficiency  of machine  learning  algorithms.  When  the  purpose  of the  algorithm  is  to  save  space  and  computation,  however,  these  concerns  are paramount. 
1.1  RELATED  WORK 
Traditionally, memory management has been  relegated  to a single,  general-purpose  allocator.  When performance is  critical, software  developers will frequently  build a  custom memory manager which  they  believe is tuned  to optimize the  performance  of the  program.  Not  only  is  this hand construction  inefficient  in  terms of the pro(cid:173) gramming time required,  this  ""optimization"" may seriously  degrade the  program's  performance if it does not accurately reflect  the program's use  [Wilson et al., 1995].  Customalloc [Grunwald and  Zorn,  1992]  monitors program runs on  benchmark in(cid:173) puts  to  determine  the  most  commonly  requested  block  sizes.  It then  produces  a  set of memory allocation routines which  are customized to efficiently allocate those  block sizes.  Other memory requests  are still handled by a general purpose allocator. 
Barrett  and Zorn  [1993]  studied lifetime prediction based on benchmark inputs.  At  each allocation request, the call graph (the list of nested  procedure/function calls in  effect  at the time)  and the object  size  was used  to identify  an  allocation  site.  If all  allocations from  a  particular site  were  short-lived  on  the  benchmark  inputs,  their  algorithm predicted that future allocations would also be short-lived.  Their method  produced  mixed  results  at  lifetime prediction,  but  demonstrated  the  savings  that  such  predictions could bring. 
In  this  paper,  we  discuss  an  approach  to  lifetime  prediction  which  uses  learned  decision  trees.  In  the  next  section,  we  first  discuss  the  identification  of relevant  state features  by  a  decision  tree.  Section  3 discusses  in greater  detail  the problem  of lifetime  prediction.  Section  4  describes  the  empirical  results  of applying  this  approach  to several  benchmark programs, and Section  5 discusses  the implications  of these  results  and directions for  future  work. 
Predicting Lifetimes in Dynamically Allocated Memory 
941 
2  FEATURE  SELECTION WITH  A  DECISION TREE 
Barrett and Zorn 's approach captures state information in the form of the program's  call  graph  at  the  time of an  allocation  request,  which  is  recorded  to  a  fixed  pre(cid:173) determined depth.  This graph,  plus the request  size, specifies  an  allocation  ""site"";  statistics are gathered separately for each site.  A drawback of this approach is that  it forces  a  division for  each  distinct  call  graph,  preventing  generalization across  ir(cid:173) relevant  features.  Computationally, it  requires  maintaining an  explicit  call  graph  (information that  the  program  would  not  normally provide),  as  well  as  storing  a  potentially large  table of call  sites from  which  to make predictions.  It also  ignores  other potentially useful  information, such as the parameters of the functions on the  call stack,  and the contents of heap  memory and the program registers  at the time  of the request. 
Ideally, we  would  like  to examine as  much  of the program state  as  possible  at the  time of each  allocation request,  and  automatically extract  those  pieces  of informa(cid:173) tion that best  allow  predicting how  the requested  block  will be used.  Decision  tree  algorithms are  useful  for  this sort  of task.  A  decision  tree  divides  inputs on  basis  of how  each  input feature  improves  ""purity""  of the  tree's  leaves.  Inputs  that  are  statistically  irrelevant  for  prediction  are  not  used  in  any  splits;  the  tree's final  set  of decisions examine only input features  that improve its predictive performance. 
Regardless of the parsimony of the final tree however,  training a tree with the entire  program state as a feature vector is  computationally infeasible. In our experiments,  detailed  below,  we  arbitrarily  used  the  top  20  words  on  the  stack,  along  with  the  request  size,  as  an approximate indicator of program state.  On the target  machine  (a Sparcstation) , we found that including program registers in the feature set made  no significant difference,  and so  dropped  them from  consideration for  efficiency. 
3  LIFETIME  PREDICTION 
The characteristic of memory requests that we  would like to predict  is  the lifetime  of the  block - how  long it  will  be  before  the  requested  memory is  returned  to the  central  pool.  Accurate  lifetime  prediction  lets  one  segregate  memory  into  short(cid:173) term, long-term and permanent storage.  To this end,  we  have  used  a  decision  tree  learning  algorithm to  derive  rules  that distinguish  ""short-lived""  and  ""permanent""  allocations from the general  pool of allocation requests. 
For short-lived blocks, one  can  create a  very  simple and efficient  allocation scheme  [Barrett  and  Zorn,  1993].  For  ""permanent""  blocks,  allocation  is  also  simple  and  cheap, because  the  allocator  does  not  need  to  compute and  store  any of the  infor(cid:173) mation that would normally be required  to keep  track of the block and return it to  the pool when freed . 
One  complication is  that of unequal loss for  different  types of incorrect  predictions.  An appropriately routed memory request may save dozens of instruction cycles,  but  an  inappropriately  routed  one  may  cost  hundreds.  The  cost  in  terms  of memory  may  also  be  unequal:  a  short-lived  block  that  is  incorrectly  predicted  to  be  ""per(cid:173) manent""  will  permanently tie up  the space occupied  by  the block  (if it is  allocated  via a  method  that  can  not  be  freed).  A  ""permanent""  block,  however,  that  is  in(cid:173) correctly  predicted  to  be  short-lived  may  pollute  the  allocator's short-term space  by  preventing a  large segment of otherwise free  memory from  being reclaimed  (see  Barrett  and  Zorn  for  examples). 
These  risks  translate  into  a  time-space  tradeoff that  depends  on  the  properties  of 
942 
D. A.  Cohn and S.  Singh 
the specific  allocators used  and the space limitations of the target machine.  For our  experiments,  we  arbitrarily defined  false  positives and false  negatives to have equal  loss,  except  where  noted  otherwise.  Other  cases  may  be  handled  by  reweighting  the  splitting  criterion,  or  by  rebalancing  the  training  inputs  (as  described  in  the  following section)."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/ab1a4d0dd4d48a2ba1077c4494791306-Abstract.html,Reinforcement Learning for Mixed Open-loop and Closed-loop Control,"Eric A. Hansen, Andrew G. Barto, Shlomo Zilberstein","Closed-loop  control  relies  on  sensory  feedback  that  is  usually  as(cid:173) sumed  to  be  free .  But  if sensing  incurs  a  cost,  it  may  be  cost(cid:173) effective  to  take  sequences  of actions  in  open-loop  mode.  We  de(cid:173) scribe  a  reinforcement  learning  algorithm  that  learns  to  combine  open-loop  and  closed-loop  control  when  sensing  incurs  a  cost.  Al(cid:173) though  we  assume reliable  sensors,  use  of open-loop control means  that  actions  must  sometimes  be  taken  when  the  current  state  of  the  controlled  system  is  uncertain.  This  is  a  special  case  of the  hidden-state  problem  in  reinforcement  learning,  and  to  cope,  our  algorithm relies  on short-term memory.  The main result  of the pa(cid:173) per is a rule that significantly limits exploration of possible  memory  states  by  pruning  memory states  for  which  the  estimated  value  of  information is  greater than its cost.  We  prove that  this  rule  allows  convergence  to an optimal  policy."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/ae5e3ce40e0404a45ecacaaf05e5f735-Abstract.html,Computing with Infinite Networks,Christopher K. I. Williams,"For  neural  networks  with  a  wide  class  of weight-priors,  it  can  be  shown  that  in  the  limit  of an  infinite  number of hidden  units  the  prior over functions  tends to a  Gaussian process.  In  this paper an(cid:173) alytic forms are derived for the covariance function of the Gaussian  processes  corresponding  to networks with sigmoidal and Gaussian  hidden  units.  This  allows predictions  to  be  made efficiently  using  networks  with an infinite number of hidden units,  and shows  that,  somewhat paradoxically, it may be  easier  to compute with infinite  networks  than finite  ones."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/b20bb95ab626d93fd976af958fbc61ba-Abstract.html,Regression with Input-Dependent Noise: A Bayesian Treatment,"Christopher M. Bishop, Cazhaow S. Quazaz","In  most  treatments  of the  regression  problem  it  is  assumed  that  the distribution of target data can be described by a  deterministic  function  of the inputs, together with additive Gaussian noise hav(cid:173) ing constant variance.  The use of maximum likelihood to train such  models then  corresponds to the minimization of a  sum-of-squares  error function.  In  many applications a more realistic model would  allow  the  noise  variance  itself to  depend  on  the  input  variables.  However, the use of maximum likelihood to train such models would  give highly biased  results.  In  this paper we  show  how  a  Bayesian  treatment  can  allow  for  an  input-dependent  variance  while  over(cid:173) coming the bias of maximum likelihood."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/b24d516bb65a5a58079f0f3526c87c57-Abstract.html,The Generalisation Cost of RAMnets,"Richard Rohwer, Michal Morciniec","Given unlimited computational resources,  it is  best  to use  a  crite(cid:173) rion of minimal expected generalisation error to select  a model and  determine its  parameters.  However,  it may be  worthwhile to sac(cid:173) rifice  some  generalisation  performance  for  higher  learning  speed.  A  method  for  quantifying sub-optimality is  set  out  here,  so  that  this  choice  can  be  made  intelligently.  Furthermore,  the  method  is  applicable  to  a  broad  class  of models,  including  the  ultra-fast  memory-based methods such  as  RAMnets.  This brings the added  benefit  of providing,  for  the  first  time,  the  means  to  analyse  the  generalisation  properties of such  models in a  Bayesian framework ."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/b3ba8f1bee1238a2f37603d90b58898d-Abstract.html,Multilayer Neural Networks: One or Two Hidden Layers?,"Graham Brightwell, Claire Kenyon, Hélène Paugam-Moisy","We study the number of hidden layers required by a multilayer neu(cid:173) ral network with threshold  units to compute a function  f  from n d  to {O, I}.  In dimension d  =  2,  Gibson  characterized  the functions  computable with just one hidden layer, under  the assumption that  there  is no  ""multiple intersection  point""  and that f  is only defined  on a compact set.  We consider the restriction of f  to the neighbor(cid:173) hood of a  multiple intersection  point or of infinity,  and give  neces(cid:173) sary  and sufficient  conditions for  it  to  be  locally computable with  one  hidden  layer.  We  show  that  adding  these  conditions  to  Gib(cid:173) son's  assumptions  is  not  sufficient  to  ensure  global  computability  with one hidden layer,  by exhibiting a new  non-local configuration,  the  ""critical cycle"",  which  implies  that f  is  not  computable  with  one  hidden  layer."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/b495ce63ede0f4efc9eec62cb947c162-Abstract.html,Improving the Accuracy and Speed of Support Vector Machines,"Christopher J. C. Burges, Bernhard Schölkopf","Support  Vector  Learning  Machines  (SVM)  are  finding  application  in  pattern  recognition,  regression  estimation , and  operator inver(cid:173) sion  for  ill-posed  problems.  Against  this  very  general  backdrop ,  any methods for  improving the  generalization  performance, or for  improving  the  speed  in  test  phase,  of SVMs  are  of increasing  in(cid:173) terest.  In this paper we  combine two such  techniques on  a  pattern  recognition problem. The method for improving generalization per(cid:173) formance  (the  ""virtual support  vector""  method)  does  so  by  incor(cid:173) porating known invariances of the problem.  This method achieves  a  drop  in  the error  rate on  10,000 NIST  test  digit images of 1.4%  to  1.0%.  The method for  improving the speed  (the  ""reduced  set""  method) does so by approximating the support vector decision sur(cid:173) face.  We  apply  this method to achieve  a  factor  of fifty  speedup  in  test phase over  the  virtual support vector machine.  The combined  approach  yields a  machine which  is  both  22  times faster  than  the  original machine, and which has better generalization performance,  achieving  1.1 % error.  The virtual support vector  method is  appli(cid:173) cable  to any SVM  problem  with  known  invariances.  The  reduced  set  method is  applicable to any support  vector machine."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/b51a15f382ac914391a58850ab343b00-Abstract.html,Adaptive Access Control Applied to Ethernet Data,Timothy X. Brown,"This paper presents a method that decides which combinations of traffic  can be accepted on a packet data link,  so that quality of service  (QoS)  constraints can be met. The method uses samples of QoS  results at dif(cid:173) ferent load conditions to build a neural  network decision function.  Pre(cid:173) vious  similar approaches  to  the  problem  have  a  significant  bias.  This  bias is likely to  occur in any real system and results in accepting loads  that miss QoS targets by orders of magnitude. Preprocessing the data to  either remove  the  bias  or provide  a  confidence  level,  the  method  was  applied  to  sources  based  on  difficult-to-analyze  ethernet  data  traces.  With this data, the method produces an accurate access control function  that  dramatically  outperforms  analytic  alternatives.  Interestingly,  the  results depend on throwing away more than 99% of the data."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/b571ecea16a9824023ee1af16897a582-Abstract.html,An Adaptive WTA using Floating Gate Technology,"W. Fritz Kruger, Paul E. Hasler, Bradley A. Minch, Christof Koch","We  have  designed,  fabricated,  and  tested  an  adaptive  Winner(cid:173) Take-All  (WTA)  circuit  based  upon  the  classic  WTA  of Lazzaro,  et  al  [IJ.  We  have  added  a  time  dimension  (adaptation)  to  this  circuit to make the input derivative an important factor in winner  selection.  To  accomplish  this,  we  have  modified  the  classic  WTA  circuit  by  adding floating  gate transistors which  slowly  null  their  inputs over time.  We  present a  simplified analysis and experimen(cid:173) tal data of this adaptive WTA fabricated in a standard CMOS 2f.tm  process. 
1  Winner-Take-All Circuits 
In a WTA network, each cell has one input and one output.  For any set of inputs, the  outputs will all be at zero except for the one which is from the cell with the maximum  input.  One way to accomplish this is by a global nonlinear inhibition coupled with a  self-excitation term [2J.  Each cell  inhibits all others while exciting itself;  thus a cell  with even a slightly greater input than the others will excite itself up to its maximal  state  and  inhibit  the  others  down  to  their  minimal  states.  The  WTA  function  is  important for  many classical neural  nets that involve  competitive  learning,  vector  quantization  and  feature  mapping.  The  classic  WTA  network  characterized  by  Lazzaro et.  al.  [IJ  is  an  elegant,  simple  circuit  that shares just one  common  line  among all  cells of the network to propagate the inhibition. 
Our motivation  to  add  adaptation  comes  from  the  idea of saliency  maps.  Picture  a  saliency  map  as  a  large  number  of cells  each  of which  encodes  an  analog  value 
An Adaptive wrA using Floating Gate Technology"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/bb04af0f7ecaee4aae62035497da1387-Abstract.html,Representation and Induction of Finite State Machines using Time-Delay Neural Networks,"Daniel S. Clouse, C. Lee Giles, Bill G. Horne, Garrison W. Cottrell","This work investigates the representational and inductive capabili(cid:173) ties of time-delay neural networks (TDNNs)  in general, and of two  subclasses of TDNN, those  with delays only on the inputs (IDNN),  and those which  include delays on hidden  units (HDNN) . Both ar(cid:173) chitectures are capable of representing  the same class of languages,  the  definite  memory machine (DMM)  languages, but the delays on  the  hidden  units  in  the  HDNN  helps  it  outperform  the  IDNN  on  problems composed of repeated  features  over short  time windows."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/bb7946e7d85c81a9e69fee1cea4a087c-Abstract.html,Probabilistic Interpretation of Population Codes,"Richard S. Zemel, Peter Dayan, Alexandre Pouget","We  present  a  theoretical  framework  for  population  codes  which  generalizes  naturally  to  the  important  case  where  the  population  provides  information  about  a  whole  probability  distribution  over  an  underlying  quantity  rather  than  just  a  single  value.  We  use  the framework  to analyze two existing models,  and to suggest and  evaluate a  third model for  encoding such probability distributions."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/c0a271bc0ecb776a094786474322cb82-Abstract.html,"Analog VLSI Circuits for Attention-Based, Visual Tracking","Timothy K. Horiuchi, Tonia G. Morris, Christof Koch, Stephen P. DeWeerth","A one-dimensional visual tracking chip has been implemented us(cid:173) ing neuromorphic, analog VLSI techniques to model selective visual  attention in the control of saccadic and smooth pursuit eye move(cid:173) ments. The chip incorporates focal-plane processing to compute  image saliency and a winner-take-all circuit to select a feature for  tracking. The target position and direction of motion are reported  as the target moves across the array. We demonstrate its function(cid:173) ality in a closed-loop system which performs saccadic and smooth  pursuit tracking movements using a one-dimensional mechanical  eye."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/c1e39d912d21c91dce811d6da9929ae8-Abstract.html,Online Learning from Finite Training Sets: An Analytical Case Study,"Peter Sollich, David Barber","We  analyse  online  learning  from  finite  training  sets  at  non(cid:173) infinitesimal  learning  rates  TJ.  By  an  extension  of statistical  me(cid:173) chanics  methods,  we  obtain  exact  results  for  the  time-dependent  generalization  error  of  a  linear  network  with  a  large  number  of  weights  N.  We  find,  for  example,  that  for  small training  sets  of  size  p  ~ N,  larger  learning rates  can  be  used  without compromis(cid:173) ing  asymptotic  generalization  performance  or  convergence  speed.  Encouragingly,  for  optimal  settings  of  TJ  (and,  less  importantly,  weight decay ,)  at given final  learning time, the generalization per(cid:173) formance of online learning is  essentially  as good as  that of offline  learning."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/c44e503833b64e9f27197a484f4257c0-Abstract.html,Spatiotemporal Coupling and Scaling of Natural Images and Human Visual Sensitivities,Dawei W. Dong,"We  study  the spatiotemporal  correlation  in  natural  time-varying  images  and  explore  the  hypothesis  that  the visual system  is  con(cid:173) cerned  with  the  optimal  coding  of visual  representation  through  spatiotemporal  decorrelation  of  the  input  signal.  Based  on  the  measured spatiotemporal power spectrum, the transform needed to  decorrelate input signal is derived analytically and then compared  with the actual processing observed in psychophysical experiments."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/c4851e8e264415c4094e4e85b0baa7cc-Abstract.html,Using Curvature Information for Fast Stochastic Search,"Genevieve B. Orr, Todd K. Leen","We  present an algorithm for  fast  stochastic gradient descent  that  uses  a  nonlinear adaptive momentum scheme to optimize the late  time convergence rate.  The algorithm makes  effective  use  of cur(cid:173) vature information,  requires  only  O(n)  storage and computation,  and  delivers  convergence  rates  close  to  the theoretical  optimum.  We demonstrate the technique on linear and large nonlinear back(cid:173) prop networks. 
Improving Stochastic  Search 
Learning algorithms that perform gradient  descent  on a  cost function can  be for(cid:173) mulated in either stochastic (on-line)  or batch form.  The stochastic version  takes  the form 
Wt+l  =  Wt  +  J1.t  G( Wt, Xt  ) 
(1)  where  Wt  is  the  current  weight  estimate,  J1.t  is  the learning  rate,  G  is  minus  the  instantaneous gradient  estimate, and  Xt  is  the input at time  t i .  One obtains the  corresponding batch mode learning rule by taking J1.  constant and averaging Gover  all  x. 
Stochastic  learning  provides  several  advantages  over  batch  learning.  For  large  datasets the batch average is  expensive to compute.  Stochastic learning eliminates  the averaging.  The stochastic update can  be regarded as  a  noisy  estimate of the  batch update, and this intrinsic noise can reduce the likelihood of becoming trapped  in poor local optima [1,  2J. 
1 We  assume  that  the  inputs  are i.i.d.  This is  achieved  by random  sampling with  re(cid:173)
placement from  the training data. 
Using Curvature Informationfor Fast Stochastic Search 
607 
The noise must be reduced late in the training to allow  weights to converge.  After  settling within the basin of a local optimum W.,  learning rate annealing allows con(cid:173) vergence of the weight error v ==  W  - w •.  It is well-known that the expected squared  weight  error, E[lv12]  decays at its maximal rate ex:  l/t with the annealing schedule  flo/to  FUrthermore to achieve this rate one must have flo  > flcnt  =  1/(2Am in) where  Amin  is the smallest eigenvalue of the Hessian at w.  [3,  4,  5, and references therein].  Finally the optimal flo, which gives the lowest possible value of E[lv12] is flo  = 1/ A.  In multiple dimensions the optimal learning  rate  matrix is  fl(t) = (l/t) 1-£-1 ,where  1-£  is  the Hessian at the local optimum.  Incorporating this curvature information into stochastic learning is  difficult for  two  reasons.  First, the Hessian is  not available since the point of stochastic learning is  not to  perform averages  over  the training data.  Second,  even if the Hessian  were  available, optimal learning requires its inverse - which is  prohibitively expensive to  compute 2. 
The primary result of this paper is that one can achieve an algorithm that behaves  optimally,  i.e.  as if one  had incorporated the inverse  of the full  Hessian,  without  the storage or  computational  burden.  The  algorithm,  which  requires  only  V(n)  storage and computation (n = number of weights in the network), uses an adaptive  momentum parameter, extending our earlier work  [7]  to fully  non-linear problems.  We demonstrate the performance on several large back-prop networks trained with  large datasets. 
Implementations of stochastic learning typically use a constant learning rate during  the early part of training (what Darken and Moody [4]  call the search phase) to ob(cid:173) tain exponential convergence towards a local optimum, and then switch to annealed  learning (called the converge phase).  We use Darken and Moody's adaptive search  then converge (ASTC)  algorithm to determine the point at which to switch to  l/t  annealing.  ASTC was  originally conceived as a  means to insure flo  > flcnt  during  the annealed phase, and we  compare its performance with adaptive momentum as  well.  We  also provide a  comparison with conjugate gradient optimization. 
1  Momentum in Stochastic Gradient Descent 
The adaptive momentum algorithm we  propose was  suggested by  earlier  work  on  convergence rates for  annealed learning with constant momentum.  In this  section  we  summarize the relevant results of that work. 
Extending (1)  to include momentum leaves  the learning rule 
wt+ 1  = Wt  + flt  G ( Wt, x t)  + f3  ( Wt  - Wt -1 ) 
(2)  where f3  is  the momentum parameter constrained so  that  0  < f3  <  1.  Analysis  of  the dynamics of the expected squared weight  error E[ Ivl2  ]  with flt = flo/t learning  rate annealing [7, 8]  shows that at late times, learning proceeds as for the algorithm  without momentum, but with a  scaled or  effective  learning rate 
flo  fleff  =  1 _  f3"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/c54e7837e0cd0ced286cb5995327d1ab-Abstract.html,ARC-LH: A New Adaptive Resampling Algorithm for Improving ANN Classifiers,"Friedrich Leisch, Kurt Hornik","We introduce arc-Ih,  a new algorithm for improvement of ANN clas(cid:173) sifier  performance,  which  measures  the  importance of patterns  by  aggregated network output errors.  On several  artificial benchmark  problems,  this  algorithm compares favorably  with other  resample  and combine techniques."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/c70daf247944fe3add32218f914c75a6-Abstract.html,Estimating Equivalent Kernels for Neural Networks: A Data Perturbation Approach,A. Neil Burgess,"We  describe  the  notion  of  ""equivalent  kernels""  and  suggest  that  this  provides a framework  for comparing different classes of regression models,  including  neural  networks  and  both  parametric  and  non-parametric  statistical techniques.  Unfortunately,  standard techniques break down  when  faced with models, such as neural networks,  in which there is more than one  ""layer"" of adjustable parameters.  We propose an algorithm which overcomes  this limitation,  estimating the equivalent kernels for  neural network models  using  a  data  perturbation approach.  Experimental  results  indicate  that  the  networks do  not  use  the  maximum possible  number of degrees of freedom,  that  these  can  be  controlled  using  regularisation  techniques  and  that  the  equivalent kernels learnt by the network vary both  in ""size""  and in ""shape""  in different regions of the input space."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/c850371fda6892fbfd1c5a5b457e5777-Abstract.html,Monotonicity Hints,"Joseph Sill, Yaser S. Abu-Mostafa","A hint is any piece of side information about the target function to  be  learned.  We  consider  the monotonicity hint,  which  states  that  the function  to be learned  is  monotonic in some or all of the input  variables.  The  application of mono tonicity  hints  is  demonstrated  on  two  real-world  problems- a  credit  card  application task,  and  a  problem in medical diagnosis.  A measure of the monotonicity error  of a candidate function is defined and an objective function for  the  enforcement  of monotonicity is  derived  from  Bayesian  principles.  We report experimental results which show that using monotonicity  hints leads to a statistically significant improvement in performance  on  both problems."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/c8ba76c279269b1c6bc8a07e38e78fa4-Abstract.html,A Convergence Proof for the Softassign Quadratic Assignment Algorithm,"Anand Rangarajan, Alan L. Yuille, Steven Gold, Eric Mjolsness","The  softassign  quadratic  assignment  algorithm  has  recently  emerged as an effective strategy for  a variety of optimization prob(cid:173) lems in pattern recognition and combinatorial optimization.  While  the  effectiveness  of the algorithm  was  demonstrated  in  thousands  of simulations,  there  was  no  known  proof of convergence.  Here,  we  provide a  proof of convergence for  the most general form of the  algorithm."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/cd758e8f59dfdf06a852adad277986ca-Abstract.html,Removing Noise in On-Line Search using Adaptive Batch Sizes,Genevieve B. Orr,"Stochastic  (on-line)  learning  can  be  faster  than  batch  learning.  However,  at late times,  the learning rate must be annealed to re(cid:173) move  the noise  present  in  the stochastic weight  updates.  In  this  annealing phase,  the convergence rate (in  mean square)  is  at best  proportional to  l/T where T  is  the number of input presentations.  An alternative is  to increase the batch size to remove the noise.  In  this paper we explore convergence for LMS using 1)  small but fixed  batch sizes  and  2)  an adaptive batch size.  We  show  that the best  adaptive  batch schedule  is  exponential  and  has  a  rate  of  conver(cid:173) gence which is  the same as for  annealing, Le.,  at best proportional  to  l/T."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/d282ef263719ab842e05382dc235f69e-Abstract.html,Size of Multilayer Networks for Exact Learning: Analytic Approach,"André Elisseeff, Hélène Paugam-Moisy","This  article  presents  a  new  result  about  the  size  of a  multilayer  neural network computing real outputs for exact learning of a finite  set of real samples.  The architecture of the network is feedforward,  with  one  hidden  layer  and several  outputs.  Starting from  a  fixed  training set,  we  consider  the  network  as  a  function  of its  weights.  We  derive,  for  a  wide family  of transfer  functions,  a  lower  and  an  upper  bound  on  the  number  of hidden  units  for  exact  learning,  given  the size  of the  dataset  and  the  dimensions of the  input and  output spaces."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/d38901788c533e8286cb6400b40b386d-Abstract.html,Support Vector Regression Machines,"Harris Drucker, Christopher J. C. Burges, Linda Kaufman, Alex J. Smola, Vladimir Vapnik","A  new  regression  technique  based  on  Vapnik's  concept  of  support  vectors  is  introduced.  We compare  support  vector  regression  (SVR)  with  a  committee regression  technique  (bagging)  based  on  regression  trees  and  ridge regression  done in  feature space.  On  the basis of these  experiments,  it  is  expected  that  SVR  will  have  advantages  in  high  dimensionality space because SVR optimization does not depend on the  dimensionality of the input space."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/d759175de8ea5b1d9a2660e45554894f-Abstract.html,A Hierarchical Model of Visual Rivalry,Peter Dayan,"Binocular rivalry is  the alternating percept that can result when  the two eyes see different scenes.  Recent psychophysical evidence  supports an account for one component of binocular rivalry similar  to that for other bistable percepts.  We  test the hypothesisl9, 16, 18  that  alternation  can be  generated by competition  between  top(cid:173) down cortical explanations for  the inputs,  rather than by direct  competition between the  inputs.  Recent neurophysiological ev(cid:173) idence  shows  that some binocular neurons  are  modulated  with  the changing percept; others are not, even if they are selective be(cid:173) tween the stimuli presented to the eyes.  We  extend our model to  a hierarchy to address these effects."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/d94e18a8adb4cc0f623f7a83b1ac75b4-Abstract.html,Dynamically Adaptable CMOS Winner-Take-All Neural Network,"Kunihiko Iizuka, Masayuki Miyamoto, Hirofumi Matsui",The major  problem  that  has  prevented  practical  application  of analog  neuro-LSIs  has  been  poor  accuracy  due  to  fluctuating  analog  device  characteristics  inherent  in  each  device  as  a  result  of  manufacturing.  This paper proposes a dynamic control architecture that  allows analog  silicon  neural  networks  to  compensate  for  the  fluctuating  device  characteristics  and  adapt  to  a  change  in  input  DC  level.  We  have  applied  this  architecture to  compensate  for  input  offset  voltages of an  analog  CMOS  WTA (Winner-Take-AlI)  chip  that we  have  fabricated.  Experimental data show the effectiveness of the architecture.
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/da11e8cd1811acb79ccf0fd62cd58f86-Abstract.html,Ensemble Methods for Phoneme Classification,"Steve R. Waterhouse, Gary Cook","This paper investigates a  number of ensemble methods for  improv(cid:173) ing  the  performance of phoneme classification  for  use  in  a  speech  recognition system.  Two ensemble methods are described; boosting  and mixtures of experts,  both in isolation and in combination.  Re(cid:173) sults are presented on two speech recognition databases:  an isolated  word database and a large vocabulary continuous speech database.  These results show that principled ensemble methods such as boost(cid:173) ing  and  mixtures provide superior  performance to  more naive  en(cid:173) semble methods such  as  averaging."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/dabd8d2ce74e782c65a973ef76fd540b-Abstract.html,Maximum Likelihood Blind Source Separation: A Context-Sensitive Generalization of ICA,"Barak A. Pearlmutter, Lucas C. Parra","In the square linear blind source separation problem, one must find  a  linear  unmixing operator  which  can detangle  the result  Xi(t)  of  mixing n  unknown independent sources 8i(t)  through an unknown  n  x  n  mixing matrix A( t)  of causal linear filters:  Xi  =  E j  aij * 8 j .  We cast the problem as one of maximum likelihood density estima(cid:173) tion,  and  in  that framework  introduce an  algorithm  that searches  for  independent components using both temporal and spatial cues.  We  call the resulting algorithm  ""Contextual ICA,""  after the  (Bell  and  Sejnowski  1995)  Infomax  algorithm,  which  we  show  to  be  a  special case of cICA.  Because cICA  can make use  of the temporal  structure of its input, it is  able separate in a  number of situations  where  standard methods  cannot,  including  sources  with  low  kur(cid:173) tosis,  colored  Gaussian sources,  and  sources  which  have  Gaussian  histograms. 
1  The Blind Source Separation Problem 
Consider  a  set  of n  indepent  sources  81 (t), . .. ,8n (t).  We  are  given  n  linearly dis(cid:173) torted sensor  reading  which  combine  these  sources,  Xi  =  E j  aij8j,  where  aij  is  a  filter  between source j  and  sensor i,  as  shown in figure  1a.  This can be expressed  as 
Xi(t) = 2: 2: aji(r)8j(t - r) = 2: aji * 8j"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/dc4c44f624d600aa568390f1f1104aa0-Abstract.html,Temporal Low-Order Statistics of Natural Sounds,"Hagai Attias, Christoph E. Schreiner","In order to process  incoming sounds  efficiently,  it is  advantageous  for the auditory system to be adapted to the statistical structure of  natural auditory scenes.  As a first step in investigating the relation  between  the system  and its inputs,  we  study low-order  statistical  properties in several sound ensembles  using a  filter  bank analysis.  Focusing on the amplitude and phase in different frequency bands,  we  find  simple  parametric  descriptions  for  their  distribution  and  power  spectrum  that  are valid  for  very  different  types  of sounds.  In  particular,  the  amplitude  distribution  has  an  exponential  tail  and  its  power  spectrum  exhibits  a  modified  power-law  behavior,  which is  manifested by self-similarity and long-range temporal cor(cid:173) relations.  Furthermore,  the statistics for  different  bands  within  a  given  ensemble  are  virtually  identical,  suggesting  translation  in(cid:173) variance along the cochlear  axis.  These results  show  that natural  sounds are highly redundant, and have possible implications to the  neural code used  by the auditory system."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/dc87c13749315c7217cdc4ac692e704c-Abstract.html,Limitations of Self-organizing Maps for Vector Quantization and Multidimensional Scaling,Arthur Flexer,"The  limitations  of  using  self-organizing  maps  (SaM)  for  either  clustering/vector  quantization  (VQ)  or  multidimensional  scaling  (MDS)  are  being  discussed  by  reviewing  recent  empirical findings  and the relevant theory.  SaM 's remaining ability of doing both VQ  and MDS  at the same time is challenged  by  a  new  combined  tech(cid:173) nique  of online  K-means  clustering  plus  Sammon  mapping of the  cluster  centroids.  SaM are shown to perform significantly worse in  terms of quantization error , in  recovering the structure of the clus(cid:173) ters  and  in  preserving  the  topology  in  a  comprehensive  empirical  study using a  series  of multivariate normal clustering  problems."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/dc960c46c38bd16e953d97cdeefdbc68-Abstract.html,A Spike Based Learning Neuron in Analog VLSI,"Philipp Häfliger, Misha Mahowald, Lloyd Watts","Many  popular  learning  rules  are  formulated  in  terms  of continu(cid:173) ous,  analog inputs  and outputs.  Biological  systems,  however,  use  action  potentials,  which  are  digital-amplitude  events  that  encode  analog  information  in  the  inter-event  interval.  Action-potential  representations are now  being used  to advantage in neuromorphic  VLSI  systems as  well.  We  report on a  simple learning rule,  based  on  the  Riccati  equation  described  by  Kohonen  [1],  modified  for  action-potential  neuronal  outputs.  We  demonstrate  this  learning  rule in an analog VLSI chip that uses volatile capacitive storage for  synaptic weights.  We  show that our time-dependent learning rule  is  sufficient  to achieve  approximate weight  normalization and  can  detect temporal correlations in spike trains. 
A Spike Based Learning Neuron in Analog VLSI"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/dfd7468ac613286cdbb40872c8ef3b06-Abstract.html,One-unit Learning Rules for Independent Component Analysis,"Aapo Hyvärinen, Erkki Oja","Neural one-unit learning rules for the problem of Independent Com(cid:173) ponent Analysis (ICA)  and blind source separation are introduced.  In  these  new  algorithms,  every  ICA  neuron  develops  into  a  sepa(cid:173) rator that finds  one of the independent  components.  The learning  rules  use  very  simple  constrained  Hebbianjanti-Hebbian  learning  in  which  decorrelating feedback  may  be  added.  To  speed  up  the  convergence of these stochastic gradient descent rules, a novel com(cid:173) putationally efficient  fixed-point  algorithm is  introduced."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/e00406144c1e7e35240afed70f34166a-Abstract.html,Analysis of Temporal-Diffference Learning with Function Approximation,"John N. Tsitsiklis, Benjamin Van Roy","We  present  new results  about  the temporal-difference learning al(cid:173) gorithm,  as  applied  to  approximating  the  cost-to-go  function  of  a  Markov  chain  using  linear  function  approximators.  The  algo(cid:173) rithm we  analyze performs on-line updating of a  parameter vector  during a  single endless  trajectory of an aperiodic irreducible finite  state Markov chain.  Results include convergence (with probability  1),  a  characterization of the limit  of convergence,  and a  bound on  the resulting approximation error.  In addition to establishing new  and  stronger  results  than  those  previously  available,  our  analysis  is  based  on  a  new  line  of  reasoning  that  provides  new  intuition  about the dynamics of temporal-difference learning.  Furthermore,  we  discuss  the  implications  of two  counter-examples with  regards  to the Significance  of on-line updating and linearly  parameterized  function  approximators."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/e034fb6b66aacc1d48f445ddfb08da98-Abstract.html,Fast Network Pruning and Feature Extraction by using the Unit-OBS Algorithm,"Achim Stahlberger, Martin Riedmiller","The  algorithm described  in  this  article  is  based  on  the  OBS  algo(cid:173) rithm by  Hassibi,  Stork  and  Wolff  ([1]  and  [2]).  The  main  disad(cid:173) vantage of OBS  is  its high complexity.  OBS needs  to calculate the  inverse  Hessian  to delete only one  weight  (thus  needing  much time  to  prune  a  big  net) .  A  better  algorithm should  use  this matrix to  remove more than only one  weight , because  calculating the inverse  Hessian  takes  the  most time in  the  OBS  algorithm.  The  algorithm,  called  Unit- OBS,  described  in  this  article  is  a  method to overcome this disadvantage.  This algorithm only  needs  to calculate the inverse Hessian once to remove one whole unit thus  drastically  reducing  the  time to prune big nets.  A  further  advantage  of Unit- OBS  is  that  it  can  be  used  to  do  a  feature  extraction  on  the  input  data.  This  can  be  helpful  on  the  understanding of unknown  problems."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/e1d5be1c7f2f456670de3d53c7b54f4a-Abstract.html,Learning with Noise and Regularizers in Multilayer Neural Networks,"David Saad, Sara A. Solla","Sara A.  Solla 
AT &T Research  Labs 
Holmdel, NJ  07733,  USA  solla@research .at t .com 
We  study  the  effect  of  noise  and  regularization  in  an  on-line  gradient-descent  learning scenario for  a  general  two-layer student  network  with  an  arbitrary  number  of hidden  units.  Training ex(cid:173) amples  are  randomly  drawn  input  vectors  labeled  by  a  two-layer  teacher  network  with an arbitrary number of hidden  units; the ex(cid:173) amples are corrupted  by Gaussian noise  affecting either the output  or  the  model itself.  We  examine the  effect  of both  types  of noise  and  that  of  weight-decay  regularization  on  the  dynamical evolu(cid:173) tion of the order parameters and the generalization error in various  phases  of the learning process."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/e3251075554389fe91d17a794861d47b-Abstract.html,The Learning Dynamcis of a Universal Approximator,"Ansgar H. L. West, David Saad, Ian T. Nabney","The learning properties of a  universal  approximator, a normalized  committee machine with  adjustable  biases,  are studied for  on-line  back-propagation learning.  Within  a  statistical  mechanics  frame(cid:173) work,  numerical  studies  show  that this  model  has  features  which  do not exist in  previously studied two-layer network models  with(cid:173) out adjustable biases, e.g., attractive suboptimal symmetric phases  even for  realizable cases and noiseless data."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html,Gaussian Processes for Bayesian Classification via Hybrid Monte Carlo,"David Barber, Christopher K. I. Williams","The full  Bayesian  method  for  applying  neural  networks  to  a  pre(cid:173) diction  problem is  to set  up  the prior/hyperprior structure for  the  net  and then  perform the necessary  integrals.  However,  these  inte(cid:173) grals are not tractable analytically, and Markov Chain Monte Carlo  (MCMC)  methods  are  slow,  especially  if the  parameter  space  is  high-dimensional.  Using  Gaussian  processes  we  can  approximate  the weight space integral analytically, so  that only a small number  of hyperparameters  need  be  integrated  over  by  MCMC  methods.  We  have applied  this  idea to classification  problems, obtaining ex(cid:173) cellent  results on  the  real-world  problems investigated so far ."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/e6d8545daa42d5ced125a4bf747b3688-Abstract.html,Genetic Algorithms and Explicit Search Statistics,Shumeet Baluja,"The  genetic  algorithm  (GA) is  a  heuristic  search  procedure  based  on  mechanisms  abstracted from population genetics. In a previous paper [Baluja & Caruana,  1995],  we  showed  that  much  simpler  algorithms,  such  as  hillcIimbing  and  Population(cid:173) Based Incremental  Learning (PBIL), perform comparably  to  GAs on  an  optimiza(cid:173) tion  problem  custom  designed  to  benefit  from  the  GA's  operators.  This  paper  extends these results in two directions. First, in a large-scale empirical comparison  of problems that have been reported  in GA literature, we show that on many prob(cid:173) lems,  simpler  algorithms  can  perform  significantly  better than  GAs.  Second,  we  describe when crossover is useful, and show how it can be incorporated into PBIL. 
1  IMPLICIT VS. EXPLICIT SEARCH STATISTICS 
Although there has recently been controversy in the genetic algorithm (GA) community as  to whether GAs should be used for static function optimization, a large amount of research  has been, and continues to be, conducted in this direction [De Jong,  1992]. Since much of  GA research focuses  on optimization (most often in static environments), this study exam(cid:173) ines the performance of GAs in these domains.  In the standard GA, candidate solutions are encoded as fixed length binary vectors. The ini(cid:173) tial group of potential solutions is chosen randomly. At each generation, the fitness of each  solution  is  calculated;  this  is  a  measure  of how  well  the  solution  optimizes  the  objective  function.  The subsequent generation is created through a process of selection, recombina(cid:173) tion,  and mutation. Recombination operators merge the information contained within pairs  of selected ""parents"" by  placing random subsets of the information from  both parents into  their respective  positions  in  a  member of the  subsequent  generation.  The  fitness  propor(cid:173) tional  selection  works  as  selective  pressure;  higher fitness  solution  strings  have  a  higher  probability of being selected for recombination. Mutations are used to help preserve diver(cid:173) sity in the population by introducing random changes into the solution strings. The GA uses  the population to implicitly maintain statistics about the search space. The selection, cross(cid:173) over, and mutation operators can be viewed as mechanisms of extracting the implicit statis(cid:173) tics from  the population to choose the  next set of points to sample.  Details of GAs can be  found in [Goldberg,  1989] [Holland,  1975]. 
Population-based incremental learning  (PBIL) is  a combination of genetic  algorithms  and  competitive  learning  [Baluja,  1994].  The  PBIL  algorithm  attempts  to  explicitly  maintain  statistics about the search space to decide where to sample next. The object of the algorithm  is to create a real valued probability vector which, when sampled, reveals high quality solu(cid:173) tion  vectors  with  high  probability.  For example,  if a  good  solution can  be  encoded  as  a  string  of alternating  O's  and  l's,  a  suitable  final  probability  vector  would  be  0.01,  0.99,  0.01, 0.99, etc. The PBIL algorithm and parameters are shown in Figure  1.  Initially, the values of the probability vector are initialized to 0.5. Sampling from this vec(cid:173) tor yields random solution vectors because the  probability  of generating a  I  or 0  is equal.  As search progresses,  the values in the probability vector gradually shift to represent high"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/e702e51da2c0f5be4dd354bb3e295d37-Abstract.html,Learning Exact Patterns of Quasi-synchronization among Spiking Neurons from Data on Multi-unit Recordings,"Laura Martignon, Kathryn B. Laskey, Gustavo Deco, Eilon Vaadia","This paper develops arguments for a family of temporal  log-linear models  to  represent spatio-temporal correlations  among  the  spiking  events  in  a  group of neurons.  The models can represent not just pairwise  correlations  but also correlations of higher order.  Methods  are discussed  for  inferring  the existence or absence of correlations and estimating their strength.  A  frequentist  and  a  Bayesian  approach  to  correlation  detection  are  compared.  The frequentist method is based on G 2 statistic with estimates  obtained via the  Max-Ent  principle.  In  the  Bayesian  approach  a  Markov  Chain  Monte  Carlo  Model  Composition  (MC3)  algorithm  is  applied  to  search  over  connectivity  structures  and  Laplace's  method  is  used  to  approximate their posterior probability.  Performance  of the  methods  was  tested on synthetic data.  The  methods  were  applied  to  experimental  data  obtained  by  the  fourth  author by  means  of measurements  carried  out  on  behaving Rhesus monkeys at the Hadassah Medical School  of the  Hebrew  University.  As  conjectured,  neural  connectivity  structures  need  not  be  neither hierarchical nor decomposable. 
Learning Quasi-synchronization Patterns among Spiking Neurons 
77"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/eb86d510361fc23b59f18c1bc9802cc6-Abstract.html,Efficient Nonlinear Control with Actor-Tutor Architecture,Kenji Doya,"A  new  reinforcement  learning architecture for  nonlinear  control is  proposed.  A  direct  feedback  controller,  or  the actor,  is  trained  by  a  value-gradient  based  controller,  or  the  tutor.  This  architecture  enables both efficient use of the value function and simple computa(cid:173) tion for  real-time implementation.  Good performance was  verified  in  multi-dimensional  nonlinear  control  tasks  using  Gaussian  soft(cid:173) max networks."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/f09696910bdd874a99cd74c8f05b5c44-Abstract.html,Interpolating Earth-science Data using RBF Networks and Mixtures of Experts,"Ernest Wan, Don Bone","We present a mixture of experts (ME) approach to interpolate sparse,  spatially correlated earth-science data. Kriging is an interpolation  method which uses a global covariation model estimated from the data  to take account of the spatial dependence in the data. Based on the  close relationship between kriging and the radial basis function (RBF)  network (Wan & Bone, 1996), we use a mixture of generalized RBF  networks to partition the input space into statistically correlated  regions and learn the local covariation model of the data in each  region. Applying the ME approach to simulated and real-world data,  we show that it is able to achieve good partitioning of the input space,  learn the local covariation models and improve generalization."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/f29b38f160f87ae86df31cee1982066f-Abstract.html,Statistically Efficient Estimations Using Cortical Lateral Connections,"Alexandre Pouget, Kechen Zhang","Coarse  codes  are  widely  used  throughout  the  brain to encode  sen(cid:173) sory  and  motor  variables.  Methods  designed  to  interpret  these  codes,  such  as population vector analysis, are either inefficient, i.e.,  the variance of the estimate is much larger than the smallest possi(cid:173) ble  variance,  or biologically implausible, like  maximum likelihood.  Moreover,  these  methods  attempt  to  compute  a  scalar  or  vector  estimate  of the  encoded  variable.  Neurons  are  faced  with  a  simi(cid:173) lar  estimation problem .  They  must  read  out  the  responses  of the  presynaptic  neurons,  but,  by  contrast,  they  typically  encode  the  variable  with  a  further  population  code  rather  than  as  a  scalar.  We  show  how  a  non-linear  recurrent  network  can  be  used  to  per(cid:173) form these estimation in an optimal way while keeping the estimate  in  a  coarse  code  format.  This  work  suggests  that  lateral  connec(cid:173) tions  in  the  cortex  may  be  involved  in  cleaning  up  uncorrelated  noise  among neurons  representing  similar variables."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/f3f1b7fc5a8779a9e618e1f23a7b7860-Abstract.html,Recursive Algorithms for Approximating Probabilities in Graphical Models,"Tommi Jaakkola, Michael I. Jordan","We  develop  a  recursive  node-elimination formalism for  efficiently  approximating large probabilistic networks.  No  constraints are set  on  the  network  topologies.  Yet  the  formalism can  be straightfor(cid:173) wardly  integrated  with exact  methods whenever  they  are/become  applicable.  The approximations we  use  are controlled:  they  main(cid:173) tain consistently upper and lower bounds on the desired quantities  at  all  times.  We  show  that  Boltzmann  machines,  sigmoid  belief  networks,  or any  combination  (i.e.,  chain  graphs)  can  be  handled  within  the same framework.  The accuracy  of the methods is  veri(cid:173) fied  experimentally."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/f4573fc71c731d5c362f0d7860945b88-Abstract.html,Clustering via Concave Minimization,"Paul S. Bradley, Olvi L. Mangasarian, W. Nick Street","The problem of assigning m  points in the n-dimensional real space  Rn  to k  clusters is  formulated  as  that of determining k  centers in  Rn  such  that  the  sum  of distances  of  each  point  to  the  nearest  center is  minimized.  If a polyhedral distance is  used,  the problem  can be formulated as that of minimizing a piecewise-linear concave  function  on  a  polyhedral  set  which  is  shown  to  be  equivalent  to  a  bilinear  program:  minimizing  a  bilinear  function  on  a  polyhe(cid:173) dral  set.  A  fast  finite  k-Median  Algorithm  consisting  of solving  few  linear  programs in  closed  form  leads  to a  stationary point  of  the bilinear program.  Computational testing on  a number of real(cid:173) world  databases  was  carried  out.  On  the  Wisconsin  Diagnostic  Breast  Cancer  (WDBC)  database,  k-Median  training set  correct(cid:173) ness was comparable to that of the k-Mean Algorithm,  however its  testing set correctness was  better.  Additionally,  on the Wisconsin  Prognostic  Breast  Cancer  (WPBC)  database,  distinct  and  clini(cid:173) cally  important  survival  curves  were  extracted  by  the  k-Median  Algorithm,  whereas  the  k-Mean  Algorithm  failed  to  obtain such  distinct survival curves for  the same database."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/f47330643ae134ca204bf6b2481fec47-Abstract.html,Balancing Between Bagging and Bumping,Tom Heskes,"We  compare  different  methods  to  combine  predictions  from  neu(cid:173) ral networks  trained on different  bootstrap samples of a  regression  problem.  One  of these  methods,  introduced  in  [6]  and  which  we  here  call  balancing,  is  based  on  the  analysis  of the  ensemble gen(cid:173) eralization error into an ambiguity term  and  a  term incorporating  generalization  performances of individual networks.  We  show  how  to  estimate  these  individual  errors  from  the  residuals  on  valida(cid:173) tion  patterns.  Weighting  factors  for  the  different  networks  follow  from  a  quadratic  programming problem.  On a  real-world problem  concerning  the  prediction  of sales  figures  and  on  the  well-known  Boston  housing  data set,  balancing clearly  outperforms  other  re(cid:173) cently  proposed  alternatives as  bagging  [1]  and bumping [8]. 
1  EARLY STOPPING AND  BOOTSTRAPPING 
Stopped  training is  a  popular  strategy  to  prevent  overfitting  in  neural  networks.  The  complete  data set  is  split  up  into  a  training  and  a  validation  set.  Through  learning  the  weights  are  adapted  in  order  to  minimize  the  error  on  the  training  data.  Training is  stopped  when  the  error  on  the  validation data starts  increasing.  The final  network depends on the accidental subdivision in training and validation  set ,  and often  also on  the,  usually  random,  initial weight  configuration  and chosen  minimization procedure.  In  other  words , early stopped  neural  networks  are  highly  unstable:  small changes in the data or different  initial conditions can produce large  changes in the estimate.  As  argued in [1 , 8],  with unstable estimators it is  advisable  to  resample,  i.e.,  to  apply  the  same  procedure  several  times  using  different  sub(cid:173) divisions  in  training  and  validation set  and  perhaps  starting from  different  initial 
RWCP:  Real  World Computing  Partnership;  SNN:  Foundation for  Neural Networks. 
Balancing Between Bagging and Bumping 
467 
configurations.  In  the  neural  network  literature  resampling  is  often  referred  to  as  training ensembles of neural networks  [3,  6].  In  this paper,  we  will discuss  methods  for combining the outputs of networks obtained through such a repetitive procedure. 
First,  however,  we  have to choose  how  to generate  the subdivisions in  training and  validation sets.  Options are, among others, k-fold cross-validation, subsampling and  bootstrapping.  In  this  paper  we  will  consider  bootstrapping [2]  which  is  based  on  the idea that  the available data set  is  nothing  but  a  particular  realization  of some  probability distribution.  In principle,  one  would  like  to do inference  on  this  ""true""  yet unknown probability distribution.  A natural thing to do is then to define an em(cid:173) pirical distribution.  With so-called  naive  bootstrapping  the empirical distribution  is  a  sum of delta peaks on the available data points, each  with  probability content  l/Pdata  with Pdata  the  number  of patterns.  A  bootstrap  sample  is  a  collection  of  Pdata  patterns drawn with replacement from this empirical probability distribution.  Some  of  the  data  points  will  occur  once,  some  twice  and  some  even  more  than  twice  in  this  bootstrap  sample.  The  bootstrap sample is  taken  to  be  the  training  set,  all  patterns  that do not  occur  in  a  particular  bootstrap sample constitute  the  validation set.  For  large Pdata,  the  probability that  a  pattern  becomes  part of the  validation set  is  (1  - l/Pdata)Pda.ta.  ~ l/e ~ 0.368.  An  advantage of bootstrapping  over  other  resampling  techniques  is  that  most  statistical  theory  on  resampling  is  nowadays based on the  bootstrap. 
Using  naive  bootstrapping we  generate  nrun  training and validation sets  out of our  complete  data set  of Pdata  input-output  combinations  {iI', tl'}.  In  this  paper  we  will  restrict  ourselves  to  regression  problems with, for  notational convenience,  just  one  output  variable.  We  keep  track  of a  matrix  with  components  q;  indicating  whether  pattern p  is  part of the  validation set for  run  i  (q; = 1)  or of the  training  set  (qf  =  0).  On  each  subdivision  we  train  and  stop  a  neural  network  with  one  layer of nhidden  hidden  units.  The output or  of network  i  with  weight  vector  w( i)  on input il' reads"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/f7cade80b7cc92b991cf4d2806d6bd78-Abstract.html,Self-Organizing and Adaptive Algorithms for Generalized Eigen-Decomposition,"Chanchal Chatterjee, Vwani P. Roychowdhury","The paper is developed in  two parts where we discuss a new approach  to self-organization in a single-layer linear feed-forward network.  First,  two novel algorithms for self-organization are derived from  a two-layer  linear hetero-associative network performing a one-of-m classification,  and trained with the constrained least-mean-squared classification error  criterion.  Second, two adaptive algorithms are derived from  these self(cid:173) organizing  procedures  the  principal  generalized  eigenvectors  of  two  correlation  matrices  from  two  sequences  of  random  vectors. These novel  adaptive  algorithms can  be  implemented  in  a  single-layer  linear  feed-forward  network.  We  give  a  rigorous  convergence  analysis  of the  adaptive  algorithms  by  using  stochastic  approximation theory. As an example, we consider a problem of online  signal detection in digital mobile communications."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/f91e24dfe80012e2a7984afa4480a6d6-Abstract.html,Multi-Grid Methods for Reinforcement Learning in Controlled Diffusion Processes,Stephan Pareigis,"Reinforcement learning methods for  discrete and semi-Markov de(cid:173) cision  problems  such  as  Real-Time  Dynamic  Programming  can  be  generalized  for  Controlled  Diffusion  Processes.  The  optimal  control  problem  reduces  to  a  boundary  value  problem  for  a  fully  nonlinear  second-order  elliptic  differential  equation  of Hamilton(cid:173) Jacobi-Bellman  (HJB-)  type.  Numerical  analysis  provides  multi(cid:173) grid methods for this kind of equation.  In the case of Learning Con(cid:173) trol, however, the systems of equations on the various grid-levels are  obtained  using  observed  information  (transitions  and  local  cost).  To  ensure  consistency,  special  attention  needs  to  be  directed  to(cid:173) ward  the  type  of time  and  space  discretization  during  the  obser(cid:173) vation.  An  algorithm for  multi-grid  observation is  proposed.  The  multi-grid algorithm is demonstrated on a simple queuing problem."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/f93882cbd8fc7fb794c1011d63be6fb6-Abstract.html,Noisy Spiking Neurons with Temporal Coding have more Computational Power than Sigmoidal Neurons,Wolfgang Maass,"We exhibit a novel  way of simulating sigmoidal  neural  nets by  net(cid:173) works  of  noisy  spiking  neurons  in  temporal  coding.  Furthermore  it  is  shown  that  networks  of noisy  spiking  neurons  with  temporal  coding  have  a  strictly  larger  computational  power  than  sigmoidal  neural  nets with  the same number of units. 
1 
Introduction and Definitions 
We  consider  a  formal  model  SNN  for  a  §piking neuron  network  that  is  basically  a  reformulation  of  the  spike  response  model  (and  of  the  leaky  integrate  and  fire  model)  without  using  6-functions  (see  [Maass,  1996a]  or  [Maass,  1996b]  for  further  backgrou nd). 
An  SNN consists of a finite set V  of spiking neurons, a set  E  ~ V  x V  of synapses, a  weight wu,v  2:  0 and  a  response function cu,v  :  R+  --+  R  for  each synapse {u, v}  E  E  (where R+  := {x E R: x  2:  O})  , and  a  threshold function  8 v  :  R+  --+  R+  for  each  neuron  v  E  V  . 
If Fu  ~ R+ is  the set of firing  times of a neuron u  , then  the  potential at the trigger  zone of neuron  v  at time t is  given  by"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html,Edges are the 'Independent Components' of Natural Scenes.,"Anthony J. Bell, Terrence J. Sejnowski","Field  (1994)  has  suggested  that  neurons  with  line  and  edge  selectivities  found  in  primary  visual  cortex  of  cats  and  monkeys  form  a  sparse,  dis(cid:173) tributed representation of natural scenes,  and Barlow  (1989)  has  reasoned  that such responses should emerge from an unsupervised learning algorithm  that attempts to find  a  factorial  code  of independent  visual  features.  We  show  here  that non-linear  'infomax',  when  applied  to  an ensemble  of nat(cid:173) ural  scenes,  produces  sets of visual  filters  that are localised  and oriented.  Some  of these  filters  are  Gabor-like  and  resemble  those  produced  by  the  sparseness-maximisation network of Olshausen & Field (1996).  In addition,  the outputs  of these filters  are as  independent  as  possible,  since  the info(cid:173) max network is  able to perform Independent Components Analysis  (ICA).  We  compare the resulting ICA filters  and their associated basis  functions,  with other decorrelating filters produced by Principal Components Analysis  (PCA)  and zero-phase whitening filters  (ZCA).  The ICA filters  have more  sparsely distributed (kurtotic) outputs on natural scenes.  They also resem(cid:173) ble the receptive fields  of simple  cells  in  visual  cortex,  which  suggests that  these neurons form an information-theoretic co-ordinate system for images. 
1"
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/fb2fcd534b0ff3bbed73cc51df620323-Abstract.html,For Valid Generalization the Size of the Weights is More Important than the Size of the Network,Peter L. Bartlett,"This paper shows that if a large neural network is used for a pattern  classification  problem, and the learning algorithm finds  a  network  with  small  weights  that  has  small  squared  error  on  the  training  patterns,  then  the generalization performance depends  on the size  of the  weights  rather  than  the  number  of weights.  More  specifi(cid:173) cally,  consider  an i-layer feed-forward  network of sigmoid units, in  which  the  sum  of the  magnitudes  of the  weights  associated  with  each  unit is  bounded  by  A.  The  misclassification probability con(cid:173) verges  to an error estimate (that is closely  related to squared error  on  the  training  set)  at  rate  O((cA)l(l+1)/2J(log n)jm)  ignoring  log  factors,  where  m  is  the  number of training  patterns,  n  is  the  input  dimension,  and  c is  a  constant.  This may explain  the  gen(cid:173) eralization performance of neural  networks,  particularly when  the  number of training examples is considerably smaller than the num(cid:173) ber  of weights.  It also  supports  heuristics  (such  as  weight  decay  and early stopping)  that attempt to keep  the weights small during  training."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html,Neural Network Modeling of Speech and Music Signals,Alex Röbel,Time  series  prediction  is  one of the  major  applications  of neural  net(cid:173) works.  After a short introduction into the basic theoretical foundations  we argue that the iterated prediction of a dynamical system may  be in(cid:173) terpreted as  a model of the system dynamics.  By  means of RBF  neural  networks  we describe a modeling approach and  extend it to be able  to  model instationary systems.  As a practical test for the capabilities of the  method we investigate the modeling of musical and  speech signals  and  demonstrate that the  model  may  be  used  for  synthesis  of musical  and  speech signals.
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/fd5c905bcd8c3348ad1b35d7231ee2b1-Abstract.html,Rapid Visual Processing using Spike Asynchrony,"Simon J. Thorpe, Jacques Gautrais","We have investigated the possibility that  rapid  processing  in  the  visual  system  could  be  achieved  by  using  the  order  of  firing  in  different  neurones as  a  code,  rather than  more  conventional  firing  rate  schemes.  Using  SPIKENET,  a  neural  net  simulator  based  on  integrate-and-fire  neurones and  in  which  neurones  in  the  input  layer function  as  analog(cid:173) to-delay  converters,  we  have  modeled  the  initial  stages  of  visual  processing. Initial  results  are extremely  promising.  Even  with  activity  in  retinal  output  cells  limited  to  one  spike  per  neuron  per  image  (effectively ruling out any form of rate coding), sophisticated processing  based on asynchronous activation was nonetheless possible."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/fe2d010308a6b3799a3d9c728ee74244-Abstract.html,Visual Cortex Circuitry and Orientation Tuning,"Trevor Mundel, Alexander Dimitrov, Jack D. Cowan","A  simple  mathematical model  for  the large-scale circuitry of  pri(cid:173) mary  visual  cortex  is  introduced.  It  is  shown  that  a  basic  cor(cid:173) tical  architecture  of  recurrent  local  excitation  and  lateral  inhi(cid:173) bition  can  account  quantitatively  for  such  properties  as  orien(cid:173) tation  tuning.  The  model  can  also  account  for  such  local  ef(cid:173) fects  as  cross-orientation suppression.  It is  also  shown  that  non(cid:173) local state-dependent coupling between similar orientation patches,  when  added  to  the  model,  can  satisfactorily  reproduce  such  ef(cid:173) fects as non-local iso--orientation suppression, and non-local cross(cid:173) orientation enhancement.  Following this an account is given of per(cid:173) ceptual  phenomena involving  object segmentation,  such  as  ""pop(cid:173) out"", and the direct  and indirect tilt illusions."
1996,https://papers.nips.cc/paper_files/paper/1996,https://papers.nips.cc/paper_files/paper/1996/hash/fe51510c80bfd6e5d78a164cd5b1f688-Abstract.html,Orientation Contrast Sensitivity from Long-range Interactions in Visual Cortex,"Klaus Pawelzik, Udo Ernst, Fred Wolf, Theo Geisel","Recently Sill ito and coworkers (Nature 378, pp.  492,1995) demon(cid:173) strated that stimulation beyond the classical receptive field  (cRF)  can not only modulate, but radically change a neuron's response to  oriented stimuli.  They revealed  that patch-suppressed cells  when  stimulated  with  contrasting orientations  inside  and  outside  their  cRF  can strongly  respond  to  stimuli  oriented  orthogonal to their  nominal  preferred  orientation.  Here  we  analyze  the  emergence  of  such  complex  response patterns in  a  simple  model  of primary  vi(cid:173) sual  cortex.  We  show that the observed sensitivity for  orientation  contrast  can  be  explained  by  a  delicate  interplay  between  local  isotropic interactions and patchy long-range connectivity between  distant iso-orientation domains.  In particular we demonstrate that  the observed properties might arise without specific connections be(cid:173) tween sites with  cross-oriented cRFs."
