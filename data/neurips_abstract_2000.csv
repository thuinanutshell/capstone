year,proceeding_link,paper_link,title,authors,abstract
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html,Reinforcement Learning with Function Approximation Converges to a Region,Geoffrey J. Gordon,"Many  algorithms  for  approximate reinforcement  learning  are  not  known  to  converge.  In  fact,  there  are  counterexamples  showing  that the adjustable weights in some algorithms may oscillate within  a region rather than converging to a point.  This paper shows that,  for  two  popular algorithms,  such  oscillation  is  the  worst  that  can  happen:  the  weights  cannot  diverge,  but  instead  must  converge  to a  bounded region.  The algorithms are SARSA(O)  and V(O);  the  latter algorithm was used in the well-known TD-Gammon program."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/052335232b11864986bb2fa20fa38748-Abstract.html,Redundancy and Dimensionality Reduction in Sparse-Distributed Representations of Natural Objects in Terms of Their Local Features,Penio S. Penev,"Low-dimensional representations are key to solving problems in high(cid:173) level vision, such as face compression and recognition. Factorial coding  strategies for reducing the redundancy present in natural images on the  basis of their second-order statistics have been successful in account(cid:173) ing for both psychophysical and neurophysiological properties of early  vision. Class-specific representations are presumably formed later, at  the higher-level stages of cortical processing. Here we show that when  retinotopic factorial codes are derived for ensembles of natural objects,  such as human faces, not only redundancy, but also dimensionality is re(cid:173) duced. We also show that objects are built from parts in a non-Gaussian  fashion which allows these local-feature codes to have dimensionalities  that are substantially lower than the respective Nyquist sampling rates."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/0533a888904bd4867929dffd884d60b8-Abstract.html,Who Does What? A Novel Algorithm to Determine Function Localization,"Ranit Aharonov-Barki, Isaac Meilijson, Eytan Ruppin","We  introduce  a  novel  algorithm,  termed  PPA  (Performance  Prediction  Algorithm),  that  quantitatively  measures  the  contributions  of elements  of a neural system to  the tasks it performs.  The algorithm identifies the  neurons or areas which participate in a cognitive or behavioral task, given  data about performance decrease in a small set of lesions.  It also allows  the  accurate  prediction  of performances  due  to  multi-element  lesions.  The effectiveness  of the  new  algorithm is  demonstrated  in  two  models  of recurrent neural  networks with  complex interactions among  the ele(cid:173) ments.  The algorithm is  scalable and applicable to  the  analysis of large  neural  networks.  Given  the  recent advances  in  reversible  inactivation  techniques,  it has  the  potential  to  significantly  contribute  to  the  under(cid:173) standing of the  organization of biological nervous systems, and to  shed  light on  the  long-lasting debate about local  versus distributed computa(cid:173) tion in the brain."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/059fdcd96baeb75112f09fa1dcc740cc-Abstract.html,"Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping","Rich Caruana, Steve Lawrence, C. Lee Giles","The conventional wisdom is that backprop nets with excess hidden units  generalize  poorly.  We  show  that  nets  with  excess  capacity  generalize  well when  trained with backprop and early  stopping.  Experiments  sug(cid:173) gest two reasons for this:  1) Overfitting can vary significantly in different  regions of the model.  Excess capacity allows better fit to regions of high  non-linearity,  and  backprop often  avoids  overfitting  the  regions  of low  non-linearity.  2)  Regardless  of size,  nets  learn  task  subcomponents  in  similar sequence.  Big nets pass  through stages  similar to those learned  by  smaller nets.  Early  stopping can  stop training the large net  when  it  generalizes  comparably  to  a  smaller net.  We  also  show  that conjugate  gradient can yield worse generalization because it overfits regions of low  non-linearity when learning to fit regions of high non-linearity."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/0609154fa35b3194026346c9cac2a248-Abstract.html,Color Opponency Constitutes a Sparse Representation for the Chromatic Structure of Natural Scenes,"Te-Won Lee, Thomas Wachtler, Terrence J. Sejnowski","The human visual system encodes the chromatic signals  conveyed  by the  three types  of retinal  cone  photoreceptors  in  an  opponent  fashion.  This  color  opponency  has  been  shown  to  constitute  an  efficient encoding by spectral decorrelation of the receptor signals.  We analyze the spatial and chromatic structure of natural scenes by  decomposing the spectral images into a set of linear basis functions  such  that  they  constitute  a  representation  with  minimal  redun(cid:173) dancy.  Independent  component  analysis  finds  the  basis  functions  that  transforms  the  spatiochromatic  data  such  that  the  outputs  (activations)  are statistically as independent  as  possible,  i.e.  least  redundant.  The  resulting  basis functions  show  strong  opponency  along  an  achromatic  direction  (luminance  edges),  along  a  blue(cid:173) yellow  direction,  and along  a red-blue direction.  Furthermore, the  resulting activations have very sparse distributions, suggesting that  the use  of color  opponency in the human visual system achieves a  highly  efficient  representation of colors.  Our findings  suggest that  color opponency is a result of the properties of natural spectra and  not  solely  a  consequence of the overlapping cone spectral sensitiv(cid:173) ities. 
1  Statistical structure of natural scenes 
Efficient  encoding of visual  sensory information is  an important  task for  informa(cid:173) tion processing systems  and  its  study  may  provide  insights  into  coding  principles  of biological visual  systems.  An  important goal  of sensory information  processing 
Electronic  version available  at www. cnl. salk . edu/ """"tewon. 
is  to transform the  input  signals  such  that the  redundancy  between  the  inputs  is  reduced.  In natural scenes, the image intensity is  highly predictable from neighbor(cid:173) ing  measurements  and  an  efficient  representation  preserves  the  information  while  the  neuronal output  is  minimized.  Recently,  several methods  have been  proposed  for finding efficient codes for  achromatic images of natural scenes  [1,  2,  3, 4].  While  luminance dominates the structure of the visual world, color vision provides impor(cid:173) tant  additional  information  about  our  environment.  Therefore,  we  are  interested  in efficient,  i.e.  redundancy reducing representations for  the chromatic structure of  natural scenes. 
2  Learning  efficient  representation for  chromatic image 
Our goal was to find  efficient  representations of the chromatic sensory information  such that its spatial and chromatic redundancy is reduced significantly.  The method  we  used for  finding  statistically efficient  representations is  independent  component  analysis  (ICA).  ICA  is  a  way of finding  a linear non-orthogonal co-ordinate system  in multivariate data that minimizes mutual information among the axial projections  of the data.  The directions of the axes of this co-ordinate system  (basis functions)  are determined by both second and higher-order statistics of the original data, com(cid:173) pared to Principal Component Analysis  (PCA) which is used solely in second order  statistics  and  has  orthogonal  basis  functions.  The  goal  of ICA  is  to  perform  a  linear transform which  makes the resulting source outputs as  statistically indepen(cid:173) dent  from  each  other  as  possible  [5].  ICA  assumes  an  unknown  source  vector  s  with mutually independent  components Si.  A small patch of the observed image is  stretched into a vector x  that can be represented as a linear combination of sources  components Si  such that"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/069654d5ce089c13f642d19f09a3d1c0-Abstract.html,Learning Segmentation by Random Walks,"Marina Meila, Jianbo Shi","We present a new view of image segmentation by pairwise simi(cid:173) larities. We interpret the similarities as edge flows in a Markov  random walk and study the eigenvalues and eigenvectors of the  walk's transition matrix. This interpretation shows that spectral  methods for clustering and segmentation have a probabilistic foun(cid:173) dation. In particular, we prove that the Normalized Cut method  arises naturally from our framework. Finally, the framework pro(cid:173) vides a principled method for learning the similarity function as a  combination of features."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/06a15eb1c3836723b53e4abca8d9b879-Abstract.html,A PAC-Bayesian Margin Bound for Linear Classifiers: Why SVMs work,"Ralf Herbrich, Thore Graepel","We  present a  bound on the generalisation error of linear classifiers  in  terms  of  a  refined  margin  quantity  on  the  training  set.  The  result  is  obtained  in  a  PAC- Bayesian framework  and  is  based  on  geometrical  arguments  in  the space of linear  classifiers.  The  new  bound constitutes an exponential improvement of the so far tightest  margin bound by Shawe-Taylor et al.  [8]  and scales logarithmically  in  the inverse margin.  Even  in  the case  of less  training  examples  than input dimensions sufficiently large margins lead to non-trivial  bound values and - plexity  term.  Furthermore,  the  classical  margin  is  too  coarse  a  measure for  the  essential  quantity  that controls the generalisation  error:  the  volume  ratio  between  the  whole  hypothesis  space  and  the subset of consistent hypotheses.  The practical relevance of the  result  lies  in the fact  that the well-known support vector machine  is optimal w.r.t. the new bound only if the feature vectors are all of  the same length.  As a consequence we recommend to use SVMs on  normalised feature  vectors only - a  recommendation that is  well  supported  by our  numerical experiments on two  benchmark  data  sets. 
for maximum margins -
to a  vanishing com(cid:173)"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/0731460a8a5ce1626210cbf4385ae0ef-Abstract.html,Active Learning for Parameter Estimation in Bayesian Networks,"Simon Tong, Daphne Koller","Bayesian  networks  are  graphical  representations  of probability  distributions.  In  virtually  all  of the  work  on  learning  these  networks,  the  assumption  is  that  we  are  presented  with  a data  set consisting  of randomly  generated instances  from  the  underlying  distribution.  In  many  situations, however, we  also  have  the  option  of active  learning, where  we  have  the  possibility of guiding the  sampling process by  querying  for certain types  of samples.  This  paper addresses the problem of estimating the parameters of Bayesian networks in  an  active  learning  setting.  We  provide  a  theoretical  framework  for  this  problem,  and  an  algorithm  that chooses  which  active  learning  queries  to  generate  based  on  the  model  learned  so  far.  We present experimental results showing that our active learning algorithm can significantly  reduce the need for training data in  many situations."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/07a4e20a7bbeeb7a736682b26b16ebe8-Abstract.html,A Tighter Bound for Graphical Models,"Martijn A. R. Leisink, Hilbert J. Kappen","We  present  a  method to bound  the  partition function  of a  Boltz(cid:173) mann machine neural network with any odd order polynomial. This  is  a  direct  extension  of the mean field  bound,  which  is  first  order.  We  show  that  the  third  order  bound  is  strictly  better  than  mean  field.  Additionally  we  show  the  rough  outline  how  this  bound  is  applicable  to  sigmoid belief networks.  Numerical  experiments  in(cid:173) dicate  that  an error  reduction  of a  factor  two  is  easily  reached  in  the region  where  expansion based  approximations are  useful."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/0950ca92a4dcf426067cfd2246bb5ff3-Abstract.html,Occam's Razor,"Carl Edward Rasmussen, Zoubin Ghahramani","The Bayesian paradigm apparently only sometimes gives rise to Occam's  Razor;  at  other times  very  large models perform well.  We  give  simple  examples of both kinds of behaviour. The two views are reconciled when  measuring complexity of functions, rather than of the machinery used to  implement them.  We analyze the complexity of functions for some linear  in the parameter models that are  equivalent to  Gaussian Processes, and  always find Occam's Razor at work."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/09b15d48a1514d8209b192a8b8f34e48-Abstract.html,Exact Solutions to Time-Dependent MDPs,"Justin A. Boyan, Michael L. Littman",We  describe an extension of the Markov decision process model in  which  a  continuous time  dimension  is  included in  the state space.  This  allows  for  the  representation  and  exact  solution  of  a  wide  range of problems in  which  transitions or rewards  vary over time.  We  examine  problems based on  route planning with  public trans(cid:173) portation and telescope observation scheduling.
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/09fb05dd477d4ae6479985ca56c5a12d-Abstract.html,An Information Maximization Approach to Overcomplete and Recurrent Representations,"Oren Shriki, Haim Sompolinsky, Daniel D. Lee","The principle of maximizing mutual  information is  applied to  learning  overcomplete and recurrent representations.  The underlying model con(cid:173) sists  of a network of input units driving a larger number of output units  with recurrent interactions.  In the limit of zero noise, the network is de(cid:173) terministic  and  the mutual information can be related  to  the  entropy  of  the output units.  Maximizing this entropy with respect to  both the feed(cid:173) forward connections as well as the recurrent interactions results in simple  learning rules for both sets of parameters.  The conventional independent  components (ICA) learning algorithm can be recovered as a special case  where  there  is  an  equal  number of output  units  and  no  recurrent  con(cid:173) nections.  The application of these new  learning rules is  illustrated on a  simple two-dimensional input example."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/0e087ec55dcbe7b2d7992d6b69b519fb-Abstract.html,A Linear Programming Approach to Novelty Detection,"Colin Campbell, Kristin P. Bennett","Novelty detection involves modeling the normal behaviour of a sys(cid:173) tem hence  enabling detection of any divergence from normality.  It  has  potential applications in  many areas  such  as  detection  of ma(cid:173) chine  damage  or  highlighting  abnormal features  in  medical data.  One  approach  is  to  build  a  hypothesis  estimating  the  support  of  the normal data i.e. constructing a function which is positive in the  region  where  the  data is  located  and  negative elsewhere.  Recently  kernel  methods  have  been  proposed  for  estimating the  support  of  a  distribution  and  they  have  performed well  in  practice - training  involves solution of a  quadratic programming problem.  In this pa(cid:173) per we propose a simpler kernel method for  estimating the support  based  on  linear  programming.  The  method  is  easy  to  implement  and  can learn large  datasets rapidly.  We  demonstrate the method  on medical and fault detection datasets. 
1"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/11108a3dbfe4636cb40b84b803b2fff6-Abstract.html,Programmable Reinforcement Learning Agents,"David Andre, Stuart J. Russell","We present an expressive agent design language for reinforcement learn(cid:173) ing that allows the user to constrain the policies considered by the learn(cid:173) ing process.The language includes standard features  such as  parameter(cid:173) ized subroutines, temporary interrupts, aborts, and memory variables, but  also  allows  for  unspecified choices  in  the  agent program.  For learning  that which isn't specified, we present provably convergent learning algo(cid:173) rithms.  We  demonstrate by  example that agent programs written in the  language are concise as well as modular.  This facilitates state abstraction  and the transferability of learned skills."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/11f524c3fbfeeca4aa916edcb6b6392e-Abstract.html,Learning Joint Statistical Models for Audio-Visual Fusion and Segregation,"John W. Fisher III, Trevor Darrell, William T. Freeman, Paul A. Viola","People can  understand complex auditory  and  visual  information,  often  using one to disambiguate the other.  Automated analysis, even at a low(cid:173) level,  faces  severe challenges, including  the  lack of accurate  statistical  models  for  the  signals,  and  their high-dimensionality  and  varied  sam(cid:173) pling rates.  Previous approaches  [6]  assumed simple parametric models  for the joint distribution which, while tractable, cannot capture the com(cid:173) plex signal relationships. We learn the joint distribution of the visual and  auditory  signals using a non-parametric approach.  First,  we project the  data into  a maximally  informative,  low-dimensional subspace,  suitable  for density estimation.  We  then  model the  complicated stochastic rela(cid:173) tionships between the  signals  using  a nonparametric density  estimator.  These  learned densities  allow  processing across  signal  modalities.  We  demonstrate,  on  synthetic and  real  signals,  localization  in  video  of the  face that is  speaking in  audio, and,  conversely, audio enhancement of a  particular speaker selected from the video."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/13168e6a2e6c84b4b7de9390c0ef5ec5-Abstract.html,Convergence of Large Margin Separable Linear Classification,Tong Zhang,"Large  margin  linear classification  methods  have  been  successfully  ap(cid:173) plied to many applications.  For a linearly separable problem, it is known  that under appropriate assumptions, the expected misclassification error  of the computed ""optimal hyperplane"" approaches zero at a rate propor(cid:173) tional  to  the  inverse  training  sample  size.  This  rate  is  usually charac(cid:173) terized  by the margin and the maximum norm of the input data.  In  this  paper,  we  argue  that another quantity,  namely  the robustness of the  in(cid:173) put data distribution,  also  plays  an  important role  in characterizing  the  convergence behavior of expected misclassification error.  Based on  this  concept of robustness,  we  show that for a large margin  separable linear  classification problem, the expected misclassification error may converge  exponentially in the number of training sample size."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/136f951362dab62e64eb8e841183c2a9-Abstract.html,Emergence of Movement Sensitive Neurons' Properties by Learning a Sparse Code for Natural Moving Images,"Rafal Bogacz, Malcolm W. Brown, Christophe G. Giraud-Carrier","Olshausen  &  Field  demonstrated  that  a  learning  algorithm  that  attempts  to  generate  a  sparse  code  for  natural  scenes  develops  a  complete  family  of localised,  oriented,  bandpass  receptive  fields,  similar  to  those  of  'simple  cells'  in  VI.  This  paper  describes  an  algorithm  which  finds  a  sparse  code  for  sequences  of images  that  preserves information  about the  input.  This  algorithm when  trained  on  natural  video  sequences  develops  bases  representing  the  movement in particular directions  with  particular speeds,  similar to  the  receptive  fields  of  the  movement-sensitive  cells  observed  in  cortical  visual  areas.  Furthermore,  to  previous  approaches  to  learning direction  selectivity,  the  timing  of neuronal  activity  encodes  the  phase  of the  movement,  so  the  precise  timing  of spikes is crucially important to  the information encoding."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/139f0874f2ded2e41b0393c4ac5644f7-Abstract.html,High-temperature Expansions for Learning Models of Nonnegative Data,Oliver B. Downs,"Recent  work  has  exploited  boundedness  of data  in  the  unsupervised  learning of new types of generative model.  For nonnegative data it was  recently  shown  that the  maximum-entropy generative  model  is  a  Non(cid:173) negative Boltzmann Distribution  not  a  Gaussian  distribution,  when  the  model is  constrained to match the  first and second order statistics of the  data.  Learning for practical sized problems is made difficult by the need  to  compute  expectations  under  the  model  distribution.  The  computa(cid:173) tional  cost of Markov  chain Monte  Carlo  methods  and  low  fidelity  of  naive  mean  field  techniques has  led  to  increasing interest in  advanced  mean  field  theories  and  variational  methods.  Here I  present a  second(cid:173) order mean-field approximation for the Nonnegative Boltzmann Machine  model,  obtained  using  a  ""high-temperature"" expansion.  The  theory  is  tested  on  learning  a bimodal 2-dimensional model,  a high-dimensional  translationally  invariant distribution,  and  a generative  model for hand(cid:173) written digits."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/14cfdb59b5bda1fc245aadae15b1984a-Abstract.html,A Support Vector Method for Clustering,"Asa Ben-Hur, David Horn, Hava T. Siegelmann, Vladimir Vapnik","We  present a novel  method for clustering using the support vector ma(cid:173) chine approach.  Data points are  mapped to  a high  dimensional  feature  space, where support vectors are used to define a sphere enclosing them.  The boundary of the sphere forms in data space a set of closed contours  containing the data.  Data points enclosed by each contour are defined as a  cluster. As the width parameter of the Gaussian kernel is decreased, these  contours fit  the data more tightly and  splitting of contours occurs.  The  algorithm  works  by  separating clusters  according  to  valleys  in  the un(cid:173) derlying probability distribution,  and  thus clusters can take on  arbitrary  geometrical shapes.  As in other SV algorithms, outliers can be dealt with  by introducing a soft margin constant leading to smoother cluster bound(cid:173) aries.  The  structure of the data is  explored by  varying the two parame(cid:173) ters.  We  investigate the dependence of our method on  these parameters  and apply it to several data sets."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/155fa09596c7e18e50b58eb7e0c6ccb4-Abstract.html,Incremental and Decremental Support Vector Machine Learning,"Gert Cauwenberghs, Tomaso Poggio","An on-line recursive algorithm for training support vector machines, one  vector  at  a  time,  is  presented.  Adiabatic  increments  retain  the  Kuhn(cid:173) Tucker  conditions  on  all  previously  seen  training  data,  in  a  number  of steps  each computed analytically.  The  incremental procedure is  re(cid:173) versible, and decremental ""unlearning"" offers an  efficient method to ex(cid:173) actly  evaluate  leave-one-out generalization performance.  Interpretation  of decremental unlearning in feature space sheds light on the relationship  between generalization and geometry of the data."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/15d185eaa7c954e77f5343d941e25fbd-Abstract.html,Active Inference in Concept Learning,"Jonathan D. Nelson, Javier R. Movellan","People  are  active  experimenters,  not  just  passive  observers,  constantly  seeking  new  information  relevant  to  their  goals.  A  reasonable  approach  to  active  information  gathering  is  to  ask  questions  and  conduct  experiments  that  maximize  the  expected  information  gain,  given  current  beliefs  (Fedorov  1972,  MacKay  1992,  Oaksford  &  Chater  1994).  In  this  paper  we  present  results  on  an  exploratory  experiment  designed  to  study  people's  active  information  gathering  behavior  on  a  concept  task  (Tenenbaum  2000).  The  results  of the  experiment are  analyzed  in  terms  of the  expected  information  gain  of the  questions  asked  by  subjects."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/17c3433fecc21b57000debdf7ad5c930-Abstract.html,The Interplay of Symbolic and Subsymbolic Processes in Anagram Problem Solving,"David B. Grimes, Michael Mozer","Although connectionist models have provided insights into the nature of  perception and motor control, connectionist accounts of higher cognition  seldom go  beyond an  implementation of traditional  symbol-processing  theories.  We  describe  a connectionist constraint satisfaction  model  of  how  people  solve  anagram problems.  The  model exploits  statistics  of  English  orthography,  but  also  addresses  the  interplay  of  sub symbolic  and  symbolic  computation  by  a  mechanism  that  extracts  approximate  symbolic representations (partial orderings of letters) from sub symbolic  structures  and  injects  the  extracted  representation back into  the  model  to  assist  in  the  solution  of the  anagram.  We  show  the  computational  benefit of this extraction-injection process and discuss its relationship to  conscious mental processes and  working memory.  We  also  account for  experimental data concerning the difficulty of anagram solution based on  the orthographic structure of the anagram string and the target word. 
Historically,  the  mind  has  been  viewed  from  two  opposing  computational perspectives.  The  symbolic  perspective  views  the  mind  as  a  symbolic  information processing engine.  According  to  this  perspective,  cognition  operates  on  representations  that  encode  logical  relationships among discrete  symbolic elements,  such  as  stacks  and  structured trees,  and  cognition involves basic operations such as  means-ends analysis  and best-first search.  In  contrast,  the  subsymbolic perspective views  the  mind  as  performing statistical inference,  and involves basic operations such as constraint-satisfaction search.  The data structures on  which these operations take place are numerical vectors. 
In some domains of cognition, significant progress has been made through analysis from  one computational perspective or the other. The thesis of our work is that many of these do(cid:173) mains might be understood more completely by focusing on the  interplay of subsymbolic  and  symbolic information processing.  Consider the higher-cognitive domain  of problem  solving.  At an  abstract level of description, problem solving tasks can readily be formal(cid:173) ized in  terms  of symbolic representations  and  operations.  However,  the  neurobiological  hardware that underlies human cognition appears to be  subsymbolic-representations are  noisy and  graded,  and  the brain  operates and  adapts in  a continuous fashion  that is  diffi(cid:173) cult to characterize in discrete symbolic terms.  At some level-between the computational  level  of the  task  description  and  the  implementation level  of human  neurobiology-the  symbolic and  subsymbolic accounts must come into contact with  one another.  We  focus  on this point of contact by proposing mechanisms by  which symbolic representations can  modulate sub symbolic processing, and mechanisms by which subsymbolic representations 
are made symbolic. We conjecture that these mechanisms can not only provide an account  for the  interplay of symbolic and sub symbolic processes  in cognition, but that  they  form  a sensible computational strategy  that outperforms purely subsymbolic computation, and  hence, symbolic reasoning makes sense from an evolutionary perspective. 
In this paper, we apply our approach to a high-level cognitive task, anagram problem solv(cid:173) ing.  An  anagram is  a nonsense  string  of letters  whose letters  can be rearranged to  form  a word.  For example, the solution to  the  anagram puzzle RYTEHO  is  THEORY.  Anagram  solving is  a interesting task because it taps  higher cognitive abilities and issues of aware(cid:173) ness, it has a tractable state space, and interesting psychological data is available to model. 
1  A Sub symbolic Computational Model 
We  start by presenting a purely subsymbolic model  of anagram processing.  By  subsym(cid:173) bolic,  we  mean  that the  model  utilizes  only  English  orthographic statistics  and  does  not  have access to an English lexicon. We will argue that this model proves insufficient to ex(cid:173) plain human performance on anagram problem solving. However, it is a key component of  a hybrid symbolic-subsymbolic model  we propose, and is thus described in detail. 
1.1  Problem Representation 
A computational  model  of anagram  processing  must represent letter orderings.  For ex(cid:173) ample, the model must be capable of representing a solution such as  ,  or any  permutation of the  letters  such as  .  (The  symbols ""<"" and  "">"" will be used  to  delimit the  beginning  and  end  of a string,  respectively.)  We  adopted  a representation  of letter strings in  which a string is  encoded by the  set of letter pairs  (hereafter, bigrams)  contained in the string; for example, the bigrams in   are: .  The delimiters <  and  >  are treated as  ordinary symbols of the alphabet.  We  capture letter pairings in a symbolic letter-ordering matrix, or symbolic ordering for short.  Figure  lea) shows the matrix, in which the rows indicate the first letter of the bigram, and  the  columns indicate the  second.  A cell  of the  matrix contains a value of I  if the  corre(cid:173) sponding bigram is present in the string.  (This matrix formalism and all procedures in the  paper can be extended to handle strings with repeated letters, which we do not have space to  discuss.) The matrix columns and rows can be thought of as consisting of all letters from  A  to z, along with the delimiters <  and>. However, in the Figure we have omitted rows and  columns corresponding to  letters  not present in the  anagram.  Similarly,  we  have  omitted  the <  from the column space and the> from row space, as they could not by definition be  part of any bigram.  The seven bigrams indicated by  the seven ones in the Figure uniquely  specify the string THEORY. 
As  we've described the matrix, cells contain the truth  value of the proposition that a par(cid:173) ticular bigram appears in the  string being represented.  However,  the  cell  values  have  an  interesting alternative interpretation:  as  the probability that a particular bigram is  present.  Figure  l(b) illustrates  a  matrix  of this  sort,  which  we  call  a subsymbolic letter ordering  matrix, or subsymbolic ordering for short.  In the Figure, the bigram TH  occurs with prob(cid:173) ability  0.8.  Although  the  symbolic  orderings  are  obviously  a subset  of the  sub symbolic  orderings, the two representations play critically disparate roles in  our model, and thus are  treated as  separate entities. 
To  formally  characterize symbolic  and  subsymbolic ordering matrices,  we  define a mask  vector,  /-£,  having N  = 28 elements,  corresponding to  the  26  letters  of the  alphabet plus  the  two  delimiters.  Element i  of the  mask,  /-£i,  is  set  to  one  if the  corresponding letter  appears in the anagram string and zero if it does not.  In both the symbolic and sub symbolic  orderings, the matrices are constrained such that elements in row i  and column i  must sum 
E  H  0  R  T  Y  >  1  0  0 
<  0  0  0  0  E  H"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/19de10adbaa1b2ee13f77f679fa1483a-Abstract.html,Using the Nyström Method to Speed Up Kernel Machines,"Christopher K. I. Williams, Matthias Seeger",Abstract Unavailable
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/1e913e1b06ead0b66e30b6867bf63549-Abstract.html,Sex with Support Vector Machines,"Baback Moghaddam, Ming-Hsuan Yang","Nonlinear  Support  Vector  Machines  (SVMs)  are  investigated  for  visual sex classification with low resolution  ""thumbnail""  faces  (21- by-12  pixels)  processed  from  1,755  images  from  the  FE RET face  database.  The  performance  of SVMs  is  shown  to  be  superior  to  traditional pattern classifiers (Linear, Quadratic, Fisher Linear Dis(cid:173) criminant,  Nearest-Neighbor)  as  well  as  more  modern  techniques  such as Radial Basis Function (RBF) classifiers and large ensemble(cid:173) RBF  networks.  Furthermore, the  SVM  performance  (3.4%  error)  is  currently the best result reported in the open literature."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html,Recognizing Hand-written Digits Using Hierarchical Products of Experts,"Guy Mayraz, Geoffrey E. Hinton","The  product  of  experts  learning  procedure  [1]  can  discover  a  set  of  stochastic binary features that constitute a non-linear generative model of  handwritten images of digits.  The quality of generative models learned  in this way can be assessed by learning a separate model for each class of  digit and  then comparing the unnormalized probabilities of test images  under the  10 different class-specific models.  To  improve discriminative  performance, it is helpful to learn a hierarchy of separate models for each  digit class.  Each model in the hierarchy has one layer of hidden units and  the nth level model is trained on data that consists of the activities of the  hidden  units  in  the  already  trained  (n  - l)th  level  model.  After train(cid:173) ing, each level produces a separate, unnormalized log probabilty score.  With a three-level hierarchy for each of the 10 digit classes, a test image  produces 30 scores  which  can  be used  as  inputs to  a supervised,  logis(cid:173) tic classification network that is trained on separate data.  On the MNIST  database, our system is comparable with current state-of-the-art discrimi(cid:173) native methods, demonstrating that the product of experts learning proce(cid:173) dure can produce effective generative models of high-dimensional data. 
1  Learning products of stochastic binary experts 
Hinton [1] describes a learning algorithm for probabilistic generative models that are com(cid:173) posed  of a  number of experts.  Each  expert specifies  a probability  distribution  over  the  visible variables and the experts are combined by  multiplying these distributions together  and renormalizing. 
(1) 
where d  is  a data vector in a discrete space, Om  is  all  the parameters of individual model  m, Pm(dIOm) is  the probability of d under model m, and  c  is  an  index over all  possible  vectors in  the data space. 
A Restricted Boltzmann machine [2,  3]  is  a special case of a product of experts in  which  each expert is  a single,  binary  stochastic  hidden unit that has  symmetrical connections to  a  set of visible units,  and connections between the hidden units are  forbidden.  Inference  in an RBM is  much easier than in a general Boltzmann machine and it is also much easier 
than in a causal belief net because there is  no explaining away.  There is  therefore no need  to  perform any iteration to determine the activities of the hidden units.  The hidden states,  Sj ,  are conditionally independent given the visible states,  Si,  and the distribution of Sj  is  given by the standard logistic function :"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/201d7288b4c18a679e48b31c72c30ded-Abstract.html,APRICODD: Approximate Policy Construction Using Decision Diagrams,"Robert St-Aubin, Jesse Hoey, Craig Boutilier","We propose a method of approximate dynamic programming for Markov  decision processes (MDPs) using algebraic decision diagrams  (ADDs).  We produce near-optimal value functions and policies with much lower  time  and  space  requirements  than  exact  dynamic  programming.  Our  method reduces  the  sizes  of the  intermediate value functions  generated  during value iteration by replacing the values at the terminals of the ADD  with  ranges  of values.  Our method is  demonstrated on  a class  of large  MDPs (with up to 34 billion states), and we compare the results with the  optimal value functions."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/2647c1dba23bc0e0f9cdf75339e120d2-Abstract.html,Four-legged Walking Gait Control Using a Neuromorphic Chip Interfaced to a Support Vector Learning Algorithm,"Susanne Still, Bernhard Schölkopf, Klaus Hepp, Rodney J. Douglas","To  control  the  walking gaits  of a four-legged robot we  present a novel  neuromorphic  VLSI  chip  that  coordinates  the  relative  phasing  of the  robot's legs similar to how spinal Central Pattern Generators are believed  to  control  vertebrate  locomotion  [3].  The  chip  controls  the  leg  move(cid:173) ments by  driving motors  with time varying voltages which are  the  out(cid:173) puts of a small network of coupled oscillators.  The characteristics of the  chip's  output voltages  depend  on  a  set  of input parameters.  The rela(cid:173) tionship between input parameters and output voltages can be computed  analytically for an idealized system.  In  practice, however,  this  ideal  re(cid:173) lationship is only approximately true due to transistor mismatch and off(cid:173) sets.  Fine tuning of the chip's input parameters is done automatically by  the robotic system, using an unsupervised Support Vector (SV) learning  algorithm introduced  recently  [7].  The learning  requires  only  that  the  description of the desired output is  given.  The machine learns from (un(cid:173) labeled) examples how to set the parameters to the chip in order to obtain  a desired motor behavior."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/29530de21430b7540ec3f65135f7323c-Abstract.html,A Mathematical Programming Approach to the Kernel Fisher Algorithm,"Sebastian Mika, Gunnar Rätsch, Klaus-Robert Müller","We investigate a new kernel-based classifier:  the Kernel Fisher Discrim(cid:173) inant (KFD). A mathematical programming formulation based on the ob(cid:173) servation that KFD maximizes the average margin permits an interesting  modification of the original KFD algorithm yielding the sparse KFD. We  find  that  both,  KFD  and  the  proposed sparse  KFD,  can  be  understood  in an unifying probabilistic context.  Furthermore, we  show connections  to Support Vector Machines and Relevance Vector Machines.  From this  understanding,  we  are  able  to  outline  an  interesting  kernel-regression  technique based upon the KFD  algorithm.  Simulations support the  use(cid:173) fulness  of our approach."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/2d1b2a5ff364606ff041650887723470-Abstract.html,Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task,"Brian Sallans, Geoffrey E. Hinton","The problem of reinforcement learning in large factored Markov decision  processes is explored. The Q-value of a state-action pair is approximated  by the free energy of a product of experts network.  Network parameters  are learned on-line using a modified SARSA algorithm which minimizes  the inconsistency of the Q-values of consecutive state-action pairs.  Ac(cid:173) tions are chosen based on the current value estimates by fixing the current  state and sampling actions from the network using Gibbs sampling. The  algorithm is  tested  on  a co-operative multi-agent task.  The product of  experts model is found to perform comparably to table-based Q-Iearning  for small instances of the task,  and  continues to perform well  when the  problem becomes too large for a table-based representation."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/2f25f6e326adb93c5787175dda209ab6-Abstract.html,Dopamine Bonuses,"Sham Kakade, Peter Dayan","Substantial data support a temporal difference (TO) model of  dopamine (OA) neuron activity in which the cells provide a global  error signal for reinforcement learning. However, in certain cir(cid:173) cumstances, OA activity seems anomalous under the TO model,  responding to non-rewarding stimuli. We address these anoma(cid:173) lies by suggesting that OA cells multiplex information about re(cid:173) ward bonuses, including Sutton's exploration bonuses and Ng et  al's non-distorting shaping bonuses. We interpret this additional  role for OA in terms of the unconditional attentional and psy(cid:173) chomotor effects of dopamine, having the computational role of  guiding exploration."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/3214a6d842cc69597f9edf26df552e43-Abstract.html,Sparse Greedy Gaussian Process Regression,"Alex J. Smola, Peter L. Bartlett","We  present  a  simple  sparse  greedy  technique  to  approximate  the  maximum a  posteriori estimate of Gaussian Processes  with much  improved  scaling  behaviour  in  the  sample  size  m.  In  particular,  computational  requirements  are  O(n2m),  storage  is  O(nm),  the  cost  for  prediction  is  0 ( n)  and  the  cost  to  compute  confidence  bounds  is  O(nm),  where  n  «:  m.  We  show  how  to  compute  a  stopping  criterion,  give  bounds  on  the  approximation  error,  and  show applications to large scale problems."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/333222170ab9edca4785c39f55221fe7-Abstract.html,Large Scale Bayes Point Machines,"Ralf Herbrich, Thore Graepel","also known as the Bayes point -
The concept of averaging over classifiers is fundamental to the  Bayesian analysis of learning. Based on this viewpoint, it has re(cid:173) cently been demonstrated for linear classifiers that the centre of  mass of version space (the set of all classifiers consistent with the  training set) - exhibits excel(cid:173) lent generalisation abilities. However, the billiard algorithm as pre(cid:173) sented in [4] is restricted to small sample size because it requires  o (m 2 ) of memory and 0 (N . m2 ) computational steps where m  is the number of training patterns and N is the number of random  draws from the posterior distribution. In this paper we present a  method based on the simple perceptron learning algorithm which  allows to overcome this algorithmic drawback. The method is al(cid:173) gorithmically simple and is easily extended to the multi-class case.  We present experimental results on the MNIST data set of hand(cid:173) written digits which show that Bayes point machines (BPMs) are  competitive with the current world champion, the support vector  machine. In addition, the computational complexity of BPMs can  be tuned by varying the number of samples from the posterior.  Finally, rejecting test points on the basis of their (approximative)  posterior probability leads to a rapid decrease in generalisation er(cid:173) ror, e.g. 0.1% generalisation error for a given rejection rate of 10%."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/39027dfad5138c9ca0c474d71db915c3-Abstract.html,Efficient Learning of Linear Perceptrons,"Shai Ben-David, Hans-Ulrich Simon","We  consider  the  existence  of efficient  algorithms for  learning the  class of half-spaces in ~n in the agnostic learning model (Le.,  mak(cid:173) ing no prior assumptions on the example-generating distribution).  The resulting combinatorial problem - finding  the best  agreement  half-space  over  an  input  sample - is  NP  hard  to  approximate to  within some constant factor.  We  suggest  a  way to circumvent this  theoretical bound by introducing a new measure of success for  such  algorithms.  An  algorithm is  IL-margin  successful  if the agreement  ratio of the half-space it outputs is as good as that of any half-space  once training points that are inside the IL-margins of its separating  hyper-plane are  disregarded.  We  prove  crisp  computational com(cid:173) plexity results with respect  to this success measure:  On one hand,  for  every positive IL,  there exist efficient  (poly-time) IL-margin suc(cid:173) cessful  learning  algorithms.  On  the  other  hand,  we  prove  that  unless  P=NP, there is  no  algorithm that runs  in time polynomial  in  the  sample  size  and  in  1/ IL  that  is  IL-margin  successful  for  all  IL> O."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/3c947bc2f7ff007b86a9428b74654de5-Abstract.html,Gaussianization,"Scott Saobing Chen, Ramesh A. Gopinath","High dimensional data modeling is difficult mainly because the so-called  ""curse of dimensionality"". We propose a technique called ""Gaussianiza(cid:173) tion"" for high dimensional density estimation, which alleviates the curse  of dimensionality by exploiting the independence structures in  the data.  Gaussianization is  motivated from  recent developments in  the statistics  literature:  projection pursuit, independent component analysis and Gaus(cid:173) sian  mixture  models  with  semi-tied  covariances.  We  propose  an  iter(cid:173) ative  Gaussianization  procedure  which  converges  weakly:  at  each  it(cid:173) eration,  the  data is  first  transformed  to  the  least dependent coordinates  and then each coordinate is  marginally Gaussianized by univariate tech(cid:173) niques.  Gaussianization offers density estimation sharper than traditional  kernel  methods and radial  basis function  methods.  Gaussianization can  be viewed as efficient solution of nonlinear independent component anal(cid:173) ysis and high dimensional projection pursuit."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/3fab5890d8113d0b5a4178201dc842ad-Abstract.html,Error-correcting Codes on a Bethe-like Lattice,"Renato Vicente, David Saad, Yoshiyuki Kabashima",We  analyze Gallager codes by employing a simple mean-field approxi(cid:173) mation that distorts the model geometry and preserves important interac(cid:173) tions between sites.  The method naturally recovers the probability prop(cid:173) agation decoding algorithm as  an extremization of a proper free-energy.  We  find  a thermodynamic phase transition that coincides with informa(cid:173) tion theoretical upper-bounds and explain the practical code performance  in terms of the free-energy landscape.
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/414e773d5b7e5c06d564f594bf6384d0-Abstract.html,Homeostasis in a Silicon Integrate and Fire Neuron,"Shih-Chii Liu, Bradley A. Minch","In this work, we explore homeostasis in a silicon integrate-and-fire neu(cid:173) ron.  The neuron adapts its firing rate over long time periods on the order  of seconds or minutes so that it returns to its spontaneous firing rate after  a  lasting  perturbation.  Homeostasis  is  implemented  via  two  schemes.  One  scheme  looks  at  the  presynaptic  activity  and  adapts  the  synaptic  weight depending  on  the  presynaptic  spiking rate.  The  second scheme  adapts the synaptic ""threshold"" depending on the neuron's activity.  The  threshold is  lowered if the  neuron's activity  decreases over a long  time  and  is  increased  for  prolonged  increase  in  postsynaptic  activity.  Both  these  mechanisms  for  adaptation use  floating-gate  technology.  The re(cid:173) sults shown here are measured from  a chip fabricated in  a 2-J.lm  CMOS  process."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/44968aece94f667e4095002d140b5896-Abstract.html,Incorporating Second-Order Functional Knowledge for Better Option Pricing,"Charles Dugas, Yoshua Bengio, François Bélisle, Claude Nadeau, René Garcia","Incorporating prior knowledge of a particular task into the  architecture  of a learning algorithm can greatly improve generalization performance.  We  study  here  a case where we  know  that the  function  to be learned is  non-decreasing in two of its  arguments and convex in  one of them.  For  this purpose we propose a class of functions similar to multi-layer neural  networks but (1) that has those properties, (2) is a universal approximator  of continuous functions  with  these  and  other properties.  We  apply  this  new class  of functions  to  the task of modeling the price of call  options.  Experiments show improvements on regressing the price of call options  using the new types of function classes that incorporate the a priori con(cid:173) straints."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html,Spike-Timing-Dependent Learning for Oscillatory Networks,"Silvia Scarpetta, Zhaoping Li, John A. Hertz","We  apply to oscillatory networks a  class of learning rules in which  synaptic weights change proportional to pre- and post-synaptic ac(cid:173) tivity,  with  a  kernel  A(r) measuring the effect  for  a  postsynaptic  spike a time r  after the presynaptic one.  The resulting synaptic ma(cid:173) trices have an outer-product form in which the oscillating patterns  are  represented  as  complex  vectors.  In  a  simple  model,  the even  part of A(r) enhances the resonant response to learned stimulus by  reducing the effective  damping, while the odd part determines the  frequency of oscillation.  We relate our model to the olfactory cortex  and hippocampus and their presumed roles  in forming  associative  memories and input representations."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/44a2e0804995faf8d2e3b084a1e2db1d-Abstract.html,On Reversing Jensen's Inequality,"Tony Jebara, Alex Pentland","Jensen's inequality is a powerful mathematical tool and one of the  workhorses in statistical learning. Its applications therein include the EM  algorithm, Bayesian estimation and Bayesian inference. Jensen com(cid:173) putes simple lower bounds on otherwise intractable quantities such as  products of sums and latent log-likelihoods. This simplification then per(cid:173) mits operations like integration and maximization. Quite often (i.e. in  discriminative learning) upper bounds are needed as well. We derive and  prove an efficient analytic inequality that provides such variational upper  bounds. This inequality holds for latent variable mixtures of exponential  family distributions and thus spans a wide range of contemporary statis(cid:173) tical models. We also discuss applications of the upper bounds including  maximum conditional likelihood, large margin discriminative models and  conditional Bayesian inference. Convergence, efficiency and prediction  results are shown. 1"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/4888241374e8c62ddd9b4c3cfd091f96-Abstract.html,From Mixtures of Mixtures to Adaptive Transform Coding,"Cynthia Archer, Todd K. Leen","We establish a principled framework for adaptive transform cod(cid:173) ing. Transform coders are often constructed by concatenating an ad  hoc choice of transform with suboptimal bit allocation and quan(cid:173) tizer design. Instead, we start from a probabilistic latent variable  model in the form of a mixture of constrained Gaussian mixtures.  From this model we derive a transform coding algorithm, which is  a constrained version of the generalized Lloyd algorithm for vector  quantizer design. A byproduct of our derivation is the introduc(cid:173) tion of a new transform basis, which unlike other transforms (PCA,  DCT, etc.) is explicitly optimized for coding. Image compression  experiments show adaptive transform coders designed with our al(cid:173) gorithm improve compressed image signal-to-noise ratio up to 3 dB  compared to global transform coding and 0.5 to 2 dB compared to  other adaptive transform coders."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html,Algorithmic Stability and Generalization Performance,"Olivier Bousquet, André Elisseeff","We  present a novel way of obtaining PAC-style bounds on the gen(cid:173) eralization error of learning algorithms, explicitly using their stabil(cid:173) ity properties.  A stable learner is one for which the learned solution  does  not  change much with small  changes in the training set.  The  bounds we  obtain do not depend on any measure of the complexity  of the hypothesis space  (e.g.  VC  dimension)  but rather depend on  how  the  learning  algorithm  searches  this  space,  and  can  thus  be  applied  even when the VC  dimension is  infinite.  We  demonstrate  that regularization networks possess the required stability property  and apply our method to obtain new bounds on their generalization  performance."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/4e87337f366f72daa424dae11df0538c-Abstract.html,The Kernel Trick for Distances,Bernhard Schölkopf,"A method is described which, like the kernel trick in  support vector ma(cid:173) chines  (SVMs),  lets  us  generalize distance-based  algorithms to  operate  in  feature  spaces,  usually  nonlinearly  related  to  the  input  space.  This  is  done  by  identifying  a  class  of kernels  which  can  be  represented  as  norm-based distances in Hilbert spaces.  It turns out that common kernel  algorithms,  such as  SVMs and kernel PCA,  are actually really distance  based algorithms and can be run with that class of kernels, too.  As  well  as  providing  a  useful  new  insight  into  how  these  algorithms  work, the present work can form the basis for conceiving new algorithms."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/5103c3584b063c431bd1268e9b5e76fb-Abstract.html,Higher-Order Statistical Properties Arising from the Non-Stationarity of Natural Signals,"Lucas C. Parra, Clay Spence, Paul Sajda","We  present  evidence  that  several  higher-order  statistical  proper(cid:173) ties of natural images and signals can be explained by a  stochastic  model  which  simply  varies  scale of an otherwise  stationary Gaus(cid:173) sian  process.  We  discuss  two  interesting  consequences.  The first  is  that  a  variety of natural signals  can be related  through a  com(cid:173) mon  model  of spherically invariant  random  processes,  which  have  the attractive property that the joint densities can be constructed  from the one dimensional marginal.  The second is that in some cas(cid:173) es the non-stationarity assumption and only second order methods  can be explicitly exploited to find  a  linear basis that is  equivalent  to  independent  components  obtained  with  higher-order  methods.  This is  demonstrated on spectro-temporal components of speech."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/52d2752b150f9c35ccb6869cbf074e48-Abstract.html,Probabilistic Semantic Video Indexing,"Milind R. Naphade, Igor Kozintsev, Thomas S. Huang","We  propose a  novel probabilistic framework for  semantic video in(cid:173) dexing.  We  define  probabilistic  multimedia  objects  (multijects)  to  map  low-level  media  features  to  high-level  semantic  labels.  A  graphical network of such multijects (multinet) captures scene con(cid:173) text  by discovering intra-frame  as  well  as  inter-frame  dependency  relations  between  the  concepts.  The  main contribution is  a  novel  application  of  a  factor  graph  framework  to  model  this  network.  We  model  relations  between  semantic  concepts  in  terms  of their  co-occurrence as well  as the temporal dependencies  between these  concepts  within video  shots.  Using  the sum-product algorithm  [1]  for  approximate or exact inference in these factor graph multinets,  we  attempt  to  correct  errors made  during isolated  concept  detec(cid:173) tion by  forcing  high-level constraints.  This results  in  a  significant  improvement in the overall detection performance."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/537de305e941fccdbba5627e3eefbb24-Abstract.html,Accumulator Networks: Suitors of Local Probability Propagation,"Brendan J. Frey, Anitha Kannan","One  way  to  approximate  inference  in  richly-connected  graphical  models  is  to  apply  the  sum-product  algorithm  (a.k.a.  probabil(cid:173) ity propagation algorithm), while  ignoring the fact  that the graph  has cycles.  The sum-product  algorithm can  be directly applied in  Gaussian networks  and in  graphs for  coding,  but for  many condi(cid:173) tional probability functions  - including the sigmoid function  - di(cid:173) rect  application of the sum-product  algorithm is  not possible.  We  introduce  ""accumulator networks""  that  have low  local  complexity  (but exponential global complexity) so  the sum-product algorithm  can be directly applied.  In an accumulator network, the probability  of a child given its parents is  computed by accumulating the inputs  from the parents in a Markov chain or more generally a tree.  After  giving  expressions  for  inference  and  learning  in  accumulator  net(cid:173) works,  we  give  results  on the  ""bars problem""  and on the problem  of extracting translated, overlapping faces  from  an image."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/56f9f88906aebf4ad985aaec7fa01313-Abstract.html,Sparse Representation for Gaussian Process Models,"Lehel Csató, Manfred Opper",We develop an approach for a sparse representation for Gaussian Process  (GP) models in order to overcome the limitations of GPs caused by large  data sets.  The method is based on a combination of a Bayesian online al(cid:173) gorithm together with a sequential construction of a relevant subsample  of the  data  which  fully  specifies  the  prediction  of the  model.  Experi(cid:173) mental results on toy examples and large real-world data sets indicate the  efficiency of the approach.
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/57c0531e13f40b91b3b0f1a30b529a1d-Abstract.html,Hippocampally-Dependent Consolidation in a Hierarchical Model of Neocortex,"Szabolcs Káli, Peter Dayan","In  memory consolidation, declarative memories  which  initially  require  the  hippocampus  for  their recall,  ultimately  become independent of it.  Consolidation has  been the focus of numerous experimental and qualita(cid:173) tive modeling studies, but only little quantitative exploration. We present  a consolidation model  in  which  hierarchical  connections in  the  cortex,  that  initially  instantiate  purely  semantic  information  acquired  through  probabilistic  unsupervised learning,  come  to  instantiate episodic  infor(cid:173) mation  as  well.  The hippocampus is  responsible  for  helping complete  partial input patterns before consolidation is  complete, while also train(cid:173) ing the cortex to perform appropriate completion by itself."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/596dedf4498e258e4bdc9fd70df9a859-Abstract.html,Automated State Abstraction for Options using the U-Tree Algorithm,"Anders Jonsson, Andrew G. Barto","Learning  a  complex  task  can  be  significantly  facilitated  by  defining  a  hierarchy  of subtasks.  An  agent  can  learn  to  choose  between  various  temporally abstract actions, each solving an assigned subtask, to accom(cid:173) plish the overall task.  In this paper, we study hierarchical learning using  the  framework of options.  We  argue  that to take full  advantage  of hier(cid:173) archical  structure,  one  should perform option-specific  state  abstraction,  and  that if this  is  to  scale to larger tasks,  state abstraction should be au(cid:173) tomated.  We adapt McCallum's U-Tree algorithm to automatically build  option-specific  representations  of the  state  feature  space,  and  we  illus(cid:173) trate  the  resulting  algorithm  using  a  simple  hierarchical  task.  Results  suggest  that  automated  option-specific  state  abstraction  is  an  attractive  approach to making hierarchical learning systems more effective."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/59bcda7c438bad7d2afffe9e2fed00be-Abstract.html,Structure Learning in Human Causal Induction,"Joshua B. Tenenbaum, Thomas L. Griffiths","We use graphical models to explore the question of how people learn sim(cid:173) ple causal relationships from data.  The two leading psychological theo(cid:173) ries  can both be seen as  estimating the parameters  of a fixed  graph.  We  argue  that a complete account of causal  induction should  also consider  how people learn the underlying causal graph structure, and we propose  to model this inductive process as  a Bayesian inference.  Our argument is  supported through the discussion of three data sets."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/59e0b2658e9f2e77f8d4d83f8d07ca84-Abstract.html,Minimum Bayes Error Feature Selection for Continuous Speech Recognition,"George Saon, Mukund Padmanabhan","We consider the problem of designing a linear transformation ()  E lRPx n,  of rank p  ~ n, which  projects the features of a classifier x  E  lRn  onto  y  =  ()x  E  lRP  such  as  to  achieve  minimum Bayes  error (or probabil(cid:173) ity  of misclassification).  Two  avenues  will  be  explored:  the  first  is  to  maximize the ()-average divergence between the  class  densities  and  the  second is  to minimize the union Bhattacharyya bound in the range of ().  While both  approaches yield  similar performance in  practice,  they  out(cid:173) perform standard  LDA  features  and  show  a  10%  relative improvement  in  the  word  error rate  over state-of-the-art cepstral  features  on  a  large  vocabulary telephony speech recognition task."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/5b6ba13f79129a74a3e819b78e36b922-Abstract.html,Bayesian Video Shot Segmentation,"Nuno Vasconcelos, Andrew Lippman","Prior knowledge about video  structure  can be used both as  a means  to  improve the peiformance of content analysis and to extract features  that  allow  semantic  classification.  We  introduce  statistical models for two  important components of this  structure,  shot duration and activity,  and  demonstrate  the  usefulness  of these  models  by introducing  a  Bayesian  formulation for  the  shot segmentation problem.  The  new formulations  is  shown  to  extend standard  thresholding  methods  in  an  adaptive  and  intuitive way,  leading to  improved segmentation accuracy."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/5bce843dd76db8c939d5323dd3e54ec9-Abstract.html,Kernel Expansions with Unlabeled Examples,"Martin Szummer, Tommi Jaakkola","Modern  classification  applications  necessitate  supplementing  the  few  available  labeled examples  with  unlabeled examples  to  improve classi(cid:173) fication performance.  We  present a new  tractable algorithm for exploit(cid:173) ing unlabeled examples in discriminative classification.  This is achieved  essentially by expanding the input vectors into longer feature vectors via  both labeled and unlabeled examples. The resulting classification method  can be interpreted as a discriminative kernel density estimate and is read(cid:173) ily trained via the EM algorithm, which in this case is both discriminative  and achieves the optimal solution.  We provide, in addition, a purely dis(cid:173) criminative formulation  of the  estimation  problem by  appealing  to  the  maximum entropy  framework.  We  demonstrate  that  the  proposed  ap(cid:173) proach requires very few  labeled examples for high classification accu(cid:173) racy."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/5fa9e41bfec0725742cc9d15ef594120-Abstract.html,Universality and Individuality in a Neural Code,"Elad Schneidman, Naama Brenner, Naftali Tishby, Robert R. de Ruyter van Steveninck, William Bialek","The  problem  of neural  coding  is  to  understand  how  sequences  of  action potentials (spikes)  are related to sensory stimuli, motor out(cid:173) puts,  or  (ultimately)  thoughts  and  intentions.  One clear  question  is whether the same coding rules are used  by  different  neurons, or  by  corresponding  neurons  in  different  individuals.  We  present  a  quantitative formulation of this problem using ideas from  informa(cid:173) tion theory, and apply this approach to the analysis of experiments  in the fly  visual  system.  We  find  significant  individual  differences  in  the  structure of the  code,  particularly  in  the  way  that tempo(cid:173) ral  patterns of spikes  are  used  to  convey information  beyond that  available from  variations in spike  rate.  On the other hand,  all  the  flies  in our ensemble exhibit  a  high coding efficiency,  so  that every  spike carries the same amount of information in all the individuals.  Thus  the  neural  code  has  a  quantifiable  mixture  of individuality  and universality."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/61b1fb3f59e28c67f3925f3c79be81a1-Abstract.html,Generalized Belief Propagation,"Jonathan S. Yedidia, William T. Freeman, Yair Weiss","Belief propagation  (BP)  was  only  supposed  to  work  for  tree-like  networks but works surprisingly well in many applications involving  networks  with  loops,  including  turbo  codes.  However,  there  has  been  little  understanding  of  the  algorithm  or  the  nature  of  the  solutions it finds  for  general graphs.  We  show  that  BP  can  only  converge  to  a  stationary  point  of an  approximate free  energy,  known as the Bethe free  energy in statis(cid:173) tical physics.  This result  characterizes BP fixed-points  and makes  connections with variational approaches to approximate inference.  More importantly, our analysis lets us  build  on the progress made  in  statistical  physics  since  Bethe's  approximation was  introduced  in 1935.  Kikuchi and others have shown how to construct more ac(cid:173) curate free energy approximations, of which Bethe's approximation  is  the  simplest.  Exploiting  the  insights  from  our analysis,  we  de(cid:173) rive generalized belief propagation (GBP) versions ofthese Kikuchi  approximations.  These  new  message  passing  algorithms  can  be  significantly more accurate than ordinary BP, at an adjustable in(cid:173) crease in complexity.  We  illustrate such  a  new  GBP algorithm on  a grid Markov network and show that it gives much more accurate  marginal probabilities than those found  using ordinary BP."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/645098b086d2f9e1e0e939c27f9f2d6f-Abstract.html,Multiple Timescales of Adaptation in a Neural Code,"Adrienne L. Fairhall, Geoffrey D. Lewen, William Bialek, Robert R. de Ruyter van Steveninck","Many neural systems extend their dynamic range by  adaptation.  We ex(cid:173) amine the  timescales  of adaptation  in  the  context of dynamically  mod(cid:173) ulated rapidly-varying stimuli,  and demonstrate in the  fly  visual system  that  adaptation  to  the  statistical  ensemble  of the  stimulus  dynamically  maximizes information transmission about the time-dependent stimulus.  Further, while the rate response has  long transients, the adaptation takes  place on timescales consistent with optimal variance estimation."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/65699726a3c601b9f31bf04019c8593c-Abstract.html,Speech Denoising and Dereverberation Using Probabilistic Models,"Hagai Attias, John C. Platt, Alex Acero, Li Deng","This paper presents a unified probabilistic framework for denoising and  dereverberation of speech signals. The framework transforms the denois(cid:173) ing and dereverberation problems into Bayes-optimal signal estimation.  The  key  idea is  to  use  a  strong  speech  model  that  is  pre-trained on  a  large data set of clean speech.  Computational efficiency is  achieved by  using variational EM, working in  the frequency domain, and employing  conjugate priors.  The framework covers both single and multiple micro(cid:173) phones.  We  apply this  approach to noisy reverberant speech signals and  get results substantially better than standard methods."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/65fc52ed8f88c81323a418ca94cec2ed-Abstract.html,"Modelling Spatial Recall, Mental Imagery and Neglect","Suzanna Becker, Neil Burgess","We present a computational model of the neural mechanisms in the pari(cid:173) etal and  temporal lobes that support spatial navigation, recall of scenes  and  imagery  of the  products  of recall.  Long  term  representations  are  stored  in  the  hippocampus,  and  are  associated  with  local  spatial  and  object-related features  in the parahippocampal region.  Viewer-centered  representations are dynamically generated from long term memory in the  parietal part of the  model.  The model thereby  simulates recall  and  im(cid:173) agery  of locations and  objects in  complex environments.  After parietal  damage,  the  model exhibits hemispatial neglect in  mental imagery that  rotates  with  the  imagined perspective of the  observer,  as  in  the  famous  Milan  Square experiment  [1].  Our model  makes  novel  predictions for  the  neural representations  in  the  parahippocampal and  parietal  regions  and for behavior in healthy volunteers and neuropsychological patients."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/68148596109e38cf9367d27875e185be-Abstract.html,Adaptive Object Representation with Hierarchically-Distributed Memory Sites,Bosco S. Tjan,"Theories  of  object  recognition  often  assume  that  only  one  representa(cid:173) tion  scheme  is  used  within  one  visual-processing  pathway.  Versatility  of  the  visual  system  comes  from  having  multiple  visual-processing  pathways,  each  specialized  in  a  different  category  of  objects.  We  propose  a  theoretically  simpler  alternative,  capable  of  explaining  the  same set of data and  more.  A  single primary visual-processing pathway,  loosely  modular,  is  assumed.  Memory  modules  are  attached  to  sites  along  this  pathway.  Object-identity  decision  is  made  independently  at  each  site.  A  site's  response  time  is  a  monotonic-decreasing  function  of  its  confidence  regarding  its  decision.  An  observer's  response  is  the  first-arriving  response  from  any  site.  The  effective  representation(s)  of  such  a system,  determined empirically,  can appear to  be  specialized for  different  tasks  and  stimuli,  consistent  with  recent  clinical  and  functional-imaging  findings.  This,  however,  merely  reflects  a  decision  being  made  at  its  appropriate  level  of abstraction.  The  system  itself is  intrinsically flexible and adaptive."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/68c694de94e6c110f42e587e8e48d852-Abstract.html,Text Classification using String Kernels,"Huma Lodhi, John Shawe-Taylor, Nello Cristianini, Christopher J. C. H. Watkins","We  introduce  a  novel  kernel  for  comparing two  text  documents.  The  kernel  is  an  inner  product  in  the  feature  space  consisting  of  all  subsequences  of  length  k.  A  subsequence  is  any  ordered  se(cid:173) quence  of k characters occurring in the text though not necessarily  contiguously.  The subsequences  are  weighted  by  an exponentially  decaying factor  of their  full  length  in  the  text,  hence  emphasising  those  occurrences  which  are  close  to  contiguous.  A  direct  compu(cid:173) tation of this feature vector  would involve a  prohibitive amount of  computation  even  for  modest  values  of k,  since  the  dimension  of  the feature space grows exponentially with k.  The paper describes  how despite this fact the inner product can be efficiently evaluated  by a dynamic programming technique.  A preliminary experimental  comparison of the performance of the kernel compared with a  stan(cid:173) dard  word  feature  space  kernel  results. 
[6]  is  made showing  encouraging"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/69d1fc78dbda242c43ad6590368912d4-Abstract.html,"Model Complexity, Goodness of Fit and Diminishing Returns","Igor V. Cadez, Padhraic Smyth","We  investigate a  general characteristic of the trade-off in  learning  problems  between  goodness-of-fit  and  model  complexity.  Specifi(cid:173) cally we  characterize a general class of learning problems where the  goodness-of-fit  function  can  be  shown  to  be  convex  within  first(cid:173) order  as  a  function  of model  complexity.  This  general  property  of  ""diminishing  returns""  is  illustrated  on  a  number  of real  data  sets  and learning problems, including finite  mixture modeling and  multivariate linear regression. 
Introduction, Motivation,  and Related Work 
1  Assume  we  have  a  data set  D  = {Xl, X2, ... , x n },  where  the  X i  could  be  vectors,  sequences,  etc.  We  consider  modeling  the  data set  D  using  models  indexed  by  a  complexity index k,  1 :::;  k :::;  kmax •  For example, the models could be finite  mixture  probability  density  functions  (PDFs)  for  vector  Xi'S  where  model  complexity  is  indexed by the number of components k  in the mixture.  Alternatively, the modeling  task  could  be to  fit  a  conditional  regression  model  y  = g(Zk)  + e,  where  now  y  is  one of the variables in the vector  X  and  Z  is  some subset of size  k  of the remaining  components in the  X  vector. 
Such learning tasks can typically be characterized by the existence of a  model and  a  loss  function.  A fitted  model of complexity  k  is  a  function  of the data points  D  and depends on a  specific  set of fitted  parameters B.  The loss  function  (goodness(cid:173) of-fit)  is  a  functional  of the  model  and  maps  each specific  model to  a  scalar  used  to evaluate the model,  e.g.,  likelihood for  density  estimation or sum-of-squares for  regression. 
Figure 1 illustrates a typical empirical curve for loss function versus complexity, for  mixtures  of  Markov  models  fitted  to  a  large  data set  of 900,000  sequences.  The  complexity k  is the number of Markov models being used in the mixture (see  Cadez  et  al.  (2000)  for  further  details  on  the  model  and  the  data set).  The  empirical  curve  has  a  distinctly  concave  appearance,  with large  relative  gains  in fit  for  low  complexity models and much more modest relative gains for high complexity models.  A  natural  question  is  whether  this  concavity  characteristic  can  be  viewed  as  a  general phenomenon in learning and under what assumptions on model classes and 
Nwnber of M Ixture Cmnponen1S  11] 
Figure 1:  Log-likelihood scores for  a  Markov mixtures data set. 
loss  functions  the  concavity  can  be  shown  to  hold.  The  goal  of this  paper  is  to  illustrate that in fact  it is  a  natural characteristic for  a  broad range of problems in  mixture modeling and linear regression. 
We  note of course that for  generalization that using goodness-of-fit  alone  will  lead  to  the  selection  of  the  most  complex  model  under  consideration  and  will  not  in  general select  the  model  which  generalizes  best  to new  data.  Nonetheless  our pri(cid:173) mary focus  of interest  in  this  paper is  how  goodness-of-fit  loss  functions  (such  as  likelihood and squared error, defined on the training data D)  behave in general as a  function of model complexity k.  Our concavity results have a number of interesting  implications.  For example, for  model  selection methods which  add a  penalty term  to the goodness-of-fit  (e.g., BIC), the resulting score function as a function of model  complexity will  be unimodal as a  function  of complexity k  within first  order. 
Li and Barron (1999)  have shown that for finite mixture models the expected value  of the log-likelihood for  any  k  is  bounded  below  by  a  function  of the  form  -C /k  where  C  is  a  constant  which  is  independent  of k.  The  results  presented  here  are  complementary in the sense that we show that the actual maximizing log-likelihood  itself is  concave  to  first-order  as  a  function  of k.  Furthermore,  we  obtain  a  more  general principle of ""diminishing returns,"" including both finite mixtures and subset  selection in regression."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/6ae07dcb33ec3b7c814df797cbda0f87-Abstract.html,Fast Training of Support Vector Classifiers,"Fernando Pérez-Cruz, Pedro Luis Alarcón-Diana, Angel Navia-Vázquez, Antonio Artés-Rodríguez","In this communication we present a new algorithm for solving Support  Vector Classifiers (SVC) with large training data sets. The new algorithm  is based on an Iterative Re-Weighted Least Squares procedure which is  used to  optimize the SVc. Moreover,  a novel sample selection strategy  for the working set is presented,  which randomly chooses  the working  set among the training  samples that do  not fulfill  the stopping criteria.  The validity  of both proposals,  the  optimization procedure and sample  selection  strategy,  is  shown  by  means  of computer experiments  using  well-known data sets."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/6be5336db2c119736cf48f475e051bfe-Abstract.html,Some New Bounds on the Generalization Error of Combined Classifiers,"Vladimir Koltchinskii, Dmitriy Panchenko, Fernando Lozano","In this paper we develop the method of bounding the generalization error  of a classifier in terms of its margin distribution which was introduced in  the recent papers of Bartlett and Schapire, Freund, Bartlett and Lee.  The  theory of Gaussian and empirical processes allow us  to prove the margin  type inequalities for the most general functional classes, the complexity  of the class being measured via the so called Gaussian complexity func(cid:173) tions.  As  a  simple  application  of our results,  we  obtain  the  bounds  of  Schapire, Freund, Bartlett and Lee for the generalization error of boost(cid:173) ing.  We  also  substantially improve the results  of Bartlett on bounding  the generalization error of neural networks in terms  of h -norms of the  weights  of neurons.  Furthermore, under additional assumptions  on  the  complexity of the class  of hypotheses we provide some tighter bounds,  which in  the  case  of boosting improve the results  of Schapire,  Freund,  Bartlett and Lee. 
1 
Introduction and margin type inequalities for general functional  classes 
Let (X, Y) be a random couple, where X  is  an  instance in a space Sand Y  E  {-I, I} is  a label.  Let 9 be a set of functions from  S into JR.  For 9  E g, sign(g(X)) will  be used as  a predictor (a classifier) of the unknown label Y.  If the distribution of (X, Y) is unknown,  then  the choice of the predictor is  based on  the  training data  (Xl, Yl ), ... , (Xn, Yn) that  consists ofn i.i.d.  copies of (X, Y). The goal ofleaming is to find a predictor 9 E 9 (based  on  the  training data)  whose generalization (classification) error JP'{Yg(X)  :::;  O}  is  small  enough.  We  will  first  introduce some probabilistic bounds for  general  functional classes  and then give several examples of their applications to bounding the generalization error of  boosting and neural networks.  We omit all the proofs and refer an interested reader to [5]. 
Let  (8, A, P)  be  a  probability  space  and  let F  be  a  class  of measurable functions  from  (8, A)  into  lR.  Let  {Xd  be  a  sequence  of  i.i.d.  random  variables  taking  values  in  (8, A) with common distribution P.  Let Pn  be the empirical measure based on the sample  (Xl,'""  ,Xn), Pn  := n- l  E~=l c5x ""  where c5x denotes the probability distribution con(cid:173) centrated at the point x.  We  will denote P! := Is !dP, Pn!  := Is !dPn, etc.  In  what  follows, £OO(F)  denotes the Banach  space of uniformly bounded real  valued functions on  F  with the norm IIYII.:F  := sUPfE.:F 1Y(f)I,  Y  E £OO(F).  Define  n 
n 
where {gi} is a sequence of i.i.d.  standard normal random variables, independent of {Xi}'  We will call n  t-+  Gn(F) the Gaussian complexity function of the class F. One can find in  the literature (see, e.g.  [11]) various upper bounds on such quantities as Gn (F) in terms of  entropies, VC-dimensions, etc. 
We give below a bound in terms of margin cost functions (compare to [6, 7]) and Gaussian  complexities.  Let 
Theorem 1  For all t  > 0, 
lP'{ =3/  E F: P{! :-::;  O}  > 

Let us consider a special family of cost functions.  Assume that cP  is a fixed non increasing  Lipschitz function from IR into IR such that cp(x)  2:  (1 + sgn( -x)) /2 for all x E lR. One can  easily observe that L( cpU 15))  :-::;  L( cP )15- 1 .  Applying Theorem 1 to the class of Lipschitz  functions 

Theorem 2  For all t > 0, 
lP'{3!  E F: P{! :-::;  O}  > 
inf  [Pncp(L)  +  2y'2irL(cp) Gn(F)  aE[O,l] 

cogIOg~(2c5-l)r/2] + t:n2 }:-::;  2exp{-2t2}."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/6e79ed05baec2754e25b4eac73a332d2-Abstract.html,Beyond Maximum Likelihood and Density Estimation: A Sample-Based Criterion for Unsupervised Learning of Complex Models,"Sepp Hochreiter, Michael Mozer","The goal of many unsupervised learning procedures is to bring two  probability  distributions  into  alignment.  Generative  models  such  as  Gaussian mixtures and Boltzmann machines can be cast in this  light,  as  can recoding models  such  as ICA  and projection pursuit.  We propose a novel sample-based error measure for these classes of  models, which applies even in situations where maximum likelihood  (ML)  and  probability  density  estimation-based  formulations  can(cid:173) not be applied,  e.g.,  models that are nonlinear  or have intractable  posteriors.  Furthermore,  our  sample-based  error  measure  avoids  the difficulties of approximating a  density function.  We  prove that  with  an unconstrained  model,  (1)  our approach  converges  on  the  correct solution as the number of samples goes to infinity,  and  (2)  the expected solution of our approach in the generative framework  is  the  ML  solution.  Finally,  we  evaluate our approach via simula(cid:173) tions of linear and nonlinear models  on  mixture of Gaussians and  ICA problems.  The experiments show the broad applicability and  generality of our approach."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html,A Neural Probabilistic Language Model,"Yoshua Bengio, Réjean Ducharme, Pascal Vincent","A goal  of statistical language modeling is  to  learn  the joint probability  function of sequences of words.  This is intrinsically difficult because of  the curse of dimensionality:  we propose to fight it with its own weapons.  In the proposed approach one learns simultaneously (1) a distributed rep(cid:173) resentation for each word (i.e.  a similarity between words) along with (2)  the probability function for word sequences, expressed with these repre(cid:173) sentations.  Generalization is  obtained because a sequence of words that  has  never been seen before gets  high probability if it is  made of words  that are similar to words forming an already seen sentence.  We report on  experiments using neural networks for the probability function, showing  on  two  text  corpora that  the  proposed approach  very  significantly  im(cid:173) proves on a state-of-the-art trigram model."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/7302e3f5e7c072aea8801faf8a492be0-Abstract.html,Support Vector Novelty Detection Applied to Jet Engine Vibration Spectra,"Paul M. Hayton, Bernhard Schölkopf, Lionel Tarassenko, Paul Anuzis","A system has  been developed to  extract diagnostic information from jet  engine carcass vibration data.  Support Vector Machines applied to nov(cid:173) elty  detection provide a measure  of how  unusual  the  shape  of a  vibra(cid:173) tion signature is, by learning a representation of normality.  We describe  a novel method for  Support Vector Machines  of including information  from a second class for novelty detection and give results from the appli(cid:173) cation to Jet Engine vibration analysis."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/7385db9a3f11415bc0e9e2625fae3734-Abstract.html,`N-Body' Problems in Statistical Learning,"Alexander G. Gray, Andrew W. Moore","We  present  efficient  algorithms for  all-point-pairs  problems ,  or  'N(cid:173) body '-like problems, which  are  ubiquitous in statistical learning.  We  focus on six examples, including nearest-neighbor classification, kernel  density  estimation,  outlier  detection ,  and  the  two-point  correlation.  These include any problem which abstractly requires  a comparison of  each  of the  N  points  in  a  dataset  with  each  other  point  and  would  naively  be  solved  using  N 2  distance  computations.  In  practice  N  is  often  large  enough  to  make this  infeasible.  We  present  a  suite  of  new  geometric  t echniques  which  are  applicable  in  principle  to  any  'N-body'  computation  including  large-scale  mixtures  of  Gaussians,  RBF neural networks,  and HMM 's.  Our  algorithms exhibit favorable  asymptotic  scaling  and  are  empirically  several  orders  of magnitude  faster  than  the  naive  computation,  even  for  small datasets.  We  are  aware of no exact  algorithms for  these  problems which  are  more effi(cid:173) cient  either  empirically  or  theoretically.  In  addition,  our framework  yields  simple and  elegant  algorithms.  It also  permits two  important  generalizations  beyond  the  standard  all-point-pairs  problems,  which  are  more difficult.  These  are  represented  by  our final  examples,  the  multiple two-point correlation  and the  notorious n-point correlation."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/73e0f7487b8e5297182c5a711d20bf26-Abstract.html,A Silicon Primitive for Competitive Learning,"David Hsu, Miguel Figueroa, Chris Diorio","Competitive  learning  is  a  technique  for  training  classification  and  clustering  networks.  We  have  designed  and  fabricated  an  11- transistor primitive,  that  we  term  an  automaximizing  bump  circuit,  that  implements  competitive  learning  dynamics.  The  circuit  per(cid:173) forms  a  similarity  computation,  affords  nonvolatile  storage,  and  implements  simultaneous  local  adaptation  and  computation.  We  show  that  our  primitive  is  suitable  for  implementing  competitive  learning  in  VLSI,  and  demonstrate  its  effectiveness  in  a  standard  clustering task."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/7503cfacd12053d309b6bed5c89de212-Abstract.html,Automatic Choice of Dimensionality for PCA,Thomas P. Minka,"A central issue  in  principal component analysis  (PCA)  is  choosing the  number of principal components to be retained.  By  interpreting PCA  as  density estimation, we show how to  use Bayesian model selection to es(cid:173) timate the true dimensionality of the data.  The resulting estimate is sim(cid:173) ple to  compute yet guaranteed to pick the correct dimensionality, given  enough data.  The estimate involves an integral over the Steifel manifold  of k-frames, which is difficult to compute exactly.  But after choosing an  appropriate parameterization  and  applying  Laplace's  method,  an  accu(cid:173) rate and practical estimator is obtained. In simulations, it is convincingly  better than cross-validation and  other proposed algorithms,  plus it runs  much faster."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/77369e37b2aa1404f416275183ab055f-Abstract.html,Propagation Algorithms for Variational Bayesian Learning,"Zoubin Ghahramani, Matthew J. Beal","Variational  approximations  are  becoming  a  widespread  tool  for  Bayesian learning of graphical models.  We  provide  some theoret(cid:173) ical  results for  the variational updates in a  very  general family  of  conjugate-exponential graphical  models.  We  show  how  the  belief  propagation  and  the junction  tree  algorithms  can  be  used  in  the  inference step of variational Bayesian learning.  Applying these re(cid:173) sults to the Bayesian analysis of linear-Gaussian state-space models  we  obtain a  learning procedure that exploits the Kalman smooth(cid:173) ing propagation, while  integrating over  all  model parameters.  We  demonstrate how  this can be used to infer the hidden state dimen(cid:173) sionality of the state-space model in a variety of synthetic problems  and one  real high-dimensional data set."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/7e9e346dc5fd268b49bf418523af8679-Abstract.html,An Adaptive Metric Machine for Pattern Classification,"Carlotta Domeniconi, Jing Peng, Dimitrios Gunopulos","Nearest neighbor  classification assumes locally constant class con(cid:173) ditional  probabilities.  This  assumption  becomes  invalid  in  high  dimensions with finite  samples due to the curse of dimensionality.  Severe  bias  can be  introduced  under these  conditions  when  using  the  nearest  neighbor  rule.  We  propose  a  locally  adaptive  nearest  neighbor  classification  method to try to  minimize  bias.  We  use  a  Chi-squared distance analysis to compute a flexible  metric for pro(cid:173) ducing neighborhoods that are elongated along less relevant feature  dimensions and constricted along most influential ones.  As a result,  the class conditional probabilities tend to be smoother in the mod(cid:173) ified neighborhoods, whereby better classification performance can  be achieved.  The efficacy of our method is validated and compared  against other techniques using a variety of real world data."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/7ffd85d93a3e4de5c490d304ccd9f864-Abstract.html,On Iterative Krylov-Dogleg Trust-Region Steps for Solving Neural Networks Nonlinear Least Squares Problems,"Eiji Mizutani, James Demmel","This  paper  describes  a  method of dogleg  trust-region  steps,  or re(cid:173) stricted  Levenberg-Marquardt  steps,  based  on  a  projection  pro(cid:173) cess  onto the  Krylov subspaces  for  neural  networks  nonlinear least  squares problems.  In particular, the linear conjugate gradient (CG)  method  works  as  the  inner iterative  algorithm for  solving  the  lin(cid:173) earized  Gauss-Newton normal equation, whereas  the  outer nonlin(cid:173) ear algorithm repeatedly  takes so-called  ""Krylov-dogleg""  steps, re(cid:173) lying only on matrix-vector multiplication without explicitly form(cid:173) ing the Jacobian matrix or the Gauss-Newton model Hessian.  That  is,  our  iterative  dogleg  algorithm  can  reduce  both  operational  counts  and  memory space  by  a  factor  of O(n)  (the number of pa(cid:173) rameters)  in comparison with a  direct  linear-equation solver.  This  memory-less property is  useful  for  large-scale  problems."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/82cadb0649a3af4968404c9f6031b233-Abstract.html,Stability and Noise in Biochemical Switches,William Bialek,"Many processes in biology, from the regulation of gene expression in  bacteria to memory in the brain, involve switches constructed from  networks of biochemical reactions.  Crucial molecules are present in  small numbers, raising questions about noise and stability.  Analysis  of noise in simple reaction schemes indicates that switches stable for  years  and  switchable  in  milliseconds  can be  built  from  fewer  than  one hundred molecules.  Prospects for direct tests of this prediction,  as well  as implications, are discussed."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/8562ae5e286544710b2e7ebe9858833b-Abstract.html,Computing with Finite and Infinite Networks,Ole Winther,"Using  statistical mechanics  results, I calculate learning curves  (average  generalization error) for Gaussian processes  (GPs)  and Bayesian  neural  networks  (NNs) used for regression.  Applying the results to learning a  teacher defined by a two-layer network, I can  directly compare GP and  Bayesian NN learning. I find that a GP in general requires CJ (d S )-training  examples  to  learn  input features  of order  s  (d  is  the  input dimension),  whereas  a  NN  can  learn  the  task  with  order  the  number of adjustable  weights training examples.  Since a GP can be considered as  an  infinite  NN, the results show that even  in the Bayesian approach, it is important  to limit the complexity of the learning machine.  The theoretical findings  are confirmed in simulations with analytical GP learning and a NN mean  field algorithm."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/85f007f8c50dd25f5a45fca73cad64bd-Abstract.html,Hierarchical Memory-Based Reinforcement Learning,"Natalia Hernandez-Gardiol, Sridhar Mahadevan","Sridhar  Mahadevan 
Department of Computer Science 
Michigan State University  East Lansing,  MI  48824  mahadeva@cse.msu.edu 
A  key  challenge  for  reinforcement  learning  is  scaling  up  to  large  partially  observable  domains.  In this paper,  we  show  how  a  hier(cid:173) archy of behaviors can be used to create and select among variable  length short-term memories  appropriate for  a task.  At  higher lev(cid:173) els  in  the  hierarchy,  the  agent  abstracts  over  lower-level  details  and  looks  back  over  a  variable  number  of high-level  decisions  in  time.  We  formalize  this  idea  in  a  framework  called  Hierarchical  Suffix Memory (HSM).  HSM  uses a memory-based SMDP learning  method  to  rapidly  propagate  delayed  reward  across  long  decision  sequences.  We  describe  a  detailed  experimental  study  comparing  memory  vs.  hierarchy  using  the  HSM  framework  on  a  realistic  corridor navigation task."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/865dfbde8a344b44095495f3591f7407-Abstract.html,Second Order Approximations for Probability Models,"Hilbert J. Kappen, Wim Wiegerinck","In  this  paper,  we  derive  a  second  order mean  field  theory  for  directed  graphical  probability  models.  By  using  an  information  theoretic  argu(cid:173) ment  it  is  shown  how  this  can  be  done  in  the  absense  of a  partition  function.  This method is  a direct generalisation of the well-known TAP  approximation for  Boltzmann Machines.  In  a numerical  example,  it is  shown  that the  method  greatly  improves  the  first  order mean  field  ap(cid:173) proximation.  For a restricted class of graphical models, so-called single  overlap graphs, the second order method has comparable complexity to  the first order method.  For sigmoid belief networks, the method is shown  to be particularly fast and effective."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/8c3039bd5842dca3d944faab91447818-Abstract.html,Feature Selection for SVMs,"Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimiliano Pontil, Tomaso Poggio, Vladimir Vapnik","We introduce a method of feature selection for Support Vector Machines.  The method is based upon finding those features which minimize bounds  on  the leave-one-out error.  This  search can be efficiently performed via  gradient descent.  The resulting  algorithms  are  shown to  be  superior to  some standard feature selection algorithms on both toy data and real-life  problems of face  recognition, pedestrian detection  and  analyzing DNA  micro array data."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/8c8a58fa97c205ff222de3685497742c-Abstract.html,Direct Classification with Indirect Data,Timothy X. Brown,"We classify an input space according to the outputs of a real-valued  function.  The  function  is  not  given,  but  rather  examples  of the  function.  We  contribute a  consistent classifier that  avoids the un(cid:173) necessary complexity of estimating the function."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/8d55a249e6baa5c06772297520da2051-Abstract.html,Constrained Independent Component Analysis,"Wei Lu, Jagath C. Rajapakse","The  paper  presents  a  novel  technique  of constrained  independent  component analysis  (CICA)  to introduce constraints into the clas(cid:173) sical ICA and solve the constrained optimization problem by using  Lagrange  multiplier  methods.  This  paper  shows  that  CICA  can  be used to order the resulted independent components in a specific  manner and normalize the demixing matrix in the signal separation  procedure.  It can systematically eliminate the ICA's indeterminacy  on permutation and dilation.  The experiments demonstrate the use  of CICA  in  ordering of  independent  components  while  providing  normalized demixing processes.  Keywords:  Independent  component  analysis, constrained indepen(cid:173) dent component analysis, constrained optimization, Lagrange mul(cid:173) tiplier methods"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/8d9fc2308c8f28d2a7d2f6f48801c705-Abstract.html,A Gradient-Based Boosting Algorithm for Regression Problems,"Richard S. Zemel, Toniann Pitassi","In adaptive boosting, several weak learners trained sequentially  are  combined  to  boost the  overall  algorithm performance.  Re(cid:173) cently adaptive boosting methods for classification problems have  been derived as gradient descent algorithms. This formulation jus(cid:173) tifies  key elements and parameters in the methods, all chosen to  optimize a single common objective function.  We propose an anal(cid:173) ogous formulation for adaptive boosting of regression problems,  utilizing a novel objective function that leads to a simple boosting  algorithm.  We prove that this method reduces training error, and  compare its performance to other regression methods. 
The aim of boosting algorithms is to ""boost"" the small advantage that a hypothesis  produced by a weak learner can achieve over random guessing, by using the weak  learning procedure several times on a sequence of carefully constructed distribu(cid:173) tions.  Boosting methods, notably AdaBoost (Freund  &  Schapire,  1997), are  sim(cid:173) ple yet powerful algorithms that are easy to implement and yield excellent results  in practice.  Two  crucial elements of boosting algorithms are the way in which a  new distribution is constructed for the learning procedure to produce the next hy(cid:173) pothesis in the sequence, and the way in which hypotheses are combined to pro(cid:173) duce a highly accurate output.  Both of these involve a set of parameters, whose  values appeared to be determined in an ad hoc maImer.  Recently boosting algo(cid:173) rithms have been derived as gradient descent algorithms (Breiman, 1997; Schapire  & Singer, 1998; Friedman et al., 1999; Mason et al., 1999). These formulations justify  the parameter values as all serving to optimize a single common objective function.  These optimization formulations of boosting originally developed for classification  problems have recently been applied to regression problems. However, key prop(cid:173) erties of these regression boosting methods deviate significantly from the classifica(cid:173) tion boosting approach. We propose a new boosting algorithm for regression prob(cid:173) lems, also derived from a central objective function, which retains these properties.  In this paper, we describe the original boosting algorithm and summarize boosting  methods for regression.  We present our method and provide a simple proof that  elucidates conditions under which convergence  on training error can be guaran(cid:173) teed. We propose a probabilistic framework that clarifies the relationship between  various  optimization-based boosting methods.  Finally,  we summarize empirical  comparisons between our method and others on some standard problems. 
1  A  Brief Summary of Boosting Methods 
Adaptive boosting methods are simple modular algorithms that operate as follows.  Let 9  :  X  -t Y be the function to be learned, where the label set Y is  finite,  typ(cid:173) ically binary-valued.  The algorithm uses a learning procedure, which has access  to n  training examples, {(Xl, Y1),  ... , (xn, Yn)},  drawn randomly from X  x  Yac(cid:173) cording to distribution D; it outputs a hypothesis I  : X  -t Y,  whose error is the  expected value of a loss function on I(x) , g(x), where X  is chosen according to D.  Given f, cl  > 0 and access to random examples, a strong learning procedure outputs  with probability 1 - cl  a hypothesis with error at most f, with running time polyno(cid:173) mial in 1/ f,  1/ cl  and the number of examples.  A weak learning procedure satisfies  the same conditions, but where f need only be better than random guessing.  Schapire (1990)  showed that any weak learning procedure, denoted WeakLeam,  can be efficiently transformed (""boosted"") into a  strong learning procedure.  The  AdaBoost algorithm achieves  this by calling WeakLeam multiple times, in a  se(cid:173) quence of T  stages, each time presenting it with a different distribution over a fixed  training set and finally combining all of the hypotheses. The algorithm maintains a  weight w:  for each training example i at stage i, and a distribution D t  is computed  by normalizing these weights. The algorithm loops through these steps: 

At stagei, the distribution D t  is given to WeakLeam, which generates a hy(cid:173) pothesis It- The error rate ft  of It w.r.t.  D t is:  ft  =  2::i f,(x');t'y ' wU 2::7=1 w~  w:  * (ft/ (l -
The new training distribution is obtained from the new weights:  W;+l"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/90599c8fdd2f6e7a03ad173e2f535751-Abstract.html,The Early Word Catches the Weights,"Mark A. Smith, Garrison W. Cottrell, Karen L. Anderson","The strong correlation between the frequency of words and their naming  latency has been well documented.  However, as  early as  1973, the  Age  of Acquisition (AoA) of a word was  alleged to be the actual  variable of  interest, but these studies  seem to  have been ignored in  most of the lit(cid:173) erature.  Recently, there has been a resurgence of interest in AoA. While  some studies have shown that frequency has no effect when AoA is con(cid:173) trolled for,  more recent studies have found independent contributions of  frequency and AoA. Connectionist models have repeatedly shown strong  effects  of frequency,  but  little  attention  has  been  paid  to  whether they  can also  show  AoA effects.  Indeed,  several researchers have explicitly  claimed  that they  cannot  show  AoA  effects.  In  this  work,  we  explore  these claims using a simple feed forward neural network.  We find  a sig(cid:173) nificant  contribution  of AoA  to  naming  latency,  as  well  as  conditions  under which frequency provides an independent contribution."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/90e1357833654983612fb05e3ec9148c-Abstract.html,The Manhattan World Assumption: Regularities in Scene Statistics which Enable Bayesian Inference,"James M. Coughlan, Alan L. Yuille","Preliminary work by the authors made use of the so-called  ""Man(cid:173) hattan  world""  assumption  about  the  scene  statistics  of  city  and  indoor scenes.  This assumption stated that such  scenes were built  on a  cartesian grid which led to regularities in the image edge gra(cid:173) dient  statistics.  In this paper we  explore the general applicability  of this  assumption  and show  that,  surprisingly,  it holds  in a  large  variety of less structured environments including rural scenes.  This  enables us, from  a single image, to determine the orientation of the  viewer relative to the scene structure and also to detect target ob(cid:173) jects  which  are  not  aligned  with  the  grid.  These  inferences  are  performed  using  a  Bayesian  model  with  probability  distributions  (e.g.  on the image gradient statistics)  learnt from  real data."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/944626adf9e3b76a3919b50dc0b080a4-Abstract.html,Processing of Time Series by Neural Circuits with Biologically Realistic Synaptic Dynamics,"Thomas Natschläger, Wolfgang Maass, Eduardo D. Sontag, Anthony M. Zador","Experimental data show that biological synapses behave quite differently  from the symbolic synapses in common artificial neural network models.  Biological synapses are dynamic, i.e., their ""weight"" changes on  a short  time  scale  by  several  hundred percent in  dependence of the  past  input  to  the  synapse.  In  this  article  we  explore  the  consequences  that  these  synaptic  dynamics  entail  for  the  computational  power  of feedforward  neural  networks. We  show that gradient descent suffices to  approximate  a given  (quadratic) filter  by  a rather small  neural  system  with  dynamic  synapses.  We  also  compare our network model to  artificial  neural net(cid:173) works  designed  for  time  series  processing.  Our  numerical  results  are  complemented by  theoretical analysis which  show  that even with just a  single hidden layer such networks can approximate a surprisingly large  large  class  of nonlinear filters:  all  filters  that  can  be  characterized  by  Volterra series. This result is robust with regard to various changes in the  model for synaptic dynamics."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/958adb57686c2fdec5796398de5f317a-Abstract.html,Machine Learning for Video-Based Rendering,"Arno Schödl, Irfan A. Essa","We  present  techniques  for  rendering  and  animation  of  realistic  scenes  by  analyzing  and  training  on  short  video  sequences.  This  work extends the new paradigm for computer animation, video  tex(cid:173) tures,  which  uses  recorded video  to generate  novel  animations  by  replaying the  video  samples  in  a  new  order.  Here  we  concentrate  on  video  sprites,  which  are  a  special  type  of  video  texture.  In  video  sprites,  instead of storing whole  images,  the  object  of inter(cid:173) est  is  separated  from  the  background  and  the  video  samples  are  stored as  a sequence of alpha-matted sprites with associated veloc(cid:173) ity information.  They can be  rendered anywhere  on the screen to  create a novel animation of the object.  We present methods to cre(cid:173) ate  such  animations  by  finding  a  sequence  of sprite  samples  that  is  both  visually  smooth  and  follows  a  desired  path.  To  estimate  visual  smoothness,  we  train  a  linear  classifier  to  estimate  visual  similarity  between video  samples.  If the  motion  path is  known in  advance,  we  use  beam search to find  a  good sample sequence.  We  can specify the motion interactively by precomputing the sequence  cost  function  using Q-Iearning."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/95e6834d0a3d99e9ea8811855ae9229d-Abstract.html,Discovering Hidden Variables: A Structure-Based Approach,"Gal Elidan, Noam Lotner, Nir Friedman, Daphne Koller","A serious problem in learning probabilistic models is the presence of hid(cid:173) den variables. These variables are not observed, yet interact with several  of the observed variables.  As  such,  they induce seemingly complex de(cid:173) pendencies  among  the  latter.  In  recent years,  much  attention  has  been  devoted to  the  development of algorithms for  learning parameters,  and  in  some cases  structure, in  the presence of hidden variables.  In this  pa(cid:173) per,  we  address  the  related  problem of detecting  hidden  variables  that  interact with the observed variables.  This problem is of interest both for  improving our understanding of the domain and as a preliminary step that  guides the learning procedure towards promising models.  A very natural  approach is  to  search for ""structural  signatures"" of hidden variables - substructures in the learned network that tend to  suggest the presence of  a hidden variable.  We  make this  basic  idea concrete, and  show  how  to  integrate it with structure-search algorithms.  We evaluate this method on  several synthetic and real-life datasets, and show that it performs surpris(cid:173) ingly well."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/96c5c28becf18e71190460a9955aa4d8-Abstract.html,Natural Sound Statistics and Divisive Normalization in the Auditory System,"Odelia Schwartz, Eero P. Simoncelli","We  explore  the  statistical  properties  of  natural  sound  stimuli  pre(cid:173) processed  with  a  bank  of linear  filters.  The  responses  of such  filters  exhibit a striking form of statistical dependency, in  which the  response  variance of each filter grows with the response amplitude of filters tuned  for  nearby  frequencies.  These  dependencies  may  be  substantially  re(cid:173) duced  using  an  operation  known  as  divisive  normalization,  in  which  the  response  of each  filter  is  divided  by  a  weighted  sum  of the  recti(cid:173) fied responses of other filters.  The weights may  be chosen to  maximize  the independence of the normalized responses for  an ensemble of natu(cid:173) ral  sounds.  We  demonstrate that the resulting model  accounts for  non(cid:173) linearities in the response characteristics of the auditory nerve, by com(cid:173) paring model simulations to electrophysiological recordings. In previous  work  (NIPS,  1998)  we  demonstrated  that  an  analogous  model derived  from the statistics of natural images accounts for non-linear properties of  neurons in primary visual cortex.  Thus, divisive normalization appears to  be a generic mechanism for eliminating a type of statistical dependency  that is prevalent in natural signals of different modalities. 
Signals in the real world are highly structured.  For example, natural sounds typically con(cid:173) tain both harmonic and rythmic structure. It is reasonable to assume that biological auditory  systems are designed to represent these structures in an efficient manner [e.g.,  1,2]. Specif(cid:173) ically, Barlow hypothesized that a role of early sensory processing is to remove redundancy  in the sensory input, resulting in a set of neural responses that are statistically independent. 
Experimentally, one can test this hypothesis by examining the statistical properties of neural  responses under natural stimulation conditions [e.g.,  3,4], or the statistical dependency of  pairs (or groups) of neural responses.  Due to  their technical difficulty, such  multi-cellular  experiments are only recently becoming possible, and the earliest reports in vision appear  consistent with  the hypothesis [e.g.,  5].  An  alternative approach, which we  follow here,  is  to  develop a neural  model from  the statistics  of natural  signals  and show that response  properties of this model are similar to those of biological sensory neurons. 
A number of researchers have derived linear filter models using statistical criterion. For vi(cid:173) sual images, this results in  linear filters  localized in frequency, orientation and phase [6, 7]. 
Similar work in audition has yielded filters localized in frequency and phase [8].  Although  these linear models provide an important starting point for neural modeling, sensory neu(cid:173) rons  are highly  nonlinear.  In  addition,  the  statistical properties of natural signals  are  too  complex to expect a linear transformation to result in an independent set of components. 
Recent results  indicate  that nonlinear gain control plays  an important role in neural pro(cid:173) cessing.  Ruderman and Bialek  [9] have shown that division by a local estimate of standard  deviation can increase the entropy of responses of center-surround filters to natural images.  Such  a model is consistent with  the properties of neurons in the retina and lateral genicu(cid:173) late nucleus.  Heeger and colleagues have shown that the  nonlinear behaviors of neurons  in primary visual cortex may  be described using a form of gain control known as  divisive  normalization [10], in which the response of a linear kernel is rectified and divided by the  sum  of other rectified kernel  responses  and  a constant.  We  have  recently  shown that the  responses  of oriented linear filters  exhibit nonlinear statistical dependencies that may  be  substantially reduced using  a  variant  of this  model,  in  which the  normalization signal  is  computed from a weighted sum of other rectified kernel responses [11,  12]. The resulting  model, with weighting parameters determined from image statistics, accounts qualitatively  for physiological nonlinearities observed in primary visual cortex. 
In this paper, we demonstrate that the responses of bandpass linear filters  to natural sounds  exhibit  striking  statistical  dependencies,  analogous  to  those  found  in  visual  images.  A  divisive  normalization procedure can  substantially remove these dependencies.  We  show  that this model, with parameters optimized for a collection of natural sounds, can account  for nonlinear behaviors of neurons at the level of the auditory nerve. Specifically, we show  that:  1) the shape offrequency tuning curves varies with sound pressure level, even though  the underlying linear filters are fixed; and 2) superposition of a non-optimal tone suppresses  the response of a linear filter in a divisive fashion, and the amount of suppression depends  on the distance between the frequency of the tone and the preferred frequency of the filter. 
1  Empirical observations of natural sound statistics 
The basic  statistical  properties of natural  sounds,  as  observed through a linear filter,  have  been previously documented by  Attias  [13].  In particular, he showed that,  as  with visual  images, the spectral energy falls roughly according to a power law, and that the histograms  of filter responses are more kurtotic than  a Gaussian  (i.e.,  they have  a sharp peak at zero,  and very long tails). 
Here we examine the joint statistical properties of a pair of linear filters  tuned for nearby  temporal frequencies.  We choose a fixed  set of filters  that have been widely used in  mod(cid:173) eling  the peripheral auditory system [14].  Figure 1 shows joint histograms of the instanta(cid:173) neous responses of a particular pair of linear filters  to five different types of natural sound,  and white noise.  First note that the responses are approximately decorrelated: the expected  value of the y-axis value is roughly zero for all values of the x-axis variable. The responses  are  not,  however,  statistically  independent:  the  width  of the  distribution  of responses  of  one  filter  increases  with  the response  amplitude  of the  other filter.  If the  two  responses  were  statistically  independent,  then  the response of the  first  filter  should not provide any  information about the distribution of responses of the other filter.  We have found that this  type  of variance  dependency  (sometimes  accompanied by  linear correlation) occurs  in  a  wide range of natural  sounds, ranging from animal  sounds  to  music.  We  emphasize that  this dependency is  a property of natural sounds, and is not due purely to our choice of lin(cid:173) ear filters.  For example, no such dependency is  observed when the input consists of white  noise (see Fig.  1). 
The  strength  of this  dependency  varies  for  different  pairs  of linear  filters .  In  addition,  we  see  this  type  of dependency between instantaneous responses  of a single  filter  at two"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/9a1de01f893e0d2551ecbb7ce4dc963e-Abstract.html,Regularized Winnow Methods,Tong Zhang,"In theory, the Winnow multiplicative update has certain advantages over  the Perceptron additive update when there are many irrelevant attributes.  Recently,  there has  been much effort on enhancing the Perceptron algo(cid:173) rithm by  using regularization,  leading to  a class  of linear classification  methods called support vector machines.  Similarly, it is also possible to  apply the regularization idea to the Winnow algorithm, which gives meth(cid:173) ods  we  call regularized Winnows.  We  show  that the resulting methods  compare with the basic Winnows in  a similar way  that a support vector  machine  compares  with  the  Perceptron.  We  investigate  algorithmic is(cid:173) sues and learning properties of the derived methods.  Some experimental  results will also be provided to illustrate different methods."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/9f6992966d4c363ea0162a056cb45fe5-Abstract.html,FaceSync: A Linear Operator for Measuring Synchronization of Video Facial Images and Audio Tracks,"Malcolm Slaney, Michele Covell","FaceSync  is  an  optimal  linear  algorithm  that  finds  the  degree  of syn(cid:173) chronization  between  the  audio  and  image  recordings  of  a  human  speaker.  Using canonical correlation,  it finds  the best direction  to com(cid:173) bine  all  the  audio  and  image data,  projecting  them  onto  a  single  axis.  FaceSync uses  Pearson's correlation to  measure the degree of synchro(cid:173) nization between the audio and image data. We derive the optimal linear  transform to combine the audio and visual information and describe an  implementation that avoids  the  numerical problems caused by comput(cid:173) ing the correlation matrices. 
1  Motivation  In many applications, we want to know about the synchronization between an audio signal  and  the corresponding image data.  In a teleconferencing system,  we might want to know  which of the several people imaged by a camera is  heard by the microphones; then, we can  direct the camera to the speaker. In post-production for a film,  clean audio dialog is  often  dubbed over the  video;  we  want to  adjust the  audio  signal  so  that the lip-sync is  perfect.  When analyzing a film,  we want to know when the person talking is in the shot, instead of  off camera. When evaluating the quality of dubbed films,  we can measure of how  well  the  translated words and audio fit  the actor's face. 
This paper describes an algorithm, FaceSync, that measures the degree of synchronization  between the video image of a face and the associated audio signal. We can do this task by  synthesizing the talking face,  using techniques  such  as Video  Rewrite  [1], and then com(cid:173) paring the synthesized video with the test video. That process, however, is expensive. Our  solution finds  a linear operator that, when applied to the audio and video signals, generates  an  audio-video-synchronization-error  signal.  The  linear  operator  gathers  information  from throughout the image and thus allows us to do the computation inexpensively.  Hershey and Movellan  [2]  describe an approach based on measuring the mutual informa(cid:173) tion between the audio signal and individual pixels in the  video. The correlation between  the audio signal, x, and one pixel in the image y,  is  given by Pearson's correlation,  r.  The  mutual information between these two  variables is  given by f(x,y)  = -1/2  log(l-?). They  create movies that show the regions of the video that have high correlation with the audio; 

Currently at IBM Almaden Research, 650 Harry Road, San Jose, CA 95120.  2.  Currently at Yes Video. com, 2192 Fortune Drive, San Jose, CA 95131."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/9fdb62f932adf55af2c0e09e55861964-Abstract.html,Mixtures of Gaussian Processes,Volker Tresp,"We introduce the mixture of Gaussian processes (MGP) model which is  useful for applications in which the optimal bandwidth of a map is input  dependent.  The MGP is  derived from the mixture of experts model and  can also be used for modeling general conditional probability densities.  We discuss how  Gaussian processes -in particular in form of Gaussian  process classification, the support vector machine and the MGP model(cid:173) can be used for quantifying the dependencies in graphical models."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/a19acd7d2689207f9047f8cb01357370-Abstract.html,What Can a Single Neuron Compute?,"Blaise Agüera y Arcas, Adrienne L. Fairhall, William Bialek","In this  paper  we  formulate  a  description  of the  computation  per(cid:173) formed  by  a  neuron  as  a  combination  of  dimensional  reduction  and nonlinearity.  We  implement this description for  the Hodgkin(cid:173) Huxley model,  identify the most  relevant  dimensions  and find  the  nonlinearity.  A  two  dimensional  description  already  captures  a  significant  fraction  of the information that  spikes  carry about  dy(cid:173) namic inputs.  This description  also shows that computation in the  Hodgkin-Huxley  model  is  more  complex  than  a  simple  integrate(cid:173) and-fire or perceptron model."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/a3545bd79d31f9a72d3a78690adf73fc-Abstract.html,Tree-Based Modeling and Estimation of Gaussian Processes on Graphs with Cycles,"Martin J. Wainwright, Erik B. Sudderth, Alan S. Willsky","We  present  the embedded  trees  algorithm,  an  iterative technique  for  estimation  of Gaussian  processes  defined  on  arbitrary  graphs.  By exactly solving a series of modified problems on embedded span(cid:173) ning  trees,  it  computes  the  conditional  means  with  an  efficiency  comparable to or better than other techniques.  Unlike other meth(cid:173) ods,  the  embedded trees  algorithm  also  computes  exact  error  co(cid:173) variances.  The error  covariance  computation  is  most  efficient  for  graphs in  which  removing a  small number of edges  reveals an em(cid:173) bedded  tree.  In  this  context,  we  demonstrate  that  sparse  loopy  graphs  can  provide  a  significant  increase  in  modeling  power  rela(cid:173) tive to trees,  with only a  minor increase in  estimation complexity."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/aa2a77371374094fe9e0bc1de3f94ed9-Abstract.html,Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks,"Xiaohui Xie, Richard H. R. Hahnloser, H. Sebastian Seung","It has long been known that lateral inhibition in neural networks can lead  to a winner-take-all competition, so that only a single neuron is active at  a steady  state.  Here we  show how  to  organize lateral inhibition so that  groups  of neurons  compete to  be  active.  Given  a  collection  of poten(cid:173) tially overlapping groups, the inhibitory connectivity is  set by a formula  that can be interpreted as arising from a simple learning rule.  Our analy(cid:173) sis demonstrates that such inhibition generally results in  winner-take-all  competition between  the  given  groups,  with  the exception of some de(cid:173) generate cases.  In a broader context, the network serves  as  a particular  illustration  of the  general  distinction  between  permitted  and  forbidden  sets,  which was introduced recently.  From this  viewpoint, the computa(cid:173) tional function  of our network is  to  store and retrieve memories as  per(cid:173) mitted sets of coactive neurons. 
In  traditional  winner-take-all  networks,  lateral  inhibition  is  used  to  enforce  a  localized,  or ""grandmother cell"" representation  in  which  only a  single  neuron  is  active  [1,  2,  3,  4].  When used  for  unsupervised learning,  winner-take-all  networks  discover representations  similar to  those learned by  vector quantization  [5].  Recently  many  research efforts have  focused on unsupervised learning algorithms for sparsely distributed representations [6, 7].  These algorithms lead to  networks in which groups of multiple neurons are coactivated to  represent an  object.  Therefore, it is of great interest to find ways of using lateral inhibition  to mediate winner-take-all competition between groups of neurons, as this could be useful  for learning sparsely distributed representations. 
In this paper, we show how winner-take-all competition between groups of neurons can be  learned.  Given a collection of potentially overlapping groups, the inhibitory connectivity  is  set by  a simple formula  that can  be interpreted as  arising from an  online learning rule.  To show that the resulting network functions as advertised, we perform a stability analysis.  If the strength of inhibition is sufficiently great, and the group organization satisfies certain  conditions, we show that the only sets of neurons that can be coactivated at a stable steady  state are  the  given groups and their subsets.  Because of the competition between groups,  only  one group can  be  activated  at  a time.  In  general,  the identity  of the  winning group  depends on the initial conditions of the network dynamics. If the groups are ordered by the  aggregate input that each receives, the possible winners are those above a cutoff that is set  by inequalities to be specified. 
1  Basic definitions 
Let m  groups of neurons be given, where group membership is specified by the matrix 
fl = {I  if the ith neuron is in the ath group  ,  ° otherwise 
(1) 
We will  assume that every neuron belongs to at least one group l, and every group contains  at  least  one  neuron.  A  neuron is  allowed to  belong  to  more than  one group,  so  that the  groups are potentially overlapping.  The inhibitory synaptic connectivity of the network is  defined in terms of the group membership, 
Ji ' = lIm (1  _ ~a ~'!) = {o"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/ab7314887865c4265e896c6e209d1cd6-Abstract.html,Foundations for a Circuit Complexity Theory of Sensory Processing,"Robert A. Legenstein, Wolfgang Maass","We introduce total wire length as salient complexity measure for an anal(cid:173) ysis of the circuit complexity of sensory processing in biological neural  systems and neuromorphic engineering. This new complexity measure is  applied to a set of basic computational problems that apparently need to  be solved by circuits for translation- and scale-invariant sensory process(cid:173) ing.  We  exhibit new  circuit design strategies  for these  new  benchmark  functions that can be implemented within realistic complexity bounds, in  particular with linear or almost linear total  wire length."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/ac5dab2e99eee9cf9ec672e383691302-Abstract.html,Bayes Networks on Ice: Robotic Search for Antarctic Meteorites,"Liam Pedersen, Dimitrios Apostolopoulos, William Whittaker","A  Bayes  network  based  classifier  for  distinguishing  terrestrial  rocks  from  meteorites  is  implemented  onboard  the  Nomad  robot.  Equipped with a camera,  spectrometer and eddy current sensor, this  robot searched the  ice  sheets of Antarctica and autonomously made  the first robotic identification of a meteorite, in January 2000 at the  Elephant Moraine.  This paper discusses  rock classification from  a  robotic platform, and describes the system onboard Nomad."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/ad4cc1fb9b068faecfb70914acc63395-Abstract.html,Learning and Tracking Cyclic Human Motion,"Dirk Ormoneit, Hedvig Sidenbladh, Michael J. Black, Trevor Hastie","We  present  methods  for  learning  and  tracking  human  motion  in  video.  We  estimate  a  statistical model of typical  activities from  a  large  set  of 3D  periodic  human  motion data  by  segmenting these  data automatically into  ""cycles"".  Then  the  mean  and  the  princi(cid:173) pal components of the cycles  are  computed using  a  new  algorithm  that  accounts  for  missing  information  and  enforces  smooth  tran(cid:173) sitions  between  cycles.  The  learned  temporal  model  provides  a  prior probability distribution over human motions that can be used  in  a  Bayesian  framework  for  tracking  human subjects  in  complex  monocular video sequences  and recovering  their  3D  motion."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/af3303f852abeccd793068486a391626-Abstract.html,The Use of MDL to Select among Computational Models of Cognition,"In Jae Myung, Mark A. Pitt, Shaobo Zhang, Vijay Balasubramanian","How  should  we  decide  among  competing  explanations  of  a  cognitive  process  given  limited  observations?  The  problem  of  model  selection  is  at  the  heart of progress  in  cognitive  science.  In  this paper, Minimum Description Length (MDL) is introduced as a  method  for  selecting  among  computational  models  of  cognition.  We  also  show  that  differential  geometry  provides  an  intuitive  understanding  of  what  drives  model  selection  in  MDL.  Finally,  adequacy  of  MDL  is  demonstrated  in  two  areas  of  cognitive  modeling. 
1  Model  Selection  and  Model  Complexity 
The development and  testing of computational models  of cognitive processing are  a  central focus in cognitive science.  A model embodies a solution to  a problem whose  adequacy  is  evaluated by  its  ability  to  mimic  behavior by  capturing  the  regularities  underlying  observed data.  This enterprise  of model  selection is  challenging because  of the  competing  goals  that  must be  satisfied.  Traditionally,  computational  models  of  cognition  have  been  compared  using  one  of  many  goodness-of-fit  measures.  However, use  of such a measure can result in the choice of a model that over-fits the  data,  one that captures idiosyncracies in  the particular data set (i.e.,  noise) over and  above  the  underlying regularities  of interest.  Such models  are  considered  complex,  in that the  inherent flexibility  in  the  model enables it to  fit  diverse patterns of data.  As a group,  they can be characterized as having many parameters that are combined  in  a  highly  nonlinear fashion  in  the  model  equation.  They do  not  assume  a  single  structure in  the data. Rather,  the model  contains  multiple  structures; each  obtained  by finely  tuning the parameter values of the  model,  and thus can fit a wide range of  data patterns. In  contrast, simple models,  frequently  with few  parameters, assume a  specific structure in the data, which will manifest itself as a narrow range of similar  data  patterns.  Only  when  one  of these  patterns  occurs  will  the  model  fit  the  data  well. 
The  problem  of over-fitting data due  to  model complexity  suggests that the goal  of  model selection should instead be to select the model that generalizes best to all data  samples  that  arise  from  the  same  underlying  regularity,  thus  capturing  only  the  regularity,  not  the  noise.  To  achieve  this  goal,  the  selection  method  must  be  sensitive  to  the  complexity  of  a  model.  There  are  at  least  two  independent  dimensions  of  model  complexity.  They  are  the  number  of  free  parameters  of  a 
model and  its functional form,  which refers to the way the parameters are combined  in  the  model  equation.  For  instance,  it  seems  unlikely  that  two  one-parameter  models,  y  =  ex and  y  =  x9,  are equally complex in their ability to  fit data.  The two  dimensions  of model  complexity  (number  of parameters  and  functional  form)  and  their interplay can improve a  model's fit  to  the  data,  without necessarily improving  generalizability. 
The trademark of a good model  selection procedure, then, is  its ability to  satisfy two  opposing  goals.  A  model  must be sufficiently complex  to  describe  the  data  sample  accurately,  but  without  over-fitting  the  data  and  thus  losing  generalizability.  To  achieve this end, we need a theoretically well-justified measure of model  complexity  that takes into account the number of parameters and the functional form  of a model.  In  this  paper, we  introduce Minimum  Description Length  (MDL)  as  an  appropriate  method  of selecting  among  mathematical  models  of cognition.  We  also  show  that  MDL  has  an  elegant  geometric  interpretation  that  provides  a  clear,  intuitive  understanding of the meaning of complexity in  MDL.  Finally, application examples  of MDL are presented in two areas of cognitive modeling. 
1.1  Minimum  Description  Length 
The central thesis of model  selection is the estimation of a model's generalizability.  One  approach  to  assessing  generalizability  is  the  Minimum  Description  Length  (MDL)  principle  [1].  It  provides  a  theoretically  well-grounded  measure  of  complexity that is sensitive to both dimensions of complexity and also lends itself to  intuitive,  geometric interpretations.  MDL was developed  within  algorithmic coding  theory  to  choose  the  model  that permits  the  greatest  compression of data.  A  model  family f  with  parameters e assigns  the  likelihood f(yle)  to  a  given  set  of observed  data y . The full  form  of the MDL measure for  such a model family is  given below. 
MDL = -In! (yISA) + ~ln( ; ) + In f dS.jdetl(S) 
where SA is  the  parameter  that  maximizes  the  likelihood,  k  is  the  number  of  parameters  in  the  model,  N  is  the  sample  size  and  I(e)  is  the  Fisher  information  matrix.  MDL  is  the  length  in  bits  of the  shortest  possible  code  that  describes  the  data with  the  help  of a  model.  In  the context of cognitive modeling,  the  model that  minimizes  MDL  uncovers  the  greatest  amount  of  regularity  (i.e.,  knowledge)  underlying  the  data  and  therefore  should  be  selected.  The  first,  maximized  log  likelihood term  is  the  lack-of-fit measure,  and  the second and third  terms constitute  the  intrinsic  complexity  of  the  model.  In  particular,  the  third  term  captures  the  effects of complexity due to functional form,  reflected through I(e).  We will call the  latter  two  terms  together  the  geometric  complexity  of  the  model,  for  reasons  that  will  become clear in the remainder of this paper. 
MDL arises  as  a  finite  series  of terms  in  an  asymptotic  expansion  of the  Bayesian  posterior probability  of a  model  given  the  data for  a  special  form  of the  parameter  prior  density  [2] .  Hence  in  essence,  minimization  of  MDL  is  equivalent  to  maximization  of  the  Bayesian  posterior  probability.  In  this  paper  we  present  a  geometric  interpretation  of  MDL,  as  well  as  Bayesian  model  selection  [3],  that  provides an  elegant and  intuitive framework for  understanding model complexity, a  central concept in model  selection. 
2  Differential  Geometric  Interpretation of MDL 
From  a  geometric  perspective,  a  parametric  model  family  of  probability  distributions forms  a Riemannian manifold embedded in  the space of all  probability 
distributions  [4].  Every  distribution  is  a  point  in  this  space,  and  the  collection  of  points created by  varying  the  parameters of the model  gives rise  to  a  hyper-surface  in  which  ""similar""  distributions  are  mapped  to  ""nearby"" points.  The infinitesimal  distance  between  points  separated  by  the  infinitesimal  parameter differences  de;  is  given by  ds 2  = Y' k.  g .. (8 )d8 ; d8  j  where g ij(e) is the Riemannian metric tensor. The  Fisher information,  lij(e),  is  the natural  metric  on  a manifold of distributions in the  context of statistical inference [4].  We argue that the MDL measure of model fitness  has an  attractive interpretation in  such  a geometric context."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/b139e104214a08ae3f2ebcce149cdf6e-Abstract.html,Active Support Vector Machine Classification,"Olvi L. Mangasarian, David R. Musicant","An active set strategy is applied to the dual of a simple  reformula(cid:173) tion of the standard quadratic program of a  linear support vector  machine.  This  application  generates  a  fast  new  dual  algorithm  that consists of solving a  finite  number of linear equations,  with a  typically large dimensionality equal to the number of points to  be  classified.  However, by making novel use of the Sherman-Morrison(cid:173) Woodbury formula, a  much smaller matrix of the order of the orig(cid:173) inal  input  space  is  inverted at each step.  Thus,  a  problem  with a  32-dimensional input space and 7 million points required inverting  positive definite symmetric matrices of size 33 x 33 with a total run(cid:173) ning time of 96  minutes  on a  400 MHz  Pentium II.  The algorithm  requires  no  specialized  quadratic or linear  programming code,  but  merely a  linear equation solver which is  publicly  available."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/b432f34c5a997c8e7c806a895ecc5e25-Abstract.html,Decomposition of Reinforcement Learning for Admission Control of Self-Similar Call Arrival Processes,Jakob Carlström,"This paper presents predictive gain scheduling, a technique for simplify(cid:173) ing reinforcement learning problems by decomposition. Link admission  control  of self-similar call traffic is  used  to  demonstrate the  technique.  The control problem is  decomposed into  on-line prediction of near-fu(cid:173) ture call arrival rates, and precomputation of policies for Poisson call ar(cid:173) rival  processes.  At  decision  time,  the  predictions  are  used  to  select  among the policies. Simulations show that this technique results in  sig(cid:173) nificantly  faster learning  without any performance loss,  compared to  a  reinforcement  learning controller that does not decompose the problem."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/b4568df26077653eeadf29596708c94b-Abstract.html,Generalizable Singular Value Decomposition for Ill-posed Datasets,"Ulrik Kjems, Lars Kai Hansen, Stephen C. Strother","We demonstrate that statistical analysis of ill-posed data sets is  subject to a bias, which can be observed when projecting indepen(cid:173) dent test set examples onto a basis defined by the training exam(cid:173) ples. Because the training examples in an ill-posed data set do not  fully span the signal space the observed training set variances in  each basis vector will be too high compared to the average vari(cid:173) ance of the test set projections onto the same basis vectors. On  basis of this understanding we introduce the Generalizable Singu(cid:173) lar Value Decomposition (GenSVD) as a means to reduce this bias  by re-estimation of the singular values obtained in a conventional  Singular Value Decomposition, allowing for a generalization perfor(cid:173) mance increase of a subsequent statistical model. We demonstrate  that the algorithm succesfully corrects bias in a data set from a  functional PET activation study of the human brain. 
1 
Ill-posed Data Sets 
An ill-posed data set has more dimensions in each example than there are examples.  Such data sets occur in many fields of research typically in connection with image  measurements. The associated statistical problem is that of extracting structure  from the observed high-dimensional vectors in the presence of noise. The statistical  analysis can be done either supervised (Le. modelling with target values: classifi(cid:173) cation, regresssion) or unsupervised (modelling with no target values: clustering,  PCA, ICA). In both types of analysis the ill-posedness may lead to immediate prob(cid:173) lems if one tries to apply conventional statistical methods of analysis, for example  the empirical covariance matrix is prohibitively large and will be rank-deficient. 
A common approach is to use Singular Value Decomposition (SVD) or the analogue  Principal Component Analysis (PCA) to reduce the dimensionality of the data. Let  the N observed i-dimensional samples Xj, j = L .N, collected in the data matrix  X = [Xl ... XN] of size I x N, I> N . The SVD-theorem states that such a matrix  can be decomposed as"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Abstract.html,Vicinal Risk Minimization,"Olivier Chapelle, Jason Weston, Léon Bottou, Vladimir Vapnik","The  Vicinal  Risk  Minimization  principle establishes  a  bridge  between  generative models  and  methods derived from  the  Structural Risk Mini(cid:173) mization Principle such  as  Support Vector Machines  or Statistical  Reg(cid:173) ularization.  We  explain how  VRM  provides  a  framework  which  inte(cid:173) grates a number of existing algorithms, such as Parzen windows, Support  Vector Machines, Ridge Regression, Constrained Logistic Classifiers and  Tangent-Prop.  We  then  show  how  the approach implies new  algorithm(cid:173) s for solving problems usually associated with generative models.  New  algorithms  are  described for  dealing with pattern recognition problems  with very different pattern distributions and dealing with unlabeled data.  Preliminary empirical results are presented."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/bf201d5407a6509fa536afc4b380577e-Abstract.html,Sparse Kernel Principal Component Analysis,Michael E. Tipping,"'Kernel'  principal  component  analysis  (PCA)  is  an  elegant  non(cid:173) linear  generalisation  of the  popular  linear  data  analysis  method,  where  a  kernel function  implicitly  defines  a  nonlinear transforma(cid:173) tion into a feature space wherein standard PCA is  performed.  Un(cid:173) fortunately,  the  technique  is  not  'sparse',  since  the  components  thus obtained are expressed in terms of kernels associated with ev(cid:173) ery training vector.  This paper shows  that by  approximating the  covariance matrix in  feature  space by a  reduced  number of exam(cid:173) ple vectors, using a  maximum-likelihood approach, we may obtain  a highly sparse form  of kernel PCA without loss of effectiveness."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/c164bbc9d6c72a52c599bbb43d8db8e1-Abstract.html,Sparsity of Data Representation of Optimal Kernel Machine and Leave-one-out Estimator,Adam Kowalczyk,"Vapnik's result that the expectation of the generalisation error ofthe opti(cid:173) mal hyperplane is bounded by the expectation of the ratio of the number  of support vectors to  the  number of training examples is  extended to  a  broad class of kernel machines.  The class includes Support Vector Ma(cid:173) chines  for  soft margin classification and regression,  and  Regularization  Networks with a variety of kernels and cost functions.  We  show that key  inequalities in Vapnik's result become equalities once ""the classification  error"" is replaced by ""the margin error"", with the latter defined as  an in(cid:173) stance with positive cost.  In particular we show that expectations of the  true margin error and the empirical margin error are equal, and that the  sparse solutions for kernel machines are possible only if the cost function  is ""partially"" insensitive."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/c366c2c97d47b02b24c3ecade4c40a01-Abstract.html,Rate-coded Restricted Boltzmann Machines for Face Recognition,"Yee Whye Teh, Geoffrey E. Hinton","We  describe  a  neurally-inspired,  unsupervised  learning  algorithm  that  builds  a non-linear generative model for pairs  of face  images from  the  same individual.  Individuals are then recognized by  finding  the highest  relative probability pair among all pairs that consist of a test image and  an image whose identity is known.  Our method compares favorably with  other methods in the literature. The generative model consists of a single  layer of rate-coded, non-linear feature detectors  and  it has  the property  that,  given a data vector,  the  true posterior probability distribution  over  the feature detector activities can be inferred rapidly without iteration or  approximation. The weights of the feature detectors are learned by com(cid:173) paring the correlations of pixel intensities and feature activations in two  phases: When the network is observing real data and when it is observing  reconstructions of real data generated from the feature activations."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/c44799b04a1c72e3c8593a53e8000c78-Abstract.html,Shape Context: A New Descriptor for Shape Matching and Object Recognition,"Serge Belongie, Jitendra Malik, Jan Puzicha","We  develop  an  approach  to  object  recognition  based  on  match(cid:173) ing shapes and using a resulting measure of similarity in a  nearest  neighbor  classifier.  The  key  algorithmic  problem  here  is  that  of  finding  pointwise  correspondences  between  an  image shape  and  a  stored  prototype  shape.  We  introduce  a  new  shape  descriptor,  the  shape  context,  which  makes  this  possible,  using  a  simple  and  robust algorithm.  The shape context at a point captures the distri(cid:173) bution over relative positions of other shape points and thus sum(cid:173) marizes  global  shape in  a  rich,  local  descriptor.  We  demonstrate  that  shape  contexts  greatly  simplify  recovery  of correspondences  between points of two given shapes.  Once shapes are aligned, shape  contexts are used to define a robust score for measuring shape sim(cid:173) ilarity.  We  have  used  this  score  in  a  nearest-neighbor  classifier  for  recognition of hand written  digits  as  well  as  3D  objects,  using  exactly  the  same  distance  function.  On  the  benchmark  MNIST  dataset  of handwritten  digits,  this  yields  an  error  rate  of 0.63%,  outperforming other published techniques."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/c45008212f7bdf6eab6050c2a564435a-Abstract.html,"Position Variance, Recurrence and Perceptual Learning","Zhaoping Li, Peter Dayan","Stimulus  arrays  are  inevitably  presented  at  different  positions  on  the  retina in  visual  tasks,  even those that nominally require fixation.  In par(cid:173) ticular, this applies to many perceptual learning tasks.  We show that per(cid:173) ceptual inference or discrimination in the face of positional variance has a  structurally different quality from inference about fixed position stimuli,  involving  a particular,  quadratic,  non-linearity rather than  a purely  lin(cid:173) ear discrimination.  We show the advantage taking this non-linearity into  account has for discrimination, and suggest it as a role for recurrent con(cid:173) nections in area VI, by demonstrating the superior discrimination perfor(cid:173) mance of a recurrent network.  We propose that learning the feedforward  and recurrent neural connections for  these tasks corresponds to  the fast  and slow components of learning observed in perceptual learning tasks."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/c8cbd669cfb2f016574e9d147092b5bb-Abstract.html,Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks,"Richard H. R. Hahnloser, H. Sebastian Seung","Ascribing computational principles to neural feedback circuits is an  important problem in theoretical neuroscience.  We  study symmet(cid:173) ric  threshold-linear  networks  and  derive  stability  results  that  go  beyond the insights that  can  be  gained  from  Lyapunov theory or  energy functions.  By applying linear analysis to subnetworks com(cid:173) posed of coactive  neurons,  we  determine the  stability of potential  steady states.  We find that stability depends on two types of eigen(cid:173) modes.  One  type  determines  global  stability  and  the  other  type  determines whether or not multistability is possible.  We  can prove  the  equivalence  of  our  stability  criteria  with  criteria  taken  from  quadratic  programming.  Also,  we  show  that  there  are  permitted  sets  of neurons that  can be coactive at  a  steady state and forbid(cid:173) den sets that cannot.  Permitted sets are clustered in the sense that  subsets of permitted sets are permitted and supersets of forbidden  sets  are forbidden.  By  viewing  permitted sets as memories  stored  in the synaptic connections,  we  can provide a formulation of long(cid:173) term memory that is more general than the traditional perspective  of fixed  point attractor networks. 
A Lyapunov-function can be used to prove that a given set of differential equations is  convergent.  For  example,  if a  neural network possesses a  Lyapunov-function,  then  for  almost  any  initial  condition,  the  outputs  of the  neurons  converge  to  a  stable  steady  state.  In  the  past,  this  stability-property  was  used  to  construct  attractor  networks  that  associatively  recall  memorized  patterns.  Lyapunov  theory  applies  mainly to symmetric networks in which neurons have monotonic activation functions  [1,  2].  Here  we  show that the restriction of activation functions  to threshold-linear  ones  is  not  a  mere  limitation,  but  can  yield  new  insights  into  the  computational  behavior of recurrent networks  (for  completeness, see  also  [3]). 
We present three main theorems about the neural responses to constant inputs.  The  first theorem provides necessary and sufficient conditions on the synaptic weight ma(cid:173) trix for  the existence of a  globally  asymptotically stable set  of fixed  points.  These  conditions can  be expressed in terms of copositivity,  a  concept from  quadratic pro(cid:173) gramming and linear complementarity theory.  Alternatively, they can be expressed  in terms of certain eigenvalues and eigenvectors of submatrices of the synaptic weight  matrix, making a connection to linear systems theory.  The theorem guarantees that 
the network will produce a steady state response to any constant input.  We  regard  this  response as the computational output of the network,  and its characterization  is the topic of the second  and third theorems. 
In the second theorem, we introduce the idea of permitted and forbidden sets.  Under  certain  conditions  on  the  synaptic  weight  matrix,  we  show  that  there  exist  sets  of neurons that  are  ""forbidden""  by the recurrent  synaptic  connections from  being  coactivated at a stable steady state, no matter what input is applied.  Other sets are  ""permitted,""  in  the  sense  that they  can be coactivated for  some  input.  The same  conditions  on  the  synaptic  weight  matrix  also  lead  to  conditional  multistability,  meaning that there exists an input for  which there is  more than one  stable steady  state.  In other words, forbidden  sets and conditional multistability are inseparable  concepts. 
The existence of permitted and forbidden sets suggests a new way of thinking about  memory in neural networks.  When an input is applied, the network must select a set  of active neurons,  and this selection is  constrained to be one of the permitted sets.  Therefore  the permitted  sets  can  be  regarded as  memories  stored  in  the  synaptic  connections. 
Our  third  theorem  states  that  there  are  constraints  on  the  groups  of  permitted  and  forbidden  sets  that  can  be  stored  by  a  network.  No  matter  which  learning  algorithm is  used  to  store memories,  active  neurons  cannot  arbitrarily  be  divided  into  permitted  and  forbidden  sets,  because  subsets  of permitted  sets  have  to  be  permitted and supersets of forbidden  sets have to be forbidden. 
1  Basic definitions 
Our theory is  applicable to the network dynamics 

dx· - '  + x · =  b· + ""W· ·x ·  1  dt"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/c91591a8d461c2869b9f535ded3e213e-Abstract.html,Competition and Arbors in Ocular Dominance,Peter Dayan,"Hebbian  and  competitive Hebbian  algorithms  are  almost ubiquitous  in  modeling pattern formation in cortical development.  We  analyse in  the(cid:173) oretical  detail  a  particular  model  (adapted  from  Piepenbrock  &  Ober(cid:173) mayer, 1999) for the development of Id stripe-like patterns, which places  competitive and interactive cortical influences, and free and restricted ini(cid:173) tial arborisation onto a common footing."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/ca460332316d6da84b08b9bcf39b687b-Abstract.html,Learning Switching Linear Models of Human Motion,"Vladimir Pavlovic, James M. Rehg, John MacCormick","The human  figure  exhibits  complex and  rich  dynamic  behavior that is  both nonlinear and  time-varying.  Effective models  of human dynamics  can be learned from motion capture data using switching linear dynamic  system  (SLDS)  models.  We  present results  for human  motion  synthe(cid:173) sis, classification, and visual tracking using learned SLDS models.  Since  exact inference in SLDS is intractable, we present three approximate in(cid:173) ference algorithms and compare their performance.  In particular, a new  variational  inference algorithm  is  obtained by  casting the  SLDS  model  as  a Dynamic  Bayesian  Network.  Classification experiments  show  the  superiority of SLDS over conventional HMM's for our problem domain."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/cb79f8fa58b91d3af6c9c991f63962d3-Abstract.html,New Approaches Towards Robust and Adaptive Speech Recognition,"Hervé Bourlard, Samy Bengio, Katrin Weber","In this paper, we discuss some new research directions in automatic  speech recognition  (ASR),  and which  somewhat  deviate  from  the  usual approaches.  More  specifically,  we  will  motivate  and  briefly  describe  new  approaches  based  on  multi-stream  and  multi/band  ASR. These approaches extend the standard hidden Markov model  (HMM)  based approach by assuming that the different (frequency)  channels representing the speech signal are processed by different  (independent)  ""experts"", each expert focusing  on a  different  char(cid:173) acteristic of the signal, and that the different stream likelihoods (or  posteriors) are combined at some (temporal) stage to yield a global  recognition output.  As  a further  extension to multi-stream ASR,  we  will  finally  introduce  a  new  approach,  referred  to  as  HMM2,  where the HMM  emission probabilities are estimated via state spe(cid:173) cific feature based HMMs responsible for merging the stream infor(cid:173) mation and modeling their possible correlation. 
1  Multi-Channel Processing in ASR 
Current automatic speech recognition systems are based on (context-dependent or  context-independent)  phone  models  described  in  terms  of  a  sequence  of  hidden  Markov model  (HMM)  states, where each HMM  state is assumed to be character(cid:173) ized  by  a  stationary probability  density  function.  Furthermore, time  correlation,  and consequently  the  dynamic  of the  signal,  inside  each  HMM  state is  also  usu(cid:173) ally  disregarded  (although the use  of temporal  delta and  delta-delta features  can  capture some  of this  correlation).  Consequently,  only medium-term  dependencies  are captured via the topology of the HMM  model,  while  short-term and long-term  dependencies are usually very poorly modeled.  Ideally, we  want to design a partic(cid:173) ular HMM  able to accommodate multiple time-scale characteristics so that we  can  capture phonetic properties, as well as syllable structures and {long term) invariants  that are more robust to noise.  It is,  however,  clear that those  different  time-scale  features  will  also  exhibit  different  levels  of stationarity  and  will  require  different  HMM  topologies to capture their dynamics.  There are many potential advantages to such a multi-stream approach, including: 

The  definition of a  principled way to merge different  temporal knowledge  sources such as  acoustic and visual inputs, even if the temporal sequences  are  not  synchronous  and  do  not  have  the  same  data  rate  - see  [13]  for  further discussion about this. 
Possibility to incorporate multiple time resolutions  (as  part of a  structure 

with multiple unit lengths,  such as phon(l  and syllable). 

As  a  particular  case  of multi-stream  processing,  mufti-band  ASR  [2,  5],  involving the independent processing and combination of partial frequency  bands,  have many potential advantages briefly discussed below. 

In the following,  we  will not discuss the underlying algorithms  (more or less  ""com(cid:173) plex""  variants of Viterbi decoding), nor detailed experimental results  (see,  e.g.,  [4]  for  recent results).  Instead, we  will mainly focus  on the combination strategy and  discuss different variants arounds the same formalism. 
2  Multiband-based  ASR 
2.1  General Formalism 
As  a  particular case  of the  multi-stream paradigm, we  have been investigating an  ASR approach based on independent processing and combination of frequency sub(cid:173) bands.  The general idea, as illustrated in Fig. 1, is to split the whole frequency band  (represented in terms of critical bands)  into a few  subbands on which different rec(cid:173) ognizers are independently applied.  The resulting probabilities are then combined  for  recognition later in the process at some segmental level.  Starting from  critical  bands, acoustic processing is now performed independently for each frequency band,  yielding K  input streams, each being associated with a particular frequency band."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/cd14821dab219ea06e2fd1a2df2e3582-Abstract.html,"Place Cells and Spatial Navigation Based on 2D Visual Feature Extraction, Path Integration, and Reinforcement Learning","Angelo Arleo, Fabrizio Smeraldi, Stéphane Hug, Wulfram Gerstner","We  model  hippocampal place cells  and head-direction cells by combin(cid:173) ing allothetic  (visual) and idiothetic (proprioceptive) stimuli.  Visual in(cid:173) put, provided by a video camera on a miniature robot, is preprocessed by  a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsu(cid:173) pervised Hebbian learning is employed to  incrementally build a popula(cid:173) tion of localized overlapping place fields.  Place cells serve as basis func(cid:173) tions for reinforcement learning.  Experimental results for goal-oriented  navigation of a mobile robot are presented."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/cd63a3eec3319fd9c84c942a08316e00-Abstract.html,Kernel-Based Reinforcement Learning in Average-Cost Problems: An Application to Optimal Portfolio Choice,"Dirk Ormoneit, Peter W. Glynn","Many  approaches  to  reinforcement  learning  combine  neural  net(cid:173) works  or other  parametric function  approximators with  a  form  of  temporal-difference  learning  to  estimate  the  value  function  of  a  Markov  Decision  Process.  A  significant disadvantage of those  pro(cid:173) cedures  is that the resulting learning algorithms are frequently un(cid:173) stable.  In  this  work,  we  present  a  new,  kernel-based  approach  to  reinforcement learning which overcomes this difficulty and provably  converges  to a  unique solution.  By contrast to existing algorithms,  our  method  can  also  be  shown  to  be  consistent  in  the  sense  that  its  costs  converge  to  the  optimal costs  asymptotically.  Our  focus  is  on learning in  an average-cost  framework  and on a  practical  ap(cid:173) plication to  the optimal portfolio choice problem."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/d072677d210ac4c03ba046120f0802ec-Abstract.html,A New Approximate Maximal Margin Classification Algorithm,Claudio Gentile,"A new  incremental learning algorithm is  described which approximates  the maximal margin hyperplane w.r.t.  norm p  ~ 2 for  a set of linearly  separable data.  Our algorithm, called ALMAp  (Approximate Large Mar- gin  algorithm  w.r.t.  norm p),  takes  0  ((P~21;;2) corrections  to  sepa(cid:173) rate the  data with p-norm margin larger than  (1  - 0:) ,,(,  where,,(  is  the  p-norm margin of the  data  and  X  is  a bound on  the p-norm of the  in(cid:173) stances.  ALMAp  avoids quadratic  (or higher-order) programming meth(cid:173) ods.  It is very easy to implement and is as fast as on-line algorithms, such  as  Rosenblatt's perceptron.  We  report on some experiments comparing  ALMAp  to  two  incremental  algorithms:  Perceptron  and  Li  and  Long's  ROMMA.  Our algorithm seems  to  perform quite better than  both.  The  accuracy levels achieved by ALMAp are slightly inferior to those obtained  by Support vector Machines (SVMs).  On the other hand, ALMAp is quite  faster and easier to implement than standard SVMs training algorithms."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/d1e946f4e67db4b362ad23818a6fb78a-Abstract.html,Ensemble Learning and Linear Response Theory for ICA,"Pedro A. d. F. R. Højen-Sørensen, Ole Winther, Lars Kai Hansen","We  propose a general Bayesian framework for performing independent  component analysis  (leA) which  relies  on  ensemble  learning  and  lin(cid:173) ear response theory  known from  statistical physics.  We  apply it to both  discrete and continuous sources. For the continuous source the underde(cid:173) termined (overcomplete) case is  studied.  The naive mean-field approach  fails in this case whereas linear response theory-which gives an improved  estimate  of covariances-is  very  efficient.  The  examples  given  are  for  sources without temporal correlations. However, this derivation can eas(cid:173) ily  be  extended  to  treat  temporal  correlations.  Finally,  the  framework  offers a simple way  of generating new  leA algorithms without needing  to define the prior distribution of the sources explicitly."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/d25414405eb37dae1c14b18d6a2cac34-Abstract.html,Regularization with Dot-Product Kernels,"Alex J. Smola, Zoltán L. Óvári, Robert C. Williamson","In  this  paper  we  give  necessary  and  sufficient  conditions  under  which  kernels  of dot  product  type  k(x, y)  =  k(x . y)  satisfy Mer(cid:173) cer's  condition  and  thus  may  be  used  in  Support  Vector  Ma(cid:173) chines  (SVM),  Regularization  Networks  (RN)  or  Gaussian  Pro(cid:173) cesses  (GP).  In  particular,  we  show  that  if the  kernel  is  analytic  (i.e.  can be expanded in a  Taylor series),  all expansion coefficients  have to be nonnegative.  We  give an explicit functional form for  the  feature map by calculating its eigenfunctions and eigenvalues."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/d305281faf947ca7acade9ad5c8c818c-Abstract.html,From Margin to Sparsity,"Thore Graepel, Ralf Herbrich, Robert C. Williamson","We  present  an  improvement  of Novikoff's  perceptron convergence  theorem.  Reinterpreting this mistake bound as a margin dependent  sparsity guarantee allows us to give a PAC-style generalisation er(cid:173) ror bound for the classifier learned by the perceptron learning algo(cid:173) rithm.  The bound value crucially depends on the margin a support  vector machine would achieve on the same data set using the same  kernel.  Ironically, the bound yields better guarantees than are cur(cid:173) rently available for  the support vector solution itself."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/d4b2aeb2453bdadaa45cbe9882ffefcf-Abstract.html,On a Connection between Kernel PCA and Metric Multidimensional Scaling,Christopher K. I. Williams,"In this paper we  show that the kernel peA algorithm of Sch6lkopf  et al  (1998) can be interpreted as a form of metric multidimensional  scaling  (MDS)  when  the kernel function  k(x, y)  is  isotropic,  i.e.  it  depends  only  on  Ilx - yll.  This leads to a  metric  MDS  algorithm  where the desired  configuration of points is  found  via the solution  of an eigenproblem rather than through the iterative optimization  of the  stress  objective function.  The  question  of kernel  choice  is  also  discussed."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/d523773c6b194f37b938d340d5d02232-Abstract.html,One Microphone Source Separation,Sam T. Roweis,"Source separation,  or computational auditory  scene analysis , attempts to extract  individual  acoustic objects from  input which contains  a mixture of sounds  from  different  sources,  altered  by  the  acoustic  environment.  Unmixing  algorithms  such  as  lCA  and  its  extensions  recover sources  by  reweighting  multiple  obser(cid:173) vation sequences, and thus cannot operate when only  a single observation  signal  is  available.  I present a technique  called  refiltering  which  recovers  sources  by  a nonstationary  reweighting  (""masking"")  of frequency  sub-bands  from  a  single  recording,  and  argue for  the  application  of statistical algorithms to  learning this  masking  function .  I  present results  of a  simple  factorial  HMM  system  which  learns on recordings of single speakers and can then separate mixtures using only  one observation signal by  computing the masking function and then refiltering. 
1  Learning from data in computational auditory scene analysis  Imagine listening to many pianos being played simultaneously. If each pianist were striking  keys randomly it would be very  difficult to  tell  which note came from which piano.  But  if each  were  playing  a  coherent  song,  separation  would  be  much  easier  because  of the  structure of music.  Now  imagine teaching a computer to  do the separation by  showing it  many musical scores as ""training data"".  Typical auditory perceptual input contains a mix(cid:173) ture of sounds from different sources, altered by the acoustic environment.  Any biological  or  artificial  hearing  system  must  extract individual  acoustic  objects  or  streams  in  order  to  do  successful localization,  denoising and recognition.  Bregman [1]  called this  process  auditory scene analysis in analogy to vision.  Source separation, or computational auditory  scene analysis  (CASA) is  the  practical realization  of this  problem via computer analysis  of microphone recordings and  is  very  similar to the musical task described above.  It has  been investigated by research groups with different emphases. The CASA community have  focused on both multiple and single microphone source separation problems under highly  realistic  acoustic  conditions,  but  have  used  almost  exclusively  hand  designed  systems  which include substantial knowledge of the human auditory system and its psychophysical  characteristics  (e.g. [2,3]).  Unfortunately,  it is  difficult  to  incorporate large  amounts  of  detailed  statistical  knowledge  about  the  problem  into  such  an  approach.  On  the  other  hand, machine learning researchers, especially those working on independent components  analysis  (lCA) and related algorithms, have focused  on the case  of multiple microphones  in  simplified mixing environments and have used powerful ""blind"" statistical  techniques.  These  ""unmixing""  algorithms  (even  those  which  attempt  to  recover  more  sources  than  signals) cannot operate on single recordings. Furthermore, since they often depend only on  the joint amplitude histogram of the observations they can be very sensitive to the details of  filtering and reverberation in the environment. The goal of this paper is to bring together the  robust representations of CAS A and methods which  learn from  data to  solve a restricted  version  of the  source  separation problem - isolating  acoustic  objects  from  only  a single  microphone recording. 
2  Refiltering vs.  unmixing  Unmixing algorithms reweight multiple simultaneous recordings mk (t)  (generically called  microphones) to form a new source object s(t):"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/d54e99a6c03704e95e6965532dec148b-Abstract.html,Interactive Parts Model: An Application to Recognition of On-line Cursive Script,"Predrag Neskovic, Philip C. Davis, Leon N. Cooper","In  this  work,  we  introduce  an  Interactive  Parts  (IP)  model as  an  alternative  to  Hidden  Markov  Models  (HMMs).  We  tested  both  models  on  a  database of on-line  cursive  script.  We  show  that  im(cid:173) plementations of HMMs  and the IP model, in which  all letters  are  assumed  to have  the same average width, give  comparable results.  However , in contrast to  HMMs,  the  IP model can handle duration  modeling without  an increase  in  computational complexity."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/d757719ed7c2b66dd17dcee2a3cb29f4-Abstract.html,A New Model of Spatial Representation in Multimodal Brain Areas,"Sophie Denève, Jean-René Duhamel, Alexandre Pouget","Most  models  of spatial representations in  the cortex  assume  cells  with limited receptive fields that are defined in a particular egocen(cid:173) tric frame  of reference.  However,  cells  outside  of primary sensory  cortex  are  either  gain  modulated  by  postural  input  or  partially  shifting.  We  show  that  solving  classical  spatial  tasks,  like  sen(cid:173) sory prediction, multi-sensory integration, sensory-motor transfor(cid:173) mation and motor control requires more complicated intermediate  representations  that  are  not  invariant  in  one  frame  of  reference.  We  present  an  iterative  basis  function  map  that  performs  these  spatial tasks optimally with gain modulated and partially shifting  units,  and tests it against  neurophysiological and neuropsycholog(cid:173) ical data. 
In  order  to  perform  an  action  directed  toward  an  object,  it  is  necessary  to  have  a  representation  of  its  spatial  location.  The  brain  must  be  able  to  use  spatial  cues coming from  different modalities (e.g.  vision, audition, touch, proprioception),  combine  them  to  infer  the  position  of the  object,  and  compute  the  appropriate  movement. 
These  cues  are  in  different  frames  of reference  corresponding  to  different  sensory  or  motor  modalities.  Visual  inputs  are  primarily  encoded  in  retinotopic  maps,  auditory  inputs  are  encoded  in  head  centered  maps  and  tactile  cues  are  encoded  in  skin-centered maps.  Going from  one frame  of reference to the other might  seem  easy.  For  example,  the  head-centered  position  of an  object  can  be  approximated  by the sum of its retinotopic position and the eye position.  However,  positions are  represented by population codes in the brain, and computing a  head-centered map  from  a  retinotopic map  is  a  more  complex computation than the underlying sum.  Moreover,  as  we  get  closer  to  sensory-motor  areas  it  seems  reasonable  to  assume"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/d7657583058394c828ee150fada65345-Abstract.html,Feature Correspondence: A Markov Chain Monte Carlo Approach,"Frank Dellaert, Steven M. Seitz, Sebastian Thrun, Charles E. Thorpe","When  trying  to  recover  3D  structure  from  a  set  of  images,  the  most difficult  problem is  establishing  the correspondence  between  the measurements.  Most existing  approaches  assume that features  can be tracked across frames,  whereas methods that exploit rigidity  constraints to facilitate matching do so only under restricted  cam(cid:173) era  motion.  In  this  paper  we  propose  a  Bayesian  approach  that  avoids  the  brittleness  associated  with  singling out  one  ""best""  cor(cid:173) respondence,  and instead consider the distribution over all possible  correspondences.  We  treat  both  a  fully  Bayesian  approach  that  yields  a  posterior  distribution,  and  a  MAP  approach  that  makes  use of EM  to maximize this posterior.  We show how  Markov chain  Monte  Carlo methods can  be  used  to implement these  techniques  in practice,  and present  experimental results on real  data."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/d7fd118e6f226a71b5f1ffe10efd0a78-Abstract.html,Stagewise Processing in Error-correcting Codes and Image Restoration,"K. Y. Michael Wong, Hidetoshi Nishimori","We  introduce  stagewise  processing  in  error-correcting  codes  and  image restoration, by extracting information from the former stage  and  using  it  selectively  to  improve  the  performance  of the  latter  one.  Both  mean-field  analysis  using  the  cavity  method  and  sim(cid:173) ulations  show  that  it  has  the  advantage  of  being  robust  against  uncertainties in hyperparameter estimation."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/dc40b7120e77741d191c0d2b82cea7be-Abstract.html,The Kernel Gibbs Sampler,"Thore Graepel, Ralf Herbrich","We  present an algorithm that samples the hypothesis space of ker(cid:173) nel classifiers.  Given a uniform prior over normalised weight vectors  and  a  likelihood  based on  a  model  of label  noise leads to a  piece(cid:173) wise  constant  posterior that  can  be  sampled  by  the  kernel  Gibbs  sampler (KGS).  The KGS is a Markov Chain Monte Carlo method  that  chooses  a  random  direction  in  parameter space  and  samples  from the resulting piecewise constant density along the line chosen.  The KGS  can be used  as  an analytical tool for  the exploration of  Bayesian transduction, Bayes point machines,  active learning, and  evidence-based model selection on small data sets that are contam(cid:173) inated with  label noise.  For a  simple toy example  we  demonstrate  experimentally how  a Bayes point machine based on the KGS  out(cid:173) performs  an  SVM  that  is  incapable  of  taking  into  account  label  noise."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/dc5d637ed5e62c36ecb73b654b05ba2a-Abstract.html,Learning Sparse Image Codes using a Wavelet Pyramid Architecture,"Bruno A. Olshausen, Phil Sallee, Michael S. Lewicki","We  show  how  a  wavelet  basis  may  be  adapted  to  best  represent  natural images  in  terms  of sparse coefficients.  The  wavelet  basis,  which  may  be  either  complete  or  overcomplete,  is  specified  by  a  small number of spatial functions  which  are  repeated across space  and combined in a  recursive fashion  so  as  to be self-similar across  scale.  These functions are adapted to minimize the estimated code  length under a model that assumes images are composed of a linear  superposition of sparse,  independent  components.  When  adapted  to natural images, the wavelet  bases take on  different  orientations  and they evenly tile the orientation domain, in stark contrast to the  standard,  non-oriented  wavelet  bases  used  in  image  compression.  When  the  basis  set  is  allowed  to  be  overcomplete,  it  also  yields  higher coding efficiency than standard wavelet  bases."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/dea9ddb25cbf2352cf4dec30222a02a5-Abstract.html,Weak Learners and Improved Rates of Convergence in Boosting,"Shie Mannor, Ron Meir","The  problem  of  constructing  weak  classifiers  for  boosting  algo(cid:173) rithms is  studied.  We  present an algorithm that produces a  linear  classifier that is guaranteed to achieve an error better than random  guessing for  any distribution on the data.  While this weak  learner  is not useful for  learning in general, we show that under reasonable  conditions on the distribution it yields an effective weak learner for  one-dimensional  problems.  Preliminary  simulations  suggest  that  similar  behavior  can  be  expected  in  higher  dimensions,  a  result  which  is  corroborated by  some  recent  theoretical  bounds.  Addi(cid:173) tionally, we  provide improved convergence rate bounds for the gen(cid:173) eralization error in situations where the empirical error can be made  small,  which  is  exactly the  situation  that  occurs  if weak  learners  with guaranteed performance that is better than random guessing  can be established."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/dfce06801e1a85d6d06f1fdd4475dacd-Abstract.html,Keeping Flexible Active Contours on Track using Metropolis Updates,"Trausti T. Kristjansson, Brendan J. Frey","Condensation, a form of likelihood-weighted particle filtering,  has been  successfully used to infer the shapes of highly constrained ""active"" con(cid:173) tours in video sequences.  However, when the contours are highly flexible  (e.g.  for tracking fingers of a hand), a computationally burdensome num(cid:173) ber of particles is needed to successfully approximate the contour distri(cid:173) bution.  We  show how  the Metropolis algorithm can be used to update a  particle set representing a distribution  over contours  at each  frame in a  video sequence.  We compare this method to condensation using a video  sequence that requires highly  flexible  contours,  and  show  that the  new  algorithm performs dramatically better that the condensation algorithm.  We  discuss  the  incorporation of this  method  into  the  ""active  contour""  framework where a shape-subspace is  used constrain shape variation."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/e06f967fb0d355592be4e7674fa31d26-Abstract.html,Data Clustering by Markovian Relaxation and the Information Bottleneck Method,"Naftali Tishby, Noam Slonim","We introduce a new,  non-parametric and principled, distance based  clustering  method.  This  method  combines  a  pairwise  based  ap(cid:173) proach  with  a  vector-quantization  method which  provide  a  mean(cid:173) ingful  interpretation  to  the  resulting  clusters.  The  idea  is  based  on  turning  the  distance  matrix  into  a  Markov  process  and  then  examine the decay  of mutual-information during  the relaxation of  this  process.  The  clusters  emerge  as  quasi-stable  structures  dur(cid:173) ing  this  relaxation,  and  then  are  extracted  using  the  information  bottleneck method.  These  clusters  capture  the  information about  the  initial point  of the  relaxation  in  the  most  effective  way.  The  method can cluster data with no geometric or other bias and makes  no  assumption about the underlying distribution."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/e0ab531ec312161511493b002f9be2ee-Abstract.html,Balancing Multiple Sources of Reward in Reinforcement Learning,Christian R. Shelton,"For many problems which  would be natural for reinforcement learning,  the reward signal is not a single scalar value but has multiple scalar com(cid:173) ponents.  Examples of such problems include agents  with multiple goals  and agents  with  multiple users.  Creating a single reward value by com(cid:173) bining  the  multiple  components  can throwaway  vital  information and  can lead  to incorrect solutions.  We describe the multiple reward source  problem  and  discuss  the  problems  with  applying  traditional  reinforce(cid:173) ment learning.  We  then present an  new  algorithm for finding a solution  and results on simulated environments."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/e1314fc026da60d837353d20aefaf054-Abstract.html,Temporally Dependent Plasticity: An Information Theoretic Account,"Gal Chechik, Naftali Tishby","The paradigm of Hebbian learning has recently received a novel in(cid:173) terpretation with the discovery of synaptic plasticity that depends  on the relative timing of pre and post  synaptic spikes.  This paper  derives a temporally dependent learning rule from the basic princi(cid:173) ple of mutual information maximization and studies its relation to  the experimentally observed  plasticity.  We  find  that  a  supervised  spike-dependent learning rule sharing similar structure with the ex(cid:173) perimentally observed plasticity increases mutual information to a  stable near  optimal  level.  Moreover,  the  analysis  reveals  how  the  temporal structure of time-dependent learning rules is  determined  by the temporal filter  applied by neurons over their inputs.  These  results suggest experimental prediction as to the dependency of the  learning rule on neuronal biophysical parameters"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/e5b294b70c9647dcf804d7baa1903918-Abstract.html,Analysis of Bit Error Probability of Direct-Sequence CDMA Multiuser Demodulators,Toshiyuki Tanaka,"We analyze the bit error probability of multiuser demodulators for direct(cid:173) sequence binary phase-shift-keying (DSIBPSK) CDMA channel with ad(cid:173) ditive gaussian noise. The problem of multiuser demodulation is cast  into the finite-temperature decoding problem, and replica analysis is ap(cid:173) plied to evaluate the performance of the resulting MPM (Marginal Pos(cid:173) terior Mode) demodulators, which include the optimal demodulator and  the MAP demodulator as special cases. An approximate implementa(cid:173) tion of demodulators is proposed using analog-valued Hopfield model  as a naive mean-field approximation to the MPM demodulators, and its  performance is also evaluated by the replica analysis. Results of the per(cid:173) formance evaluation shows effectiveness of the optimal demodulator and  the mean-field demodulator compared with the conventional one, espe(cid:173) cially in the cases of small information bit rate and low noise level."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/e74c0d42b4433905293aab661fcf8ddb-Abstract.html,A Variational Mean-Field Theory for Sigmoidal Belief Networks,"Chiranjib Bhattacharyya, S. Sathiya Keerthi",A variational derivation of Plefka's mean-field theory is  presented.  This  theory  is  then  applied  to  sigmoidal  belief networks  with  the  aid of further approximations.  Empirical evaluation on small scale  networks  show  that  the  proposed  approximations  are  quite  com(cid:173) petitive.
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/e8dfff4676a47048d6f0c4ef899593dd-Abstract.html,Robust Reinforcement Learning,"Jun Morimoto, Kenji Doya","This paper proposes a  new  reinforcement  learning  (RL)  paradigm  that explicitly takes into account input disturbance as well as mod(cid:173) eling errors.  The use of environmental models  in  RL  is  quite pop(cid:173) ular  for  both  off-line  learning  by  simulations  and  for  on-line  ac(cid:173) tion planning.  However, the difference between the model and the  real environment can lead to unpredictable, often unwanted results.  Based on the theory of H oocontrol, we consider a  differential game  in  which  a  'disturbing'  agent  (disturber)  tries  to  make  the  worst  possible  disturbance  while  a  'control'  agent  (actor)  tries  to  make  the best control input.  The problem is formulated as finding a min(cid:173) max solution of a  value function that takes into account the norm  of the output deviation and the norm of the disturbance.  We derive  on-line  learning  algorithms  for  estimating  the  value  function  and  for  calculating the worst disturbance and the best  control in refer(cid:173) ence to the value function.  We  tested the paradigm, which we call  ""Robust  Reinforcement  Learning  (RRL),""  in  the task  of inverted  pendulum.  In  the  linear  domain,  the  policy  and  the  value  func(cid:173) tion learned by the on-line algorithms coincided with those derived  analytically  by the linear  H ootheory.  For a  fully  nonlinear  swing(cid:173) up task, the control by RRL  achieved  robust  performance against  changes in the pendulum weight  and friction  while a  standard RL  control could not  deal with such environmental changes."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/eb1e78328c46506b46a4ac4a1e378b91-Abstract.html,Explaining Away in Weight Space,"Peter Dayan, Sham Kakade","Explaining  away  has  mostly  been  considered  in  terms  of inference  of  states  in  belief networks.  We  show  how  it can  also  arise in a Bayesian  context in  inference about  the  weights  governing relationships  such  as  those  between stimuli  and reinforcers in  conditioning experiments such  as  bacA,'Ward blocking.  We  show how  explaining away  in  weight  space  can be accounted for using an  extension of a Kalman filter model; pro(cid:173) vide a new  approximate way  of looking at the Kalman gain  matrix as  a  whitener for  the correlation  matrix  of the  observation process;  suggest  a network implementation of this  whitener using an  architecture due to  Goodall; and show that the resulting model exhibits backward blocking."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/ecd62de20ea67e1c2d933d311b08178a-Abstract.html,Improved Output Coding for Classification Using Continuous Relaxation,"Koby Crammer, Yoram Singer","Output coding is  a general  method for  solving  multiclass  problems  by  reducing them to  multiple binary classification problems.  Previous re(cid:173) search on output coding has employed, almost solely, predefined discrete  codes. We describe an algorithm that improves the performance of output  codes by relaxing them  to  continuous codes.  The relaxation procedure  is  cast  as  an  optimization  problem and  is  reminiscent of the  quadratic  program for support vector machines.  We describe experiments with the  proposed algorithm, comparing it to  standard discrete output codes.  The  experimental results indicate that continuous relaxations of output codes  often improve the generalization performance, especially for short codes."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/ed519dacc89b2bead3f453b0b05a4a8b-Abstract.html,Learning Curves for Gaussian Processes Regression: A Framework for Good Approximations,"Dörthe Malzahn, Manfred Opper","Based  on  a  statistical mechanics  approach,  we  develop  a  method  for approximately computing average case learning curves for Gaus(cid:173) sian  process  regression  models.  The  approximation  works  well  in  the large  sample size  limit  and for  arbitrary dimensionality  of the  input space.  We explain how the approximation can be systemati(cid:173) cally improved and argue that similar techniques can be applied to  general likelihood models."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f0bbac6fa079f1e00b2c14c1d3c6ccf0-Abstract.html,Sequentially Fitting ``Inclusive'' Trees for Inference in Noisy-OR Networks,"Brendan J. Frey, Relu Patrascu, Tommi Jaakkola, Jodi Moran","An  important  class  of problems  can  be  cast  as inference  in noisy(cid:173) OR Bayesian networks,  where  the binary state of each variable is  a  logical  OR of noisy  versions  of the  states of the  variable's  par(cid:173) ents.  For example, in medical diagnosis, the presence of a symptom  can be expressed as a noisy-OR of the diseases that may cause the  symptom  - on  some  occasions,  a  disease  may fail  to  activate  the  symptom.  Inference  in  richly-connected noisy-OR networks is  in(cid:173) tractable,  but  approximate methods  (e .g.,  variational techniques)  are  showing  increasing  promise  as  practical  solutions.  One  prob(cid:173) lem  with  most  approximations  is  that  they  tend  to  concentrate  on  a  relatively  small  number  of  modes  in  the  true  posterior,  ig(cid:173) noring  other plausible  configurations of the  hidden  variables.  We  introduce a  new  sequential variational method for  bipartite noisy(cid:173) OR networks, that favors  including  all modes of the true posterior  and models the  posterior distribution  as  a  tree.  We  compare this  method with other approximations using an ensemble of networks  with network statistics that are comparable to the QMR-DT med(cid:173) ical diagnostic network. 
Inclusive variational approximations 
1  Approximate algorithms for probabilistic inference are gaining in popularity and are  now even being incorporated into VLSI hardware (T. Richardson, personal commu(cid:173) nication).  Approximate  methods include  variational techniques  (Ghahramani  and  Jordan 1997; Saul et al.  1996; Frey and Hinton 1999; Jordan et al.  1999), local prob(cid:173) ability propagation (Gallager 1963; Pearl 1988; Frey 1998; MacKay 1999a; Freeman  and Weiss 2001) and Markov chain Monte Carlo (Neal 1993; MacKay 1999b).  Many  algorithms have been proposed in each of these classes. 
One  problem that  most  of the  above  algorithms  suffer  from  is  a  tendency to  con(cid:173) centrate on a  relatively  small  number of modes  of the  target  distribution  (the  dis(cid:173) tribution  being  approximated).  In  the  case  of medical  diagnosis,  different  modes  correspond to different  explanations of the symptoms.  Markov  chain  Monte  Carlo  methods are usually guaranteed to eventually sample from  all  the modes,  but this  may take an extremely long time, even when tempered transitions  (Neal  1996)  are 
(a)"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f0fcf351df4eb6786e9bb6fc4e2dee02-Abstract.html,Whence Sparseness?,Carl van Vreeswijk,"It has been shown that the receptive fields of simple cells in VI can be ex(cid:173) plained by assuming optimal encoding, provided that an extra constraint  of sparseness  is  added.  This  finding  suggests that there is  a reason, in(cid:173) dependent of optimal representation, for sparseness.  However this work  used  an  ad  hoc  model for the  noise.  Here I show that,  if a biologically  more plausible noise model, describing neurons as  Poisson processes, is  used  sparseness does  not have to  be  added as  a constraint.  Thus I con(cid:173) clude that sparseness is not a feature that evolution has striven for,  but is  simply the result of the evolutionary pressure towards an  optimal repre(cid:173) sentation."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html,Periodic Component Analysis: An Eigenvalue Method for Representing Periodic Structure in Speech,"Lawrence K. Saul, Jont B. Allen","An  eigenvalue  method  is  developed  for  analyzing  periodic  structure in  speech.  Signals are analyzed by  a matrix diagonalization reminiscent of  methods  for principal component analysis  (PCA)  and  independent com(cid:173) ponent analysis (ICA).  Our method-called periodic component analysis  (1l""CA)-uses  constructive interference to  enhance periodic  components  of the frequency  spectrum and  destructive interference  to  cancel  noise.  The front end emulates important aspects of auditory processing, such as  cochlear filtering, nonlinear compression, and insensitivity to phase, with  the  aim  of approaching the robustness of human  listeners.  The  method  avoids the inefficiencies of autocorrelation at the pitch period:  it does not  require  long  delay  lines,  and  it correlates  signals  at  a clock rate  on  the  order  of the  actual pitch,  as  opposed  to  the  original  sampling rate.  We  derive its cost function and present some experimental results."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f26dab9bf6a137c3b6782e562794c2f2-Abstract.html,Partially Observable SDE Models for Image Sequence Recognition Tasks,"Javier R. Movellan, Paul Mineiro, Ruth J. Williams","This paper explores a framework for recognition of image sequences  using partially observable stochastic differential equation (SDE)  models. Monte-Carlo importance sampling techniques are used for  efficient estimation of sequence likelihoods and sequence likelihood  gradients. Once the network dynamics are learned, we apply the  SDE models to sequence recognition tasks in a manner similar to  the way Hidden Markov models (HMMs) are commonly applied.  The potential advantage of SDEs over HMMS is the use of contin(cid:173) uous state dynamics. We present encouraging results for a video  sequence recognition task in which SDE models provided excellent  performance when compared to hidden Markov models."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f2d887e01a80e813d9080038decbbabb-Abstract.html,Combining ICA and Top-Down Attention for Robust Speech Recognition,"Un-Min Bae, Soo-Young Lee","We  present  an  algorithm  which  compensates  for  the  mismatches  between characteristics of real-world problems and assumptions of  independent  component analysis algorithm.  To  provide additional  information  to  the  ICA  network,  we  incorporate top-down  selec(cid:173) tive  attention.  An  MLP  classifier is  added to the separated signal  channel  and  the  error  of  the  classifier  is  backpropagated  to  the  ICA  network.  This backpropagation process results  in  estimation  of expected ICA  output  signal for  the top-down  attention.  Then,  the unmixing matrix is  retrained according to a  new  cost function  representing the backpropagated error as well  as independence.  It  modifies the density of recovered signals to the density appropriate  for  classification.  For noisy speech  signal recorded in real environ(cid:173) ments,  the  algorithm  improved  the  recognition  performance  and  showed robustness against parametric changes."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f31b20466ae89669f9741e047487eb37-Abstract.html,A Comparison of Image Processing Techniques for Visual Speech Recognition Applications,"Michael S. Gray, Terrence J. Sejnowski, Javier R. Movellan","We  examine  eight  different  techniques  for  developing  visual  rep(cid:173) resentations  in  machine  vision  tasks.  In  particular  we  compare  different  versions  of  principal  component  and  independent  com(cid:173) ponent  analysis  in  combination  with  stepwise  regression  methods  for  variable selection.  We  found  that local methods,  based on the  statistics of image patches, consistently outperformed global meth(cid:173) ods based on the statistics of entire images.  This result is consistent  with  previous  work  on  emotion  and facial  expression  recognition.  In  addition, the use of a stepwise regression technique for  selecting  variables and regions of interest substantially boosted performance."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f39ae9ff3a81f499230c4126e01f421b-Abstract.html,Learning Continuous Distributions: Simulations With Field Theoretic Priors,"Ilya Nemenman, William Bialek","Learning of a smooth but nonparametric probability density can  be reg(cid:173) ularized using methods of Quantum Field Theory. We implement a field  theoretic  prior numerically,  test its  efficacy,  and  show  that  the  free  pa(cid:173) rameter of the  theory  (,smoothness scale') can  be determined self con(cid:173) sistently by the data; this forms an infinite dimensional generalization of  the  MDL principle.  Finally,  we  study  the implications of one's choice  of the prior and the parameterization and conclude that the  smoothness  scale  determination  makes  density  estimation  very  weakly  sensitive  to  the choice of the prior, and that even wrong choices can be advantageous  for small data sets. 
One of the central problems in  learning is  to  balance  'goodness of fit'  criteria against the  complexity of models.  An  important development in the Bayesian approach was  thus  the  realization  that there does  not need to  be  any  extra penalty for  model complexity:  if we  compute the total probability that data are generated by a model, there is a factor from the  volume in  parameter space-the 'Occam factor' -that discriminates  against models  with  more parameters  [1,  2].  This works remarkably welJ  for  systems with a finite  number of  parameters and creates a complexity  'razor'  (after  'Occam's razor') that is  almost equiv(cid:173) alent to  the  celebrated Minimal Description  Length (MDL)  principle  [3].  In addition,  if  the a priori distributions involved are strictly Gaussian, the ideas have also been proven to  apply to some infinite-dimensional (nonparametric) problems [4].  It is not clear, however,  what happens  if we  leave  the  finite  dimensional  setting  to  consider nonparametric prob(cid:173) lems  which  are  not Gaussian,  such  as  the  estimation of a smooth probability density.  A  possible route to  progress on  the  nonparametric problem was  opened by  noticing  [5]  that  a Bayesian prior for density estimation is  equivalent to  a quantum field  theory  (QFT).  In  particular, there are field  theoretic methods for computing the infinite dimensional analog  of the Occam factor,  at least asymptotically for large numbers of examples.  These obser(cid:173) vations have led to  a number of papers  [6,  7,  8,  9]  exploring alternative formulations and  their implications  for  the  speed  of learning.  Here  we  return  to  the  original  formulation  of Ref.  [5]  and  use  numerical methods to  address  some of the questions left open by  the  analytic work [10]:  What is  the result of balancing the infinite dimensional Occam factor  against the  goodness  of fit?  Is  the  QFT inference optimal  in using alJ  of the  information  relevant for learning [II]? What happens if our learning problem is strongly atypical of the  prior distribution?  Following Ref. [5], if N  i. i. d. samples {Xi}, i  = 1 ... N, are observed, then the probability 
that a particular density Q(x) gave rise to these data is given by  P[Q(x)] rr~1 Q(Xi)"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f442d33fa06832082290ad8544a8da27-Abstract.html,Algebraic Information Geometry for Learning Machines with Singularities,Sumio Watanabe,"Algebraic  geometry is  essential  to learning theory.  In  hierarchical  learning  machines  such  as  layered  neural  networks  and  gaussian  mixtures,  the  asymptotic normality does not hold, since Fisher in(cid:173) formation matrices are singular.  In this paper , the rigorous asymp(cid:173) totic form of the stochastic complexity is  clarified  based on resolu(cid:173) tion of singularities and two  different  problems  are studied.  (1)  If  the  prior  is  positive,  then the  stochastic  complexity is  far smaller  than BIO,  resulting in the smaller generalization error than regular  statistical models, even when the true distribution is  not contained  in  the  parametric  model.  nate  free  and  equal  to  zero  at  singularities,  is  employed  then the  stochastic  complexity  has  the  same  form  as  BIO.  It  is  useful  for  model selection,  but not  for  generalization. 
(2)  If Jeffreys'  prior,  which  is  coordi(cid:173)"
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f45a1078feb35de77d26b3f7a52ef502-Abstract.html,The Missing Link - A Probabilistic Model of Document Content and Hypertext Connectivity,"David A. Cohn, Thomas Hofmann","We  describe  a joint probabilistic  model  for  modeling  the  contents  and  inter-connectivity  of document collections such  as  sets of web pages  or  research  paper archives.  The  model  is  based  on  a  probabilistic  factor  decomposition  and  allows  identifying  principal  topics  of the  collection  as well as  authoritative documents within those topics.  Furthermore, the  relationships between topics is mapped out in order to build a predictive  model of link content.  Among the many applications of this approach are  information retrieval and search, topic identification, query disambigua(cid:173) tion, focused web crawling, web authoring, and bibliometric analysis."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f4a4da9aa7eadfd23c7bdb7cf57b3112-Abstract.html,The Use of Classifiers in Sequential Inference,"Vasin Punyakanok, Dan Roth","We  study  the  problem  of combining  the  outcomes  of several  different  classifiers in a way that provides a coherent inference that satisfies some  constraints.  In particular, we develop two general approaches for an im(cid:173) portant subproblem - identifying phrase structure.  The first is  a Marko(cid:173) vian approach that extends standard HMMs to allow the use of a rich ob(cid:173) servation  structure  and  of general  classifiers  to  model  state-observation  dependencies.  The second is  an extension of constraint satisfaction for(cid:173) malisms.  We  develop efficient combination algorithms under both mod(cid:173) els and study them experimentally in the context of shallow parsing."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f542eae1949358e25d8bfeefe5b199f1-Abstract.html,Finding the Key to a Synapse,"Thomas Natschläger, Wolfgang Maass","Experimental data have shown that synapses are heterogeneous: different  synapses respond with different sequences of amplitudes of postsynaptic  responses to the same spike train.  Neither the role of synaptic dynamics  itself nor the  role  of the  heterogeneity of synaptic  dynamics  for  com(cid:173) putations in neural circuits is  well  understood.  We present in this article  methods that make it feasible to compute for a given synapse with known  synaptic parameters the spike train that is optimally fitted  to the synapse,  for example in the  sense that it produces the largest sum of postsynap(cid:173) tic responses.  To  our surprise we find  that most of these optimally fitted  spike  trains  match  common firing  patterns  of specific  types  of neurons  that are discussed in the literature."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f5c3dd7514bf620a1b85450d2ae374b1-Abstract.html,The Unscented Particle Filter,"Rudolph van der Merwe, Arnaud Doucet, Nando de Freitas, Eric A. Wan","In this paper, we  propose a  new  particle filter  based on sequential  importance sampling.  The algorithm uses  a  bank of unscented fil(cid:173) ters to obtain the importance proposal distribution.  This proposal  has  two  very  ""nice""  properties.  Firstly,  it  makes  efficient  use  of  the  latest  available  information  and,  secondly,  it  can  have  heavy  tails.  As  a  result,  we  find  that  the  algorithm  outperforms  stan(cid:173) dard  particle filtering  and  other  nonlinear  filtering  methods  very  substantially.  This  experimental finding  is  in  agreement  with the  theoretical  convergence  proof for  the  algorithm.  The  algorithm  also includes resampling and (possibly) Markov chain Monte Carlo  (MCMC)  steps."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/f9d1152547c0bde01830b7e8bd60024c-Abstract.html,Algorithms for Non-negative Matrix Factorization,"Daniel D. Lee, H. Sebastian Seung","Non-negative matrix factorization (NMF) has previously been shown to 
be a useful decomposition for multivariate data. Two different multi- 
plicative algorithms for NMF are analyzed. They differ only slightly in 
the multiplicative factor used in the update rules. One algorithm can be 
shown to minimize the conventional least squares error while the other 
minimizes the generalized Kullback-Leibler divergence. The monotonic 
convergence of both algorithms can be proven using an auxiliary func- 
tion analogous to that used for proving convergence of the Expectation- 
Maximization algorithm. The algorithms can also be interpreted as diag- 
onally rescaled gradient descent, where the rescaling factor is optimally 
chosen to ensure convergence."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/faacbcd5bf1d018912c116bf2783e9a1-Abstract.html,Divisive and Subtractive Mask Effects: Linking Psychophysics and Biophysics,"Barbara Zenger, Christof Koch","We describe an analogy between psychophysically measured effects  in  contrast  masking,  and  the  behavior  of a  simple  integrate-and(cid:173) fire  neuron  that  receives  time-modulated  inhibition.  In  the  psy(cid:173) chophysical experiments, we tested observers ability to discriminate  contrasts of peripheral  Gabor patches  in  the  presence  of collinear  Gabor flankers.  The data reveal a complex interaction pattern that  we  account  for  by  assuming  that  flankers  provide  divisive  inhibi(cid:173) tion  to  the  target  unit  for  low  target  contrasts,  but  provide  sub(cid:173) tractive  inhibition  to  the  target  unit  for  higher  target  contrasts.  A similar switch from divisive to subtractive inhibition is  observed  in  an integrate-and-fire unit that receives  inhibition  modulated in  time such that the cell spends part of the time in  a high-inhibition  state  and  part  of  the  time  in  a  low-inhibition  state.  The  simi(cid:173) larity  between  the effects  suggests  that  one  may  cause  the  other.  The biophysical model makes testable predictions for  physiological  single-cell  recordings."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/fb8feff253bb6c834deb61ec76baa893-Abstract.html,Factored Semi-Tied Covariance Matrices,Mark J. F. Gales,"A new  form  of covariance modelling for Gaussian mixture models and  hidden Markov models is  presented.  This is  an  extension to  an  efficient  form of covariance modelling used in  speech recognition, semi-tied co(cid:173) variance matrices.  In the standard form of semi-tied covariance matrices  the covariance matrix is  decomposed into a highly shared decorrelating  transform and a component-specific diagonal covariance matrix. The use  of a factored decorrelating transform is presented in this paper. This fac(cid:173) toring effectively increases the number of possible transforms without in(cid:173) creasing the number of free parameters. Maximum likelihood estimation  schemes for all the model parameters are presented including the compo(cid:173) nent/transform assignment,  transform and component parameters.  This  new  model  form  is  evaluated on  a large vocabulary  speech  recognition  task.  It is  shown that using this  factored  form of covariance modelling  reduces the word error rate."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/fc4ddc15f9f4b4b06ef7844d6bb53abf-Abstract.html,Noise Suppression Based on Neurophysiologically-motivated SNR Estimation for Robust Speech Recognition,"Jürgen Tchorz, Michael Kleinschmidt, Birger Kollmeier","A  novel  noise  suppression  scheme  for  speech  signals  is  proposed  which  is  based  on  a  neurophysiologically-motivated estimation  of  the  local  signal-to-noise  ratio  (SNR)  in  different  frequency  chan(cid:173) nels.  For  SNR-estimation,  the  input  signal  is  transformed  into  so-called  Amplitude Modulation Spectrograms  (AMS),  which rep(cid:173) resent both spectral and temporal characteristics of the respective  analysis  frame,  and  which  imitate  the  representation  of modula(cid:173) tion  frequencies  in  higher  stages  of the  mammalian  auditory sys(cid:173) tem.  A neural network is  used to analyse AMS  patterns generated  from  noisy  speech  and  estimates  the  local  SNR.  Noise  suppres(cid:173) sion  is  achieved  by  attenuating  frequency  channels  according  to  their SNR. The noise suppression algorithm is evaluated in speaker(cid:173) independent  digit  recognition  experiments  and  compared to noise  suppression by Spectral Subtraction."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/ff1418e8cc993fe8abcfe3ce2003e5c5-Abstract.html,Dendritic Compartmentalization Could Underlie Competition and Attentional Biasing of Simultaneous Visual Stimuli,"Kevin A. Archie, Bartlett W. Mel","Neurons in area V4 have relatively large receptive fields  (RFs), so multi(cid:173) ple visual features  are simultaneously ""seen"" by these cells.  Recordings  from  single  V 4  neurons  suggest  that  simultaneously  presented  stimuli  compete  to  set  the  output  firing  rate,  and  that  attention  acts  to  isolate  individual  features  by  biasing  the  competition in  favor  of the  attended  object.  We propose that both stimulus competition and attentional bias(cid:173) ing arise from the spatial segregation of afferent synapses onto different  regions of the excitable dendritic tree of V 4 neurons. The pattern of feed(cid:173) forward, stimulus-driven inputs follows from  a Hebbian rule:  excitatory  afferents  with  similar RFs  tend  to  group  together on  the  dendritic  tree,  avoiding randomly located inhibitory inputs with similar RFs.  The same  principle  guides  the  formation  of inputs  that  mediate  attentional  mod(cid:173) ulation.  Using  both  biophysically  detailed  compartmental  models  and  simplified models of computation in single neurons, we demonstrate that  such an architecture could account for the response properties and atten(cid:173) tional modulation of V 4 neurons.  Our results  suggest an important role  for nonlinear dendritic conductances in extrastriate cortical processing."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/ff7d0f525b3be596a51fb919492c099c-Abstract.html,Smart Vision Chip Fabricated Using Three Dimensional Integration Technology,"Hiroyuki Kurino, M. Nakagawa, Kang Wook Lee, Tomonori Nakamura, Yuusuke Yamada, Ki Tae Park, Mitsumasa Koyanagi","The  smart  VISIOn  chip  has  a  large  potential  for  application  in  general  purpose  high  speed  image  processing  systems.  In  order  to  fabricate  smart  vision  chips  including  photo  detector  compactly,  we  have  proposed  the  application  of  three  dimensional  LSI  technology  for  smart  vision  chips.  Three  dimensional  technology  has  great  potential  to  realize  new  neuromorphic  systems  inspired  by not only  the biological function but also  the  biological structure.  In this paper, we describe our three dimensional LSI technology for  neuromorphic circuits and the design of smart vision chips ."
2000,https://papers.nips.cc/paper_files/paper/2000,https://papers.nips.cc/paper_files/paper/2000/hash/fface8385abbf94b4593a0ed53a0c70f-Abstract.html,"A Productive, Systematic Framework for the Representation of Visual Structure","Shimon Edelman, Nathan Intrator","We describe a unified framework for the understanding of struc(cid:173) ture representation in primate vision. A model derived from this  framework is shown to be effectively systematic in that it has the  ability to interpret and associate together objects that are related  through a rearrangement of common ""middle-scale"" parts, repre(cid:173) sented as image fragments. The model addresses the same concerns  as previous work on compositional representation through the use  of what+where receptive fields and attentional gain modulation. It  does not require prior exposure to the individual parts, and avoids  the need for abstract symbolic binding. 
1 The problem of structure representation 
The focus of theoretical discussion in visual object processing has recently started to  shift from problems of recognition and categorization to the representation of object  structure. Although view- or appearance-based solutions for these problems proved  effective on a variety of object classes [1], the ""holistic"" nature of this approach  - the lack of explicit representation of relational structure - limits its appeal as a  general framework for visual representation [2]. 
The main challenges in the processing of structure are productivity and system(cid:173) aticity, two traits commonly attributed to human cognition. A visual system is  productive if it is open-ended, that is, if it can deal effectively with a potentially  infinite set of objects. A visual representation is systematic if a well-defined change  in the spatial configuration of the object (e.g., swapping top and bottom parts)  causes a principled change in the representation (e.g., the interchange of the rep(cid:173) resentations of top and bottom parts [3, 2]). A solution commonly offered to the  twin problems of productivity and systematicity is compositional representation, in  which symbols standing for generic parts drawn from a small repertoire are bound  together by categorical symbolically coded relations [4]. 
2 The Chorus of Fragments 
In visual representation, the need for symbolic binding may be alleviated by us(cid:173) ing location in the visual field in lieu of the abstract frame that encodes object  structure. Intuitively, the constituents of the object are then bound to each other  by virtue of residing in their proper places in the visual field; this can be thought  of as a pegboard, whose spatial structure supports the arrangement of parts sus(cid:173) pended from its pegs. This scheme exhibits shallow compositionality, which can  be enhanced by allowing the ""pegboard"" mechanism to operate at different spatial  scales, yielding effective systematicity across levels of resolution. Coarse coding the  constituents (e.g., representing each object fragment in terms of its similarities to  some basis shapes) will render the scheme productive. We call this approach to the  representation of structure the Chorus of Fragments (CoF; [5]). 
2.1 Neurobiological building blocks 
What+ Where cells. The representation of spatially anchored object fragments pos(cid:173) tulated by the CoF model can be supported by what+where neurons, each tuned  both to a certain shape class and to a certain range of locations in the visual field.  Such cells have been found in the monkey in areas V 4 and posterior IT [6], and in  the prefrontal cortex [7]. 
Attentional gain fields. To decouple the representation of object structure from its  location in the visual field, one needs a version of the what+where mechanism in  which the response of the cell depends not merely on the location of the stimulus  with respect to fixation (as in classical receptive fields), but also on its location  with respect to the focus of attention. Indeed, modulatory effects of object-centered  attention on classical RF structure (gain fields) have been found in area V 4 [8]. 
2.2 
Implemented model 
Our implementation of the CoF model involves what+where cells with attention(cid:173) modulated gain fields, and is aimed at productive and systematic treatment of  composite shapes in object-centered coordinates. It operates directly on gray-level  images, pre-processed by a model of the primary visual cortex [9], with complex(cid:173) cell responses modified to use the MAX operation suggested in [10]. In the model,  one what+where unit is assigned to the top and one to the bottom fragment of the  visual field, each extracted by an appropriately configured Gaussian gain profile  (Figure 2, left). The units are trained (1) to discriminate among five objects,  (2) to tolerate translation within the hemifield, and (3) to provide an estimate of  the reliability of its output, through an autoassociation mechanism attempting to  reconstruct the stimulus image [11, 12]. Within each hemifield, the five outputs  of a unit can provide a coarse coding of novel objects belonging to the familiar  category, in a manner useful for translation-tolerant recognition [13]. The reliability  estimate carries information about category, allowing outputs for objects from other  categories to be squelched. Most importantly, due to the spatial localization of the  unit's receptive field, the system can distinguish between different configurations of  the same shapes, while noting the fragment-wise similarities. 
We assume that during learning the system performs multiple fixations of the target  object, effectively providing the what+where units with a basis for spanning the"
