year,proceeding_link,paper_link,title,authors,abstract
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/01e00f2f4bfcbb7505cb641066f2859b-Abstract.html,Regular and Irregular Gallager-zype Error-Correcting Codes,"Yoshiyuki Kabashima, Tatsuto Murayama, David Saad, Renato Vicente",The  performance  of  regular  and  irregular  Gallager-type  error(cid:173) correcting code  is  investigated  via  methods  of statistical  physics.  The transmitted codeword comprises products of the original mes(cid:173) sage  bits  selected  by  two  randomly-constructed  sparse  matrices;  the  number  of  non-zero  row/column  elements  in  these  matrices  constitutes  a  family  of codes.  We  show  that  Shannon's  channel  capacity may  be saturated in equilibrium for  many  of the regular  codes while slightly lower performance is obtained for others which  may  be  of higher  practical  relevance.  Decoding  aspects  are  con(cid:173) sidered  by employing the TAP  approach  which  is  identical  to the  commonly used  belief-propagation-based decoding.  We  show that  irregular codes may saturate Shannon's capacity but with improved  dynamical properties.
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/02f039058bd48307e6f653a2005c9dd2-Abstract.html,An MEG Study of Response Latency and Variability in the Human Visual System During a Visual-Motor Integration Task,"Akaysha C. Tang, Barak A. Pearlmutter, Tim A. Hely, Michael Zibulevsky, Michael P. Weisend","Human  reaction  times  during  sensory-motor  tasks  vary  consider(cid:173) ably.  To  begin to understand how  this variability arises, we  exam(cid:173) ined neuronal populational response time variability at early versus  late  visual  processing  stages.  The  conventional  view  is  that  pre(cid:173) cise temporal information is gradually lost as information is passed  through a  layered network of mean-rate  ""units.""  We  tested in hu(cid:173) mans  whether  neuronal  populations  at different  processing  stages  behave like mean-rate ""units"".  A blind source separation algorithm  was  applied to MEG signals from  sensory-motor integration tasks.  Response  time  latency  and  variability  for  multiple  visual  sources  were estimated by  detecting single-trial stimulus-locked events for  each  source.  In  two  subjects  tested  on  four  visual  reaction  time  tasks,  we  reliably identified sources  belonging to early and late vi(cid:173) sual processing stages.  The standard deviation of response latency  was smaller for  early rather than late processing stages.  This sup(cid:173) ports the  hypothesis that human populational response  time vari(cid:173) ability increases from  early to late visual  processing stages."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/08e6bea8e90ba87af3c9554d94db6579-Abstract.html,The Entropy Regularization Information Criterion,"Alex J. Smola, John Shawe-Taylor, Bernhard Schölkopf, Robert C. Williamson","Effective methods of capacity control via uniform convergence bounds  for function expansions have been largely limited to Support Vector ma(cid:173) chines,  where  good  bounds  are  obtainable  by  the  entropy  number ap(cid:173) proach. We extend these methods to systems with expansions in terms of  arbitrary (parametrized) basis functions and a wide range of regulariza(cid:173) tion methods covering the whole range of general linear additive models.  This is  achieved by  a data dependent analysis of the eigenvalues of the  corresponding design matrix."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/0efbe98067c6c73dba1250d2beaa81f9-Abstract.html,Invariant Feature Extraction and Classification in Kernel Spaces,"Sebastian Mika, Gunnar Rätsch, Jason Weston, Bernhard Schölkopf, Alex J. Smola, Klaus-Robert Müller","In hyperspectral imagery one pixel  typically  consists of a  mixture  of the  reflectance  spectra of several  materials,  where  the  mixture  coefficients  correspond  to  the  abundances of the  constituting  ma(cid:173) terials.  We  assume linear combinations of reflectance spectra with  some additive normal sensor noise and derive a  probabilistic MAP  framework  for  analyzing  hyperspectral  data.  As  the  material re(cid:173) flectance characteristics are not know  a priori, we face  the problem  of  unsupervised  linear  unmixing.  The  incorporation  of  different  prior  information  (e.g.  positivity  and  normalization  of the  abun(cid:173) dances)  naturally  leads  to  a  family  of  interesting  algorithms,  for  example  in  the  noise-free  case  yielding  an  algorithm  that  can  be  understood as constrained independent component analysis (ICA).  Simulations underline the usefulness of our theory."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/10c272d06794d3e5785d5e7c5356e9ff-Abstract.html,Correctness of Belief Propagation in Gaussian Graphical Models of Arbitrary Topology,"Yair Weiss, William T. Freeman","Local ""belief propagation"" rules of the  sort proposed by Pearl  [15]  are  guaranteed  to  converge to  the  correct  posterior probabilities  in  singly  connected graphical models. Recently, a number of researchers have em(cid:173) pirically demonstrated good performance of ""loopy belief propagation""(cid:173) using these same rules on graphs with loops.  Perhaps the most dramatic  instance is the near Shannon-limit performance of ""Turbo codes"", whose  decoding algorithm is equivalent to loopy belief propagation.  Except for the case of graphs with a single loop, there has been little theo(cid:173) retical understanding of the performance of loopy propagation. Here we  analyze belief propagation in  networks  with  arbitrary  topologies  when  the nodes in  the graph describe jointly Gaussian random variables.  We  give an analytical formula relating the true  posterior probabilities with  those calculated using loopy propagation.  We give sufficient conditions  for convergence and show that when belief propagation converges it gives  the correct posterior means for all graph  topologies,  not just networks  with a single loop.  The related ""max-product"" belief propagation algorithm finds  the max(cid:173) imum posterior probability estimate for singly connected networks.  We  show  that,  even for non-Gaussian probability distributions,  the  conver(cid:173) gence points of the max-product algorithm in loopy  networks are  max(cid:173) ima  over a  particular large  local  neighborhood of the  posterior proba(cid:173) bility.  These results  help clarify  the  empirical performance results  and  motivate  using  the  powerful  belief propagation algorithm in  a  broader  class of networks. 
Problems involving probabilistic belief propagation arise in a wide variety of applications,  including error correcting codes,  speech recognition and medical diagnosis.  If the  graph  is  singly connected, there exist local message-passing schemes to  calculate the  posterior  probability of an unobserved variable given the observed variables.  Pearl [15] derived such  a scheme for singly connected Bayesian networks and showed that this ""belief propagation""  algorithm is guaranteed to converge to the correct posterior probabilities (or ""beliefs""). 
Several groups have recently reported excellent experimental results by running algorithms 
674 
Y.  Weiss  and W T.  Freeman 
equivalent to Pearl's algorithm on networks with loops [8, 13, 6].  Perhaps the most dramatic  instance of this performance is for ""Turbo code""  [2]  error correcting codes.  These codes  have been described as ""the most exciting and potentially important development in coding  theory  in  many years"" [12]  and have recently been shown  [10,  11] to utilize an algorithm  equivalent to belief propagation in a network with loops. 
Progress in the analysis of loopy belief propagation has been made for the case of networks  with  a  single  loop  [17,  18,  4,  1].  For  these  networks,  it  can  be  shown  that  (1)  unless  all  the  compatabilities are  deterministic,  loopy belief propagation will converge.  (2) The  difference between the loopy beliefs and the true beliefs is related to the convergence rate  the faster the convergence the more exact the approximation and (3) If  of the messages - the hidden nodes are binary, then the loopy beliefs and the true beliefs are both maximized  by the same assignments, although the confidence in that assignment is wrong for the loopy  beliefs. 
In this paper we analyze belief propagation in  graphs of arbitrary topology, for nodes de(cid:173) scribing jointly Gaussian random variables.  We  give an exact formula relating the correct  marginal  posterior probabilities with  the  ones calculated using  loopy  belief propagation.  We  show that if belief propagation converges, then it will give the correct posterior means  for all graph  topologies, not just networks with  a single loop.  We  show that the covari(cid:173) ance estimates will  generally be  incorrect but present a relationship between the error in  the covariance estimates and the convergence speed.  For Gaussian or non-Gaussian vari(cid:173) ables,  we  show  that the  ""max-product"" algorithm,  which  calculates the MAP estimate in  singly connected networks, only converges to points that are maxima over a particular large  neighborhood of the posterior probability of loopy networks."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/11c484ea9305ea4c7bb6b2e6d570d466-Abstract.html,Population Decoding Based on an Unfaithful Model,"Si Wu, Hiroyuki Nakahara, Noboru Murata, Shun-ichi Amari","We study a population decoding paradigm in which the maximum likeli(cid:173) hood inference is based on an  unfaithful decoding model (UMLI). This  is usually the case for neural population decoding because the encoding  process  of the  brain  is  not exactly  known,  or because a  simplified de(cid:173) coding model  is  preferred for  saving computational cost.  We  consider  an  unfaithful  decoding model  which  neglects the  pair-wise  correlation  between neuronal activities, and prove that UMLI is asymptotically effi(cid:173) cient when the neuronal correlation is uniform or of limited-range.  The  performance of UMLI is compared with that of the maximum likelihood  inference based on  a faithful  model  and  that of the  center of mass de(cid:173) coding  method.  It turns  out  that  UMLI  has  advantages  of decreasing  the computational complexity remarkablely and maintaining a high-level  decoding  accuracy  at  the  same  time.  The  effect of correlation  on  the  decoding accuracy is also discussed."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html,Broadband Direction-Of-Arrival Estimation Based on Second Order Statistics,"Justinian P. Rosca, Joseph Ó Ruanaidh, Alexander Jourjine, Scott Rickard","N  wideband  sources  recorded  using  N  closely  spaced  receivers  can  feasibly  be separated  based  only  on  second  order statistics when  using  a physical model  of the mixing process.  In  this case  we  show that  the  parameter estimation problem can  be essentially reduced to considering  directions of arrival and attenuations of each signal.  The paper presents  two demixing methods operating in  the time and frequency  domain and  experimentally shows that it is always possible to demix signals arriving at  different angles.  Moreover,  one can use spatial cues to solve the channel  selection  problem  and  a post-processing Wiener filter  to  ameliorate the  artifacts caused by demixing."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/148510031349642de5ca0c544f31b2ef-Abstract.html,Emergence of Topography and Complex Cell Properties from Natural Images using Extensions of ICA,"Aapo Hyvärinen, Patrik O. Hoyer","Independent  component  analysis of natural images  leads to emer(cid:173) gence  of  simple  cell  properties,  Le.  linear  filters  that  resemble  wavelets  or  Gabor  functions.  In  this  paper,  we  extend  ICA  to  explain further properties of VI cells.  First, we decompose natural  images  into  independent  subspaces  instead of scalar  components.  This  model  leads  to  emergence  of phase  and  shift  invariant  fea(cid:173) tures,  similar  to  those  in  VI  complex  cells.  Second,  we  define  a  topography between the linear components obtained by ICA.  The  topographic distance between  two components is  defined  by  their  higher-order correlations, so that two components are close to each  other  in  the  topography  if  they  are  strongly  dependent  on  each  other.  This  leads  to simultaneous emergence  of both  topography  and invariances similar to complex cell  properties."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/158fc2ddd52ec2cf54d3c161f2dd6517-Abstract.html,Reinforcement Learning Using Approximate Belief States,"Andres C. Rodriguez, Ronald Parr, Daphne Koller","The problem of developing good policies for partially observable Markov  decision  problems (POMDPs) remains one of the  most challenging ar(cid:173) eas of research  in  stochastic planning.  One line of research  in  this  area  involves  the use  of reinforcement learning  with  belief states,  probabil(cid:173) ity  distributions  over  the  underlying  model  states.  This  is  a  promis(cid:173) ing method for  small problems,  but its  application is  limited  by  the  in(cid:173) tractability of computing or representing a full belief state for large prob(cid:173) lems.  Recent  work  shows  that,  in  many  settings,  we  can  maintain  an  approximate belief state, which is fairly close to the true belief state.  In  particular,  great success has  been shown  with  approximate belief states  that marginalize out correlations between  state  variables.  In  this  paper,  we investigate two methods of full belief state reinforcement learning and  one novel method for reinforcement learning using factored approximate  belief states. We compare the performance of these algorithms on several  well-known problem from the literature. Our results demonstrate the im(cid:173) portance of approximate belief state representations for large problems."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/17ed8abedc255908be746d245e50263a-Abstract.html,Building Predictive Models from Fractal Representations of Symbolic Sequences,"Peter Tiño, Georg Dorffner","We propose a novel approach for building finite memory predictive mod(cid:173) els similar in spirit to variable memory length Markov models (VLMMs).  The models are constructed by first transforming the n-block structure of  the training sequence into a spatial structure of points in a unit hypercube,  such that the  longer is the common suffix shared by any  two n-blocks,  the closer lie their point representations. Such a transformation embodies  a Markov assumption - n-blocks with long common suffixes  are likely  to produce similar continuations.  Finding a set of prediction contexts is  formulated as a resource allocation problem solved by vector quantizing  the spatial n-block representation. We compare our model with both the  classical  and variable memory length Markov models on three data sets  with different memory  and stochastic components.  Our models have  a  superior performance, yet, their construction is fully automatic, which is  shown to be problematic in the case of VLMMs."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/1c54985e4f95b7819ca0357c0cb9a09f-Abstract.html,Neural Computation with Winner-Take-All as the Only Nonlinear Operation,Wolfgang Maass,"Everybody ""knows"" that neural networks need more than a single layer  of nonlinear units to compute interesting functions.  We show that this is  false if one employs winner-take-all as nonlinear unit: 
•  Any boolean function can be computed by a single k-winner-take(cid:173)
all unit applied to weighted sums of the input variables. 
•  Any continuous  function  can be approximated arbitrarily well  by  a  single soft winner-take-all unit applied to weighted sums of the  input variables. 
•  Only positive weights are needed in these (linear) weighted sums.  This may be of interest from  the point of view of neurophysiology,  since only 15% of the synapses in the cortex are inhibitory.  In addi(cid:173) tion it is  widely believed that there are special microcircuits in the  cortex that compute winner-take-all. 
•  Our results  support the  view  that winner-take-all  is  a  very  useful 
basic computational unit in Neural VLS!:  o 
it  is  wellknown  that  winner-take-all  of n  input  variables  can  be  computed  very  efficiently  with  2n  transistors  (and  a  to(cid:173) tal  wire  length  and  area  that  is  linear  in  n)  in  analog  VLSI  [Lazzaro et at.,  1989] 
o  we show that winner-take-all is  not just useful  for special pur(cid:173) pose computations, but may serve as the only nonlinear unit for  neural circuits with universal computational power 
o  we show that any multi-layer perceptron needs quadratically in  n many gates to compute winner-take-all for n  input variables,  hence  winner-take-all  provides a  substantially  more  powerful  computational  unit  than  a  perceptron (at  about the  same  cost  of implementation in analog VLSI). 
Complete  proofs  and  further  details  to  these  results  can  be  found  in  [Maass, 2000]. 
294"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/207f88018f72237565570f8a9e5ca240-Abstract.html,Support Vector Method for Multivariate Density Estimation,"Vladimir Vapnik, Sayan Mukherjee","A  new  method  for  multivariate  density  estimation  is  developed  based  on  the  Support  Vector  Method  (SVM)  solution  of inverse  ill-posed problems.  The solution has  the form  of a  mixture of den(cid:173) sities.  This  method  with  Gaussian  kernels  compared favorably  to  both  Parzen's  method  and  the  Gaussian  Mixture  Model  method.  For synthetic data we achieve more accurate estimates for densities  of 2,  6,  12, and 40  dimensions."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/21be9a4bd4f81549a9d1d241981cec3c-Abstract.html,Leveraged Vector Machines,Yoram Singer,"We describe an iterative algorithm for building vector machines used in  classification tasks.  The algorithm builds  on  ideas from  support vector  machines, boosting, and generalized additive models. The algorithm can  be  used  with  various continuously differential functions  that bound the  discrete (0-1) classification loss and is very simple to implement. We test  the proposed algorithm with two different loss functions on synthetic and  natural data. We also describe a norm-penalized version of the algorithm  for the exponential loss function used in AdaBoost.  The performance of  the algorithm on  natural data is  comparable to  support vector machines  while typically its running time is shorter than of SVM."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/231141b34c82aa95e48810a9d1b33a79-Abstract.html,Learning Factored Representations for Partially Observable Markov Decision Processes,Brian Sallans,"The problem of reinforcement learning in a non-Markov environment is  explored using a dynamic Bayesian network, where conditional indepen(cid:173) dence assumptions between random variables are compactly represented  by network parameters.  The parameters are learned on-line, and approx(cid:173) imations are used to perform inference and to compute the optimal value  function.  The  relative  effects of inference  and  value  function  approxi(cid:173) mations on the quality of the final  policy are investigated, by learning to  solve a moderately difficult driving task. The two value function approx(cid:173) imations,  linear and quadratic, were found to  perform similarly, but the  quadratic model was more sensitive to initialization.  Both performed be(cid:173) low the level of human performance on the task.  The dynamic Bayesian  network performed comparably to  a model  using a localist hidden state  representation, while requiring exponentially fewer parameters."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/2451041557a22145b3701b0184109cab-Abstract.html,Variational Inference for Bayesian Mixtures of Factor Analysers,"Zoubin Ghahramani, Matthew J. Beal","We  present an algorithm that infers the model structure of a  mix(cid:173) ture of factor  analysers using  an efficient  and  deterministic  varia(cid:173) tional  approximation to  full  Bayesian  integration  over  model  pa(cid:173) rameters.  This  procedure  can  automatically  determine  the  opti(cid:173) mal  number  of components  and  the  local  dimensionality  of each  component  (Le.  the  number  of  factors  in  each  factor  analyser) .  Alternatively  it  can  be  used  to  infer  posterior  distributions  over  number of components and dimensionalities.  Since  all  parameters  are integrated out the method is  not prone to overfitting.  Using a  stochastic  procedure for  adding  components  it is  possible  to  per(cid:173) form  the variational optimisation incrementally and to avoid  local  maxima.  Results show that the method works very well in practice  and  correctly  infers  the  number  and  dimensionality  of  nontrivial  synthetic examples.  By  importance  sampling  from  the  variational  approximation  we  show  how  to  obtain  unbiased  estimates  of the  true  evidence,  the  exact predictive density,  and the KL divergence between the varia(cid:173) tional posterior and the true posterior, not only in this  model  but  for  variational approximations in general."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/25e2a30f44898b9f3e978b1786dcd85c-Abstract.html,Topographic Transformation as a Discrete Latent Variable,"Nebojsa Jojic, Brendan J. Frey","Invariance to topographic transformations such as translation and  shearing in  an image has  been successfully incorporated into feed(cid:173) forward  mechanisms,  e.g.,  ""convolutional neural  networks"",  ""tan(cid:173) gent propagation"".  We describe a way to add transformation invari(cid:173) ance to a generative density model by approximating the nonlinear  transformation manifold  by  a  discrete  set of transformations.  An  EM  algorithm for  the original model  can  be extended to the  new  model  by  computing expectations over the set of transformations.  We show how to add a discrete transformation variable to Gaussian  mixture modeling,  factor  analysis and mixtures of factor  analysis.  We  give results on filtering microscopy images, face  and facial  pose  clustering, and handwritten digit  modeling and recognition."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/2612aa892d962d6f8056b195ca6e550d-Abstract.html,Channel Noise in Excitable Neural Membranes,"Amit Manwani, Peter N. Steinmetz, Christof Koch","Stochastic  fluctuations  of voltage-gated  ion  channels  generate  current  and  voltage  noise  in  neuronal  membranes.  This  noise  may  be  a  criti(cid:173) cal  determinant of the efficacy of information processing within  neural  systems.  Using Monte-Carlo simulations, we carry out a systematic in(cid:173) vestigation of the relationship  between channel  kinetics  and  the result(cid:173) ing membrane  voltage  noise  using  a  stochastic  Markov  version  of the  Mainen-Sejnowski  model  of dendritic  excitability  in  cortical  neurons.  Our simulations show that kinetic  parameters which  lead to an increase  in  membrane excitability (increasing channel densities, decreasing tem(cid:173) perature)  also  lead to  an  increase in  the magnitude of the sub-threshold  voltage noise. Noise also increases as the membrane is depolarized from  rest towards threshold.  This  suggests that channel fluctuations  may  in(cid:173) terfere with a neuron's ability to function as an integrator of its synaptic  inputs and may limit the reliability  and precision of neural  information  processing."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/26751be1181460baf78db8d5eb7aad39-Abstract.html,Efficient Approaches to Gaussian Process Classification,"Lehel Csató, Ernest Fokoué, Manfred Opper, Bernhard Schottky, Ole Winther",We  present  three  simple  approximations  for  the  calculation  of  the  posterior  mean  in  Gaussian  Process  classification.  The  first  two  methods  are  related  to mean field  ideas  known  in  Statistical  Physics.  The third approach is based on Bayesian online approach  which was  motivated by recent results in the Statistical Mechanics  of Neural Networks.  We present simulation results showing:  1. that  the mean field  Bayesian evidence may be used for  hyperparameter  tuning and 2.  that the online approach may achieve a low  training  error fast.
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/270edd69788dce200a3b395a6da6fdb7-Abstract.html,Optimal Sizes of Dendritic and Axonal Arbors,Dmitri B. Chklovskii,I consider a topographic projection between two neuronal layers with dif(cid:173) ferent densities of neurons.  Given  the  number of output  neurons con(cid:173) nected to each input neuron (divergence or fan-out) and the number of  input neurons synapsing on each output neuron (convergence or fan-in) I  determine the widths of axonal and dendritic arbors which minimize the  total  volume ofaxons and dendrites.  My analytical results can be sum(cid:173) marized qualitatively in the following rule:  neurons of the sparser layer  should have arbors wider than those of the denser layer.  This agrees with  the anatomical data from retinal and cerebellar neurons whose morphol(cid:173) ogy and connectivity are known.  The rule may be used to infer connec(cid:173) tivity of neurons from their morphology.
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/28dc6b0e1b33769b4b94685e4f4d1e5c-Abstract.html,v-Arc: Ensemble Learning in the Presence of Outliers,"Gunnar Rätsch, Bernhard Schölkopf, Alex J. Smola, Klaus-Robert Müller, Takashi Onoda, Sebastian Mika","AdaBoost and other ensemble methods have successfully  been ap(cid:173) plied  to a  number  of classification  tasks,  seemingly  defying  prob(cid:173) lems of overfitting.  AdaBoost performs gradient descent in an error  function  with  respect  to the margin,  asymptotically concentrating  on  the  patterns which  are  hardest to learn.  For  very  noisy  prob(cid:173) lems,  however,  this  can  be  disadvantageous.  Indeed,  theoretical  analysis has shown that the margin distribution,  as opposed to just  the minimal margin, plays a crucial role in understanding this phe(cid:173) nomenon.  Loosely  speaking,  some  outliers  should  be  tolerated  if  this  has  the  benefit  of substantially  increasing  the  margin  on  the  remaining points.  We  propose a  new  boosting algorithm which  al(cid:173) lows for  the possibility of a  pre-specified fraction of points to lie  in  the margin area Or even on the wrong side of the decision boundary."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/299570476c6f0309545110c592b6a63b-Abstract.html,Monte Carlo POMDPs,Sebastian Thrun,"We  present  a Monte Carlo algorithm for  learning to  act  in  partially observable  Markov decision processes (POMDPs) with real-valued state and action spaces.  Our approach uses importance sampling for representing beliefs, and Monte Carlo  approximation for belief propagation.  A reinforcement learning algorithm, value  iteration, is employed to learn value functions over belief states. Finally, a sample(cid:173) based  version  of nearest  neighbor  is  used  to  generalize  across  states.  Initial  empirical results suggest that our approach works well in practical applications."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/2a27b8144ac02f67687f76782a3b5d8f-Abstract.html,A Recurrent Model of the Interaction Between Prefrontal and Inferotemporal Cortex in Delay Tasks,"Alfonso Renart, Néstor Parga, Edmund T. Rolls",A very simple model of two reciprocally connected attractor neural net(cid:173) works is studied analytically in situations similar to those encountered  in delay match-to-sample tasks with intervening stimuli and in tasks of  memory guided attention. The model qualitatively reproduces many of  the experimental data on these types of tasks and provides a framework  for the understanding of the experimental observations in the context of  the attractor neural network scenario.
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/2cb6b10338a7fc4117a80da24b582060-Abstract.html,Information Factorization in Connectionist Models of Perception,"Javier R. Movellan, James L. McClelland","We  examine  a  psychophysical  law  that  describes  the  influence  of  stimulus and context  on  perception.  According to this law  choice  probability  ratios  factorize  into  components  independently  con(cid:173) trolled by stimulus and context.  It has been argued that this  pat(cid:173) tern of results is  incompatible with feedback models of perception.  In this  paper we  examine this  claim  using neural  network models  defined via stochastic differential equations.  We show that the law  is  related to a  condition  named channel separability and has little  to do with the existence of feedback connections.  In essence, chan(cid:173) nels are separable if they converge into the response units without  direct lateral connections to other channels and if their sensors are  not  directly  contaminated  by  external  inputs  to  the  other  chan(cid:173) nels.  Implications of the analysis for  cognitive and computational  neurosicence are discussed."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/365d17770080c807a0e47ae9118d8641-Abstract.html,Hierarchical Image Probability (H1P) Models,"Clay Spence, Lucas C. Parra","We formulate a model for probability distributions on image spaces.  We  show that any distribution of images can be factored exactly into condi(cid:173) tional  distributions of feature  vectors at  one resolution  (pyramid level)  conditioned on the  image information at  lower resolutions.  We  would  like to factor this over positions in the pyramid levels to make it tractable,  but such factoring may miss long-range dependencies. To fix this, we in(cid:173) troduce hidden class  labels  at  each  pixel  in  the pyramid.  The result  is  a  hierarchical mixture of conditional probabilities,  similar  to  a  hidden  Markov model on a tree.  The model parameters can be found with max(cid:173) imum likelihood estimation using the EM algorithm.  We  have obtained  encouraging preliminary results on the problems of detecting various ob(cid:173) jects in SAR images and target recognition in optical aerial images."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/36d7534290610d9b7e9abed244dd2f28-Abstract.html,Reinforcement Learning for Spoken Dialogue Systems,"Satinder P. Singh, Michael J. Kearns, Diane J. Litman, Marilyn A. Walker","Recently, a number of authors have proposed treating dialogue systems as Markov  decision processes (MDPs). However, the practical application ofMDP algorithms  to dialogue systems faces a number of severe technical challenges. We have built a  general software tool (RLDS, for Reinforcement Learning for Dialogue Systems)  based on the MDP framework, and have applied it to dialogue corpora gathered  from two dialogue systems built at AT&T Labs. Our experiments demonstrate that  RLDS holds promise as a tool for ""browsing"" and understanding correlations in  complex, temporally dependent dialogue corpora."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/375c71349b295fbe2dcdca9206f20a06-Abstract.html,Distributed Synchrony of Spiking Neurons in a Hebbian Cell Assembly,"David Horn, Nir Levy, Isaac Meilijson, Eytan Ruppin","We  investigate the behavior of a  Hebbian  cell  assembly of spiking  neurons formed via a temporal synaptic learning curve.  This learn(cid:173) ing  function  is  based  on  recent  experimental findings .  It  includes  potentiation for  short time delays  between  pre- and  post-synaptic  neuronal spiking,  and depression for  spiking events occuring in the  reverse order.  The coupling  between the dynamics of the synaptic  learning and of the neuronal activation leads to interesting results.  We  find  that  the  cell  assembly  can  fire  asynchronously,  but  may  also  function  in  complete  synchrony,  or  in  distributed  synchrony.  The latter implies spontaneous division of the Hebbian cell  assem(cid:173) bly into groups of cells  that fire  in  a cyclic manner.  We  invetigate  the behavior of distributed synchrony both by  simulations and by  analytic calculations of the resulting synaptic distributions."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/393c55aea738548df743a186d15f3bef-Abstract.html,Image Representations for Facial Expression Coding,"Marian Stewart Bartlett, Gianluca Donato, Javier R. Movellan, Joseph C. Hager, Paul Ekman, Terrence J. Sejnowski","The  Facial  Action  Coding  System  (FACS)  (9)  is  an  objective  method  for  quantifying  facial  movement  in  terms  of  component  actions.  This  system  is  widely  used  in  behavioral  investigations  of emotion,  cognitive  processes,  and  social  interaction.  The  cod(cid:173) ing is  presently  performed by  highly trained human experts.  This  paper  explores  and  compares  techniques  for  automatically  recog(cid:173) nizing facial actions in sequences of images.  These methods include  unsupervised  learning techniques  for  finding  basis  images  such  as  principal component analysis, independent component analysis and  local  feature  analysis,  and supervised learning  techniques  such  as  Fisher's  linear  discriminants.  These  data-driven  bases  are  com(cid:173) pared to Gabor wavelets, in which the basis images are predefined.  Best  performances  were  obtained  using  the  Gabor  wavelet  repre(cid:173) sentation and the  independent  component  representation,  both of  which achieved 96%  accuracy for  classifying 12 facial  actions.  The  ICA  representation employs  2 orders of magnitude fewer  basis im(cid:173) ages than the  Gabor representation and takes 90%  less  CPU  time  to compute for new images.  The results provide converging support  for using local basis images, high spatial frequencies, and statistical  independence for  classifying facial  actions."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/3c1e4bd67169b8153e0047536c9f541e-Abstract.html,Algorithms for Independent Components Analysis and Higher Order Statistics,"Daniel D. Lee, Uri Rokni, Haim Sompolinsky","A  latent  variable  generative  model  with  finite  noise  is  used  to  de(cid:173) scribe  several  different  algorithms  for  Independent Components  Anal(cid:173) ysis  (lCA).  In  particular,  the  Fixed  Point  ICA  algorithm  is  shown  to  be equivalent to  the Expectation-Maximization algorithm for maximum  likelihood  under certain  constraints,  allowing  the conditions for  global  convergence to  be  elucidated.  The algorithms can  also  be explained by  their generic  behavior  near  a  singular point where the  size  of the  opti(cid:173) mal generative bases vanishes.  An expansion of the likelihood about this  singular point indicates the role of higher order correlations in  determin(cid:173) ing the features discovered by  ICA. The application and convergence of  these algorithms are demonstrated on a simple illustrative example."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract.html,A SNoW-Based Face Detector,"Ming-Hsuan Yang, Dan Roth, Narendra Ahuja","A novel learning approach for human face detection using a network  of linear units is  presented.  The SNoW  learning architecture is  a  sparse network  of linear functions  over a  pre-defined or incremen(cid:173) tally learned feature  space  and  is  specifically tailored for  learning  in the presence of a  very large number of features.  A wide range of  face  images in different poses,  with different expressions and under  different  lighting conditions  are  used  as  a  training  set  to  capture  the  variations of human faces.  Experimental results on commonly  used benchmark data sets of a wide range of face images show that  the  SNoW-based  approach  outperforms  methods  that  use  neural  networks,  Bayesian  methods,  support  vector  machines  and  oth(cid:173) ers.  Furthermore, learning and  evaluation using  the  SNoW-based  method are significantly more efficient  than with other methods."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/3e7e0224018ab3cf51abb96464d518cd-Abstract.html,A Winner-Take-All Circuit with Controllable Soft Max Property,Shih-Chii Liu,"I describe a silicon network consisting of a group of excitatory neu(cid:173) rons  and a  global inhibitory neuron.  The output of the inhibitory  neuron is normalized with respect to the input strengths.  This out(cid:173) put models the normalization property of the wide-field direction(cid:173) selective cells in the fly  visual system.  This normalizing property is  also useful  in any system where we  wish  the output signal to code  only the strength of the inputs, and not be dependent on the num(cid:173) ber of inputs.  The circuitry in each neuron is equivalent to that in  Lazzaro's winner-take-all  (WTA)  circuit  with one additional tran(cid:173) sistor  and  a  voltage  reference.  Just  as  in  Lazzaro's  circuit,  the  outputs of the excitatory neurons code the neuron with the largest  input.  The difference here is  that multiple winners can be chosen.  By  varying  the  voltage  reference  of the  neuron,  the  network  can  transition  between  a  soft-max  behavior  and  a  hard  WTA  behav(cid:173) ior.  I show results from  a fabricated chip of 20  neurons in a  1.2J.Lm  CMOS technology."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/404dcc91b2aeaa7caa47487d1483e48a-Abstract.html,"Bayesian Model Selection for Support Vector Machines, Gaussian Processes and Other Kernel Classifiers",Matthias Seeger,"We present a  variational Bayesian method for model selection over  families of kernels classifiers like Support Vector machines or Gaus(cid:173) sian processes.  The algorithm needs no user interaction and is  able  to adapt a large number of kernel parameters to given data without  having to sacrifice training cases for validation.  This opens the pos(cid:173) sibility to use  sophisticated families  of kernels  in  situations  where  the small  ""standard kernel""  classes  are clearly  inappropriate.  We  relate  the  method  to other work  done  on  Gaussian  processes  and  clarify the  relation between  Support Vector  machines  and  certain  Gaussian process models."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/418ef6127e44214882c61e372e866691-Abstract.html,Kirchoff Law Markov Fields for Analog Circuit Design,Richard M. Golden,"Three contributions to developing an algorithm for  assisting engi(cid:173) neers in designing analog circuits are provided in this paper.  First,  a  method  for  representing  highly  nonlinear  and  non-continuous  analog circuits using Kirchoff current law potential functions within  the context of a Markov field  is described.  Second, a relatively effi(cid:173) cient  algorithm for  optimizing the Markov field  objective function  is  briefly  described  and the  convergence proof is  briefly  sketched.  And  third,  empirical  results  illustrating  the strengths and limita(cid:173) tions  of the approach  are provided within the context  of a  JFET  transistor design problem.  The proposed algorithm generated a set  of circuit components for  the JFET circuit model  that accurately  generated the desired characteristic curves. 
1  Analog circuit design using Markov random fields 
1.1  Markov random field  models 
A Markov random field  (MRF) is a generalization of the concept of a Markov chain.  In a Markov field  one begins with a set of random variables and a  neighborhood re(cid:173) lation which  is  represented by a  graph.  Each  random  variable  will  be  assumed  in  this paper to be  a  discrete random variable which  takes on  one of a  finite  number  of possible  values.  Each  node  of the graph  indexs  a  specific  random  variable.  A  link  from  the  jth node  to  the  ith  node  indicates  that  the  conditional  probability  distribution of the ith random variable in the field  is functionally  dependent  upon  the jth random variable.  That is,  random variable j  is  a  neighbor of random  vari(cid:173) able i.  The only restriction upon the definition of a  Markov field  (Le.,  the positivity  condition)  is  that  the  probability  of every  realization  of the field  is  strictly  posi(cid:173) tive.  The essential idea behind  Markov field  design is that one specifies  a potential  (energy)  function  for  every clique in  the  neighborhood graph such  that the subset  of random  variables  associated  with  that clique  obtain their  optimal values  when  that clique'S  potential function  obtains  its  minimal  value  (for  reviews  see  [1]-[2]) . 
•  Associate Professor  at University of Texas at Dallas  (www.utdallas.eduj-901den) 
908"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/442cde81694ca09a626eeddefd1b74ca-Abstract.html,Can VI Mechanisms Account for Figure-Ground and Medial Axis Effects?,Zhaoping Li,"When a  visual image consists of a figure against a background, V1  cells are physiologically observed to give higher responses to image  regions  corresponding  to  the figure  relative  to  their  responses  to  the  background.  The  medial  axis  of the figure  also  induces  rela(cid:173) tively  higher  responses  compared  to responses  to  other  locations  in the figure  (except for  the boundary between  the figure  and the  background).  Since the receptive fields  of V1  cells  are very smal(cid:173) l  compared with  the global  scale  of the figure-ground  and  medial  axis effects, it has been suggested that these effects  may be caused  by feedback from  higher visual areas.  I show how these effects can  be  accounted  for  by  V1  mechanisms  when  the  size  of the  figure  is  small  or  is  of a  certain scale.  They are  a  manifestation  of the  processes of pre-attentive segmentation which detect and highlight  the boundaries between homogeneous image regions."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html,Policy Gradient Methods for Reinforcement Learning with Function Approximation,"Richard S. Sutton, David A. McAllester, Satinder P. Singh, Yishay Mansour","Function  approximation  is  essential  to  reinforcement  learning,  but  the standard approach of approximating a  value function and deter(cid:173) mining  a  policy  from  it  has so  far  proven theoretically  intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator,  indepen(cid:173) dent of the value function,  and is  updated according to the gradient  of expected reward with respect to the policy parameters.  Williams's  REINFORCE method and actor-critic methods are examples of this  approach.  Our  main  new  result  is  to  show  that  the  gradient  can  be  written  in  a  form  suitable  for  estimation  from  experience  aided  by  an  approximate  action-value  or  advantage  function.  Using  this  result,  we  prove for  the first  time that a  version  of policy  iteration  with arbitrary differentiable function approximation is convergent to  a  locally optimal policy. 
Large applications of reinforcement learning (RL) require the use of generalizing func(cid:173) tion approximators such neural networks,  decision-trees,  or instance-based methods.  The dominant approach for the last decade has been the value-function approach,  in  which  all  function  approximation  effort  goes  into  estimating  a  value  function,  with  the action-selection policy represented implicitly as the  ""greedy""  policy with respect  to the estimated values  (e.g.,  as the policy that selects in each state the action with  highest estimated value).  The value-function approach has worked well in many appli(cid:173) cations,  but has several limitations.  First, it is oriented toward finding  deterministic  policies, whereas the optimal policy is often stochastic, selecting different actions with  specific probabilities (e.g.,  see Singh,  Jaakkola,  and Jordan,  1994).  Second,  an arbi(cid:173) trarily small change in the estimated value of an action can cause it to be, or not be,  selected.  Such discontinuous changes have been identified as a  key obstacle to estab(cid:173) lishing  convergence  assurances for  algorithms  following  the value-function approach  (Bertsekas  and Tsitsiklis,  1996).  For example,  Q-Iearning,  Sarsa,  and dynamic pro(cid:173) gramming methods have all  been shown unable to converge to any policy for  simple  MDPs  and simple  function  approximators  (Gordon,  1995,  1996;  Baird,  1995;  Tsit(cid:173) siklis and van Roy,  1996;  Bertsekas  and Tsitsiklis,  1996).  This can occur even if the  best approximation is found at each step before changing the policy,  and whether the  notion of ""best""  is in the mean-squared-error sense or the slightly different  senses of  residual-gradient,  temporal-difference,  and dynamic-programming methods. 
In this  paper we  explore an  alternative  approach to  function  approximation  in  RL."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/4921f95baf824205e1b13f22d60357a1-Abstract.html,Lower Bounds on the Complexity of Approximating Continuous Functions by Sigmoidal Neural Networks,Michael Schmitt,"We calculate lower bounds on the size of sigmoidal neural networks  that  approximate  continuous  functions.  In  particular,  we  show  that  for  the  approximation  of  polynomials  the  network  size  has  to grow as  O((logk)1/4)  where  k  is  the degree of the polynomials.  This bound is  valid for  any input dimension, i.e.  independently of  the  number of variables.  The result  is  obtained  by  introducing a  new method employing upper bounds on the Vapnik-Chervonenkis  dimension  for  proving lower  bounds  on  the  size  of networks  that  approximate continuous functions."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/4a2ddf148c5a9c42151a529e8cbdcc06-Abstract.html,Evolving Learnable Languages,"Bradley Tonkes, Alan Blair, Janet Wiles","Recent theories suggest that language acquisition is assisted by the evolution of languages towards forms that are easily learnable. In this paper, we evolve combinatorial languages which can be learned by a recurrent neural network quickly and from relatively few ex(cid:173) amples. Additionally, we evolve languages for generalization in different ""worlds"", and for generalization from specific examples. We find that languages can be evolved to facilitate different forms of impressive generalization for a minimally biased, general pur(cid:173) pose learner. The results provide empirical support for the theory that the language itself, as well as the language environment of a learner, plays a substantial role in learning: that there is far more to language acquisition than the language acquisition device.
1
Introduction: Factors in language learnability
In exploring issues of language learnability, the special abilities of humans to learn complex languages have been much emphasized, with one dominant theory based on innate, domain-specific learning mechanisms specifically tuned to learning hu(cid:173) man languages. It has been argued that without strong constraints on the learning mechanism, the complex syntax of language could .not be learned from the sparse data that a 'child observes [1]. More recent theories challenge this claim and em(cid:173) phasize the interaction between learner and environment [~]. In addition to these two theories is the proposal that rather than ""language-savvy infants"", languages themselves adapt to human learners, and the ones that survive are ""infant-friendly languages"" [3-5]. To date, relatively few empirical studies have explored how such adaptation of language facilitates learning. Hare and Elman [6] demonstrated that
Evolving Learnable Lan~ages
67
classes of past tense forms could evolve over simulated generations in response to changes in the frequency of verbs, using neural networks. Kirby [7] showed, using a symbolic system, how compositional languages are more likely to emerge when learning is constrained to a limited set of examples. Batali [8] has evolved recurrent networks that communicate simple structured, concepts. Our argument is not that humans are general purpose learners. Rather, current research questions require exploring the nature and extent of biases that learners bring to language learning, and the ways in which languages exploit those biases [2]. Previous theories suggesting that many aspects of language were unlearnable without strong biases are graduallybrealdng down as new aspects of language are shown to be learnable with much weaker biases. Studies include the investigation of how languages may exploit biases as subtle as attention ~d memory limitations in children [9]. A complementary study has shown that general purpose learners can evolve biases in the form of initial starting weights that facilitate the learning of a family of recursive languages [10]..
In this paper we present an empirical paradigm for continuing the exploration of fac(cid:173) tors that contribute to language learnability. The paradigm we propose necessitates the evolution of languages comprising recursive sentences over symbolic strings (cid:173) languages whose sentences cannot be. conveyed without combinatorial composition of symbols drawn from a finite alphabet. The paradigm is not based on any specific natural language, but rather, it is the simplest task we could find to illustrate the point that languages with compositional structure can be evolved to be learnable from few sentences.. The simplicity of the communication task allows us to analyze the language and its generalizability, and highlight the nature of the generalization properties.
We start with the evolution of a recursive language that can be learned easily from five sentences by a minimally biased learner. We then address issues of robust learning of evolved languages, showing that different languages support generaliza(cid:173) tion in different ways. We also address a factor to which scant regard has been paid, namely that languages may evolve not just to their learners, but also to be easily generalizable from a specific set of concepts. It seems almost axiomatic that learning paradigms should sample randomly from the training domain. It may be that human languages are not learnable from random sentences, but are easily gen(cid:173) eralizable from just those examples that a child is likely to be exposed to in its In the third series of simulations, we test whether a language can environment. adapt to be learnable from a core ·set of concepts.
2 A paradig:m for exploring language learnability
We consider a simple language task in which two recurrent neural networks try to communicate a ""concept"" represented by a point in the unit interval, [0, 1] over a symbolic· channeL An encoder network sends a sequence of symbols (thresholded outputs) for each concept, which a decoder network receives and processes back into a concept (the framework is described in greater detail in [11]). For communication to be successful, the decoder's output should approximate the encoder's input for all concepts. The architecture for the encoder is a recurrent network with one input unit and five output units, and with recurrent connections from both the output and hidden units back to the hidden units. The encoder produces a sequence of up to five symbols (states of the output units) taken from ~ = {A, ....., J}, followed by the $ symbol, for each concept taken from .[0, 1]. To encode a value x E [0,1], the network
68
B. Tonkes, A. Blair and J. Wiles"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/4abe17a1c80cbdd2aa241b70840879de-Abstract.html,Large Margin DAGs for Multiclass Classification,"John C. Platt, Nello Cristianini, John Shawe-Taylor","We  present a  new  learning  architecture:  the  Decision  Directed  Acyclic  Graph  (DDAG),  which  is  used  to  combine  many  two-class  classifiers  into  a  multiclass  classifier.  For  an  N -class  problem,  the  DDAG  con(cid:173) tains N(N - 1)/2 classifiers, one for each pair of classes.  We present a  VC analysis of the case when the node classifiers are hyperplanes; the re(cid:173) sulting bound on the test error depends on N  and on the margin achieved  at  the  nodes,  but not  on  the  dimension  of the  space.  This motivates an  algorithm, DAGSVM, which operates in  a kernel-induced feature  space  and  uses  two-class maximal  margin  hyperplanes at each  decision-node  of the  DDAG.  The DAGSVM  is  substantially faster  to  train  and  evalu(cid:173) ate  than  either the  standard  algorithm  or Max Wins,  while maintaining  comparable accuracy to both of these algorithms."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/4f398cb9d6bc79ae567298335b51ba8a-Abstract.html,Approximate Planning in Large POMDPs via Reusable Trajectories,"Michael J. Kearns, Yishay Mansour, Andrew Y. Ng","We consider the problem of reliably choosing a near-best strategy from  a restricted class  of strategies TI  in  a partially observable Markov deci(cid:173) sion process (POMDP). We assume we are given the ability to simulate  the POMDP,  and study  what might be called the sample complexity - that  is,  the  amount of data one must  generate in  the  POMDP  in  order  to choose a good strategy.  We  prove upper bounds on  the sample com(cid:173) plexity  showing  that,  even  for  infinitely  large  and arbitrarily  complex  POMDPs,  the  amount  of data needed  can  be finite,  and  depends  only  linearly  on  the complexity of the  restricted strategy class  TI,  and  expo(cid:173) nentially  on  the horizon time.  This latter dependence can be eased in  a  variety  of ways,  including the  application  of gradient and  local  search  algorithms.  Our measure of complexity generalizes the classical super(cid:173) vised  learning notion  of VC  dimension to the settings  of reinforcement  learning and planning."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/4fa53be91b4933d536748a60458b9797-Abstract.html,Maximum Entropy Discrimination,"Tommi Jaakkola, Marina Meila, Tony Jebara","We present a general framework for discriminative estimation based  on the maximum entropy principle and its extensions.  All  calcula(cid:173) tions involve distributions over structures and/or parameters rather  than  specific  settings  and  reduce  to  relative  entropy  projections.  This holds  even  when  the data is  not  separable within  the chosen  parametric class,  in  the context of anomaly  detection  rather than  classification, or when the labels in the training set are uncertain or  incomplete.  Support  vector  machines  are  naturally subsumed  un(cid:173) der  this  class  and we  provide several extensions.  We  are also  able  to estimate exactly and efficiently discriminative distributions over  tree structures of class-conditional  models  within  this framework.  Preliminary experimental results  are  indicative of the potential in  these techniques."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/515ab26c135e92ed8bf3a594d67e4ade-Abstract.html,The Relaxed Online Maximum Margin Algorithm,"Yi Li, Philip M. Long","We  describe  a  new  incremental  algorithm  for  training  linear  thresh(cid:173) old  functions:  the  Relaxed  Online  Maximum  Margin  Algorithm,  or  ROMMA. ROMMA can be viewed as  an approximation to the algorithm  that repeatedly chooses the hyperplane that classifies previously seen ex(cid:173) amples  correctly  with  the  maximum  margin.  It  is  known  that  such  a  maximum-margin hypothesis can be computed by minimizing the length  of the weight vector subject to a number of linear constraints.  ROMMA  works  by  maintaining a relatively simple relaxation of these constraints  that can be efficiently updated.  We prove a mistake bound for ROMMA  that is the same as that proved for the perceptron algorithm.  Our analysis  implies that the more computationally intensive maximum-margin algo(cid:173) rithm also satisfies this mistake bound; this is the first worst-case perfor(cid:173) mance guarantee for this algorithm.  We describe some experiments us(cid:173) ing ROMMA and  a variant that updates its hypothesis more aggressively  as  batch  algorithms to recognize handwritten digits.  The computational  complexity  and  simplicity of these algorithms is  similar to  that of per(cid:173) ceptron algorithm , but their generalization is much better.  We describe a  sense  in  which the performance of ROMMA converges  to that of SVM  in the limit if bias isn't considered."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/52c5189391854c93e8a0e1326e56c14f-Abstract.html,Bayesian Modelling of fMRI lime Series,"Pedro A. d. F. R. Højen-Sørensen, Lars Kai Hansen, Carl Edward Rasmussen","We present a Hidden Markov Model (HMM) for inferring the hidden  psychological state (or neural activity) during single trial tMRI activa(cid:173) tion experiments with blocked task paradigms. Inference is based on  Bayesian methodology, using a combination of analytical and a variety  of Markov Chain Monte Carlo (MCMC) sampling techniques. The ad(cid:173) vantage of this method is that detection of short time learning effects be(cid:173) tween repeated trials is possible since inference is based only on single  trial experiments."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/52d080a3e172c33fd6886a37e7288491-Abstract.html,Bayesian Averaging is Well-Temperated,Lars Kai Hansen,"Bayesian predictions are stochastic just like predictions of any other  inference scheme that generalize from a finite sample.  While a sim(cid:173) ple variational argument shows  that Bayes averaging is  generaliza(cid:173) tion  optimal given  that  the  prior  matches  the  teacher  parameter  distribution the situation is  less  clear  if the  teacher  distribution is  unknown.  I define  a class of averaging procedures,  the temperated  likelihoods,  including  both  Bayes  averaging  with  a  uniform  prior  and  maximum likelihood estimation as  special  cases.  I  show  that  Bayes  is  generalization optimal in this family  for  any  teacher  dis(cid:173) tribution for  two  learning problems that are  analytically tractable:  learning the mean of a Gaussian and asymptotics of smooth learn(cid:173) ers."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/54e36c5ff5f6a1802925ca009f3ebb68-Abstract.html,Policy Search via Density Estimation,"Andrew Y. Ng, Ronald Parr, Daphne Koller","We  propose  a  new  approach  to  the  problem  of searching  a  space  of  stochastic controllers for a Markov decision process (MDP) or a partially  observable Markov decision process (POMDP). Following several other  authors,  our approach  is  based  on  searching  in  parameterized  families  of policies (for example, via gradient descent) to optimize solution qual(cid:173) ity.  However,  rather than  trying  to  estimate  the  values  and  derivatives  of a policy  directly,  we do  so  indirectly  using  estimates  for  the  proba(cid:173) bility  densities  that  the  policy  induces  on  states  at  the  different points  in  time.  This enables our algorithms to  exploit the many techniques for  efficient  and  robust approximate density  propagation  in  stochastic  sys(cid:173) tems.  We show how our techniques can  be applied both to deterministic  propagation schemes (where the MDP's dynamics are given explicitly in  compact form,)  and  to  stochastic  propagation schemes (where we  have  access only to a generative model, or simulator, of the MDP). We present  empirical results for both of these variants on complex problems."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/54f5f4071faca32ad5285fef87b78646-Abstract.html,Low Power Wireless Communication via Reinforcement Learning,Timothy X. Brown,This paper examines the application of reinforcement learning to a wire(cid:173) less  communication problem.  The problem requires  that channel  util(cid:173) ity  be  maximized while simultaneously minimizing battery  usage.  We  present a  solution  to  this  multi-criteria problem  that  is  able  to  signifi(cid:173) cantly reduce power consumption.  The solution uses a variable discount  factor to capture the effects of battery usage.
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/5a142a55461d5fef016acfb927fee0bd-Abstract.html,Learning to Parse Images,"Geoffrey E. Hinton, Zoubin Ghahramani, Yee Whye Teh","We  describe a class of probabilistic models  that we  call credibility  networks.  Using  parse trees as  internal representations of images,  credibility  networks  are able  to  perform  segmentation  and  recog(cid:173) nition simultaneously,  removing the need for  ad hoc  segmentation  heuristics.  Promising results  in  the  problem of segmenting  hand(cid:173) written digits  were obtained. 
1"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/5cf21ce30208cfffaa832c6e44bb567d-Abstract.html,Robust Recognition of Noisy and Superimposed Patterns via Selective Attention,"Soo-Young Lee, Michael Mozer","In many classification tasks, recognition accuracy is low because input  patterns  are  corrupted  by  noise  or  are  spatially  or  temporally  overlapping. We propose an approach to overcoming these  limitations  based  on  a  model  of human  selective  attention.  The  model,  an  early  selection filter guided by top-down attentional control, entertains each  candidate  output  class  in  sequence  and  adjusts  attentional  gain  coefficients  in  order to produce a strong response  for that class.  The  chosen class is then the one that  obtains the strongest response with the  least  modulation  of  attention.  We  present  simulation  results  on  classification of corrupted and superimposed handwritten digit patterns,  showing a significant improvement in recognition rates.  The algorithm  has  also  been  applied  in  the  domain  of  speech  recognition,  with  comparable results."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/5d79099fcdf499f12b79770834c0164a-Abstract.html,Bayesian Network Induction via Local Neighborhoods,"Dimitris Margaritis, Sebastian Thrun","In  recent  years,  Bayesian  networks  have  become  highly  successful  tool  for  di(cid:173) agnosis,  analysis,  and  decision  making  in  real-world domains.  We  present  an  efficient algorithm for  learning Bayes  networks from  data.  Our approach  con(cid:173) structs Bayesian networks by first identifying each node's Markov blankets, then  connecting nodes  in  a maximally consistent way.  In  contrast to the majority of  work, which typically uses hill-climbing approaches that may produce dense and  causally incorrect nets, our approach yields much more compact causal networks  by heeding independencies in the data.  Compact causal networks facilitate fast in(cid:173) ference and are also easier to understand. We prove that under mild assumptions,  our approach requires time polynomial in the size of the data and the number of  nodes.  A randomized variant,  also presented here,  yields comparable results at  much higher speeds."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/62889e73828c756c961c5a6d6c01a463-Abstract.html,Spiking Boltzmann Machines,"Geoffrey E. Hinton, Andrew D. Brown","We first show how to represent sharp posterior probability distribu(cid:173) tions using real valued coefficients on broadly-tuned basis functions.  Then we  show how  the precise times of spikes can be used to con(cid:173) vey the real-valued  coefficients on  the basis functions  quickly and  accurately.  Finally we  describe a  simple  simulation in which spik(cid:173) ing neurons learn to model an image sequence by fitting a dynamic  generative model. 
1  Population codes and  energy landscapes 
A perceived object is represented in the brain by the activities of many neurons, but  there is no general consensus on how the activities of individual neurons combine to  represent the multiple properties of an object.  We start by focussing on the case of  a single object that has multiple instantiation parameters such as position, velocity,  size and orientation.  We assume that each neuron has an ideal stimulus in the space  of instantiation parameters and that its activation rate or probability of activation  falls off monotonically in all directions as the actual stimulus departs from this ideal.  The semantic problem is  to define  exactly what instantiation parameters are being  represented when  the activities of many such neurons are specified. 
Hinton,  Rumelhart and  McClelland  (1986)  consider  binary neurons with  receptive  fields  that  are  convex  in  instantiation  space.  They  assume  that  when  an  object  is  present  it  activates  all  of the neurons  in whose  receptive  fields  its  instantiation  parameters  lie.  Consequently,  if it  is  known  that  only  one  object  is  present,  the  parameter  values  of the  object  must  lie  within  the  feasible  region  formed  by  the  intersection of the receptive fields  of the active neurons.  This will  be called a  con(cid:173) junctive  distributed  representation.  Assuming  that  each  receptive  field  occupies  only  a  small  fraction  of the  whole  space,  an  interesting  property  of this  type  of  ""coarse coding""  is that the bigger the receptive fields,  the more accurate the repre(cid:173) sentation.  However,  large receptive fields  lead to a  loss  of resolution when  several  objects are present simultaneously. 
When the sensory input  is  noisy,  it  is  impossible  to infer  the exact  parameters of  objects so it  makes sense  for  a  perceptual system to represent  the probability dis(cid:173) tribution  across  parameters  rather  than  just  a  single  best  estimate  or  a  feasible  region.  The full  probability distribution is  essential for  correctly  combining  infor-
Spiking Boltzmann Machines"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html,Actor-Critic Algorithms,"Vijay R. Konda, John N. Tsitsiklis",We  propose  and  analyze  a  class  of  actor-critic  algorithms  for  simulation-based  optimization  of  a  Markov  decision  process  over  a  parameterized  family  of randomized  stationary  policies.  These  are two-time-scale  algorithms in  which  the critic uses TD learning  with  a  linear approximation architecture and the actor is  updated  in  an  approximate  gradient  direction  based  on  information  pro(cid:173) vided by the critic.  We  show that the features for  the critic should  span a subspace prescribed by the choice of parameterization of the  actor.  We  conclude by discussing convergence properties and some  open problems.
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/647c722bf90a49140184672e0d3723e3-Abstract.html,Training Data Selection for Optimal Generalization in Trigonometric Polynomial Networks,"Masashi Sugiyama, Hidemitsu Ogawa","In this  paper,  we  consider the problem of active learning  in trigonomet(cid:173) ric  polynomial  networks  and  give  a  necessary  and sufficient  condition of  sample  points to  provide the  optimal generalization capability.  By ana(cid:173) lyzing the condition from  the functional analytic point of view,  we  clarify  the  mechanism  of  achieving  the  optimal  generalization  capability.  We  also  show  that  a  set  of  training  examples  satisfying  the  condition  does  not only provide the optimal generalization  but also  reduces the compu(cid:173) tational  complexity and memory required  for  the  calculation of learning  results.  Finally, examples  of sample  points  satisfying  the  condition  are  given and computer simulations are performed to demonstrate the effec(cid:173) tiveness of the proposed  active learning method."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/64f1f27bf1b4ec22924fd0acb550c235-Abstract.html,Image Recognition in Context: Application to Microscopic Urinalysis,"Xubo B. Song, Joseph Sill, Yaser S. Abu-Mostafa, Harvey Kasdan","We  propose a  new  and efficient technique for  incorporating contextual  information into object classification. Most of the current techniques face  the problem of exponential computation cost.  In this paper, we propose a  new general framework that incorporates partial context at a linear cost.  This technique  is  applied  to  microscopic  urinalysis  image  recognition,  resulting in a significant improvement of recognition rate over the context  free approach. This gain would have been impossible using conventional  context incorporation techniques. 
1  BACKGROUND: RECOGNITION IN CONTEXT 
There are a number of pattern recognition problem domains where the classification of an  object should be based on more than simply the appearance of the object itself.  In remote  sensing image classification, where each pixel is part of ground cover, a pixel is more like(cid:173) ly  to  be a glacier if it is  in  a mountainous area, than  if surrounded by  pixels of residential  areas.  In text analysis, one can expect to find certain letters occurring regularly in  particu(cid:173) lar arrangement with  other letters(qu, ee,est, tion,  etc.).  The information conveyed by  the  accompanying entities is referred to as contextual information.  Human experts apply con(cid:173) textual information in their decision making [2][ 6].  It makes sense to design techniques and  algorithms to make computers aggregate and utilize a more complete set of information in  their decision making the way human experts do.  In  pattern recognition systems, however, 
*Author for correspondence 
964 
X  B.  Song, J  Sill,  Y.  Abu-Mostafa and H.  Kasdan 
the primary (and often only) source of information used to identify an object is the set of  measurements, or features, associated with the object itself.  Augmenting this information  by incorporating context into the classification process can yield significant benefits. 
i  =  1, ... N.  With  each  object  we  associate  a  Consider  a  set  of  N  objects  Ti ,  class  label  Ci  that  is  a  member  of  a  label  set  n  =  {1 , ... , D} .  Each  object  Ti  is  characterized  by  a  set  of  measurements  Xi  E  R P,  which  we  call  a  feature  vec(cid:173) tor.  Many  techniques  [1][2][4J[6}  incorporate  context  by  conditioning  the  posterior  probability  of objects'  identities  on  the joint features  of all  accompanying objects.  i.e .•  P(Cl, C2,··· , cNlxl , . . . , XN). and then maximizing it with respectto Cl, C2, . .. , CN . It can  be  shown  thatp(cl,c2, . . . ,cNlxl, . . . ,xN)  ex  p(cllxl) ... p(CNlxN)  (~ci""""'(N\ given  certain reasonable assumptions."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/66be31e4c40d676991f2405aaecc6934-Abstract.html,Bayesian Map Learning in Dynamic Environments,Kevin P. Murphy,"We consider the problem of learning a grid-based map using a robot  with noisy sensors and actuators. We compare two approaches:  online EM, where the map is treated as a fixed parameter, and  Bayesian inference, where the map is a (matrix-valued) random  variable. We show that even on a very simple example, online EM  can get stuck in local minima, which causes the robot to get ""lost""  and the resulting map to be useless. By contrast, the Bayesian  approach, by maintaining multiple hypotheses, is much more ro(cid:173) bust. We then introduce a method for approximating the Bayesian  solution, called Rao-Blackwellised particle filtering. We show that  this approximation, when coupled with an active learning strategy,  is fast but accurate."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/6709e8d64a5f47269ed5cea9f625f7ab-Abstract.html,Better Generative Models for Sequential Data Problems: Bidirectional Recurrent Mixture Density Networks,Mike Schuster,"This  paper  describes  bidirectional  recurrent  mixture  density  net(cid:173) works,  which  can  model  multi-modal  distributions  of  the  type  P(Xt Iyf)  and  P(Xt lXI, X2 , ... ,Xt-l, yf) without  any  explicit  as(cid:173) sumptions  about  the  use  of context .  These  expressions  occur  fre(cid:173) quently  in  pattern  recognition  problems  with  sequential  data,  for  example  in  speech  recognition.  Experiments  show  that  the  pro(cid:173) posed generative models give  a higher likelihood on test data com(cid:173) pared to a  traditional modeling approach, indicating that they can  summarize  the statistical  properties of the data better."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/673271cc47c1a4e77f57e239ed4d28a7-Abstract.html,Statistical Dynamics of Batch Learning,"Song Li, K. Y. Michael Wong","An important issue in neural computing concerns the description of  learning dynamics  with  macroscopic  dynamical  variables.  Recen(cid:173) t  progress on  on-line learning only  addresses the often unrealistic  case of an infinite  training set.  We  introduce a  new  framework  to  model batch learning of restricted sets of examples, widely applica(cid:173) ble to  any learning cost function,  and fully  taking into account the  temporal correlations introduced by the recycling of the examples.  For  illustration  we  analyze  the  effects  of weight  decay  and  early  stopping during the learning of teacher-generated examples."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/6a5dfac4be1502501489fc0f5a24b667-Abstract.html,Scale Mixtures of Gaussians and the Statistics of Natural Images,"Martin J. Wainwright, Eero P. Simoncelli","The statistics of photographic images, when represented using  multiscale (wavelet) bases, exhibit two striking types of non(cid:173) Gaussian behavior. First, the marginal densities of the coefficients  have extended heavy tails. Second, the joint densities exhibit vari(cid:173) ance dependencies not captured by second-order models. We ex(cid:173) amine properties of the class of Gaussian scale mixtures, and show  that these densities can accurately characterize both the marginal  and joint distributions of natural image wavelet coefficients. This  class of model suggests a Markov structure, in which wavelet coeffi(cid:173) cients are linked by hidden scaling variables corresponding to local  image structure. We derive an estimator for these hidden variables,  and show that a nonlinear ""normalization"" procedure can be used  to Gaussianize the coefficients. 
Recent years have witnessed a surge of interest in modeling the statistics of natural  images. Such models are important for applications in image processing and com(cid:173) puter vision, where many techniques rely (either implicitly or explicitly) on a prior  density. A number of empirical studies have demonstrated that the power spectra  of natural images follow a 1/ f'Y law in radial frequency, where the exponent ""f is  typically close to two [e.g., 1]. Such second-order characterization is inadequate,  however, because images usually exhibit highly non-Gaussian behavior. For in(cid:173) stance, the marginals of wavelet coefficients typically have much heavier tails than  a Gaussian [2]. Furthermore, despite being approximately decorrelated (as sug(cid:173) gested by theoretical analysis of 1/ f processes [3]), orthonormal wavelet coefficients  exhibit striking forms of statistical dependency [4, 5]. In particular, the standard  deviation of a wavelet coefficient typically scales with the absolute values of its  neighbors [5]. 
A number of researchers have modeled the marginal distributions of wavelet coef(cid:173) ficients with generalized Laplacians, py(y) ex exp( -Iy/ AlP) [e.g. 6, 7, 8]. Special  cases include the Gaussian (p = 2) and the Laplacian (p = 1), but appropriate ex- Research supported by NSERC 1969 fellowship 160833 to MJW, and NSF CAREER grant  MIP-9796040 to EPS. 
856 
M J Wainwright and E. P. Simoncelli"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/6a81681a7af700c6385d36577ebec359-Abstract.html,Independent Factor Analysis with Temporally Structured Sources,Hagai Attias,"We  present  a  new  technique  for  time  series  analysis  based on  dy(cid:173) namic probabilistic networks.  In this approach,  the observed data  are modeled in terms of unobserved, mutually independent factors,  as in the recently introduced technique of Independent Factor Anal(cid:173) ysis  (IFA).  However,  unlike  in  IFA,  the  factors  are not  Li.d.;  each  factor has its own temporal statistical characteristics.  We  derive a  family of EM  algorithms that learn the structure of the underlying  factors  and  their  relation  to  the  data.  These  algorithms  perform  source separation and noise reduction in an integrated manner, and  demonstrate superior performance compared to IFA."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/6e62a992c676f611616097dbea8ea030-Abstract.html,Managing Uncertainty in Cue Combination,"Zhiyong Yang, Richard S. Zemel","We develop a hierarchical generative model to study cue combi(cid:173) nation.  The model maps a global shape parameter to local cue(cid:173) specific  parameters,  which in tum generate  an intensity image.  Inferring shape from images is achieved by inverting this model.  Inference produces a probability distribution at each level; using  distributions rather than a single value of underlying variables at  each stage preserves information about the validity  of each local  cue for the given image.  This allows the model, unlike standard  combination models, to adaptively weight each cue based on gen(cid:173) eral  cue  reliability  and  specific  image context.  We  describe  the  results  of a cue combination psychophysics experiment we con(cid:173) ducted that allows a direct comparison with the model. The model  provides a good fit to our data and a natural account for some in(cid:173) teresting aspects of cue combination. 
Understanding cue  combination is  a  fundamental  step in developing  computa(cid:173) tional models of visual perception, because many aspects of perception naturally  involve multiple cues, such as binocular stereo, motion, texture, and shading. It is  often formulated as a problem of inferring or estimating some relevant parameter,  e.g., depth, shape, position, by combining estimates from individual cues.  An important finding  of psychophysical studies of cue combination is that cues  vary in the degree to which they are used in different visual environments. Weights  assigned to estimates derived from a particular cue seem to reflect  its estimated  reliability  in  the  current  scene  and  viewing  conditions.  For  example,  motion  and stereo are  weighted approximately equally at near distances, but motion is  weighted more at far distances,  presumably due to distance limits on binocular  disparity.3  Experiments have also found these weightings sensitive to image ma(cid:173) nipulations; if a cue is weakened, such as by adding noise, then the uncontami(cid:173) nated cue is utilized more in making depth judgments.9 A recent study2 has shown  that observers can adjust the weighting they assign to a cue based on its relative  utility for a particular task. From these and other experiments, we can identify two  types of information that determine relative cue weightings:  (1) cue reliability:  its  relative utility in the context of the task and general viewing conditions; and (2)  region informativeness:  cue information available locally in a given image.  A central question in computational models of cue combination then concerns how  these forms of uncertainty can be combined. We propose a hierarchical generative 
870 
Z.  Yang and R.  S.  Zemel 
model. Generative models have a rich history in cue combination, as thel underlie  models of Bayesian perception that have been developed in this area. lO ,  The nov(cid:173) elty in the generative model proposed here lies in its hierarchical nature and use  of distributions throughout, which allows for both context-dependent and image(cid:173) specific uncertainty to be combined in a principled manner.  Our aims in this paper are dual: to develop a combination model that incorporates  cue reliability and region informativeness (estimated across and within images),  and to use this model to account for data and provide predictions for psychophys(cid:173) ical experiments. Another motivation for the approach here stems from our recent  probabilistic framework,11  which posits that every step of processing entails  the  representation of an entire probability distribution, rather than just a single value  of the relevant underlying variable(s).  Here we use separate local probability dis(cid:173) tributions for each cue estimated directly from an image. Combination then entails  transforming representations and integrating distributions across both space and  cues, taking across- and within-image uncertainty into account. 
1  IMAGE GENERATION 
In this paper we study the case of combining shading and texture. Standard shape(cid:173) from-shading models exclude texture, l, 8  while standard shape-from-texture mod(cid:173) els  exclude  shading.7  Experimental  results  and  computational arguments  have  supported a strong interaction between these cues}O but no model accounting for  this interaction has yet been worked out.  The shape used in our experiments is a simple surface: 
Z = B(l - x2 ), Ixl  <= 1, Iyl  <= 1 
(1) 
where Z is the height from the xy plane.  B is the only shape parameter.  Our image formation model is a hierarchical generative model (see Figure 1). The  top layer contains the global parameter B.  The second layer contains local shad(cid:173) ing and texture parameters S, T  = {Sj, 11}, where i  indexes image regions.  The  generation of local cues from a global parameter is intended to allow local uncer(cid:173) tainties to be introduced separately into the cues.  This models specific conditions  in realistic images, such as shading uncertainty due to shadows or specularities,  and texture uncertainty when prior assumptions such as isotropicity are violated.4  Here we introduce uncertainty by adding independent local noise to the underly(cid:173) ing shape parameter; this manipulation is less realistic but easier to control."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/70ece1e1e0931919438fcfc6bd5f199c-Abstract.html,Potential Boosters?,"Nigel Duffy, David P. Helmbold","Recent  interpretations  of the  Adaboost  algorithm  view  it  as  per(cid:173) forming a gradient descent on a  potential function.  Simply chang(cid:173) ing the potential function  allows  one to create new  algorithms re(cid:173) lated  to  AdaBoost.  However,  these  new  algorithms are generally  not  known  to have  the formal  boosting property.  This  paper ex(cid:173) amines  the question  of which  potential  functions  lead  to  new  al(cid:173) gorithms that are boosters.  The two main results are general sets  of  conditions  on  the  potential;  one  set  implies  that  the  resulting  algorithm is  a  booster,  while the other implies  that the algorithm  is  not.  These conditions are applied to previously studied potential  functions , such as those used by LogitBoost and  Doom II."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/712a3c9878efeae8ff06d57432016ceb-Abstract.html,Resonance in a Stochastic Neuron Model with Delayed Interaction,"Toru Ohira, Yuzuru Sato, Jack D. Cowan","We study here a simple stochastic single neuron model with delayed  self-feedback capable of generating spike  trains.  Simulations show  that its spike trains exhibit resonant behavior between ""noise""  and  ""delay"".  In  order to gain  insight  into this  resonance,  we  simplify  the model and study a  stochastic binary element whose  transition  probability  depends  on  its  state  at  a  fixed  interval  in  the  past.  With this simplified model we  can analytically compute interspike  interval histograms, and show how the resonance between noise and  delay  arises.  The resonance  is  also  observed  when  such  elements  are coupled through delayed interaction. 
1"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html,Wiring Optimization in the Brain,"Dmitri B. Chklovskii, Charles F. Stevens","The complexity of cortical circuits may be characterized by the number  of synapses per neuron.  We study the dependence of complexity on the  fraction of the cortical volume that is made up of ""wire"" (that is, ofaxons  and dendrites),  and find  that complexity is  maximized when wire takes  up about 60% of the cortical volume.  This prediction is  in  good agree(cid:173) ment with experimental observations.  A consequence of our arguments  is that any rearrangement of neurons that takes more wire would sacrifice  computational power. 
Wiring a brain presents formidable problems because of the extremely large number of con(cid:173) nections:  a microliter of cortex contains approximately 105  neurons, 109  synapses, and 4  km ofaxons, with 60% of the cortical  volume being taken up with ""wire"", half of this  by  axons and the other half by dendrites. [ 1] Each cortical neighborhood must have exactly the  right balance of components; if too many cell bodies were present in a particular mm cube,  for example, insufficient space would remain for the axons, dendrites and synapses.  Here  we ask ""What fraction of the cortical volume should be wires (axons + dendrites)?"" We ar(cid:173) gue that physiological properties ofaxons and dendrites dictate an optimal wire fraction of  0.6, just what is actually observed. 
To calculate the optimal wire fraction, we start with a real cortical region containing a fixed  number of neurons, a mm cube, for example, and imagine perturbing it by adding or sub(cid:173) tracting synapses and the axons and dendrites needed to support them.  The rules for per(cid:173) turbing the cortical cube require that the existing circuit connections and function remain  intact (except for what may have been removed in  the perturbation), that no holes are cre(cid:173) ated, and that all added (or subtracted) synapses are typical of those present; as wire volume  is added, the volume of the cube of course increases.  The ratio of the number of synapses  per neuron in  the perturbed cortex to  that in  the real  cortex is  denoted by 8,  a  parameter  we call the relative complexity.  We  require that the volume of non-wire components (cell  bodies, blood vessels,  glia, etc) is  unchanged by  our perturbation and use 4>  to denote the  volume fraction of the perturbed cortical region that is made up of wires (axons + dendrites;  4> can vary between zero and one), with the fraction for the real brain being 4>0.  The relation  between relative complexity 8 and wire volume fraction 4>  is given by the equation (derived  in Methods)"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/7283518d47a05a09d33779a17adf1707-Abstract.html,Learning from User Feedback in Image Retrieval Systems,"Nuno Vasconcelos, Andrew Lippman","We  formulate  the  problem  of retrieving  images  from  visual  databases  as  a problem of Bayesian inference.  This leads to  natural and effective  solutions for two of the most challenging issues in the design of a retrieval  system:  providing  support  for  region-based  queries  without  requiring  prior  image  segmentation,  and  accounting  for  user-feedback  during  a  retrieval  session.  We  present  a  new  learning  algorithm  that  relies  on  belief propagation to account for both positive and negative examples of  the user's interests."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/7437d136770f5b35194cb46c1653efaa-Abstract.html,Online Independent Component Analysis with Local Learning Rate Adaptation,"Nicol N. Schraudolph, Xavier Giannakopoulos","Stochastic meta-descent (SMD) is a new technique for online adap(cid:173) tation  of local  learning  rates  in  arbitrary twice-differentiable  sys(cid:173) tems.  Like matrix momentum it uses full  second-order information  while  retaining  O(n)  computational  complexity  by  exploiting  the  efficient  computation  of Hessian-vector  products.  Here  we  apply  SMD  to independent  component  analysis,  and  employ  the  result(cid:173) ing  algorithm  for  the  blind  separation  of time-varying  mixtures.  By matching individual learning rates to the rate of change in each  source signal's  mixture coefficients,  our technique is  capable of si(cid:173) multaneously tracking sources that move at very different,  a priori  unknown speeds."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/74563ba21a90da13dacf2a73e3ddefa7-Abstract.html,A Variational Baysian Framework for Graphical Models,Hagai Attias,"This paper presents a novel practical framework for Bayesian model  averaging  and  model  selection  in  probabilistic  graphical  models.  Our approach approximates full  posterior distributions over model  parameters and structures, as well as latent variables, in an analyt(cid:173) ical  manner.  These  posteriors fall  out of a  free-form  optimization  procedure,  which  naturally  incorporates  conjugate  priors.  Unlike  in  large sample  approximations,  the posteriors are  generally  non(cid:173) Gaussian and no Hessian needs to be computed.  Predictive quanti(cid:173) ties  are obtained analytically.  The resulting algorithm generalizes  the standard Expectation Maximization algorithm, and its conver(cid:173) gence  is  guaranteed.  We  demonstrate  that  this  approach  can  be  applied  to  a  large  class  of  models  in  several  domains,  including  mixture models and source separation."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/752d25a1f8dbfb2d656bac3094bfb81c-Abstract.html,Algebraic Analysis for Non-regular Learning Machines,Sumio Watanabe,"Hierarchical learning machines are non-regular and non-identifiable  statistical models, whose true parameter sets are analytic sets with  singularities.  Using  algebraic  analysis,  we  rigorously  prove  that  the  stochastic  complexity  of  a  non-identifiable  learning  machine  (ml  - 1) log log n  + const.,  is  asymptotically  equal  to  >'1  log n  - where n is the number of training samples.  Moreover we show that  the rational  number >'1  and the integer ml  can be algorithmically  calculated  using  resolution  of singularities  in  algebraic  geometry.  Also we obtain inequalities 0 < >'1  ~ d/2 and 1  ~ ml  ~ d,  where d  is  the number of parameters."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/757f843a169cc678064d9530d12a1881-Abstract.html,Model Selection in Clustering by Uniform Convergence Bounds,"Joachim M. Buhmann, Marcus Held","Unsupervised  learning  algorithms  are  designed  to  extract  struc(cid:173) ture from  data samples.  Reliable  and robust  inference  requires  a  guarantee that extracted structures are typical for the data source,  Le.,  similar  structures  have  to be  inferred  from  a  second  sample  set of the same data source.  The overfitting phenomenon in max(cid:173) imum  entropy  based  annealing  algorithms  is  exemplarily  studied  for  a  class of histogram clustering  models.  Bernstein's  inequality  for  large deviations is used to determine the maximally achievable  approximation  quality  parameterized  by  a  minimal  temperature.  Monte Carlo simulations support the proposed model selection cri(cid:173) terion by finite  temperature annealing."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/798ed7d4ee7138d49b8828958048130a-Abstract.html,Unmixing Hyperspectral Data,"Lucas C. Parra, Clay Spence, Paul Sajda, Andreas Ziehe, Klaus-Robert Müller","In hyperspectral imagery one pixel  typically  consists of a  mixture  of the  reflectance  spectra of several  materials,  where  the  mixture  coefficients  correspond  to  the  abundances of the  constituting  ma(cid:173) terials.  We  assume linear combinations of reflectance spectra with  some additive normal sensor noise and derive a  probabilistic MAP  framework  for  analyzing  hyperspectral  data.  As  the  material re(cid:173) flectance characteristics are not know  a priori, we face  the problem  of  unsupervised  linear  unmixing.  The  incorporation  of  different  prior  information  (e.g.  positivity  and  normalization  of the  abun(cid:173) dances)  naturally  leads  to  a  family  of  interesting  algorithms,  for  example  in  the  noise-free  case  yielding  an  algorithm  that  can  be  understood as constrained independent component analysis (ICA).  Simulations underline the usefulness of our theory."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/7c4ede33a62160a19586f6e26eaefacf-Abstract.html,Some Theoretical Results Concerning the Convergence of Compositions of Regularized Linear Functions,Tong Zhang,"Recently,  sample complexity bounds have been derived for problems in(cid:173) volving linear functions such as neural networks and support vector ma(cid:173) chines.  In this paper,  we extend some theoretical results in this area by  deriving dimensional independent covering number bounds for regular(cid:173) ized  linear functions under certain regularization conditions.  We  show  that such bounds lead to a class of new methods for training linear clas(cid:173) sifiers with similar theoretical advantages of the support vector machine.  Furthermore,  we also present a theoretical analysis for these new meth(cid:173) ods from the asymptotic statistical point of view.  This technique provides  better description for large sample behaviors of these algorithms."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/7d12b66d3df6af8d429c1a357d8b9e1a-Abstract.html,Inference for the Generalization Error,"Claude Nadeau, Yoshua Bengio","In order to to compare learning algorithms, experimental results reported  in  the  machine  learning  litterature often  use  statistical  tests  of signifi(cid:173) cance.  Unfortunately,  most of these  tests  do  not take  into account the  variability  due  to  the  choice  of training  set.  We  perform  a  theoretical  investigation of the variance of the cross-validation estimate of the gen(cid:173) eralization error that takes into account the  variability due to  the choice  of training  sets.  This  allows  us  to  propose  two  new  ways  to  estimate  this variance. We show, via simulations, that these new statistics perform  well relative to the statistics considered by Dietterich (Dietterich, 1998)."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/7dd0240cd412efde8bc165e864d3644f-Abstract.html,LTD Facilitates Learning in a Noisy Environment,"Paul W. Munro, Gerardina Hernández","Long-term potentiation (LTP) has long been held as a biological  substrate for associative learning. Recently, evidence has emerged  that long-term depression (LTD) results when the presynaptic cell  fires after the postsynaptic cell. The computational utility of LTD  is explored here. Synaptic modification kernels for both LTP and  LTD have been proposed by other laboratories based studies of one  postsynaptic unit. Here, the interaction between time-dependent  LTP and LTD is studied in small networks."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/7e230522657ecdc50e4249581b861f8e-Abstract.html,An Oculo-Motor System with Multi-Chip Neuromorphic Analog VLSI Control,"Oliver Landolt, Steve Gyger","A system emulating the functionality of a moving eye-hence the name  oculo-motor system-has been built and successfully tested.  It is  made  of an optical device for shifting the field of view of an image sensor by up  to 45 ° in  any direction, four neuromorphic analog VLSI circuits imple(cid:173) menting an oculo-motor control loop, and some off-the-shelf electronics.  The custom integrated circuits communicate with each other primarily by  non-arbitrated address-event buses.  The system  implements the behav(cid:173) iors of saliency-based saccadic exploration, and smooth pursuit of light  spots.  The duration of saccades ranges from  45 ms  to  100 ms,  which  is  comparable to human eye performance. Smooth pursuit operates on light  sources moving at up to 50 0 /s in the visual field."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/7eb7eabbe9bd03c2fc99881d04da9cbd-Abstract.html,Understanding Stepwise Generalization of Support Vector Machines: a Toy Model,"Sebastian Risau-Gusman, Mirta B. Gordon","In this  article  we  study the effects  of introducing structure in the  input distribution of the data to be learnt by a  simple perceptron.  We  determine the learning curves within the framework of Statis(cid:173) tical  Mechanics.  Stepwise  generalization  occurs  as  a  function  of  the number of examples when the distribution of patterns is highly  anisotropic.  Although  extremely simple,  the model  seems  to cap(cid:173) ture  the  relevant  features  of  a  class  of  Support  Vector  Machines  which was  recently shown to present this behavior."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/7fea637fd6d02b8f0adf6f7dc36aed93-Abstract.html,A Geometric Interpretation of v-SVM Classifiers,"David J. Crisp, Christopher J. C. Burges","We show that the recently proposed variant of the Support Vector  machine  (SVM)  algorithm,  known  as  v-SVM,  can  be  interpreted  as a maximal separation between subsets of the convex hulls of the  data,  which  we  call  soft  convex  hulls.  The  soft  convex  hulls  are  controlled by  choice of the parameter v.  If the intersection of the  convex hulls is empty, the hyperplane is positioned halfway between  them such that the distance between convex hulls, measured along  the normal, is  maximized; and if it is not, the hyperplane's normal  is  similarly determined  by  the  soft  convex  hulls,  but  its  position  (perpendicular  distance  from  the  origin)  is  adjusted  to  minimize  the  error  sum.  The  proposed  geometric  interpretation of v-SVM  also leads to necessary and sufficient conditions for the existence of  a choice of v  for  which  the v-SVM solution is nontrivial."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/81c650caac28cdefce4de5ddc18befa0-Abstract.html,Robust Learning of Chaotic Attractors,"Rembrandt Bakker, Jaap C. Schouten, Marc-Olivier Coppens, Floris Takens, C. Lee Giles, Cor M. van den Bleek","A fundamental problem with the modeling of chaotic time series data is that  minimizing short-term prediction errors does not guarantee a match  between the reconstructed attractors of model and experiments. We  introduce a modeling paradigm that simultaneously learns to short-tenn  predict and to locate the outlines of the attractor by a new way of nonlinear  principal component analysis. Closed-loop predictions are constrained to  stay within these outlines, to prevent divergence from the attractor. Learning  is exceptionally fast: parameter estimation for the 1000 sample laser data  from the 1991 Santa Fe time series competition took less than a minute on  a 166 MHz Pentium PC."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/8303a79b1e19a194f1875981be5bdb6f-Abstract.html,Greedy Importance Sampling,Dale Schuurmans,"I present a simple variation of importance sampling that explicitly search(cid:173) es for  important regions in the target distribution.  I prove that the tech(cid:173) nique yields unbiased estimates,  and show empirically it can reduce the  variance  of standard Monte Carlo estimators.  This is achieved  by con(cid:173) centrating samples in more significant regions of the sample space."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/831c2f88a604a07ca94314b56a4921b8-Abstract.html,Recurrent Cortical Competition: Strengthen or Weaken?,"Péter Adorján, Lars Schwabe, Christian Piepenbrock, Klaus Obermayer","We investigate the short term .dynamics of the recurrent competition and  neural  activity in the primary visual cortex in terms of information pro(cid:173) cessing and in the context of orientation selectivity.  We propose that af(cid:173) ter stimulus onset, the strength of the recurrent excitation decreases due  to  fast  synaptic depression.  As  a consequence,  the network shifts from  an  initially highly  nonlinear to a  more  linear operating  regime.  Sharp  orientation tuning is established in the first highly competitive phase.  In  the second and less competitive phase, precise signaling of multiple ori(cid:173) entations and long range modulation, e.g.,  by intra- and inter-areal con(cid:173) nections becomes possible (surround effects).  Thus the network first ex(cid:173) tracts  the  salient  features  from  the  stimulus,  and  then  starts  to  process  the details.  We  show  that  this  signal  processing  strategy  is  optimal  if  the neurons have limited bandwidth and their objective is to transmit the  maximum amount of information in any time interval beginning with the  stimulus onset."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/84c6494d30851c63a55cdb8cb047fadd-Abstract.html,Constrained Hidden Markov Models,Sam T. Roweis,"By  thinking  of each  state in  a hidden Markov  model  as  corresponding  to  some  spatial region of a fictitious topology space it is possible to naturally define neigh(cid:173) bouring states as  those which are connected in that space.  The transition matrix  can then be constrained to allow transitions only between neighbours; this means  that all valid state sequences correspond to connected paths in the topology space.  I show  how  such constrained HMMs  can learn to  discover underlying  structure  in complex sequences  of high dimensional data,  and apply  them  to  the problem  of recovering mouth movements from acoustics in continuous speech. 
1  Latent variable models for structured sequence data  Structured time-series are generated by systems whose underlying state variables change in  a continuous way but whose state to output mappings are highly nonlinear, many to one and  not smooth.  Probabilistic unsupervised learning for such sequences requires models with  two  essential features:  latent (hidden)  variables and topology in those variables.  Hidden  Markov models  (HMMs)  can be thought of as  dynamic  generalizations of discrete  state  static data models such as Gaussian mixtures, or as discrete state versions of linear dynam(cid:173) ical  systems  (LDSs)  (which are themselves dynamic generalizations of continuous latent  variable models such as factor analysis). While both HMMs and LDSs provide probabilistic  latent variable models for time-series, both have important limitations.  Traditional HMMs  have a very powerful model of the relationship between the underlying state and the associ(cid:173) ated observations because each state stores a private distribution over the output variables.  This means that any change in the hidden state can cause arbitrarily complex changes in the  output distribution.  However, it is  extremely difficult to  capture reasonable dynamics on  the discrete latent variable because in principle any state is reachable from any other state at  any time step and the next state depends only on the current state. LDSs, on the other hand,  have  an  extremely impoverished representation of the outputs as  a  function of the latent  variables since this transformation is restricted to be global and linear.  But it is somewhat  easier to capture state dynamics since the state is a multidimensional vector of continuous  variables  on which  a  matrix  ""flow"" is  acting;  this  enforces some continuity of the  latent  variables across time.  Constrained hidden Markov  models address the modeling of state  dynamics by building some topology into  the hidden state representation.  The essential  idea is to constrain the transition parameters of a conventional HMM so that the discrete(cid:173) valued hidden state evolves in a structured way.l  In particular, below I consider parameter  restrictions  which  constrain  the  state  to  evolve  as  a  discretized  version  of a  continuous  multivariate variable,  i.e.  so  that it  inscribes only  connected  paths  in  some  space.  This  lends a physical interpretation to the discrete state trajectories in an HMM. 
I A standard trick in traditional  speech applications  of HMMs  is  to  use  ""left-to-right"" transition 
matrices which are a special case of the type of constraints investigated in this paper.  However,  left(cid:173) to-right (Bakis) HMMs force state trajectories that are inherently one-dimensional and uni-directional  whereas here I also consider higher dimensional topology and free omni-directional motion. 
Constrained Hidden Markov Models 
783 
2  An illustrative game  Consider playing the following game:  divide a sheet of paper into several contiguous, non(cid:173) overlapping regions which between them cover it entirely.  In each region inscribe a symbol,  allowing symbols to be repeated in different regions. Place a pencil on the sheet and move it  around, reading out (in order) the symbols in the regions through which it passes.  Add some  noise to  the observation process  so  that some fraction  of the  time incorrect symbols  are  reported in the list instead of the correct ones. The game is to reconstruct the configuration  of regions on the sheet from only such an ordered list(s) of noisy symbols.  Of course, the  absolute scale, rotation and reflection of the sheet can never be recovered, but learning the  essential topology may be possible.2  Figure 1 illustrates this setup. 
_ _ 
1,  11,  1,  11, .. .  24(V, 21, 2, .. .  ..... 18, 19, 10,3, .. ."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/84f0f20482cde7e5eacaf7364a643d33-Abstract.html,Approximate Inference A lgorithms for Two-Layer Bayesian Networks,"Andrew Y. Ng, Michael I. Jordan","We  present  a  class  of approximate  inference  algorithms  for  graphical  models  of the  QMR-DT type.  We  give  convergence rates  for  these  al(cid:173) gorithms and  for  the  Jaakkola and  Jordan  (1999) algorithm,  and  verify  these  theoretical predictions empirically.  We  also present empirical re(cid:173) sults on the difficult QMR-DT network problem, obtaining performance  of the  new  algorithms roughly  comparable  to  the  Jaakkola and  Jordan  algorithm."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/851300ee84c2b80ed40f51ed26d866fc-Abstract.html,Perceptual Organization Based on Temporal Dynamics,"Xiuwen Liu, DeLiang L. Wang","A figure-ground  segregation  network is  proposed  based on a  novel  boundary  pair  representation.  Nodes  in  the  network  are  bound(cid:173) ary  segments  obtained  through  local  grouping.  Each  node  is  ex(cid:173) citatorily  coupled  with  the  neighboring  nodes  that  belong  to  the  same region, and inhibitorily coupled with the corresponding paired  node.  Gestalt grouping rules are incorporated by  modulating con(cid:173) nections.  The  status  of  a  node  represents  its  probability  being  figural  and  is  updated  according  to  a  differential  equation.  The  system solves the figure-ground segregation problem through tem(cid:173) poral  evolution.  Different  perceptual  phenomena,  such  as  modal  and  amodal completion,  virtual contours,  grouping and  shape de(cid:173) composition are then explained through local diffusion.  The system  eliminates combinatorial optimization and accounts for  many psy(cid:173) chophysical results with a  fixed  set of parameters."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/8698ff92115213ab187d31d4ee5da8ea-Abstract.html,Learning Informative Statistics: A Nonparametnic Approach,"John W. Fisher III, Alexander T. Ihler, Paul A. Viola","We discuss an information theoretic approach for categorizing and mod(cid:173) eling dynamic processes. The approach can learn a compact and informa(cid:173) tive statistic which summarizes past states to predict future observations.  Furthermore, the  uncertainty of the prediction is characterized nonpara(cid:173) metrically by a joint density over the learned statistic and present obser(cid:173) vation.  We  discuss the  application of the technique to both noise driven  dynamical systems and random processes sampled from a density which  is conditioned on the past. In the first case we show results in which both  the  dynamics of random walk and the  statistics of the  driving noise  are  captured.  In the second case we  present results in  which a summarizing  statistic  is  learned  on  noisy  random telegraph  waves  with  differing de(cid:173) pendencies on past states.  In both cases the algorithm yields a principled  approach for discriminating processes with differing dynamics and/or de(cid:173) pendencies.  The method is  grounded in  ideas  from  information theory  and nonparametric statistics."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/86d7c8a08b4aaa1bc7c599473f5dddda-Abstract.html,Rules and Similarity in Concept Learning,Joshua B. Tenenbaum,"This paper argues that two apparently distinct modes of generalizing con(cid:173) cepts - abstracting rules and computing similarity to exemplars - should  both be seen as special cases of a more general Bayesian learning frame(cid:173) work.  Bayes explains the specific workings of these two modes - which  rules are abstracted,  how similarity is measured - as  well  as  why gener(cid:173) alization should appear rule- or similarity-based in different situations.  This  analysis also suggests why  the rules/similarity distinction, even  if  not computationally fundamental,  may  still be useful at the algorithmic  level as part of a principled approximation to fully Bayesian learning."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/8725fb777f25776ffa9076e44fcfd776-Abstract.html,Support Vector Method for Novelty Detection,"Bernhard Schölkopf, Robert C. Williamson, Alex J. Smola, John Shawe-Taylor, John C. Platt","Suppose you are given some dataset drawn from an underlying probabil(cid:173) ity distribution P  and you want to estimate a ""simple"" subset S  of input  space such that the probability that a test point drawn from P lies outside  of S equals some a priori specified l/ between 0 and 1.  We  propose a method to  approach this  problem by trying to estimate a  function f  which is positive on S  and negative on the complement.  The  functional form of f  is given by a kernel expansion in terms of a poten(cid:173) tially small subset of the training data; it is regularized by controlling the  length of the weight vector in  an associated feature space.  We provide a  theoretical analysis of the statistical performance of our algorithm.  The algorithm is  a natural extension of the  support vector algorithm to  the case of unlabelled data."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/89f03f7d02720160f1b04cf5b27f5ccb-Abstract.html,Generalized Model Selection for Unsupervised Learning in High Dimensions,"Shivakumar Vaithyanathan, Byron Dom",We describe a Bayesian approach to  model  selection in unsupervised  learning  that  determines  both  the  feature  set  and  the  number  of  clusters. We then evaluate this scheme (based on marginal likelihood)  and  one  based  on  cross-validated  likelihood.  For  the  Bayesian  scheme  we  derive  a  closed-form  solution  of the  marginal  likelihood  by  assuming  appropriate  forms  of the  likelihood  function  and  prior.  Extensive  experiments  compare these  approaches  and  all  results  are  verified  by comparison against ground truth.  In  these experiments the  Bayesian scheme using our objective function gave better results than  cross-validation.
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/8b5700012be65c9da25f49408d959ca0-Abstract.html,An Improved Decomposition Algorithm for Regression Support Vector Machines,Pavel Laskov,"A  new  decomposition  algorithm  for  training  regression  Support  Vector  Machines  (SVM)  is  presented.  The  algorithm  builds  on  the  basic  principles  of decomposition  proposed  by  Osuna et.  al.,  and addresses the issue of optimal working set  selection.  The new  criteria for  testing optimality  of a  working set are derived.  Based  on these  criteria, the principle of  ""maximal inconsistency""  is  pro(cid:173) posed to form (approximately) optimal working sets.  Experimental  results show superior performance of the new algorithm in compar(cid:173) ison with traditional training of regression SVM without decompo(cid:173) sition.  Similar results have been previously reported on decomposi(cid:173) tion algorithms for  pattern recognition SVM. The new algorithm is  also applicable to advanced SVM formulations based on regression,  such as density estimation  and integral equation SVM. 
1"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/8b6a80c3cf2cbd5f967063618dc54f39-Abstract.html,An Analog VLSI Model of Periodicity Extraction,André van Schaik,"that extracts 
This paper presents an electronic system  the  periodicity of a sound. It uses three analogue VLSI building  blocks: a silicon cochlea, two inner-hair-cell circuits and two  spiking neuron chips. The silicon cochlea consists of a cascade of  filters. Because of the delay between two outputs from the silicon  cochlea, spike trains created at these outputs are synchronous only  for a narrow range of periodicities. In contrast to traditional band(cid:173) pass filters, where an increase in' selectivity has to be traded off  against a decrease in response time, the proposed system responds  quickly, independent of selectivity."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/8bb88f80d334b1869781beb89f7b73be-Abstract.html,From Coexpression to Coregulation: An Approach to Inferring Transcriptional Regulation among Gene Classes from Large-Scale Expression Data,"Eric Mjolsness, Tobias Mann, Rebecca Castaño, Barbara J. Wold","small-scale  gene 
regulation  networks 
We  provide  preliminary  evidence  that  eXlstmg  algorithms  for  inferring  from  gene  expression  data  can  be  adapted  to  large-scale  gene  expression  data  coming  from  hybridization  microarrays.  The essential  steps are  (1)  clustering  many  genes  by  their  expression  time-course  data  into  a  minimal  set  of  clusters  of  co-expressed  genes,  (2)  theoretically  modeling  the  various  conditions  under  which  the  time-courses  are  measured  using  a  continious-time  analog  recurrent  neural  network  for  the  cluster  mean  time-courses,  (3)  fitting  such  a  regulatory  model  to  the  cluster  mean  time  courses  by  simulated  annealing  with  weight  decay,  and  (4)  analysing  several  such  fits  for  commonalities  the  connection  matrices.  This  procedure  can  be  used  to  assess  the  adequacy  of existing  and  future  gene  expression  time-course  data  sets  for  determ ining  transcriptional  regulatory  relationships  such  as  coregulation . 
the  circuit  parameter  sets"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/8c01a75941549a705cf7275e41b21f0d-Abstract.html,Data Visualization and Feature Selection: New Algorithms for Nongaussian Data,"Howard Hua Yang, John Moody","Data  visualization  and  feature  selection  methods  are  proposed  based on the )oint mutual information and ICA.  The visualization  methods can find  many good 2-D  projections for  high dimensional  data interpretation,  which  cannot be easily found by  the other ex(cid:173) isting methods.  The new  variable selection  method is found  to be  better in eliminating redundancy in the inputs than other methods  based  on  simple mutual information.  The efficacy  of the  methods  is illustrated on a radar signal analysis problem to find  2-D viewing  coordinates for  data visualization and to select  inputs for  a  neural  network  classifier.  Keywords:  feature  selection,  joint mutual information,  ICA,  vi(cid:173) sualization, classification."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/8d420fa35754d1f1c19969c88780314d-Abstract.html,An Information-Theoretic Framework for Understanding Saccadic Eye Movements,"Tai Sing Lee, Stella X. Yu","In  this paper,  we  propose that information maximization can  pro(cid:173) vide  a  unified  framework  for  understanding  saccadic  eye  move(cid:173) ments.  In  this framework,  the mutual information among the cor(cid:173) tical  representations  of the  retinal  image,  the  priors  constructed  from  our  long  term  visual  experience,  and  a  dynamic  short-term  internal  representation  constructed  from  recent  saccades  provides  a  map for  guiding eye  navigation .  By  directing  the  eyes  to loca(cid:173) tions  of maximum complexity in  neuronal  ensemble  responses  at  each  step,  the  automatic saccadic  eye  movement system  greedily  collects information about the external world, while modifying the  neural  representations  in  the  process.  This  framework  attempts  to  connect  several  psychological  phenomena,  such  as  pop-out  and  inhibition of return,  to long term visual experience  and short term  working  memory.  It  also  provides  an  interesting  perspective  on  contextual computation and formation of neural  representation  in  the visual system."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/94e4451ad23909020c28b26ca3a13cb8-Abstract.html,Noisy Neural Networks and Generalizations,"Hava T. Siegelmann, Alexander Roitershtein, Asa Ben-Hur","In this paper we  define  a  probabilistic computational model which  generalizes many noisy neural network models, including the recent  work  of Maass and Sontag [5].  We identify weak  ergodicjty  as  the  mechanism responsible for  restriction  of the computational power  of  probabilistic  models  to  definite  languages,  independent  of the  characteristics  of the noise:  whether  it is  discrete  or analog,  or if  it  depends  on  the  input  or  not,  and  independent  of whether  the  variables  are  discrete  or continuous.  We  give examples of weakly  ergodic  models including  noisy  computational systems  with noise  depending on the current state and inputs, aggregate models,  and  computational systems which update in continuous time."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/955a1584af63a546588caae4d23840b3-Abstract.html,The Nonnegative Boltzmann Machine,"Oliver B. Downs, David J. C. MacKay, Daniel D. Lee","The nonnegative Boltzmann machine (NNBM) is a recurrent neural net(cid:173) work model that can describe multimodal nonnegative data.  Application  of maximum likelihood estimation to this model gives a learning rule that  is analogous to the binary Boltzmann machine. We examine the utility of  the mean field  approximation for the  NNBM,  and describe  how Monte  Carlo sampling techniques can be  used to  learn its parameters.  Reflec(cid:173) tive  slice  sampling  is  particularly well-suited  for  this  distribution,  and  can efficiently be implemented to sample the  distribution.  We  illustrate  learning of the NNBM on a transiationally invariant distribution, as well  as on a generative model for images of human faces."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/96a93ba89a5b5c6c226e49b88973f46e-Abstract.html,Boosting Algorithms as Gradient Descent,"Llew Mason, Jonathan Baxter, Peter L. Bartlett, Marcus R. Frean","We  provide an abstract characterization of boosting algorithms as  gradient  decsent  on  cost-functionals  in  an  inner-product  function  space.  We  prove convergence  of these functional-gradient-descent  algorithms under quite  weak conditions.  Following previous theo(cid:173) retical  results  bounding the generalization  performance of convex  combinations of classifiers in terms of general cost functions of the  margin,  we  present  a  new  algorithm  (DOOM  II)  for  performing a  gradient descent  optimization of such cost functions.  Experiments  on  several  data sets  from  the  UC  Irvine  repository  demonstrate  that DOOM II generally outperforms AdaBoost, especially in high  noise situations, and that the overfitting behaviour of AdaBoost is  predicted by our cost functions."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/96de2547f44254c97f5f4f1f402711c1-Abstract.html,Local Probability Propagation for Factor Analysis,Brendan J. Frey,"Ever since Pearl's probability propagation algorithm in graphs with  cycles  was  shown  to  produce excellent  results  for  error-correcting  decoding  a  few  years  ago,  we  have  been  curious  about  whether  local  probability  propagation  could  be  used  successfully  for  ma(cid:173) chine  learning.  One  of the simplest  adaptive models  is  the factor  analyzer,  which  is  a  two-layer  network  that  models  bottom  layer  sensory inputs as a  linear combination of top layer factors  plus in(cid:173) dependent  Gaussian  sensor noise.  We  show  that local probability  propagation in the factor analyzer network usually takes just a few  iterations to perform accurate inference, even in networks with 320  sensors and 80 factors.  We derive an expression for  the algorithm's  fixed  point and show that this fixed  point matches the exact solu(cid:173) tion in a variety of networks, even when the fixed point is  unstable.  We also show that this method can be used successfully to perform  inference for approximate EM and we give results on an online face  recognition task.  1  Factor analysis  A  simple way  to encode  input patterns is  to suppose that  each input  can  be  well(cid:173) approximated by a linear combination of component vectors,  where the amplitudes  of the vectors are modulated to match the input.  For a given training set,  the most  appropriate set  of component vectors  will  depend  on  how  we  expect  the  modula(cid:173) tion levels  to  behave  and how  we  measure the distance  between  the input  and its  approximation.  These  effects  can  be  captured  by  a  generative  probabilit~ model  that  specifies  a  distribution  p(z)  over  modulation  levels  z  =  (Zl, ... ,ZK)  and  a  distribution  p(xlz)  over  sensors  x  =  (Xl, ... ,XN)T  given  the  modulation  levels.  Principal component analysis,  independent component analysis and factor analysis  can be viewed as maximum likelihood learning in a model of this type, where we as(cid:173) sume that over the training set, the appropriate modulation levels  are independent  and the overall distortion is  given  by  the sum of the individual sensor distortions. 
In  factor  analysis,  the  modulation  levels  are  called  factors  and  the  distributions  have the following  form: 
p(Zk)  =  N(Zk; 0,1), 
p(z)  =  nf=lP(Zk) =  N(z; 0, I), 
p(xnl z) =  N(xn; E~=l AnkZk, 'l/Jn), 
(1)  The parameters of this model  are the factor  loading matrix A,  with elements  Ank,  and the diagonal sensor noise  covariance matrix  'It,  with  diagonal elements 'l/Jn.  A  belief network for  the factor analyzer is  shown in Fig.  1a.  The likelihood is 
p(xlz) =  n:=IP(xnlz) =  N(x; Az, 'It). 
p(x) = 1 N(z; 0, I)N(x; Az, 'It)dz = N(x; 0, AA T  + 'It),"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/973a5f0ccbc4ee3524ccf035d35b284b-Abstract.html,A MCMC Approach to Hierarchical Mixture Modelling,Christopher K. I. Williams,"There  are  many  hierarchical  clustering  algorithms  available,  but these  lack a firm  statistical basis.  Here we  set up a hierarchical probabilistic  mixture model, where data is  generated in  a hierarchical tree-structured  manner. Markov chain Monte Carlo (MCMC) methods are demonstrated  which can  be used to  sample from  the  posterior distribution  over trees  containing variable numbers of hidden units."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/97d98119037c5b8a9663cb21fb8ebf47-Abstract.html,The Infinite Gaussian Mixture Model,Carl Edward Rasmussen,"In a Bayesian mixture model it is not necessary a priori to limit the num(cid:173) ber of components to be finite.  In this paper an infinite Gaussian mixture  model is  presented which neatly sidesteps the difficult problem of find(cid:173) ing the ""right"" number of mixture components. Inference in the model is  done using an efficient parameter-free Markov Chain that relies entirely  on Gibbs sampling."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/9a4400501febb2a95e79248486a5f6d3-Abstract.html,Reconstruction of Sequential Data with Probabilistic Models and Continuity Constraints,Miguel Á. Carreira-Perpiñán,"We consider the problem of reconstructing a temporal discrete sequence  of multidimensional real vectors when part of the data is  missing, under  the  assumption  that  the  sequence  was  generated  by  a  continuous  pro(cid:173) cess.  A particular case of this problem is  multivariate regression, which  is  very difficult when  the  underlying mapping is one-to-many.  We  pro(cid:173) pose  an  algorithm  based  on  a joint probability  model  of the  variables  of interest,  implemented using a  nonlinear latent  variable  model.  Each  point  in  the  sequence  is  potentially reconstructed  as  any  of the  modes  of the conditional distribution of the missing  variables given the present  variables (computed using an exhaustive mode search in a Gaussian mix(cid:173) ture).  Mode selection is determined by  a dynamic programming search  that  minimises a geometric measure of the reconstructed sequence,  de(cid:173) rived from continuity constraints.  We illustrate the algorithm with a toy  example  and  apply  it  to  a real-world  inverse problem,  the  acoustic-to(cid:173) articulatory mapping.  The results  show  that the  algorithm outperforms  conditional mean imputation and multilayer perceptrons. 
1  Definition of the problem 
Consider  a  mobile  point  following  a  continuous trajectory  in  a  subset  of ]RD.  Imagine  that  it  is  possible to  obtain  a finite  number of measurements of the position of the point.  Suppose that these measurements are corrupted by noise and that sometimes part of, or all,  the variables are missing.  The problem considered here is to reconstruct the sequence from  the part of it which is observed.  In the particular case where the present variables and the  missing ones  are the  same for every  point,  the  problem is  one of multivariate regression.  If the  pattern  of missing  variables  is  more  general,  the  problem  is  one  of missing  data  reconstruction. 
Consider the problem of regression.  If the present variables uniquely identify the missing  ones  at every  point of the data set,  the problem can  be  adequately  solved  by  a  universal  function approximator, such as  a multilayer perceptron.  In  a probabilistic framework,  the  conditional mean of the missing variables given the present ones will minimise the average  squared reconstruction error [3].  However, if the underlying mapping is one-to-many, there  will  be  regions  in  the  space for  which  the  present variables do  not identify  uniquely  the  missing  ones.  In  this  case,  the  conditional  mean  mapping  will  fail,  since  it  will  give  a  compromise  value-an average of the correct ones.  Inverse  problems,  where the  inverse 
Probabilistic Sequential Data Reconstruction 
415 
of a  mapping is  one-to-many,  are  of this  type.  They  include the  acoustic-to-articulatory  mapping in speech [15], where different vocal tract shapes may produce the same acoustic  signal, or the robot arm problem [2], where different configurations of the joint angles may  place the hand in the same position. 
In some situations, data reconstruction is a means to some other objective, such as classifi(cid:173) cation or inference. Here, we deal solely with data reconstruction of temporally continuous  sequences according to  the squared error.  Our algorithm does  not apply for data sets that  either lack continuity (e.g. discrete variables) or have lost it (e.g.  due to undersampling or  shuffling). 
We follow a statistical learning approach:  we attempt to reconstruct the sequence by learn(cid:173) ing  the  mapping  from  a  training  set  drawn from  the  probability  distribution  of the  data,  rather than  by  solving  a  physical  model  of the  system.  Our  algorithm can  be  described  briefly as follows.  First, a joint density model of the data is learned in an unsupervised way  from  a sample of the datal .  Then, pointwise reconstruction is  achieved by  computing all  the  modes of the conditional distribution  of the  missing  variables given  the present ones  at the current point.  In principle, any of these modes is potentially a plausible reconstruc(cid:173) tion.  When  reconstructing a sequence,  we  repeat this  mode search  for  every point in  the  sequence,  and then  find  the  combination of modes that  minimises a geometric sequence  measure, using dynamic programming. The sequence measure is derived from local conti(cid:173) nuity constraints, e.g. the curve length. 
The algorithm is detailed in §2 to §4.  We illustrate it with a 2D toy problem in §5 and apply  it to  an  acoustic-to-articulatory-like problem in §6.  §7 discusses the results and compares  the approach with previous work.  Our  notation  is  as  follows.  We  represent the  observed  variables  in  vector form  as  t  =  (tl' ... , t D)  E  ~D. A data set (possibly a temporal sequence) is represented as  {t n } ~=l .  Groups of variables are represented by  sets of indices I, J  E  {I, ... , D}, so  that if I  =  {I, 7, 3}, then tI = (tlt7t3). 
2  Joint generative modelling using latent variables 
Our starting point is a joint probability model of the observed variables p( t).  From it,  we  can compute conditional distributions of the form p( t..71 tI) and, by picking representative  points,  derive a  (multivalued) mapping  tI  ~ t..7.  Thus,  contrarily  to  other approaches,  e.g.  [6],  we  adopt multiple pointwise  imputation.  In  §4  we  show how  to  obtain a single  reconstructed sequence of points. 
Although density estimation requires more parameters than mapping approximation, it has  a fundamental advantage [6]:  the density  model represents the relation  between any  vari(cid:173) ables,  which allows to  choose any  missing/present variable combination.  A mapping ap(cid:173) proximator treats asymmetrically some variables as inputs (present) and the rest as outputs  (missing) and can't easily deal with other relations. 
The existence of functional  relationships (even one-to-many) between the  observed  vari(cid:173) ables indicates that the data must span a low-dimensional manifold in the data space.  This  suggests the  use of latent variable  models for  modelling the joint density.  However,  it  is  possible to use other kinds of density models. 
In  latent variable  modelling the  assumption is  that the observed high-dimensional data t  is  generated from  an  underlying  low-dimensional process  defined  by  a  small  number L  of latent  variables  x  =  (Xl, ... , xL)  [1] .  The  latent  variables  are  mapped  by  a  fixed 
I In our examples we only use complete training data (i.e., with no missing data), but it is perfectly  possible to estimate a probability model with incomplete training data by using an EM algorithm [6]. 
416 
M  A.  Carreira-Perpiful.n 
transformation  into  a  D-dimensional  data  space  and  noise  is  added  there.  A  particular  model is specified by three parametric elements:  a prior distribution in latent space p(x), a  smooth mapping f  from  latent space to data space and a noise model in data space p(tlx).  Marginalising the joint probability density function p(t, x)  over the latent space gives the  distribution  in  data space, p(t).  Given  an  observed sample in  data space  {t n };;=l'  a pa(cid:173) rameter estimate can  be  found  by  maximising  the  log-likelihood,  typically  using  an  EM  algorithm.  We  consider the  following  latent  variable  models,  both of which  allow  easy  computation of conditional distributions of the form p( tJ It I  ): 
Factor analysis  [1], in which the mapping is linear, the prior in latent space is  unit Gaus(cid:173) sian and the noise model  is  diagonal Gaussian.  The density  in data space is  then  Gaussian with a constrained covariance matrix.  We  use  it as  a baseline for com(cid:173) parison with more sophisticated models. 
The generative topographic mapping (GTM)  [4]  is  a  nonlinear  latent  variable  model, 
where the mapping is  a generalised linear model, the prior in  latent space is  dis(cid:173) crete uniform and the noise model is isotropic Gaussian.  The density in data space  is then a constrained mixture of isotropic Gaussians. 
In  latent  variable  models  that  sample  the  latent space prior distribution  (like  GTM),  the  mixture centroids  in  data space  (associated to  the  latent space samples)  are  not trainable  parameters. We can then improve the density model at a higher computational cost with no  generalisation loss by increasing the number of mixture components. Note that the number  of components required  will  depend exponentially  on  the  intrinsic  dimensionality  of the  data (ideally coincident with that of the latent space, L) and not on the observed one, D. 
3  Exhaustive mode finding 
Given a conditional distribution p(tJltI), we  consider all  its  modes as  plausible predic(cid:173) tions for tJ.  This requires an  exhaustive mode search in  the space of t J .  For Gaussian  mixtures, we do this by  using a maximisation algorithm starting from each centroid2 ,  such  as a fixed-point iteration or gradient ascent combined with quadratic optimisation  [5].  In  the particular case where all  variables are missing, rather than performing a mode search,  we  return  as  predictions all  the  component centroids.  It  is  also  possible  to  obtain  error  bars at each mode by locally approximating the density function by  a normal distribution.  However,  if the dimensionality of tJ is  high,  the error bars become very  wide due to  the  curse of the dimensionality. 
An advantage of multiple pointwise imputation is the easy incorporation of extra constraints  on the missing variables.  Such constraints might include keeping only those modes that lie  in  an interval dependent on the present variables [8]  or discarding  low-probability (spuri(cid:173) ous) modes-which speeds up the reconstruction algorithm and may make it more robust. 
A faster way to generate representative points of p(tJltI) is simply to draw a fixed number  of samples from  it-which may also give robustness to poor density models.  However, in  practice this resulted in a higher reconstruction error. 
4  Continuity constraints and dynamic programming (D.P) search 
Application  of the  exhaustive mode  search  to  the  conditional  distribution  at every  point  of the  sequence produces  one  or more  candidate reconstructions  per point.  To  select  a 
2 Actually,  given  a  value  of tz, most  centroids  have  negligible  posterior probability  and  can  be  removed  from  the  mixture  with  practically  no  loss  of accuracy.  Thus,  a  large  number  of mixture  components may be used  without deteriorating excessively the computational efficiency. 
Probabilistic Sequential Data Reconstrnction 
417 
trajectory  factor an.  mean  dpmode"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html,Learning the Similarity of Documents: An Information-Geometric Approach to Document Retrieval and Categorization,Thomas Hofmann,"The  project  pursued  in  this  paper  is  to  develop  from  first  information-geometric  principles  a  general  method  for  learning  the  similarity  between  text  documents.  Each  individual  docu(cid:173) ment  is  modeled  as  a  memoryless  information source.  Based  on  a  latent  class  decomposition of the  term-document  matrix, a  low(cid:173) dimensional  (curved)  multinomial subfamily is  learned.  From this  model a  canonical similarity function - known as the Fisher  kernel  - is  derived.  Our  approach  can  be  applied  for  unsupervised  and  supervised  learning problems alike.  This in  particular covers inter(cid:173) esting  cases  where  both,  labeled and  unlabeled  data are  available.  Experiments in  automated indexing and text categorization verify  the advantages of the proposed  method."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/9fe97fff97f089661135d0487843108e-Abstract.html,Bayesian Reconstruction of 3D Human Motion from Single-Camera Video,"Nicholas R. Howe, Michael E. Leventon, William T. Freeman","The three-dimensional motion of humans is  underdetermined when  the  observation is  limited to a single camera, due to the inherent 3D ambi(cid:173) guity of 2D video.  We present a system that reconstructs the 3D motion  of human subjects from single-camera video, relying on prior knowledge  about  human motion,  learned  from  training  data,  to  resolve  those am(cid:173) biguities.  After initialization in  2D, the tracking and 3D reconstruction  is  automatic;  we show results  for several  video sequences.  The results  show the power of treating 3D body tracking as an inference problem."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/a0f3601dc682036423013a5d965db9aa-Abstract.html,Mixture Density Estimation,"Jonathan Q. Li, Andrew R. Barron","Gaussian mixtures (or so-called radial basis function networks)  for  density estimation provide a natural counterpart to sigmoidal neu(cid:173) ral networks for function fitting and approximation.  In both cases,  it is  possible to give  simple expressions for  the iterative improve(cid:173) ment of performance as components of the network are introduced  one at a time.  In particular, for mixture density estimation we show  that a k-component mixture estimated by maximum likelihood  (or  by an iterative likelihood improvement that we introduce) achieves  log-likelihood  within order  1/k of the log-likelihood  achievable by  any convex combination.  Consequences for  approximation and es(cid:173) timation  using  Kullback-Leibler  risk  are  also  given.  A  Minimum  Description Length principle selects the optimal number of compo(cid:173) nents k that minimizes the risk bound."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/a40511cad8383e5ae8ddd8b855d135da-Abstract.html,Information Capacity and Robustness of Stochastic Neuron Models,"Elad Schneidman, Idan Segev, Naftali Tishby","The  reliability  and  accuracy  of  spike  trains  have  been  shown  to  depend  on  the  nature  of  the  stimulus  that  the  neuron  encodes.  Adding  ion  channel  stochasticity  to  neuronal  models  results  in  a  macroscopic behavior that replicates the input-dependent reliabili(cid:173) ty and precision of real neurons.  We  calculate the amount of infor(cid:173) mation that an ion channel based stochastic Hodgkin-Huxley (HH)  neuron model can encode about a wide set of stimuli.  We show that  both the information  rate and the information  per  spike  of the s(cid:173) tochastic model are similar to the values  reported experimentally.  Moreover,  the  amount  of information  that  the  neuron  encodes  is  correlated with the amplitude of fluctuations in the input, and less  so with the average firing rate of the neuron.  We also show that for  the HH  ion  channel density,  the information capacity is  robust  to  changes  in  the density  of ion  channels  in  the  membrane,  whereas  changing  the  ratio  between  the  Na+  and  K+  ion  channels  has  a  considerable effect on the information that the neuron can encode.  Finally,  we  suggest  that neurons  may maximize  their information  capacity by appropriately balancing the density of the different ion  channels that underlie neuronal excitability."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/a51c896c9cb81ecb5a199d51ac9fc3c5-Abstract.html,Bayesian Transduction,"Thore Graepel, Ralf Herbrich, Klaus Obermayer","Transduction  is  an  inference  principle  that  takes  a  training sam(cid:173) ple  and aims at estimating the values of a function  at given points  contained in the so-called working sample as  opposed  to the whole  of input space  for  induction.  Transduction  provides  a  confidence  measure  on  single  predictions  rather  than  classifiers  - a  feature  particularly important for  risk-sensitive  applications.  The possibly  infinite number of functions  is  reduced  to a finite  number of equiv(cid:173) alence classes on the working sample.  A rigorous Bayesian analysis  reveals  that for  standard classification loss  we  cannot benefit  from  considering  more  than  one  test  point  at  a  time.  The  probability  of  the  label  of a  given  test  point  is  determined  as  the  posterior  measure of the corresponding subset  of hypothesis  space.  We  con(cid:173) sider the PAC setting of binary classification by linear discriminant  functions  (perceptrons)  in kernel space such that the probability of  labels  is  determined  by  the  volume  ratio  in  version  space.  We  suggest  to sample this  region  by  an  ergodic  billiard.  Experimen(cid:173) tal  results  on  real  world data indicate that Bayesian Transduction  compares  favourably  to  the  well-known  Support  Vector  Machine,  in  particular  if the  posterior  probability  of labellings  is  used  as  a  confidence  measure to exclude test  points of low  confidence."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/a588a6199feff5ba48402883d9b72700-Abstract.html,Constructing Heterogeneous Committees Using Input Feature Grouping: Application to Economic Forecasting,"Yuansong Liao, John E. Moody","The  committee  approach  has  been  proposed  for  reducing  model  uncertainty  and  improving  generalization  performance.  The  ad(cid:173) vantage of committees depends  on  (1)  the performance of individ(cid:173) ual members  and  (2)  the correlational structure of errors between  members.  This paper presents an input grouping technique for  de(cid:173) signing a  heterogeneous  committee.  With  this  technique,  all input  variables are first grouped based on their mutual information.  Sta(cid:173) tistically  similar  variables  are  assigned  to  the  same  group.  Each  member's  input  set  is  then  formed  by  input  variables  extracted  from different groups.  Our designed  committees have less error cor(cid:173) relation between its members, since each member observes different  input variable combinations.  The individual member's feature sets  contain less redundant information, because highly correlated vari(cid:173) ables will not be combined together.  The member feature sets con(cid:173) tain almost complete information, since each set contains a feature  from  each  information group.  An  empirical study for  a  noisy  and  nonstationary  economic  forecasting  problem  shows  that  commit(cid:173) tees constructed by our proposed technique outperform committees  formed using several existing techniques."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/a63fc8c5d915e1f1a40f40e6c7499863-Abstract.html,An Analysis of Turbo Decoding with Gaussian Densities,"Paat Rusmevichientong, Benjamin Van Roy","We  provide  an  analysis  of  the  turbo  decoding  algorithm  (TDA)  in  a  setting involving  Gaussian  densities.  In  this  context,  we  are  able  to show  that  the  algorithm  converges  and  that  - somewhat  surprisingly - though the density generated by the TDA may differ  significantly from  the desired posterior density,  the means of these  two densities  coincide. 
1"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/a941493eeea57ede8214fd77d41806bc-Abstract.html,Probabilistic Methods for Support Vector Machines,Peter Sollich,"I  describe  a  framework  for  interpreting Support  Vector  Machines  (SVMs)  as  maximum  a  posteriori  (MAP)  solutions  to  inference  problems with Gaussian Process priors.  This can provide intuitive  guidelines  for  choosing  a  'good'  SVM  kernel.  It  can  also  assign  (by  evidence  maximization)  optimal values  to parameters such  as  the noise level C which cannot be determined unambiguously from  properties of the MAP  solution alone  (such as  cross-validation er(cid:173) ror) . I illustrate this using a simple approximate expression for the  SVM  evidence.  Once  C  has  been determined,  error bars on SVM  predictions can also  be obtained. 
1  Support  Vector Machines:  A  probabilistic framework 
Support  Vector  Machines  (SVMs)  have  recently  been  the  subject  of  intense  re(cid:173) search  activity  within  the  neural  networks  community;  for  tutorial  introductions  and overviews of recent developments see  [1,  2,  3].  One of the open questions that  remains is  how  to set the 'tunable' parameters of an SVM  algorithm:  While meth(cid:173) ods for  choosing the width of the kernel function and the noise parameter C  (which  controls  how  closely  the  training  data  are  fitted)  have  been  proposed  [4,  5]  (see  also, very recently,  [6]), the effect of the overall shape of the kernel function remains  imperfectly understood [1].  Error bars  (class probabilities) for  SVM  predictions - important for safety-critical applications, for example - are also difficult to obtain.  In this paper I suggest that a probabilistic interpretation of SVMs  could be used to  tackle these problems.  It shows that the SVM  kernel defines  a  prior over functions  on the input space, avoiding the need to think in terms of high-dimensional feature  spaces.  It also allows one to define quantities such as the evidence  (likelihood) for a  set of hyperparameters (C,  kernel amplitude Ko  etc).  I give a simple approximation  to the  evidence  which  can  then  be  maximized  to set  such  hyperparameters.  The  evidence is sensitive to the values of C and Ko individually, in contrast to properties  (such  as  cross-validation  error)  of the  deterministic  solution,  which  only  depends  on the product  CKo.  It can thfrefore be used  to  assign an unambiguous  value  to  C, from  which  error bars can be derived. 
350"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/ab2b41c63853f0a651ba9fbf502b0cd8-Abstract.html,Neural System Model of Human Sound Localization,"Craig T. Jin, Simon Carlile","This paper examines the role of biological constraints in the human audi(cid:173) tory localization process.  A psychophysical and neural system modeling  approach  was  undertaken  in which  performance  comparisons between  competing  models  and  a  human  subject  explore  the  relevant  biologi(cid:173) cally plausible  ""realism  constraints"".  The  directional  acoustical  cues,  upon which sound localization is  based,  were  derived  from  the  human  subject's  head-related transfer functions  (HRTFs).  Sound stimuli  were  generated by convolving bandpass noise with the HRTFs and were pre(cid:173) sented to both the subject and the model.  The input stimuli to the model  was processed using the Auditory Image Model of cochlear processing.  The  cochlear  data  was  then  analyzed  by  a  time-delay  neural  network  which integrated temporal and spectral information to determine the spa(cid:173) tial  location  of the  sound  source.  The  combined  cochlear  model  and  neural network provided a system model of the sound localization pro(cid:173) cess.  Human-like  localization performance  was  qualitatively  achieved  for broadband and bandpass stimuli when the model architecture incor(cid:173) porated frequency division (or tonotopicity), and was trained using vari(cid:173) able bandwidth and center-frequency sounds."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/acab0116c354964a558e65bdd07ff047-Abstract.html,A Neuromorphic VLSI System for Modeling the Neural Control of Axial Locomotion,"Girish N. Patel, Edgar A. Brown, Stephen P. DeWeerth","We have developed and tested an analog/digital VLSI system that mod(cid:173) els the coordination of biological segmental oscillators underlying axial  locomotion in animals such as leeches and lampreys. In its current form  the system consists of a chain of twelve pattern generating circuits that  are capable of arbitrary contralateral inhibitory synaptic coupling. Each  pattern generating circuit is implemented with two independent silicon  Morris-Lecar neurons with a total of 32 programmable (floating-gate  based) inhibitory synapses, and an asynchronous address-event inter(cid:173) connection element that provides synaptic connectivity and implements  axonal delay. We describe and analyze the data from a set of experi(cid:173) ments exploring the system behavior in terms of synaptic coupling."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/ae614c557843b1df326cb29c57225459-Abstract.html,Graded Grammaticality in Prediction Fractal Machines,"Shan Parfitt, Peter Tiño, Georg Dorffner","We  introduce  a  novel  method  of constructing  language  models,  which  avoids some of the problems associated  with recurrent  neu(cid:173) ral networks.  The method of creating a Prediction Fractal Machine  (PFM)  [1]  is  briefly described  and some experiments are presented  which  demonstrate the suitability of PFMs for  language modeling.  PFMs  distinguish  reliably  between  minimal  pairs,  and  their  be(cid:173) havior is  consistent  with the hypothesis  [4]  that wellformedness  is  'graded' not absolute.  A discussion of their potential to offer fresh  insights into language acquisition and processing follows."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/b0f2ad44d26e1a6f244201fe0fd864d1-Abstract.html,Learning Sparse Codes with a Mixture-of-Gaussians Prior,"Bruno A. Olshausen, K. Jarrod Millman","We  describe  a  method  for  learning  an  overcomplete  set  of  basis  functions  for  the  purpose of modeling  sparse structure in images.  The  sparsity  of the  basis  function  coefficients  is  modeled  with  a  mixture-of-Gaussians  distribution.  One  Gaussian  captures  non(cid:173) active  coefficients  with  a  small-variance  distribution  centered  at  zero,  while one or more other Gaussians capture active coefficients  with a large-variance distribution.  We show that when the prior is  in such  a form,  there exist efficient  methods for  learning the basis  functions  as  well  as  the parameters of the prior.  The performance  of the  algorithm  is  demonstrated  on  a  number  of test  cases  and  also  on  natural  images.  The  basis  functions  learned  on  natural  images  are similar to those  obtained with other methods,  but the  sparse form of the coefficient distribution is much better described.  Also, since the parameters of the prior are adapted to the data, no  assumption about sparse  structure in the  images  need  be  made  a  priori,  rather it is  learned from  the data."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/b147a61c1d07c1c999560f62add6dbc7-Abstract.html,Semiparametric Approach to Multichannel Blind Deconvolution of Nonminimum Phase Systems,"Liqing Zhang, Shun-ichi Amari, Andrzej Cichocki","In  this  paper we discuss  the  semi parametric statistical  model  for  blind  deconvolution.  First we  introduce a Lie Group to  the  manifold of non(cid:173) causal  FIR  filters.  Then  blind deconvolution problem is  formulated  in  the  framework of a  semiparametric  model,  and  a  family  of estimating  functions  is  derived for  blind  deconvolution.  A  natural gradient  learn(cid:173) ing algorithm is developed for training noncausal filters.  Stability of the  natural gradient algorithm is also analyzed in this framework."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/b2531e7bb29bf22e1daae486fae3417a-Abstract.html,Application of Blind Separation of Sources to Optical Recording of Brain Activity,"Holger Schoner, Martin Stetter, Ingo Schießl, John E. W. Mayhew, Jennifer S. Lund, Niall McLoughlin, Klaus Obermayer","In the analysis of data recorded by optical imaging from intrinsic signals  (measurement of changes of light reflectance from cortical tissue) the re(cid:173) moval  of noise and  artifacts  such  as  blood  vessel  patterns  is  a  serious  problem. Often bandpass filtering is used, but the underlying assumption  that a spatial frequency  exists,  which separates  the mapping component  from  other components  (especially  the  global  signal),  is  questionable.  Here we propose alternative ways of processing optical imaging data, us(cid:173) ing blind source separation techniques based on the spatial decorre1ation  of the data.  We  first  perform  benchmarks  on  artificial  data in  order  to  select  the  way  of processing, which is  most robust with respect  to  sen(cid:173) sor noise.  We then apply it to recordings of optical imaging experiments  from  macaque primary visual cortex. We show that our BSS technique is  able  to  extract ocular dominance and orientation preference maps  from  single  condition  stacks,  for  data,  where  standard  post-processing pro(cid:173) cedures  fail.  Artifacts,  especially  blood  vessel  patterns,  can  often  be  completely removed  from  the maps.  In  summary,  our method for blind  source separation using extended spatial decorrelation is a superior tech(cid:173) nique for the analysis of optical recording data."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/b29eed44276144e4e8103a661f9a78b7-Abstract.html,Spectral Cues in Human Sound Localization,"Craig T. Jin, Anna Corderoy, Simon Carlile, André van Schaik","The  differential  contribution  of the  monaural  and  interaural  spectral  cues to human sound localization was examined using a combined psy(cid:173) chophysical  and  analytical  approach.  The  cues  to  a  sound's  location  were  correlated  on an individual basis  with  the  human  localization re(cid:173) sponses to a variety of spectrally manipulated sounds.  The spectral cues  derive from the acoustical filtering of an individual's auditory periphery  which  is  characterized by the measured head-related transfer functions  (HRTFs).  Auditory localization performance was determined in  virtual  auditory space (VAS).  Psychoacoustical experiments were conducted in  which the amplitude spectra of the sound stimulus was  varied indepen(cid:173) dentlyat each ear while preserving the normal timing cues, an impossibil(cid:173) ity in the free-field environment. Virtual auditory noise stimuli were gen(cid:173) erated over earphones for a specified target direction such that there was  a ""false"" flat  spectrum at the left eardrum.  Using the subject's HRTFs,  the sound spectrum at the right eardrum was then adjusted so that either  the  true right  monaural  spectral  cue  or the  true  interaural  spectral  cue  was preserved.  All  subjects showed systematic mislocalizations in both  the true right and true interaural spectral conditions which was absent in  their control localization performance. The analysis of the different cues  along with the subjects' localization responses suggests there are signif(cid:173) icant differences in the use of the monaural and interaural spectral cues  and that the auditory system's reliance on the spectral  cues varies with  the sound condition."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/b3b43aeeacb258365cc69cdaf42a68af-Abstract.html,Robust Full Bayesian Methods for Neural Networks,"Christophe Andrieu, João F. G. de Freitas, Arnaud Doucet","In this paper, we propose a full Bayesian model for neural networks.  This model treats the model dimension (number of neurons), model  parameters, regularisation parameters and noise parameters as ran(cid:173) dom  variables  that  need  to be estimated.  We  then  propose  a  re(cid:173) versible jump Markov chain Monte Carlo (MCMC)  method to per(cid:173) form  the necessary computations.  We  find  that the results are not  only  better than the previously  reported ones,  but  also  appear to  be  robust  with  respect  to  the  prior  specification.  Moreover,  we  present a  geometric convergence theorem for  the algorithm."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/b3bbccd6c008e727785cb81b1aa08ac5-Abstract.html,A Neurodynamical Approach to Visual Attention,"Gustavo Deco, Josef Zihl","The psychophysical evidence for ""selective attention"" originates mainly  from  visual search experiments. In this work, we formulate a hierarchi(cid:173) cal  system of interconnected modules consisting in  populations of neu(cid:173) rons  for  modeling  the  underlying  mechanisms  involved  in  selective  visual  attention.  We  demonstrate  that  our  neural  system  for  visual  search  works  across the  visual  field  in  parallel  but due  to the  different  intrinsic dynamics can show the two experimentally observed modes of  visual  attention,  namely:  the  serial  and  the  parallel  search  mode.  In  other words, neither explicit model of a focus of attention nor saliencies  maps are  used.  The  focus of attention  appears as an  emergent property  of the dynamic behavior of the system. The neural population dynamics  are  handled in  the  framework  of the  mean-field  approximation.  Conse(cid:173) quently, the whole process can be expressed as a system of coupled dif(cid:173) ferential equations."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/b59a51a3c0bf9c5228fde841714f523a-Abstract.html,Dynamics of Supervised Learning with Restricted Training Sets and Noisy Teachers,"Anthony C. C. Coolen, C. W. H. Mace","We generalize a recent formalism to describe the dynamics of supervised  learning in layered neural networks, in the regime where data recycling  is  inevitable, to the case of noisy teachers.  Our theory generates reliable  predictions for the  evolution in  time of training- and generalization er(cid:173) rors, and extends the class of mathematically solvable learning processes  in large neural networks to those situations where overfitting can occur."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/b618c3210e934362ac261db280128c22-Abstract.html,Audio Vision: Using Audio-Visual Synchrony to Locate Sounds,"John R. Hershey, Javier R. Movellan","Psychophysical and physiological evidence shows that sound local(cid:173) ization of acoustic signals is strongly influenced by  their synchrony  with visual signals.  This effect,  known as ventriloquism, is at work  when  sound  coming  from  the  side  of a  TV  set  feels  as  if it  were  coming  from  the  mouth  of the  actors.  The  ventriloquism  effect  suggests that there is  important information about sound location  encoded in  the synchrony between the audio and video signals.  In  spite  of  this  evidence,  audiovisual  synchrony  is  rarely  used  as  a  source of information  in  computer  vision  tasks.  In  this  paper  we  explore the use  of audio  visual  synchrony to locate sound sources.  We developed a system that searches for regions of the visual land(cid:173) scape that correlate highly with the acoustic signals and tags them  as  likely  to contain an acoustic  source.  We  discuss  our experience  implementing the system,  present results on a  speaker localization  task and discuss potential applications of the approach."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/b865367fc4c0845c0682bd466e6ebf4c-Abstract.html,Predictive Sequence Learning in Recurrent Neocortical Circuits,"Rajesh P. N. Rao, Terrence J. Sejnowski",Neocortical circuits are dominated by massive excitatory feedback:  more  than eighty percent of the synapses made by excitatory cortical neurons  are onto other excitatory cortical neurons.  Why is there such massive re(cid:173) current excitation in the neocortex and what is its role in cortical compu(cid:173) tation? Recent neurophysiological experiments have shown that the plas(cid:173) ticity of recurrent neocortical synapses is governed by a temporally asym(cid:173) metric Hebbian learning rule.  We  describe how such a rule may  allow  the cortex to modify recurrent synapses for prediction of input sequences.  The goal is to predict the next cortical input from the recent past based on  previous experience of similar input sequences. We show that a temporal  difference learning rule for prediction used in conjunction with dendritic  back-propagating action potentials reproduces the temporally asymmet(cid:173) ric Hebbian plasticity observed physiologically. Biophysical simulations  demonstrate that a network of cortical neurons can learn to predict mov(cid:173) ing stimuli and develop direction selective responses as a consequence of  learning. The space-time response properties of model neurons are shown  to be similar to those of direction selective cells in alert monkey VI.
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/b9f94c77652c9a76fc8a442748cd54bd-Abstract.html,Differentiating Functions of the Jacobian with Respect to the Weights,"Gary William Flake, Barak A. Pearlmutter","For many problems, the correct behavior of a model depends not only on  its input-output mapping but also on properties of its Jacobian matrix, the  matrix of partial derivatives of the model's outputs with respect to its in(cid:173) puts.  We introduce the J-prop algorithm, an efficient general method for  computing the exact partial derivatives of a variety of simple functions of  the Jacobian of a model with respect to its free parameters. The algorithm  applies to any parametrized feedforward model,  including nonlinear re(cid:173) gression, multilayer perceptrons, and radial basis function networks."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/ba1b3eba322eab5d895aa3023fe78b9c-Abstract.html,Effects of Spatial and Temporal Contiguity on the Acquisition of Spatial Information,"Thea B. Ghiselli-Crippa, Paul W. Munro","Spatial  information comes in two forms:  direct spatial information (for  example, retinal position) and indirect temporal contiguity information,  since objects encountered sequentially are in general spatially close. The  acquisition  of spatial  information  by  a  neural  network  is  investigated  here. Given a spatial layout of several objects, networks are trained on a  prediction task.  Networks using temporal sequences with no direct spa(cid:173) tial information are found to  develop internal representations that show  distances correlated with distances in the external layout.  The influence  of spatial information is analyzed by providing direct spatial information  to the system during training that is  either consistent with the layout or  inconsistent with  it.  This  approach  allows  examination of the relative  contributions of spatial and temporal contiguity."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/be3e9d3f7d70537357c67bb3f4086846-Abstract.html,Agglomerative Information Bottleneck,"Noam Slonim, Naftali Tishby","We  introduce a novel distributional clustering algorithm that max(cid:173) imizes  the  mutual  information  per  cluster  between  data and  giv(cid:173) en  categories.  This  algorithm  can  be considered  as  a  bottom  up  hard  version  of  the  recently  introduced  ""Information  Bottleneck  Method"".  The algorithm is  compared with the top-down soft  ver(cid:173) sion  of the  information  bottleneck  method  and  a  relationship  be(cid:173) tween the hard and soft results is established.  We  demonstrate the  algorithm on the 20 Newsgroups data set.  For a subset of two news(cid:173) groups  we  achieve compression  by  3  orders  of magnitudes loosing  only  10%  of the original mutual information."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/c0560792e4a3c79e62f76cbf9fb277dd-Abstract.html,Spike-based Learning Rules and Stabilization of Persistent Neural Activity,"Xiaohui Xie, H. Sebastian Seung","We  analyze  the  conditions  under  which  synaptic  learning  rules  based  on  action  potential timing can  be approximated by  learning rules  based  on  firing  rates.  In  particular, we  consider a form  of plasticity in  which  synapses depress when a presynaptic spike is followed by a postsynaptic  spike, and potentiate with the opposite temporal ordering.  Such differen(cid:173) tial anti-Hebbian plasticity can be approximated under certain conditions  by a learning rule that depends on the time derivative of the postsynaptic  firing rate.  Such a learning rule acts to stabilize persistent neural activity  patterns in recurrent neural networks."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/c0d0e461de8d0024aebcb0a7c68836df-Abstract.html,Nonlinear Discriminant Analysis Using Kernel Functions,"Volker Roth, Volker Steinhage","Fishers linear discriminant analysis  (LDA)  is  a  classical multivari(cid:173) ate technique both for  dimension reduction and classification.  The  data vectors are transformed into a low dimensional subspace such  that  the  class  centroids  are  spread  out  as  much  as  possible.  In  this subspace LDA  works  as  a  simple  prototype classifier  with lin(cid:173) ear decision  boundaries.  However,  in many applications the linear  boundaries  do  not  adequately separate the  classes.  We  present  a  nonlinear generalization of discriminant analysis that uses the ker(cid:173) nel trick of representing dot products by kernel functions.  The pre(cid:173) sented algorithm allows a simple formulation of the EM-algorithm  in terms of kernel functions which leads to a unique concept for  un(cid:173) supervised  mixture analysis,  supervised discriminant  analysis  and  semi-supervised discriminant analysis with partially unlabelled ob(cid:173) servations in feature spaces."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/c1fea270c48e8079d8ddf7d06d26ab52-Abstract.html,On Input Selection with Reversible Jump Markov Chain Monte Carlo Sampling,Peter Sykacek,"In this paper we will treat input selection for a radial basis function  (RBF) like classifier within a Bayesian framework.  We approximate  the a-posteriori distribution over both model coefficients  and input  subsets  by samples drawn with Gibbs updates and reversible jump  moves.  Using some  public  datasets,  we  compare the  classification  accuracy  of the  method with  a  conventional  ARD  scheme.  These  datasets  are  also used  to  infer the  a-posteriori  probabilities of dif(cid:173) ferent  input subsets."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/c4492cbe90fbdbf88a5aec486aa81ed5-Abstract.html,Uniqueness of the SVM Solution,"Christopher J. C. Burges, David J. Crisp","We  give  necessary  and  sufficient  conditions  for  uniqueness  of the  support vector solution for the problems of pattern recognition and  regression estimation, for a general class of cost functions.  We show  that if the solution is not unique, all support vectors are necessarily  at  bound,  and  we  give  some  simple examples of non-unique solu(cid:173) tions.  We  note that uniqueness of the primal (dual)  solution does  not necessarily imply uniqueness of the dual (primal) solution.  We  show  how to compute the threshold b when the solution is  unique,  but when  all support vectors are at bound, in which case the usual  method for  determining b does  not work."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/c59b469d724f7919b7d35514184fdc0f-Abstract.html,The Parallel Problems Server: an Interactive Tool for Large Scale Machine Learning,"Charles Lee Isbell Jr., Parry Husbands","Imagine that you wish to classify data consisting of tens of thousands of ex(cid:173) amples residing in a twenty thousand dimensional space.  How can one ap(cid:173) ply standard machine learning algorithms? We describe the Parallel Prob(cid:173) lems Server (PPServer) and MATLAB*P.  In tandem they  allow users  of  networked computers to work transparently on large data sets from within  Matlab.  This  work is motivated by the desire to  bring the many  benefits  of scientific computing algorithms and computational power to  machine  learning researchers.  We  demonstrate the usefulness  of the system on  a number of tasks.  For  example,  we perform independent components analysis on very large text  corpora consisting  of tens  of thousands of documents,  making  minimal  changes  to  the original Bell  and  Sejnowski Matlab source  (Bell  and  Se(cid:173) jnowski,  1995).  Applying ML techniques to data previously beyond their  reach leads to interesting analyses of both data and algorithms."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/cc42acc8ce334185e0193753adb6cb77-Abstract.html,Search for Information Bearing Components in Speech,"Howard Hua Yang, Hynek Hermansky","In  this  paper,  we  use  mutual information to characterize  the  dis(cid:173) tributions of phonetic  and speaker/channel  information in  a  time(cid:173) frequency  space.  The  mutual information  (MI)  between  the  pho(cid:173) netic label and one feature,  and the joint mutual information (JMI)  between the phonetic label and two or three features are estimated .  The Miller's bias formulas for  entropy and mutual information es(cid:173) timates  are  extended  to include  higher  order  terms.  The  MI  and  the  JMI for  speaker/channel  recognition  are  also  estimated.  The  results  are complementary to those for phonetic classification.  Our  results  show  how  the  phonetic  information is  locally  spread  and  how  the  speaker/channel  information  is  globally  spread  in  time  and frequency."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/cdf1035c34ec380218a8cc9a43d438f9-Abstract.html,An Oscillatory Correlation Frame work for Computational Auditory Scene Analysis,"Guy J. Brown, DeLiang L. Wang","A  neural  model  is  described  which  uses  oscillatory  correlation  to  segregate speech from  interfering sound sources. The core of the model  is  a two-layer neural  oscillator network.  A sound  stream  is  represented  by  a  synchronized  population  of oscillators,  and  different  streams  are  represented  by  desynchronized  oscillator  populations.  The  model  has  been evaluated using a corpus of speech mixed with interfering sounds,  and produces an  improvement in signal-to-noise ratio for every  mixture."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/d2cdf047a6674cef251d56544a3cf029-Abstract.html,Manifold Stochastic Dynamics for Bayesian Learning,"Mark Zlochin, Yoram Baram","We propose a new Markov Chain Monte Carlo algorithm which is a gen(cid:173) eralization of the stochastic dynamics method.  The algorithm performs  exploration of the state space using its intrinsic geometric structure, facil(cid:173) itating efficient sampling of complex distributions. Applied to Bayesian  learning in neural networks, our algorithm was found to perform at least  as well as the best state-of-the-art method while consuming considerably  less time."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/d43ab110ab2489d6b9b2caa394bf920f-Abstract.html,Bifurcation Analysis of a Silicon Neuron,"Girish N. Patel, Gennady S. Cymbalyuk, Ronald L. Calabrese, Stephen P. DeWeerth",We  have developed a VLSI  silicon  neuron  and  a corresponding  mathe(cid:173) matical  model  that  is  a two  state-variable system. We  describe the  cir(cid:173) cuit implementation and compare the  behaviors  observed in  the  silicon  neuron and the mathematical model. We also perform bifurcation analy(cid:173) sis of the mathematical model by  varying the externally applied current  and show that the behaviors exhibited by the silicon neuron under corre(cid:173) sponding  conditions  are  in  good  agreement  to  those  predicted  by  the  bifurcation analysis.
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/d860bd12ce9c026814bbdfc1c573f0f5-Abstract.html,Speech Modelling Using Subspace and EM Techniques,"Gavin Smith, João F. G. de Freitas, Tony Robinson, Mahesan Niranjan","The speech waveform can be modelled as  a piecewise-stationary linear  stochastic state space system, and its parameters can be estimated using  an  expectation-maximisation (EM)  algorithm.  One problem is  the  ini(cid:173) tialisation of the EM algorithm. Standard initialisation schemes can lead  to poor formant trajectories.  But these  trajectories however are impor(cid:173) tant for  vowel  intelligibility.  The aim of this paper is to investigate the  suitability of subspace identification methods to initialise EM.  The  paper  compares  the  subspace  state  space  system  identification  (4SID) method with the EM algorithm.  The 4SID and EM methods are  similar in that they both estimate a state sequence (but using Kalman fil(cid:173) ters and Kalman smoothers respectively),  and then estimate parameters  (but using least-squares and maximum likelihood respectively). The sim(cid:173) ilarity of 4SID and EM motivates the use of 4SID to initialise EM. Also,  4SID is non-iterative and requires no initialisation, whereas EM is itera(cid:173) tive and requires initialisation.  However 4SID is sub-optimal compared  to EM in a probabilistic sense. During experiments on real speech, 4SID  methods compare favourably with conventional initialisation techniques.  They produce smoother formant trajectories, have greater frequency res(cid:173) olution, and produce higher likelihoods. 
1  Work done while in Cambridge Engineering Dept., UK. 
Speech Modelling Using Subspace and EM Techniques 
797"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/d8d31bd778da8bdd536187c36e48892b-Abstract.html,Optimal Kernel Shapes for Local Linear Regression,"Dirk Ormoneit, Trevor Hastie","Local linear regression performs very well in many low-dimensional  forecasting problems.  In high-dimensional spaces, its performance  typically  decays  due  to the  well-known  ""curse-of-dimensionality"".  A possible way to approach this problem is  by varying the  ""shape""  of the weighting kernel.  In this work we suggest a new, data-driven  method to estimating the optimal  kernel  shape.  Experiments  us(cid:173) ing an artificially generated data set and data from  the UC  Irvine  repository show the benefits of kernel shaping."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/db29450c3f5e97f97846693611f98c15-Abstract.html,Robust Neural Network Regression for Offline and Online Learning,"Thomas Briegel, Volker Tresp","We  replace  the  commonly  used  Gaussian  noise  model  in  nonlinear  regression  by  a  more  flexible  noise  model  based  on  the  Student-t(cid:173) distribution.  The degrees of freedom of the t-distribution can be  chosen  such that as  special cases either the Gaussian distribution or the Cauchy  distribution are  realized.  The  latter is  commonly used in  robust regres(cid:173) sion.  Since the t-distribution can be  interpreted as being an infinite mix(cid:173) ture of Gaussians,  parameters and  hyperparameters such  as  the  degrees  of freedom of the t-distribution can be learned from the data based on an  EM-learning algorithm.  We  show that modeling using the  t-distribution  leads  to  improved  predictors  on  real  world  data  sets.  In  particular,  if  outliers are present,  the  t-distribution  is  superior to  the  Gaussian  noise  model.  In  effect, by  adapting  the  degrees of freedom,  the  system  can  ""learn"" to distinguish  between outliers  and  non-outliers.  Especially for  online learning tasks, one is  interested in  avoiding inappropriate weight  changes  due  to  measurement  outliers  to  maintain  stable  online  learn(cid:173) ing  capability.  We  show experimentally that using the  t-distribution  as  a noise model leads to stable online learning algorithms and outperforms  state-of-the art online learning methods like  the  extended Kalman filter  algorithm."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/db957c626a8cd7a27231adfbf51e20eb-Abstract.html,Neural Network Based Model Predictive Control,"Stephen Piche, James D. Keeler, Greg Martin, Gene Boe, Doug Johnson, Mark Gerules","Model  Predictive  Control  (MPC),  a  control  algorithm which  uses  an  optimizer  to solve  for  the optimal  control  moves  over  a  future  time horizon based upon a model of the process, has become a stan(cid:173) dard control technique  in  the process industries over  the  past  two  decades.  In  most  industrial  applications,  a  linear  dynamic  model  developed using empirical data is  used even though the process it(cid:173) self is often nonlinear.  Linear models have been used because of the  difficulty  in  developing  a  generic  nonlinear  model  from  empirical  data and  the  computational expense  often involved  in  using  non(cid:173) linear models.  In  this  paper,  we  present  a  generic neural  network  based technique for developing nonlinear dynamic models from em(cid:173) pirical  data and show that these  models  can be efficiently  used  in  a  model predictive control framework.  This nonlinear MPC based  approach has been successfully implemented in a  number of indus(cid:173) trial  applications  in  the  refining,  petrochemical,  paper  and  food  industries.  Performance of the controller on  a  nonlinear industrial  process, a  polyethylene reactor, is  presented."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/df9028fcb6b065e000ffe8a4f03eeb38-Abstract.html,Coastal Navigation with Mobile Robots,"Nicholas Roy, Sebastian Thrun","The  problem  that  we  address  in  this paper  is  how  a mobile robot can  plan  in  order  to  arrive  at  its  goal  with  minimum  uncertainty.  Traditional  motion  planning algo(cid:173) rithms often assume that a mobile robot can track its position reliably, however, in real  world situations, reliable localization may not always be feasible.  Partially Observable  Markov Decision Processes (POMDPs) provide one way to maximize the certainty of  reaching  the goal  state,  but at the cost of computational  intractability for large state  spaces.  The  method  we propose  explicitly  models  the  uncertainty of the  robot's position as  a  state  variable,  and  generates  trajectories  through  the  augmented  pose-uncertainty  space.  By  minimizing  the  positional  uncertainty  at  the  goal,  the  robot reduces  the  likelihood  it  becomes  lost.  We  demonstrate  experimentally  that  coastal  navigation  reduces the uncertainty at the goal, especially with degraded localization."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/e1696007be4eefb81b1a1d39ce48681b-Abstract.html,Boosting with Multi-Way Branching in Decision Trees,"Yishay Mansour, David A. McAllester","It  is  known  that  decision  tree  learning  can  be  viewed  as  a  form  of boosting.  However,  existing boosting theorems for  decision tree  learning allow only binary-branching trees and the generalization to  multi-branching trees  is  not immediate.  Practical decision tree al(cid:173) gorithms, such  as CART and C4.5, implement a  trade-off between  the  number  of branches  and  the  improvement  in  tree  quality  as  measured  by an index function.  Here  we  give a  boosting justifica(cid:173) tion for a particular quantitative trade-off curve.  Our main theorem  states,  in  essence,  that if we  require  an  improvement proportional  to the  log  of the  number of branches  then  top-down  greedy  con(cid:173) struction of decision trees  remains an effective boosting algorithm."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/e22dd5dabde45eda5a1a67772c8e25dd-Abstract.html,Neural Representation of Multi-Dimensional Stimuli,"Christian W. Eurich, Stefan D. Wilke, Helmut Schwegler","Spatial  information comes in two forms:  direct spatial information (for  example, retinal position) and indirect temporal contiguity information,  since objects encountered sequentially are in general spatially close. The  acquisition  of spatial  information  by  a  neural  network  is  investigated  here. Given a spatial layout of several objects, networks are trained on a  prediction task.  Networks using temporal sequences with no direct spa(cid:173) tial information are found to  develop internal representations that show  distances correlated with distances in the external layout.  The influence  of spatial information is analyzed by providing direct spatial information  to the system during training that is  either consistent with the layout or  inconsistent with  it.  This  approach  allows  examination of the relative  contributions of spatial and temporal contiguity."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/e449b9317dad920c0dd5ad0a2a2d5e49-Abstract.html,Model Selection for Support Vector Machines,"Olivier Chapelle, Vladimir Vapnik","New functionals for parameter (model) selection of Support Vector Ma(cid:173) chines are introduced based on the concepts of the span of support vec(cid:173) tors and rescaling of the feature space.  It is shown that using these func(cid:173) tionals, one can both predict the best choice of parameters of the model  and the relative quality of performance for any value of parameter."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/e4873aa9a05cc5ed839561d121516766-Abstract.html,Memory Capacity of Linear vs. Nonlinear Models of Dendritic Integration,"Panayiota Poirazi, Bartlett W. Mel","Previous biophysical modeling work showed that nonlinear interac(cid:173) tions among nearby synapses located on active dendritic trees can  provide a large boost in the memory capacity of a cell  (Mel,  1992a,  1992b).  The aim of our present work is  to quantify  this  boost by  estimating  the  capacity  of  (1)  a  neuron  model  with  passive  den(cid:173) dritic  integration  where  inputs  are  combined  linearly  across  the  entire cell  followed  by a  single  global  threshold,  and  (2)  an active  dendrite  model  in  which  a  threshold  is  applied  separately  to  the  output of each branch, and the branch subtotals are combined lin(cid:173) early.  We focus  here on the limiting case of binary-valued synaptic  weights,  and derive expressions which  measure model  capacity by  estimating the number of distinct input-output functions  available  to both neuron types.  We  show that  (1)  the application of a fixed  nonlinearity to each dendritic compartment substantially increases  the model's flexibility,  (2) for a neuron of realistic size, the capacity  of the nonlinear cell can exceed that of the same-sized linear cell by  more than an order of magnitude, and (3)  the largest capacity boost  occurs for cells with a relatively large number of dendritic subunits  of relatively  small  size.  We  validated  the analysis  by  empirically  measuring memory  capacity with  randomized two-class  classifica(cid:173) tion problems, where a stochastic delta rule was used to train both  linear and nonlinear models.  We  found  that large capacity boosts  predicted for  the nonlinear  dendritic  model  were  readily  achieved  in practice. 
-http://lnc.usc.edu 
158 
P.  Poirazi and B.  W.  Mel"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/e5a4d6bf330f23a8707bb0d6001dfbe8-Abstract.html,State Abstraction in MAXQ Hierarchical Reinforcement Learning,Thomas G. Dietterich,"Many researchers have explored methods for hierarchical reinforce(cid:173) ment  learning  (RL)  with  temporal  abstractions,  in  which  abstract  actions are defined that can perform many primitive actions before  terminating.  However, little is known about learning with state ab(cid:173) stractions, in which aspects of the state space are ignored.  In previ(cid:173) ous work, we  developed the MAXQ method for  hierarchical RL.  In  this paper, we  define five  conditions under which state abstraction  can  be  combined  with  the  MAXQ  value  function  decomposition.  We  prove  that  the  MAXQ-Q  learning  algorithm  converges  under  these conditions and show experimentally that state abstraction is  important for  the successful  application of MAXQ-Q learning."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html,Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks,"Yoshua Bengio, Samy Bengio","The curse of dimensionality is  severe when modeling high-dimensional  discrete data:  the number of possible combinations of the variables ex(cid:173) plodes exponentially.  In  this  paper we propose a  new  architecture for  modeling high-dimensional data that requires resources (parameters and  computations) that grow only at most as the square of the number of vari(cid:173) ables,  using a multi-layer neural  network to represent the joint distribu(cid:173) tion of the variables as the product of conditional distributions. The neu(cid:173) ral  network can be interpreted as  a graphical model  without hidden ran(cid:173) dom variables, but in which the conditional distributions are tied through  the hidden units. The connectivity of the neural network can be pruned by  using dependency tests between the variables. Experiments on modeling  the distribution of several discrete data sets show statistically significant  improvements over other methods such as  naive Bayes and comparable  Bayesian networks,  and show  that significant improvements can be ob(cid:173) tained by pruning the network."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html,A Generative Model for Attractor Dynamics,"Richard S. Zemel, Michael Mozer","Attractor networks, which map an input space to a discrete out(cid:173) put space, are useful for pattern completion. However, designing  a net to have a given set of attractors is notoriously tricky; training  procedures are CPU intensive and often produce spurious afuac(cid:173) tors and ill-conditioned attractor basins.  These difficulties  occur  because each connection in the network participates in the encod(cid:173) ing of multiple attractors.  We describe an alternative formulation  of attractor networks in which the encoding of knowledge is local,  not distributed. Although localist attractor networks have similar  dynamics to their distributed counterparts, they are much easier  to work with and interpret. We propose a statistical formulation of  localist attract or net dynamics, which yields a convergence proof  and a mathematical interpretation of model parameters. 
Attractor networks map an input space,  usually continuous,  to a  sparse output  space composed of a discrete set of alternatives.  Attractor networks have a long  history in neural network research.  Attractor networks are often used for pattern completion, which involves filling  in  missing,  noisy,  or incorrect features  in an input pattern.  The  initial state of the  attractor net is  typically determined by the input pattern.  Over time, the state is  drawn to  one of a  predefined set of states-the attractors.  Attractor net dynam(cid:173) ics can be described by a state trajectory (Figure 1a).  An attractor net is generally  implemented by a set of visible units whose activity represents the instantaneous  state, and optionally, a set of hidden units that assist in the computation. Attractor  dynamics arise from interactions among the units.  In most formulations of afuac(cid:173) tor nets,2,3  the dynamics can be characterized by gradient descent in an energy  landscape, allowing one to partition the output space into attractor basins.  Instead  of homogeneous attractor basins, it is often desirable to sculpt basins that depend  on the recent history of the network and the arrangement of attractors in the space.  In psychological models of human cognition, for example, priming is fundamental:  after the model visits an attractor, it should be faster to fall into the same attractor  in the near future, i.e., the attractor basin should be broadened. 1 ,6  Another property of attractor nets is key to explaining behavioral data in psycho(cid:173) logical  and neurobiological  models:  the gang  effect,  in  which  the strength  of an  attractor is influenced by other attractors in its neighborhood. Figure 1b illustrates  the gang effect:  the proximity of the two rightmost afuactors creates a deeper at(cid:173) tractor basin, so that if the input starts at the origin it will get pulled to the right. 
A  Generative Model for Attractor Dynamics 
81"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/e8d92f99edd25e2cef48eca48320a1a5-Abstract.html,An Environment Model for Nonstationary Reinforcement Learning,"Samuel P. M. Choi, Dit-Yan Yeung, Nevin Lianwen Zhang","Reinforcement learning in nonstationary environments is  generally  regarded  as  an  important  and  yet  difficult  problem.  This  paper  partially addresses the problem by formalizing a subclass of nonsta(cid:173) tionary environments.  The environment model, called hidden-mode  Markov  decision  process  (HM-MDP),  assumes that environmental  changes  are  always  confined  to  a  small  number  of hidden  modes.  A  mode  basically  indexes  a  Markov  decision  process  (MDP)  and  evolves  with  time according to a  Markov  chain.  While  HM-MDP  is  a  special  case  of partially  observable  Markov  decision  processes  (POMDP), modeling an HM-MDP environment via the more gen(cid:173) eral POMDP  model  unnecessarily increases the problem complex(cid:173) ity.  A variant of the Baum-Welch algorithm is  developed for model  learning requiring less  data and time."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/e8fd4a8a5bab2b3785d794ab51fef55c-Abstract.html,Predictive App roaches for Choosing Hyperparameters in Gaussian Processes,"S. Sundararajan, S. Sathiya Keerthi","Gaussian  Processes  are  powerful  regression  models  specified  by  parametrized mean and covariance functions.  Standard approaches  to  estimate  these  parameters  (known  by  the  name  Hyperparam(cid:173) eters)  are  Maximum  Likelihood  (ML)  and  Maximum  APosterior  (MAP)  approaches.  In this paper, we  propose and investigate pre(cid:173) dictive  approaches,  namely,  maximization  of  Geisser's  Surrogate  Predictive Probability (GPP) and minimization of mean square er(cid:173) ror with  respect to GPP  (referred to as  Geisser's  Predictive mean  square  Error  (GPE))  to  estimate  the  hyperparameters.  We  also  derive  results  for  the  standard  Cross-Validation  (CV)  error  and  make  a  comparison.  These approaches are tested on  a  number of  problems and experimental results show that these approaches are  strongly competitive to existing approaches."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/e9b73bccd1762555582b513ff9d02492-Abstract.html,Acquisition in Autoshaping,"Sham Kakade, Peter Dayan","Quantitative data on the speed with which animals acquire behav(cid:173) ioral responses during classical conditioning experiments should  provide strong constraints on models of learning.  However, most  models have simply ignored these data; the few that have attempt(cid:173) ed to address them have failed by at least an order of magnitude.  We discuss key data on the speed of acquisition, and show how to  account for them using a statistically sound model of learning, in  which differential reliabilities of stimuli playa crucial role."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/ef2a4be5473ab0b3cc286e67b1f59f44-Abstract.html,Transductive Inference for Estimating Values of Functions,"Olivier Chapelle, Vladimir Vapnik, Jason Weston","We  introduce an algorithm for  estimating the values of a  function  at a set of test points  Xe+!, ... , xl+m  given a set of training points  (XI,YI), ... ,(xe,Ye)  without  estimating  (as  an  intermediate  step)  the regression function .  We demonstrate that this direct (transduc(cid:173) ti ve)  way  for  estimating  values  of the  regression  (or  classification  in  pattern  recognition)  can  be  more  accurate  than  the  tradition(cid:173) alone  based  on  two  steps,  first  estimating  the  function  and  then  calculating the values of this function  at the  points of interest."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/f0bda020d2470f2e74990a07a607ebd9-Abstract.html,Effective Learning Requires Neuronal Remodeling of Hebbian Synapses,"Gal Chechik, Isaac Meilijson, Eytan Ruppin","This paper revisits the classical neuroscience paradigm of Hebbian  learning.  We  find  that  a  necessary  requirement  for  effective  as(cid:173) sociative  memory  learning  is  that  the  efficacies  of  the  incoming  synapses  should  be  uncorrelated.  This  requirement  is  difficult  to  achieve in  a  robust  manner by  Hebbian synaptic learning,  since it  depends on network level information.  Effective learning can yet be  obtained by a neuronal process that maintains a zero sum of the in(cid:173) coming synaptic efficacies.  This normalization drastically improves  the  memory  capacity of associative networks,  from  an  essentially  bounded capacity to one that linearly scales with the network's size.  It also enables the effective storage of patterns with heterogeneous  coding levels in a single network.  Such neuronal normalization can  be successfully carried out by activity-dependent homeostasis of the  neuron's synaptic efficacies, which was recently observed in cortical  tissue.  Thus,  our findings  strongly suggest  that  effective  associa(cid:173) tive learning with Hebbian synapses alone is  biologically implausi(cid:173) ble and that Hebbian synapses must be continuously remodeled by  neuronally-driven regulatory processes in  the brain."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/f3144cefe89a60d6a1afaf7859c5076b-Abstract.html,The Relevance Vector Machine,Michael E. Tipping,"The support vector machine  (SVM)  is  a state-of-the-art technique  for regression and classification, combining excellent generalisation  properties  with  a  sparse  kernel  representation.  However,  it  does  suffer from a number of disadvantages, notably the absence of prob(cid:173) abilistic outputs, the requirement to estimate a trade-off parameter  and the need to utilise  'Mercer' kernel functions.  In this paper we  introduce the Relevance  Vector Machine  (RVM),  a Bayesian treat(cid:173) ment  of a  generalised  linear  model  of identical  functional  form  to  the SVM.  The RVM  suffers  from  none of the above disadvantages,  and examples demonstrate that for  comparable generalisation per(cid:173) formance,  the RVM  requires dramatically fewer  kernel functions. 
1"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/f50a6c02a3fc5a3a5d4d9391f05f3efc-Abstract.html,Dual Estimation and the Unscented Transformation,"Eric A. Wan, Rudolph van der Merwe, Alex T. Nelson","Dual estimation  refers  to  the problem of simultaneously  estimating the  state of a dynamic system and the model which gives rise  to the dynam(cid:173) ics.  Algorithms  include  expectation-maximization (EM), dual  Kalman  filtering,  and joint Kalman methods.  These methods have recently  been  explored in  the context of nonlinear modeling,  where a  neural network  is  used  as  the functional form of the  unknown model.  Typically, an  ex(cid:173) tended  Kalman  filter  (EKF)  or smoother  is  used  for  the  part of the  al(cid:173) gorithm that estimates the clean state given the current estimated model.  An  EKF may  also be used to estimate the weights of the  network.  This  paper points  out the flaws  in  using  the  EKF,  and  proposes an  improve(cid:173) ment based on a new approach called the unscented transformation (UT)  [3].  A  substantial performance gain is  achieved  with  the same order of  computational complexity as  that of the standard EKF.  The approach is  illustrated on several dual estimation methods."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html,Learning Statistically Neutral Tasks without Expert Guidance,"Ton Weijters, Antal van den Bosch, Eric O. Postma","Eric  Postma 
Computer Science, 
Universiteit  Maastricht,"
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/f670ef5d2d6bdf8f29450a970494dd64-Abstract.html,Gaussian Fields for Approximate Inference in Layered Sigmoid Belief Networks,"David Barber, Peter Sollich","Local ""belief propagation"" rules of the  sort proposed by Pearl  [15]  are  guaranteed  to  converge to  the  correct  posterior probabilities  in  singly  connected graphical models. Recently, a number of researchers have em(cid:173) pirically demonstrated good performance of ""loopy belief propagation""(cid:173) using these same rules on graphs with loops.  Perhaps the most dramatic  instance is the near Shannon-limit performance of ""Turbo codes"", whose  decoding algorithm is equivalent to loopy belief propagation.  Except for the case of graphs with a single loop, there has been little theo(cid:173) retical understanding of the performance of loopy propagation. Here we  analyze belief propagation in  networks  with  arbitrary  topologies  when  the nodes in  the graph describe jointly Gaussian random variables.  We  give an analytical formula relating the true  posterior probabilities with  those calculated using loopy propagation.  We give sufficient conditions  for convergence and show that when belief propagation converges it gives  the correct posterior means for all graph  topologies,  not just networks  with a single loop.  The related ""max-product"" belief propagation algorithm finds  the max(cid:173) imum posterior probability estimate for singly connected networks.  We  show  that,  even for non-Gaussian probability distributions,  the  conver(cid:173) gence points of the max-product algorithm in loopy  networks are  max(cid:173) ima  over a  particular large  local  neighborhood of the  posterior proba(cid:173) bility.  These results  help clarify  the  empirical performance results  and  motivate  using  the  powerful  belief propagation algorithm in  a  broader  class of networks. 
Problems involving probabilistic belief propagation arise in a wide variety of applications,  including error correcting codes,  speech recognition and medical diagnosis.  If the  graph  is  singly connected, there exist local message-passing schemes to  calculate the  posterior  probability of an unobserved variable given the observed variables.  Pearl [15] derived such  a scheme for singly connected Bayesian networks and showed that this ""belief propagation""  algorithm is guaranteed to converge to the correct posterior probabilities (or ""beliefs""). 
Several groups have recently reported excellent experimental results by running algorithms 
674 
Y.  Weiss  and W T.  Freeman 
equivalent to Pearl's algorithm on networks with loops [8, 13, 6].  Perhaps the most dramatic  instance of this performance is for ""Turbo code""  [2]  error correcting codes.  These codes  have been described as ""the most exciting and potentially important development in coding  theory  in  many years"" [12]  and have recently been shown  [10,  11] to utilize an algorithm  equivalent to belief propagation in a network with loops. 
Progress in the analysis of loopy belief propagation has been made for the case of networks  with  a  single  loop  [17,  18,  4,  1].  For  these  networks,  it  can  be  shown  that  (1)  unless  all  the  compatabilities are  deterministic,  loopy belief propagation will converge.  (2) The  difference between the loopy beliefs and the true beliefs is related to the convergence rate  the faster the convergence the more exact the approximation and (3) If  of the messages - the hidden nodes are binary, then the loopy beliefs and the true beliefs are both maximized  by the same assignments, although the confidence in that assignment is wrong for the loopy  beliefs. 
In this paper we analyze belief propagation in  graphs of arbitrary topology, for nodes de(cid:173) scribing jointly Gaussian random variables.  We  give an exact formula relating the correct  marginal  posterior probabilities with  the  ones calculated using  loopy  belief propagation.  We  show that if belief propagation converges, then it will give the correct posterior means  for all graph  topologies, not just networks with  a single loop.  We  show that the covari(cid:173) ance estimates will  generally be  incorrect but present a relationship between the error in  the covariance estimates and the convergence speed.  For Gaussian or non-Gaussian vari(cid:173) ables,  we  show  that the  ""max-product"" algorithm,  which  calculates the MAP estimate in  singly connected networks, only converges to points that are maxima over a particular large  neighborhood of the posterior probability of loopy networks."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/fc6709bfdf0572f183c1a84ce5276e96-Abstract.html,A Multi-class Linear Learning Algorithm Related to Winnow,Chris Mesterharm,"In  this  paper,  we  present  Committee,  a  new  multi-class  learning  algo(cid:173) rithm related  to the Winnow family  of algorithms.  Committee is  an  al(cid:173) gorithm for combining the predictions of a set of sub-experts in  the  on(cid:173) line mistake-bounded model oflearning. A sub-expert is a special type of  attribute that predicts with a distribution over a finite  number of classes.  Committee learns a linear function  of sub-experts and uses this function  to make class predictions.  We  provide bounds for Committee that show  it performs  well  when  the  target can  be represented  by  a  few  relevant  sub-experts.  We  also  show how Committee can  be  used  to solve more  traditional problems composed  of attributes.  This leads to a natural ex(cid:173) tension  that learns on  multi-class problems that contain both traditional  attributes and sub-experts."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/fc9b003bb003a298c2ad0d05e4342bdc-Abstract.html,Churn Reduction in the Wireless Industry,"Michael Mozer, Richard H. Wolniewicz, David B. Grimes, Eric Johnson, Howard Kaushansky","Competition  in  the  wireless telecommunications industry  is  rampant.  To main(cid:173) tain  profitability,  wireless  carriers  must  control  chum,  the  loss  of subscribers  who  switch  from  one  carrier  to  another.  We  explore  statistical  techniques  for  chum prediction and, based on these predictions. an optimal policy for  identify(cid:173) ing  customers to  whom  incentives  should be offered to  increase  retention.  Our  experiments are  based on  a data base of nearly 47,000 U.S. domestic subscrib(cid:173) ers,  and  includes information about  their  usage,  billing,  credit, application,  and  complaint history. We show that under a wide variety of assumptions concerning  the cost of intervention and the retention  rate resulting from  intervention, chum  prediction  and  remediation  can  yield  significant  savings  to  a  carrier.  We  also  show the importance of a data representation crafted by domain experts. 

Competition  in  the  wireless  telecommunications industry  is  rampant.  As many  as  seven  competing carriers operate in  each market.  The industry  is  extremely dynamic,  with  new  services,  technologies,  and  carriers  constantly  altering  the  landscape.  Carriers  announce  new rates and incentives weekly, hoping to entice new subscribers and to lure subscribers  away  from  the  competition.  The extent of rivalry  is  reflected  in  the  deluge  of advertise(cid:173) ments for wireless service in the daily newspaper and other mass media. 

The United States had 69 million  wireless subscribers in  1998, roughly  25% of the 

population. Some markets are further developed; for example, the subscription rate in Fin(cid:173) land is 53%.  Industry  forecasts  are for a U.S.  penetration rate of 48%  by  2003. Although  there is significant room for growth in most markets, the industry growth rate is declining  and competition is rising. Consequently, it has become crucial for wireless carriers to con(cid:173) trol  chum-the  loss  of customers  who  switch  from  one  carrier  to  another.  At  present,  domestic monthly chum rates are 2-3%  of the  customer base.  At an average cost of $400  to acquire a subscriber, churn cost the industry nearly $6.3 bilIion in  1998; the total annual  loss rose to nearly $9.6 billion when lost monthly revenue from subscriber cancellations is  considered (Luna,  1998).  It  costs roughly five  times as much to  sign on a  new subscriber  as to retain an existing one. Consequently, for a carrier with 1.5 milIion subscribers, reduc(cid:173) ing the monthly churn' rate from 2% to 1 % would yield an increase in annual earnings of at  least  $54  milIion,  and  an  increase  in  shareholder  value  of approximately  $150  million.  (Estimates are even higher when lost monthly revenue is considered; see Fowlkes, Madan,  Andrew, &  Jensen,  1999; Luna,  1998.) 

The goal  of our research is  to  evaluate  the  benefits  of predicting churn  using tech(cid:173)

niques from statistical  machine learning. We designed models that predict the probability 


936 

M.  C.  Mozer,  R.  Wolniewicz.  D.  B.  Grimes. E.  Johnson  and H.  Kaushansky 

of a subscriber churning within a short time window, and we evaluated how well these pre(cid:173) dictions  could  be  used  for  decision  making  by  estimating  potential  cost  savings  to  the  wireless carrier under a variety of assumptions concerning subscriber behavior."
1999,https://papers.nips.cc/paper_files/paper/1999,https://papers.nips.cc/paper_files/paper/1999/hash/fddd7938a71db5f81fcc621673ab67b7-Abstract.html,Recognizing Evoked Potentials in a Virtual Environment,"Jessica D. Bayliss, Dana H. Ballard","Virtual  reality  (VR)  provides  immersive  and  controllable  experimen(cid:173) tal  environments.  It expands  the  bounds  of possible  evoked  potential  (EP)  experiments by  providing complex,  dynamic  environments in  or(cid:173) der  to  study  cognition  without  sacrificing  environmental  control.  VR  also serves as a safe dynamic testbed for brain-computer .interface (BCl)  research.  However, there has been some concern about detecting EP sig(cid:173) nals in a complex VR environment.  This paper shows that EPs exist at  red,  green, and yellow stop lights in  a virtual driving environment.  Ex(cid:173) perimental results show  the existence of the  P3  EP at  ""go"" and  ""stop""  lights and  the contingent negative variation (CNY)  EP  at ""slow down""  lights.  In order to  test the  feasibility  of on-line recognition in  VR,  we  looked at recognizing the P3 EP at red stop tights and the absence of this  signal at yellow slow down lights.  Recognition results show that the P3  may successfully be used to control the brakes of a VR car at stop lights."
