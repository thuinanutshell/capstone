year,proceeding_link,paper_link,title,authors,abstract
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/026a39ae63343c68b5223a95f3e17616-Abstract.html,PAC-Bayes Learning of Conjunctions and Classification of Gene-Expression Data,"Mario Marchand, Mohak Shah","We propose a “soft greedy” learning algorithm for building small conjunctions of simple threshold functions, called rays, deﬁned on single real-valued attributes. We also propose a PAC-Bayes risk bound which is minimized for classiﬁers achieving a non-trivial tradeoﬀ between sparsity (the number of rays used) and the mag- nitude of the separating margin of each ray. Finally, we test the soft greedy algorithm on four DNA micro-array data sets."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/028ee724157b05d04e7bdcf237d12e60-Abstract.html,A Second Order Cone programming Formulation for Classifying Missing Data,"Chiranjib Bhattacharyya, Pannagadatta K. Shivaswamy, Alex J. Smola","We propose a convex optimization based strategy to deal with uncertainty          in the observations of a classification problem. We assume that instead          of a sample (xi, yi) a distribution over (xi, yi) is specified. In particu-          lar, we derive a robust formulation when the distribution is given by a          normal distribution. It leads to Second Order Cone Programming formu-          lation. Our method is applied to the problem of missing data, where it          outperforms direct imputation."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/02f657d55eaf1c4840ce8d66fcdaf90c-Abstract.html,Learning first-order Markov models for control,"Pieter Abbeel, Andrew Y. Ng","First-order Markov models have been successfully applied to many prob- lems, for example in modeling sequential data using Markov chains, and modeling control problems using the Markov decision processes (MDP) formalism. If a ﬁrst-order Markov model’s parameters are estimated from data, the standard maximum likelihood estimator considers only the ﬁrst-order (single-step) transitions. But for many problems, the ﬁrst- order conditional independence assumptions are not satisﬁed, and as a re- sult the higher order transition probabilities may be poorly approximated. Motivated by the problem of learning an MDP’s parameters for control, we propose an algorithm for learning a ﬁrst-order Markov model that ex- plicitly takes into account higher order interactions during training. Our algorithm uses an optimization criterion different from maximum likeli- hood, and allows us to learn models that capture longer range effects, but without giving up the beneﬁts of using ﬁrst-order Markov models. Our experimental results also show the new algorithm outperforming conven- tional maximum likelihood estimation in a number of control problems where the MDP’s parameters are estimated from data."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/03fa2f7502f5f6b9169e67d17cbf51bb-Abstract.html,Who's In the Picture,"Tamara L. Berg, Alexander C. Berg, Jaety Edwards, David A. Forsyth","The context in which a name appears in a caption provides powerful cues           as to who is depicted in the associated image. We obtain 44,773 face im-           ages, using a face detector, from approximately half a million captioned           news images and automatically link names, obtained using a named en-           tity recognizer, with these faces. A simple clustering method can pro-           duce fair results. We improve these results significantly by combining           the clustering process with a model of the probability that an individual           is depicted given its context. Once the labeling procedure is over, we           have an accurately labeled set of faces, an appearance model for each           individual depicted, and a natural language model that can produce ac-           curate results on captions in isolation."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html,Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging,"Vladimir Koltchinskii, Manel Martínez-ramón, Stefan Posse","We study a method of optimal data-driven aggregation of classifiers in a           convex combination and establish tight upper bounds on its excess risk           with respect to a convex loss function under the assumption that the so-           lution of optimal aggregation problem is sparse. We use a boosting type           algorithm of optimal aggregation to develop aggregate classifiers of ac-           tivation patterns in fMRI based on locally trained SVM classifiers. The           aggregation coefficients are then used to design a ""boosting map"" of the           brain needed to identify the regions with most significant impact on clas-           sification."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/058d6f2fbe951a5a56d96b1f1a6bca1c-Abstract.html,Binet-Cauchy Kernels,"Alex J. Smola, S.v.n. Vishwanathan","We propose a family of kernels based on the Binet-Cauchy theorem and its ex- tension to Fredholm operators. This includes as special cases all currently known kernels derived from the behavioral framework, diffusion processes, marginalized kernels, kernels on graphs, and the kernels on sets arising from the subspace angle approach. Many of these kernels can be seen as the extrema of a new continuum of kernel functions, which leads to numerous new special cases. As an application, we apply the new class of kernels to the problem of clustering of video sequences with encouraging results."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/06c284d3f757b15c02f47f3ff06dc275-Abstract.html,Synergistic Face Detection and Pose Estimation with Energy-Based Models,"Margarita Osadchy, Matthew L. Miller, Yann L. Cun","We describe a novel method for real-time, simultaneous multi-view face           detection and facial pose estimation. The method employs a convolu-           tional network to map face images to points on a manifold, parametrized           by pose, and non-face images to points far from that manifold. This           network is trained by optimizing a loss function of three variables: im-           age, pose, and face/non-face label. We test the resulting system, in a           single configuration, on three standard data sets  one for frontal pose,           one for rotated faces, and one for profiles  and find that its performance           on each set is comparable to previous multi-view face detectors that can           only handle one form of pose variation. We also show experimentally           that the system's accuracy on both face detection and pose estimation is           improved by training for the two tasks together.
1     Introduction
The detection of human faces in natural images and videos is a key component in a wide variety of applications of human-computer interaction, search and indexing, security, and surveillance. Many real-world applications would profit from multi-view detectors that can detect faces under a wide range of poses: looking left or right (yaw axis), up or down (pitch axis), or tilting left or right (roll axis).
In this paper we describe a novel method that not only detects faces independently of their poses, but simultaneously estimates those poses. The system is highly-reliable, runs at near real time (5 frames per second on standard hardware), and is robust against variations in yaw (90), roll (45), and pitch (60).
The method is motivated by the idea that multi-view face detection and pose estimation are so closely related that they should not be performed separately. The tasks are related in the sense that they must be robust against the same sorts of variation: skin color, glasses, facial hair, lighting, scale, expressions, etc. We suspect that, when trained together, each task can serve as an inductive bias for the other, yielding better generalization or requiring fewer training examples [2].
To exploit the synergy between these two tasks, we train a convolutional network to map face images to points on a face manifold, and non-face images to points far away from that manifold. The manifold is parameterized by facial pose. Conceptually, we can view the pose parameter as a latent variable that can be inferred through an energy-minimization process [4]. To train the machine we derive a new type of discriminative loss function that is tailored to such detection tasks.
Previous Work: Learning-based approaches to face detection abound, including real-time methods [16], and approaches based on convolutional networks [15, 3]. Most multi-view systems take a view-based approach, which involves building separate detectors for differ- ent views and either applying them in parallel [10, 14, 13, 7] or using a pose estimator to select a detector [5]. Another approach is to estimate and correct in-plane rotations before applying a single pose-specific detector [12]. Closer to our approach is that of [8], in which a number of Support Vector Regressors are trained to approximate smooth functions, each of which has a maximum for a face at a particular pose. Another machine is trained to con- vert the resulting values to estimates of poses, and a third is trained to convert the values into a face/non-face score. The resulting system is very slow.
2    Integrating face detection and pose estimation
To exploit the posited synergy between face detection and pose estimation, we must design a system that integrates the solutions to the two problems. We hope to obtain better results on both tasks, so this should not be a mere cascaded system in which the answer to one problem is used to assist in solving the other. Both answers must be derived from one underlying analysis of the input, and both tasks must be trained together.
Our approach is to build a trainable system that can map raw images X to points in a low-dimensional space. In that space, we pre-define a face manifold F (Z) that we para- meterize by the pose Z. We train the system to map face images with known poses to the corresponding points on the manifold. We also train it to map non-face images to points far away from the manifold. Proximity to the manifold then tells us whether or not an image is a face, and projection to the manifold yields an estimate of the pose.
Parameterizing the Face Manifold: We will now describe the details of the parameter- izations of the face manifold. Let's start with the simplest case of one pose parameter Z = , representing, say, yaw. If we want to preserve the natural topology and geometry of the problem, the face manifold under yaw variations in the interval [-90, 90] should be a half circle (with constant curvature). We embed this half-circle in a three-dimensional space using three equally-spaced shifted cosines.                                                                                                              Fi() = cos( - i);          i = 1, 2, 3;          = [- , ]                         (1)                                                                                      2 2 When we run the network on an image X, it outputs a vector G(X) with three components that can be decoded analytically into corresponding pose angle:                                                     3      G                                  = arctan          i=1    i(X ) cos(i)                                                     3                                                       (2)                                                            G                                                     i=1         i(X ) sin(i)
The point on the manifold closest to G(X) is just F ().
The same idea can be applied to any number of pose parameters. Let us consider the set of all faces with yaw in [-90, 90] and roll in [-45, 45]. In an abstract way, this set is isomorphic to a portion of the surface of a sphere. Consequently, we encode the pose with the product of the cosines of the two angles:                       Fij(, ) = cos( - i) cos( - j);                    i, j = 1, 2, 3;               (3) For convenience we rescale the roll angles to the range of [-90, 90]. With these parame- terizations, the manifold has constant curvature, which ensures that the effect of errors will be the same regardless of pose. Given nine components of the network's output Gij(X), we compute the corresponding pose angles as follows:      cc =             G                                                       G                 ij     ij (X ) cos(i) cos(j );         cs =           ij      ij (X ) cos(i) sin(j )      sc =             G                                                       G                 ij     ij (X ) sin(i) cos(j );         ss =           ij      ij (X ) sin(i) sin(j )    (4)               = 0.5(atan2(cs + sc, cc - ss) + atan2(sc - cs, cc + ss))               = 0.5(atan2(cs + sc, cc - ss) - atan2(sc - cs, cc + ss)) Note that the dimension of the face manifold is much lower than that of the embedding space. This gives ample space to represent non-faces away from the manifold.
3    Learning Machine
To build a learning machine for the proposed approach we refer to the Minimum Energy Machine framework described in [4].
Energy Minimization Framework: We can view our system as a scalar-value function EW (Y, Z, X), where X and Z are as defined above, Y is a binary label (Y = 1 for face, Y = 0 for a non-face), and W is a parameter vector subject to learning. EW (Y, Z, X) can be interpreted as an energy function that measures the degree of compatibility between X, Z, Y . If X is a face with pose Z, then we want: EW (1, Z, X)                                 EW (0, Z , X) for any pose Z , and EW (1, Z , X)                     EW (1, Z, X) for any pose Z = Z.
Operating the machine consists in clamping X to the observed value (the image), and finding the values of Z and Y that minimize EW (Y, Z, X):
                   (Y , Z) = argminY {Y }, Z{Z}EW (Y, Z, X)                                              (5)

where {Y } = {0, 1} and {Z} = [-90, 90][-45, 45] for yaw and roll variables. Although this inference process can be viewed probabilistically as finding the most likely configu- ration of Y and Z according to a model that attributes high probabilities to low-energy configurations (e.g. a Gibbs distribution), we view it as a non probabilistic decision mak- ing process. In other words, we make no assumption as to the finiteness of integrals over {Y }and {Z}that would be necessary for a properly normalized probabilistic model. This gives us considerable flexibility in the choice of the internal architecture of EW (Y, Z, X).
Our energy function for a face EW (1, Z, X) is defined as the distance between the point produced by the network GW (X) and the point with pose Z on the manifold F (Z):
                        EW (1, Z, X) = GW (X) - F (Z)                                                      (6)

The energy function for a non-face EW (0, Z, X) is equal to a constant T that we can interpret as a threshold (it is independent of Z and X). The complete energy function is:
                EW (Y, Z, X) = Y GW (X) - F (Z) + (1 - Y )T                                                (7)

The architecture of the machine is depicted in Figure 1. Operating this machine (find- ing the output label and pose with the smallest energy) comes down to first finding: Z = argminZ{Z}||GW (X) - F (Z)||, and then comparing this minimum distance,  GW (X) - F (Z) , to the threshold T . If it smaller than T , then X is classified as a face, otherwise X is classified as a non-face. This decision is implemented in the architecture as a switch, that depends upon the binary variable Y .
Convolutional Network: We employ a Convolutional Network as the basic architecture for the GW (X) image-to-face-space mapping function. Convolutional networks [6] are ""end- to-end"" trainable system that can operate on raw pixel images and learn low-level features and high-level representation in an integrated fashion. Convolutional nets are advantageous because they easily learn the types of shift-invariant local features that are relevant to image recognition; and more importantly, they can be replicated over large images (swept over every location) at a fraction of the cost of replicating more traditional classifiers [6]. This is a considerable advantage for building real-time systems.
We employ a network architecture similar to LeNet5 [6]. The difference is in the number of maps. In our architecture we have 8 feature maps in the bottom convolutional and subsampling layers and 20 maps in the next two layers. The last layer has 9 outputs to encode two pose parameters.
Training with a Discriminative Loss Function for Detection: We define the loss function as follows:                             1                                           1                 L(W ) =                            L                                  L                            |S                       1(W, Zi, X i) +                        0(W, X i)           (8)                                  1|                                    |S                                        iS                              0|                                               1                               iS0
                        Figure 1: Architecture of the Minimum Energy Machine.

where S1is the set of training faces, S0the set of non-faces, L1(W, Zi, Xi) and L0(W, Xi) are loss functions for a face sample (with a known pose) and non-face, respectively1.
The loss L(W ) should be designed so that its minimization for a particular positive training sample (Xi, Zi, 1), will make EW (1, Zi, Xi) < EW (Y, Z, Xi) for Y = Y i or Z = Zi. To satisfy this, it is sufficient to make EW (1, Zi, Xi) < EW (0, Z, Xi). For a particu- lar negative training sample (Xi, 0), minimizing the loss should make EW (1, Z, Xi) > EW (0, Z, Xi) = T for any Z. To satisfy this, it is sufficient to make EW (1, Z, Xi) > T .
Let W be the current parameter value, and W be the parameter value after an update caused by a single sample. To cause the machine to achieve the desired behavior, we need the parameter update to decrease the difference between the energy of the desired label and the energy of the undesired label. In our case, since EW (0, Z, X) = T is constant, the following condition on the update is sufficient to ensure the desired behavior:
Condition 1. for a face example (X, Z, 1), we must have: EW (1, Z, X) < EW (1, Z, X) For a non-face example (X, 1), we must have: EW (1, Z, X) > EW (1, Z, X)
We choose the following forms for L1 and L0:
     L1(W, 1, Z, X) = EW (1, Z, X)2;                 L0(W, 0, X) = K exp[-E(1, Z, X)]                                (9)

where K is a positive constant.
Next we show that minimizing (9) with an incremental gradient-based algorithm will satisfy condition 1. With gradient-based optimization algorithms, the parameter update formula is of the form: W = W - W = -A L                                                      W . where A is a judiciously chosen symmetric positive semi-definite matrix, and  is a small positive constant.
For Y = 1 (face): An update step will change the parameter by W = -A EW (1,Z,X)2 =                                                                                                              W -2EW (1, Z, X)A EW (1,Z,X)                                   W     . To first order (for small values of ), the resulting change in EW (1, Z, X) is given by:       E                     T                                                       T             W (1, Z, X )                                       E                              E                                   W = -2E                          W (1, Z, X )         A          W (1, Z, X ) < 0               W                               W (1, Z, X )            W                              W
because EW (1, Z, X) > 0 (it's a distance), and the quadratic form is positive. Therefore EW (1, Z, X) < EW (1, Z, X).
1Although face samples whose pose is unknown can easily be accommodated, we will not discuss this possibility here.
             100                                                                                                               100


                      95                                                                                                                    95


                      90                                                                                                                    90


                      85                                                                                                                    85


                      80                                                                                                                    80


                      75                                                                                                                    75


                      70                                                                                                                    70


                      65                                                                                                                    65

           Percentage of faces detected 60                                                                                                                                                                      Pose + detection                             60                                                                                                                                                                                                                                                                                                                Pose + detection

                                                                                               Detection only                                                                                                                                                                                                                           Pose only                                                                                                                                                                                                                                                              55                                                                                                     Percentage of yaws correctly estimated 55

                      50                                                                                                                    50                                                   0    2     4         6      8     10    12       14    16          18    20                                                 0     5            10      15         20                     25     30                                                                             False positive rate                                                                                             Yaw-error tolerance (degrees)

Figure 2: Synergy test. Left: ROC curves for the pose-plus-detection and detection-only networks. Right: frequency with which the pose-plus-detection and pose-only networks correctly estimated the yaws within various error tolerances.
For Y                  = 0 (non-face): An update step will change the parameter by W = -A K exp[-E(1,Z,X)] = K exp[-E                                                        W                                                             W (1, Z, X )] EW (1,Z,X)                                                                                                                                                                                            W          . To first order (for small values of ), the resulting change in EW (1, Z, X) is given by:
 E                                                           T                                                                                                                                            T            W (1, Z, X )                                                                                                                                                            E                                    E                                                                        W = K exp[-E                                                                                                    W (1, Z , X )              A          W (1, Z , X ) > 0              W                                                                                                     W (1, Z , X )]                                                         W                                               W

Therefore EW (1, Z, X) > EW (1, Z, X).
Running the Machine: Our detection system works on grayscale images and it applies                                                                                                                                                                                                                            the network to each image at a range of scales, stepping by a factor of                                                                                                                                                        2. The network is replicated over the image at each scale, stepping by 4 pixels in x and y (this step size is a consequence of having two, 2x2 subsampling layers). At each scale and location, the network outputs are compared to the closest point on the manifold, and the system collects a list of all instances closer than our detection threshold. Finally, after examining all scales, the system identifies groups of overlapping detections in the list and discards all but the strongest (closest to the manifold) from each group. No attempt is made to combine detections or apply any voting scheme. We have implemented the system in C. The system can detect, locate, and estimate the pose of faces that are between 40 and 250 pixels high in a 640  480 image at roughly 5 frames per second on a 2.4GHz Pentium 4.
4     Experiments and results
Using the above architecture, we built a detector to locate faces and estimate two pose parameters: yaw from left to right profile, and in-plane rotation from -45 to 45 degrees. The machine was trained to be robust against pitch variation.
In this section, we first describe the training regimen for this network, and then give the results of two sets of experiments. The first set of experiments tests whether training for the two tasks together improves performance on both. The second set allows comparisons between our system and other published multi-view detectors.
Training: Our training set consisted of 52, 850, 32x32-pixel faces from natural images collected at NEC Labs and hand annotated with appropriate facial poses (see [9] for a description of how the annotation was done). These faces were selected from a much larger annotated set to yield a roughly uniform distribution of poses from left profile to right profile, with as much variation in pitch as we could obtain. Our initial negative training data consisted of 52, 850 image patches chosen randomly from non-face areas of a variety of images. For our second set of tests, we replaced half of these with image patches obtained by running the initial version of the detector on our training images and collecting false detections. Each training image was used 5 times during training, with random variations
       100                                                                                                      100


                95                                                                                                           95


                90                                                                                                           90


                85                                                                                                           85


                80                                                                                                           80


                75                                                                                                           75


                70                                                                                                           70


                65                                                                                                           65                                                                                       Frontal          Percentage of faces detected 60                                                                                                                                                 Rotated in plane                           60                                                                                                                                                                                                                                                                                           In-plane rotation                                                                                       Profile                                                                                                           Yaw                          55                                                                                            Percentage of poses correctly estimated 55

                50                                                                                                           50                                             0    0.5    1    1.5    2    2.5    3    3.5         4    4.5    5                                                  0    5       10        15         20                     25    30                                                               False positives per image                                                                                   Pose-error tolerance (degrees)

Figure 3: Results on standard data sets. Left: ROC curves for our detector on the three data sets. The x axis is the average number of false positives per image over all three sets, so each point corresponds to a single detection threshold. Right: frequency with which yaw and roll are estimated within various error tolerances.                                                                                 in scale (from x 2 to x(1 +                                                          2)), in-plane rotation (45), brightness (20), contrast (from 0.8 to 1.3).
To train the network, we made 9 passes through this data, though it mostly converged after about the first 6 passes. Training was performed using LUSH [1], and the total training time was about 26 hours on a 2Ghz Pentium 4. At the end of training, the network had converged to an equal error rate of 5% on the training data and 6% on a separate test set of 90,000 images.
Synergy tests: The goal of the synergy test was to verify that both face detection and pose estimation benefit from learning and running in parallel. To test this claim we built three networks with almost identical architectures, but trained to perform different tasks. The first one was trained for simultaneous face detection and pose estimation (combined), the second was trained for detection only and the third for pose estimation only. The ""detection only"" network had only one output for indicating whether or not its input was a face. The ""pose only"" network was identical to the combined network, but trained on faces only (no negative examples). Figure 2 shows the results of running these networks on our 10,000 test images. In both these graphs, we see that the pose-plus-detection network had better performance, confirming that training for each task benefits the other.
Standard data sets: There is no standard data set that tests all the poses our system is designed to detect. There are, however, data sets that have been used to test more restricted face detectors, each set focusing on a particular variation in pose. By testing a single detector with all of these sets, we can compare our performance against published systems. As far as we know, we are the first to publish results for a single detector on all these data sets. The details of these sets are described below:  MIT+CMU [14, 11]  130 images for testing frontal face detectors. We count 517 faces in this set, but the standard tests only use a subset of 507 faces, because 10 faces are in the wrong pose or otherwise not suitable for the test. (Note: about 2% of the faces in the standard subset are badly-drawn cartoons, which we do not intend our system to detect. Nevertheless, we include them in the results we report.)  TILTED [12]  50 images of frontal faces with in-plane rotations. 223 faces out of 225 are in the standard subset. (Note: about 20% of the faces in the standard subset are outside of the 45 rotation range for which our system is designed. Again, we still include these in our results.)  PROFILE [13]  208 images of faces in profile. There seems to be some disagreement about the number of faces in the standard set of annotations: [13] reports using 347 faces of the 462 that we found, [5] reports using 355, and we found 353 annotations. However, these discrepencies should not significantly effect the reported results.
We counted a face as being detected if 1) at least one detection lay within a circle centered on the midpoint between the eyes, with a radius equal to 1.25 times the distance from that point to the midpoint of the mouth, and 2) that detection came at a scale within a factor of
Figure 4: Some example face detections. Each white box shows the location of a detected face. The angle of each box indicates the estimated in-plane rotation. The black crosshairs within each box indicate the estimated yaw.
                    Data set          TILTED               PROFILE             MIT+CMU      False positives per image          4.42         26.90     .47         3.36     .50         1.28  Our detector                           90%           97%      67%          83%     83%          88%  Jones & Viola [5] (tilted)             90%           95%              x                    x  Jones & Viola [5] (profile)                      x             70%          83%             x  Rowley et al [11]                      89%           96%              x                    x  Schneiderman & Kanade [13]                      x             86%          93%             x

Table 1: Comparisons of our results with other multi-view detectors. Each column shows the detec- tion rates for a given average number of false positives per image (these rates correspond to those for which other authors have reported results). Results for real-time detectors are shown in bold. Note that ours is the only single detector that can be tested on all data sets simultaneously.
two of the correct scale for the face's size. We counted a detection as a false positive if it did not lie within this range for any of the faces in the image, including those faces not in the standard subset.
The left graph in Figure 3 shows ROC curves for our detector on the three data sets. Figure 4 shows a few results on various poses. Table 1 shows our detection rates compared against other systems for which results were given on these data sets. The table shows that our results on the TILTED and PROFILE sets are similar to those of the two Jones & Viola detectors, and even approach those of the Rowley et al and Schneiderman & Kanade non- real-time detectors. Those detectors, however, are not designed to handle all variations in pose, and do not yield pose estimates.
The right side of Figure 3 shows our performance at pose estimation. To make this graph, we fixed the detection threshold at a value that resulted in about 0.5 false positives per image over all three data sets. We then compared the pose estimates for all detected faces (including those not in the standard subsets) against our manual pose annotations. Note that this test is more difficult than typical tests of pose estimation systems, where faces are first localized by hand. When we hand-localize these faces, 89% of yaws and 100% of in-plane rotations are correctly estimated to within 15."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/06fe1c234519f6812fc4c1baae25d6af-Abstract.html,The power of feature clustering: An application to object detection,"Shai Avidan, Moshe Butman","We give a fast rejection scheme that is based on image segments and          demonstrate it on the canonical example of face detection. However, in-          stead of focusing on the detection step we focus on the rejection step and          show that our method is simple and fast to be learned, thus making it          an excellent pre-processing step to accelerate standard machine learning          classifiers, such as neural-networks, Bayes classifiers or SVM. We de-          compose a collection of face images into regions of pixels with similar          behavior over the image set. The relationships between the mean and          variance of image segments are used to form a cascade of rejectors that          can reject over 99.8% of image patches, thus only a small fraction of the          image patches must be passed to a full-scale classifier. Moreover, the          training time for our method is much less than an hour, on a standard PC.          The shape of the features (i.e. image segments) we use is data-driven,          they are very cheap to compute and they form a very low dimensional          feature space in which exhaustive search for the best features is tractable."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/08f38e0434442128fab5ead6217ca759-Abstract.html,Synergies between Intrinsic and Synaptic Plasticity in Individual Model Neurons,Jochen Triesch,"This paper explores the computational consequences of simultaneous in-          trinsic and synaptic plasticity in individual model neurons. It proposes          a new intrinsic plasticity mechanism for a continuous activation model          neuron based on low order moments of the neuron's firing rate distribu-          tion. The goal of the intrinsic plasticity mechanism is to enforce a sparse          distribution of the neuron's activity level. In conjunction with Hebbian          learning at the neuron's synapses, the neuron is shown to discover sparse          directions in the input."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/09a5e2a11bea20817477e0b1dfe2cc21-Abstract.html,A Probabilistic Model for Online Document Clustering with Application to Novelty Detection,"Jian Zhang, Zoubin Ghahramani, Yiming Yang","In this paper we propose a probabilistic model for online document clus-          tering. We use non-parametric Dirichlet process prior to model the grow-          ing number of clusters, and use a prior of general English language          model as the base distribution to handle the generation of novel clusters.          Furthermore, cluster uncertainty is modeled with a Bayesian Dirichlet-          multinomial distribution. We use empirical Bayes method to estimate          hyperparameters based on a historical dataset. Our probabilistic model          is applied to the novelty detection task in Topic Detection and Tracking          (TDT) and compared with existing approaches in the literature."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/0b7e926154c1274e8b602ff0d7c133d7-Abstract.html,Non-Local Manifold Tangent Learning,"Yoshua Bengio, Martin Monperrus","We claim and present arguments to the effect that a large class of man-          ifold learning algorithms that are essentially local and can be framed as          kernel learning algorithms will suffer from the curse of dimensionality,          at the dimension of the true underlying manifold. This observation sug-          gests to explore non-local manifold learning algorithms which attempt to          discover shared structure in the tangent planes at different positions. A          criterion for such an algorithm is proposed and experiments estimating a          tangent plane prediction function are presented, showing its advantages          with respect to local manifold learning algorithms: it is able to general-          ize very far from training data (on learning handwritten character image          rotations), where a local non-parametric method fails."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/0bed45bd5774ffddc95ffe500024f628-Abstract.html,Maximal Margin Labeling for Multi-Topic Text Categorization,"Hideto Kazawa, Tomonori Izumitani, Hirotoshi Taira, Eisaku Maeda","In this paper, we address the problem of statistical learning for multi- topic text categorization (MTC), whose goal is to choose all relevant top- ics (a label) from a given set of topics. The proposed algorithm, Max- imal Margin Labeling (MML), treats all possible labels as independent classes and learns a multi-class classiﬁer on the induced multi-class cate- gorization problem. To cope with the data sparseness caused by the huge number of possible labels, MML combines some prior knowledge about label prototypes and a maximal margin criterion in a novel way. Experi- ments with multi-topic Web pages show that MML outperforms existing learning algorithms including Support Vector Machines.
1 Multi-topic Text Categorization (MTC)
This paper addresses the problem of learning for multi-topic text categorization (MTC), whose goal is to select all topics relevant to a text from a given set of topics. In MTC, multiple topics may be relevant to a single text. We thus call a set of topics label, and say that a text is assigned a label, not a topic.
In almost all previous text categorization studies (e.g. [1, 2]), the label is predicted by judging each topic’s relevance to the text. In this decomposition approach, the features speciﬁc to a topic, not a label, are regarded as important features. However, the approach may result in inefﬁcient learning as we will explain in the following example.
Imagine an MTC problem of scientiﬁc papers where quantum computing papers are as- signed multi-topic label “quantum physics (QP) & computer science (CS)”. (QP and CS are topics in this example.) Since there are some words speciﬁc to quantum computing such as “qbit1”, one can say that efﬁcient MTC learners should use such words to assign label QP & CS. However, the decomposition approach is likely to ignore these words since they are only speciﬁc to a small portion of the whole QP or CS papers (there are many more QP and CS papers than quantum computing papers), and therefore are not discriminative features for either topic QP or CS.
1Qbit is a unit of quantum information, and frequently appears in real quantum computing litera-
tures, but rarely seen in other literatures.
Symbol x(∈ Rd) t1, t2, . . . , tl T L, λ(⊂ T ) L[j] Λ(= 2T ) {(xi, Li)}m
i=1
Meaning A document vector Topics The set of all topics A label The binary representation of L. 1 if tj ∈ L and 0 otherwise. The set of all possible labels Training samples"
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/0c215f194276000be6a6df6528067151-Abstract.html,Conditional Random Fields for Object Recognition,"Ariadna Quattoni, Michael Collins, Trevor Darrell","We present a discriminative part-based approach for the recognition of          object classes from unsegmented cluttered scenes. Objects are modeled          as flexible constellations of parts conditioned on local observations found          by an interest operator. For each object class the probability of a given          assignment of parts to local features is modeled by a Conditional Ran-          dom Field (CRF). We propose an extension of the CRF framework that          incorporates hidden variables and combines class conditional CRFs into          a unified framework for part-based object recognition. The parameters of          the CRF are estimated in a maximum likelihood framework and recogni-          tion proceeds by finding the most likely class under our model. The main          advantage of the proposed CRF framework is that it allows us to relax the          assumption of conditional independence of the observed data (i.e. local          features) often used in generative approaches, an assumption that might          be too restrictive for a considerable number of object classes."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/0e4a2c65bdaddd66a53422d93daebe68-Abstract.html,"Inference, Attention, and Decision in a Bayesian Neural Architecture","Angela J. Yu, Peter Dayan","We study the synthesis of neural coding, selective attention and percep-          tual decision making. A hierarchical neural architecture is proposed,          which implements Bayesian integration of noisy sensory input and top-          down attentional priors, leading to sound perceptual discrimination. The          model offers an explicit explanation for the experimentally observed          modulation that prior information in one stimulus feature (location) can          have on an independent feature (orientation). The network's intermediate          levels of representation instantiate known physiological properties of vi-          sual cortical neurons. The model also illustrates a possible reconciliation          of cortical and neuromodulatory representations of uncertainty."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/0e900ad84f63618452210ab8baae0218-Abstract.html,Exponential Family Harmoniums with an Application to Information Retrieval,"Max Welling, Michal Rosen-zvi, Geoffrey E. Hinton","Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dom- inant modelling paradigm in many research ﬁelds. Although this ap- proach has met with considerable success, the causal semantics of these models can make it difﬁcult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undi- rected models. Inference in these “exponential family harmoniums” is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/1006ff12c465532f8c574aeaa4461b16-Abstract.html,Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation,"Erik B. Sudderth, Michael I. Mandel, William T. Freeman, Alan S. Willsky","We describe a threedimensional geometric hand model suitable for vi-           sual tracking applications. The kinematic constraints implied by the           model's joints have a probabilistic structure which is well described by           a graphical model. Inference in this model is complicated by the hand's           many degrees of freedom, as well as multimodal likelihoods caused by           ambiguous image measurements. We use nonparametric belief propaga-           tion (NBP) to develop a tracking algorithm which exploits the graph's           structure to control complexity, while avoiding costly discretization.           While kinematic constraints naturally have a local structure, self           occlusions created by the imaging process lead to complex interpenden-           cies in color and edgebased likelihood functions. However, we show           that local structure may be recovered by introducing binary hidden vari-           ables describing the occlusion state of each pixel. We augment the NBP           algorithm to infer these occlusion variables in a distributed fashion, and           then analytically marginalize over them to produce hand position esti-           mates which properly account for occlusion events. We provide simula-           tions showing that NBP may be used to refine inaccurate model initializa-           tions, as well as track hand motion through extended image sequences.
1     Introduction
Accurate visual detection and tracking of threedimensional articulated objects is a chal- lenging problem with applications in humancomputer interfaces, motion capture, and scene understanding [1]. In this paper, we develop a probabilistic method for tracking a geometric hand model from monocular image sequences. Because articulated hand mod- els have many (roughly 26) degrees of freedom, exact representation of the posterior dis- tribution over model configurations is intractable. Trackers based on extended and un- scented Kalman filters [2, 3] have difficulties with the multimodal uncertainties produced by ambiguous image evidence. This has motived many researchers to consider nonparamet- ric representations, including particle filters [4, 5] and deterministic multiscale discretiza- tions [6]. However, the hand's high dimensionality can cause these trackers to suffer catas- trophic failures, requiring the use of models which limit the hand's motion [4] or sophisti- cated prior models of hand configurations and dynamics [5, 6].
An alternative way to address the high dimensionality of articulated tracking problems is to describe the posterior distribution's statistical structure using a graphical model. Graph-
Figure 1: Projected edges (left block) and silhouettes (right block) for a configuration of the 3D structural hand model matching the given image. To aid visualization, the model is also projected following rotations by 35 (center) and 70 (right) about the vertical axis.
ical models have been used to track viewbased human body representations [7], con- tour models of restricted hand configurations [8], viewbased 2.5D ""cardboard"" models of hands and people [9], and a full 3D kinematic human body model [10]. Because the variables in these graphical models are continuous, and discretization is intractable for threedimensional models, most traditional graphical inference algorithms are inapplica- ble. Instead, these trackers are based on recently proposed extensions of particle filters to general graphs: mean field Monte Carlo in [9], and nonparametric belief propagation (NBP) [11, 12] in [10].
In this paper, we show that NBP may be used to track a threedimensional geometric model of the hand. To derive a graphical model for the tracking problem, we consider a redun- dant local representation in which each hand component is described by its own three dimensional position and orientation. We show that the model's kinematic constraints, including selfintersection constraints not captured by joint angle representations, take a simple form in this local representation. We also provide a local decomposition of the likelihood function which properly handles occlusion in a distributed fashion, a significant improvement over our earlier tracking results [13]. We conclude with simulations demon- strating our algorithm's robustness to occlusions.
2      Geometric Hand Modeling
Structurally, the hand is composed of sixteen approximately rigid components: three pha- langes or links for each finger and thumb, as well as the palm [1]. As proposed by [2, 3], we model each rigid body by one or more truncated quadrics (ellipsoids, cones, and cylin- ders) of fixed size. These geometric primitives are well matched to the true geometry of the hand, allow tracking from arbitrary orientations (in contrast to 2.5D ""cardboard"" mod- els [5, 9]), and permit efficient computation of projected boundaries and silhouettes [3]. Figure 1 shows the edges and silhouettes corresponding to a sample hand model configu- ration. Note that only a coarse model of the hand's geometry is necessary for tracking.
2.1    Kinematic Representation and Constraints
The kinematic constraints between different hand model components are well described by revolute joints [1]. Figure 2(a) shows a graph describing this kinematic structure, in which nodes correspond to rigid bodies and edges to joints. The two joints connecting the phalanges of each finger and thumb have a single rotational degree of freedom, while the joints connecting the base of each finger to the palm have two degrees of freedom (cor- responding to grasping and spreading motions). These twenty angles, combined with the palm's global position and orientation, provide 26 degrees of freedom. Forward kinematic transformations may be used to determine the finger positions corresponding to a given set of joint angles. While most modelbased hand trackers use this joint angle parameteriza- tion, we instead explore a redundant representation in which the ith rigid body is described by its position qi and orientation ri (a unit quaternion). Let xi = (qi, ri) denote this local description of each component, and x = {x1, . . . , x16} the overall hand configuration.
Clearly, there are dependencies among the elements of x implied by the kinematic con-
        (a)                           (b)                                  (c)                                    (d)

Figure 2: Graphs describing the hand model's constraints. (a) Kinematic constraints (EK ) de- rived from revolute joints. (b) Structural constraints (ES) preventing 3D component intersections. (c) Dynamics relating two consecutive time steps. (d) Occlusion consistency constraints (EO).
straints. Let EK be the set of all pairs of rigid bodies which are connected by joints, or equivalently the edges in the kinematic graph of Fig. 2(a). For each joint (i, j)  EK , define an indicator function K (x                                            i,j    i, xj ) which is equal to one if the pair (xi, xj ) are valid rigid body configurations associated with some setting of the angles of joint (i, j), and zero otherwise. Viewing the component configurations xi as random variables, the following prior explicitly enforces all constraints implied by the original joint angle representation:
                                       pK(x)                  K (x                                                                         i,j     i, xj )                                         (1)                                                        (i,j)EK

Equation (1) shows that pK (x) is an undirected graphical model, whose Markov structure is described by the graph representing the hand's kinematic structure (Fig. 2(a)).
2.2      Structural and Temporal Constraints
In reality, the hand's joint angles are coupled because different fingers can never occupy the same physical volume. This constraint is complex in a joint angle parameterization, but simple in our local representation: the position and orientation of every pair of rigid bodies must be such that their component quadric surfaces do not intersect.
We approximate this ideal constraint in two ways. First, we only explicitly constrain those pairs of rigid bodies which are most likely to intersect, corresponding to the edges ES of the graph in Fig. 2(b). Furthermore, because the relative orientations of each finger's quadrics are implicitly constrained by the kinematic prior pK (x), we may detect most intersections based on the distance between object centroids. The structural prior is then given by
                                                                                       1         ||q   p                                                                                                         i - qj || > i,j        S (x)                 S (x                               (x                                i,j     i, xj )           S                                                            i,j     i, xj ) =                                                    (2)                                                                                            0         otherwise                   (i,j)ES

where i,j is determined from the quadrics composing rigid bodies i and j. Empirically, we find that this constraint helps prevent different fingers from tracking the same image data.
In order to track hand motion, we must model the hand's dynamics. Let xt denote the                                                                                                                      i position and orientation of the ith hand component at time t, and xt = {xt1, . . . , xt16}. For each component at time t, our dynamical model adds a Gaussian potential connecting it to the corresponding component at the previous time step (see Fig. 2(c)):                                                           16                                       pT xt | xt-1 =            N xt - xt-1; 0,                                                                           i       i              i                               (3)                                                          i=1 Although this temporal model is factorized, the kinematic constraints at the following time step implicitly couple the corresponding random walks. These dynamics can be justified as the maximum entropy model given observations of the nodes' marginal variances i.
3      Observation Model
Skin colored pixels have predictable statistics, which we model using a histogram distribu- tion pskin estimated from training patches [14]. Images without people were used to create a histogram model pbkgd of nonskin pixels. Let (x) denote the silhouette of projected hand configuration x. Then, assuming pixels  are independent, an image y has likelihood                                                                                                                                           p                  p                                                                                                                             skin(u)                       C (y | x) =                          pskin(u)                         pbkgd(v)                                                            (4)                                                                                                                                      pbkgd(u)                                              u(x)                        v(x)                              u(x)
The final expression neglects the proportionality constant                                                                    p                                                                                                                 v               bkgd(v), which is inde- pendent of x, and thereby limits computation to the silhouette region [8].
3.1     Distributed Occlusion Reasoning
In configurations where there is no selfocclusion, pC (y | x) decomposes as a product of local likelihood terms involving the projections (xi) of individual hand components [13]. To allow a similar decomposition (and hence distributed inference) when there is occlu- sion, we augment the configuration xi of each node with a set of binary hidden variables zi = {zi         }                                         = 0 if pixel u in the projection of rigid body i is occluded           (u)    u. Letting zi(u) by any other body, and 1 otherwise, the color likelihood (eq. (4)) may be rewritten as                                                      16                      p                z                 16                                                                                                   i(u)                       p                                                           skin(u)                            C (y | x, z) =                                                                 =             p                                                                              p                                               C (y | xi, zi)                      (5)                                                                                   bkgd(u)                                                      i=1 u(xi)                                               i=1
Assuming they are set consistently with the hand configuration x, the hidden occlusion variables z ensure that the likelihood of each pixel in (x) is counted exactly once.
We may enforce consistency of the occlusion variables using the following function:                                                 0                         if x                                                                            = 1         (x                                                                  j occludes xi, u  (xj ), and zi(u)             j , zi                ; x                                                                                                                            (6)                            (u)      i) =        1                         otherwise
Note that because our rigid bodies are convex and nonintersecting, they can never take mutually occluding configurations. The constraint (xj, zi                                                             ; x                                                                                                                (u)      i) is zero precisely when pixel u in the projection of xi should be occluded by xj, but zi                                                        is in the unoccluded state.                                                                                                                  (u) The following potential encodes all of the occlusion relationships between nodes i and j:
                              O (x                                           (x             ; x                               ; x                                     i,j     i, zi, xj , zj ) =                       j , zi(u)       i) (xi, zj(u)                            j )               (7)                                                                           u These occlusion constraints exist between all pairs of nodes. As with the structural prior, we enforce only those pairs EO (see Fig. 2(d)) most prone to                                                        xj occlusion:        pO(x, z)                            O (x                                               i,j          i, zi, xj , zj )          (8)                              (i,j)EO                                                                                        z                        y                                                                                                                                   i(u)                           xi Figure 3 shows a factor graph for the occlusion relationships between xi and its neighbors, as well as the observation potential pC (y | xi, zi).                                                       x                    u                                                                                                            k                           The occlusion potential (xj, zi                                   ; x                                                             (u)      i) has a very                 Figure 3:                        Factor graph showing weak dependence on xi, depending only on                                                                                                    p(y | xi, zi),                  and the occlusion con- whether xi is behind xj relative to the camera.                                                    straints placed on xi by xj , xk. Dashed                                                                                                    lines denote weak dependencies. The 3.2     Modeling Edge Filter Responses                                                             plate is replicated once per pixel.

Edges provide another important hand tracking cue. Using boundaries labeled in training images, we estimated a histogram pon of the response of a derivative of Gaussian filter steered to the edge's orientation [8, 10]. A similar histogram poff was estimated for filter outputs at
randomly chosen locations. Let (x) denote the oriented edges in the projection of model configuration x. Then, again assuming pixel independence, image y has edge likelihood                                               p                      16               p               z                 16                                                                                                            i(u)      p                                             on(u)                                   on(u)           E (y | x, z)                                         =                                                  =           p                                               p                                                                                     E (y | xi, zi)     (9)                                                    off (u)                             poff(u)                                u(x)                                i=1 u(xi)                                        i=1
where we have used the same occlusion variables z to allow a local decomposition.
4         Nonparametric Belief Propagation
Over the previous sections, we have shown that a redundant, local representation of the geometric hand model's configuration xt allows p (xt | yt), the posterior distribution of the hand model at time t given image observations yt, to be written as                                                                                     16        p xt | yt                    pK(xt)pS(xt)pO(xt, zt)                                pC(yt | xt, zt)p                              , zt)                                                                                                             i      i    E (yt | xti         i         (10)                                zt                                                   i=1
The summation marginalizes over the hidden occlusion variables zt, which were needed to locally decompose the edge and color likelihoods. When  video frames are observed, the overall posterior distribution is given by                                                                       
                                     p (x | y)                         p xt | yt pT (xt | xt-1)                                                  (11)                                                                      t=1 Excluding the potentials involving occlusion variables, which we discuss in detail in Sec. 4.2, eq. (11) is an example of a pairwise Markov random field:

                                   p (x | y)                            i,j (xi, xj)           i (xi, y)                                       (12)                                                                  (i,j)E                      iV

Hand tracking can thus be posed as inference in a graphical model, a problem we propose to solve using belief propagation (BP) [15]. At each BP iteration, some node i  V calculates a message m (x                    ij     j ) to be sent to a neighbor j  (i)                                 {j | (i, j)  E}:
                mn (x                                                                                 mn-1 (x                          ij          j )                     j,i (xj , xi) i (xi, y)                           ki           i) dxi                  (13)                                                    xi                                      k(i)\j

At any iteration, each node can produce an approximation ^                                                                                                      p(xi | y) to the marginal distri- bution p (xi | y) by combining the incoming messages with the local observation:
                                          ^                                               pn(xi | y)  i (xi, yi)                          mn (x                                                                                                      ji          i)                                   (14)                                                                                       j(i)

For treestructured graphs, the beliefs ^                                                                             pn(xi | y) will converge to the true marginals p (xi | y). On graphs with cycles, BP is approximate but often highly accurate [15].
4.1        Nonparametric Representations
For the hand tracking problem, the rigid body configurations xi are sixdimensional con- tinuous variables, making accurate discretization intractable. Instead, we employ nonpara- metric, particlebased approximations to these messages using the nonparametric belief propagation (NBP) algorithm [11, 12]. In NBP, each message is represented using either a samplebased density estimate (a mixture of Gaussians) or an analytic function. Both types of messages are needed for hand tracking, as we discuss below. Each NBP message update involves two stages: sampling from the estimated marginal, followed by Monte Carlo ap- proximation of the outgoing message. For the general form of these updates, see [11]; the following sections focus on the details of the hand tracking implementation.
The hand tracking application is complicated by the fact that the orientation component ri of xi = (qi, ri) is an element of the rotation group SO(3). Following [10], we represent
orientations as unit quaternions, and use a linearized approximation when constructing den- sity estimates, projecting samples back to the unit sphere as necessary. This approximation is most appropriate for densities with tightly concentrated rotational components.
4.2    Marginal Computation
BP's estimate of the belief ^                                     p(xi | y) is equal to the product of the incoming messages from neighboring nodes with the local observation potential (see eq. (14)). NBP approximates this product using importance sampling, as detailed in [13] for cases where there is no selfocclusion. First, M samples are drawn from the product of the incoming kinematic and temporal messages, which are Gaussian mixtures. We use a recently proposed multi- scale Gibbs sampler [16] to efficiently draw accurate (albeit approximate) samples, while avoiding the exponential cost associated with direct sampling (a product of d M Gaussian mixtures contains M d Gaussians). Following normalization of the rotational component, each sample is assigned a weight equal to the product of the color and edge likelihoods with any structural messages. Finally, the computationally efficient ""rule of thumb"" heuris- tic [17] is used to set the bandwidth of Gaussian kernels placed around each sample.
To derive BP updates for the occlusion masks zi, we first cluster (xi, zi) for each hand component so that p (xt, zt | yt) has a pairwise form (as in eq. (12)). In principle, NBP could manage occlusion constraints by sampling candidate occlusion masks zi along with rigid body configurations xi. However, due to the exponentially large number of possible occlusion masks, we employ a more efficient analytic approximation.
Consider the BP message sent from xj to (zi, xi), calculated by applying eq. (13) to the occlusion potential               (x            ; x                              u      j , zi(u)        i). We assume that ^                                                                                                 p(xj | y) is well separated from any candidate xi, a situation typically ensured by the kinematic and structural constraints. The occlusion constraint's weak dependence on xi (see Fig. 3) then separates the message computation into two cases. If xi lies in front of typical xj configurations, the BP message j,i(u)(zi ) is uninformative. If x           (u)                                       i is occluded, the message approximately equals
     j,i(u)(zi          = 0) = 1                                                  = 1) = 1 - Pr [u  (x                       (u)                                           j,i(u)(zi(u)                                                 j )]    (15) where we have neglected correlations among pixel occlusion states, and where the prob- ability is computed with respect to ^                                                      p(xj | y). By taking the product of these messages k,i(u)(zi ) from all potential occluders x           (u)                                                          k and normalizing, we may determine an ap-

proximation to the marginal occlusion probability i                                            Pr[z               = 0].                                                                                    (u)                     i(u)
Because the color likelihood pC (y | xi, zi) factorizes across pixels u, the BP approximation to pC (y | xi) may be written in terms of these marginal occlusion probabilites:                                                                                                       p                  p                                                                                         skin(u)                       C (y | xi)                            i           + (1 -               )                                        (16)                                                                    (u)                  i(u)          pbkgd(u)                                            u(xi)
Intuitively, this equation downweights the color evidence at pixel u as the probability of that pixel's occlusion increases. The edge likelihood pE(y | xi) averages over zi similarly. The NBP estimate of ^                              p(xi | y) is determined by sampling configurations of xi as before, and reweighting them using these occlusionsensitive likelihood functions.
4.3    Message Propagation
To derive the propagation rule for nonocclusion edges, as suggested by [18] we rewrite the message update equation (13) in terms of the marginal distribution ^                                                                                                                          p(xi | y):                                                                                    ^                                                                                    pn-1(x                              mn (x                                                                  i | y) dx                               ij         j ) =                   j,i (xj , xi)                                     i                    (17)                                                      x                              mn-1 (x                                                         i                                 ji           i)
Our explicit use of the current marginal estimate ^                                                                              pn-1(xi | y) helps focus the Monte Carlo approximation on the most important regions of the state space. Note that messages sent
         1                        2                         1                      2 Figure 4: Refinement of a coarse initialization following one and two NBP iterations, both without (left) and with (right) occlusion reasoning. Each plot shows the projection of the five most significant modes of the estimated marginal distributions. Note the difference in middle finger estimates.

along kinematic, structural, and temporal edges depend only on the belief ^                                                                                   p(xi | y) follow- ing marginalization over occlusion variables zi.
Details and pseudocode for the message propagation step are provided in [13]. For kine- matic constraints, we sample uniformly among permissable joint angles, and then use forward kinematics to propagate samples from ^                                                          pn-1(xi | y) /mn-1 (x                                                                          ji      i) to hypothesized configurations of xj. Following [12], temporal messages are determined by adjusting the bandwidths of the current marginal estimate ^                                                      p(xi | y) to match the temporal covariance i. Because structural potentials (eq. (2)) equal one for all state configurations outside some ball, the ideal structural messages are not finitely integrable. We therefore approximate the structural message m (x                         ij    j ) as an analytic function equal to the weights of all kernels in ^ p(xi | y) outside a ball centered at qj, the position of xj."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/1102a326d5f7c9e04fc3c89d0ede88c9-Abstract.html,An Investigation of Practical Approximate Nearest Neighbor Algorithms,"Ting Liu, Andrew W. Moore, Ke Yang, Alexander G. Gray","This paper concerns approximate nearest neighbor searching algorithms,          which have become increasingly important, especially in high dimen-          sional perception areas such as computer vision, with dozens of publica-          tions in recent years. Much of this enthusiasm is due to a successful new          approximate nearest neighbor approach called Locality Sensitive Hash-          ing (LSH). In this paper we ask the question: can earlier spatial data          structure approaches to exact nearest neighbor, such as metric trees, be          altered to provide approximate answers to proximity queries and if so,          how? We introduce a new kind of metric tree that allows overlap: certain          datapoints may appear in both the children of a parent. We also intro-          duce new approximate k-NN search algorithms on this structure. We          show why these structures should be able to exploit the same random-          projection-based approximations that LSH enjoys, but with a simpler al-          gorithm and perhaps with greater efficiency. We then provide a detailed          empirical evaluation on five large, high dimensional datasets which show          up to 31-fold accelerations over LSH. This result holds true throughout          the spectrum of approximation levels."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/110eec23201d80e40d0c4a48954e2ff5-Abstract.html,Hierarchical Distributed Representations for Statistical Language Modeling,"John Blitzer, Fernando Pereira, Kilian Q. Weinberger, Lawrence K. Saul","Statistical language models estimate the probability of a word occurring            in a given context. The most common language models rely on a discrete            enumeration of predictive contexts (e.g., n-grams) and consequently fail            to capture and exploit statistical regularities across these contexts. In this            paper, we show how to learn hierarchical, distributed representations of            word contexts that maximize the predictive value of a statistical language            model. The representations are initialized by unsupervised algorithms for            linear and nonlinear dimensionality reduction [14], then fed as input into            a hierarchical mixture of experts, where each expert is a multinomial dis-            tribution over predicted words [12]. While the distributed representations            in our model are inspired by the neural probabilistic language model of            Bengio et al. [2, 3], our particular architecture enables us to work with            significantly larger vocabularies and training corpora. For example, on a            large-scale bigram modeling task involving a sixty thousand word vocab-            ulary and a training corpus of three million sentences, we demonstrate            consistent improvement over class-based bigram models [10, 13]. We            also discuss extensions of our approach to longer multiword contexts.
1      Introduction
Statistical language models are essential components of natural language systems for human-computer interaction. They play a central role in automatic speech recognition [11], machine translation [5], statistical parsing [8], and information retrieval [15]. These mod- els estimate the probability that a word will occur in a given context, where in general a context specifies a relationship to one or more words that have already been observed. The simplest, most studied case is that of n-gram language modeling, where each word is predicted from the preceding n-1 words. The main problem in building these models is that the vast majority of word combinations occur very infrequently, making it difficult to estimate accurate probabilities of words in most contexts.
Researchers in statistical language modeling have developed a variety of smoothing tech- niques to alleviate this problem of data sparseness. Most smoothing methods are based on simple back-off formulas or interpolation schemes that discount the probability of observed events and assign the ""leftover"" probability mass to events unseen in training [7]. Unfortu- nately, these methods do not typically represent or take advantage of statistical regularities
among contexts. One expects the probabilities of rare or unseen events in one context to be related to their probabilities in statistically similar contexts. Thus, it should be possible to estimate more accurate probabilities by exploiting these regularities.
Several approaches have been suggested for sharing statistical information across contexts. The aggregate Markov model (AMM) of Saul and Pereira [13] (also discussed by Hofmann and Puzicha [10] as a special case of the aspect model) factors the conditional probability table of a word given its context by a latent variable representing context ""classes"". How- ever, this latent variable approach is difficult to generalize to multiword contexts, as the size of the conditional probability table for class given context grows exponentially with the context length.
The neural probabilistic language model (NPLM) of Bengio et al. [2, 3] achieved signifi- cant improvements over state-of-the-art smoothed n-gram models [6]. The NPLM encodes contexts as low-dimensional continuous vectors. These are fed to a multilayer neural net- work that outputs a probability distribution over words. The low-dimensional vectors and the parameters of the network are trained simultaneously to minimize the perplexity of the language model. This model has no difficulty encoding multiword contexts, but its training and application are very costly because of the need to compute a separate normalization for the conditional probabilities associated to each context.
In this paper, we introduce and evaluate a statistical language model that combines the advantages of the AMM and NPLM. Like the NPLM, it can be used for multiword con- texts, and like the AMM it avoids per-context normalization. In our model, contexts are represented as low-dimensional real vectors initialized by unsupervised algorithms for di- mensionality reduction [14]. The probabilities of words given contexts are represented by a hierarchical mixture of experts (HME) [12], where each expert is a multinomial distri- bution over predicted words. This tree-structured mixture model allows a rich dependency on context without expensive per-context normalization. Proper initialization of the dis- tributed representations is crucial; in particular, we find that initializations from the results of linear and nonlinear dimensionality reduction algorithms lead to better models (with significantly lower test perplexities) than random initialization.
In practice our model is several orders of magnitude faster to train and apply than the NPLM, enabling us to work with larger vocabularies and training corpora. We present re- sults on a large-scale bigram modeling task, showing that our model also leads to significant improvements over comparable AMMs.
2    Distributed representations of words
Natural language has complex, multidimensional semantics. As a trivial example, consider the following four sentences:
                   The vase broke.      The vase contains water.                        The window broke.    The window contains water.

The bottom right sentence is syntactically valid but semantically meaningless. As shown by the table, a two-bit distributed representation of the words ""vase"" and ""window"" suffices to express that a vase is both a container and breakable, while a window is breakable but can- not be a container. More generally, we expect low dimensional continuous representations of words to be even more effective at capturing semantic regularities.
Distributed representations of words can be derived in several ways. In a given corpus of text, for example, consider the matrix of bigram counts whose element Cij records the number of times that word wj follows word wi. Further, let pij = Cij/                  C                                                                                   k    ik denote the conditional frequencies derived from these counts, and let pi denote the V -dimensional
frequency vector with elements pij, where V is the vocabulary size. Note that the vectors pi themselves provide a distributed representation of the words wi in the corpus. For large vocabularies and training corpora, however, this is an extremely unwieldy representation, tantamount to storing the full matrix of bigram counts. Thus, it is natural to seek a lower dimensional representation that captures the same information. To this end, we need to map each vector pi to some d-dimensional vector xi, with d               V . We consider two methods in dimensionality reduction for this problem. The results from these methods are then used to initialize the HME architecture in the next section.
2.1      Linear dimensionality reduction
The simplest form of dimensionality reduction is principal component analysis (PCA). PCA computes a linear projection of the frequency vectors pi into the low dimensional subspace that maximizes their variance. The variance-maximizing subspace of dimension- ality d is spanned by the top d eigenvectors of the frequency vector covariance matrix. The eigenvalues of the covariance matrix measure the variance captured by each axis of the subspace. The effect of PCA can also be understood as a translation and rotation of the frequency vectors pi, followed by a truncation that preserves only their first d elements.
2.2      Nonlinear dimensionality reduction
Intuitively, we would like to map the vectors pi into a low dimensional space where se- mantically similar words remain close together and semantically dissimilar words are far apart. Can we find a nonlinear mapping that does this better than PCA? Weinberger et al. recently proposed a new solution to this problem based on semidefinite programming [14].
Let xi denote the image of pi under this mapping. The mapping is discovered by first learning the V V matrix of Euclidean squared distances [1] given by Dij = |xi - xj|2. This is done by balancing two competing goals: (i) to co-locate semantically similar words, and (ii) to separate semantically dissimilar words. The first goal is achieved by fixing the distances between words with similar frequency vectors to their original values. In particu- lar, if pj and pk lie within some small neighborhood of each other, then the corresponding element Djk in the distance matrix is fixed to the value |pj - pk|2. The second goal is achieved by maximizing the sum of pairwise squared distances ijDij. Thus, we push the words in the vocabulary as far apart as possible subject to the constraint that the distances between semantically similar words do not change.
The only freedom in this optimization is the criterion for judging that two words are se- mantically similar. In practice, we adopt a simple criterion such as k-nearest neighbors in the space of frequency vectors pi and choose k as small as possible so that the resulting neighborhood graph is connected [14].
The optimization is performed over the space of Euclidean squared distance matrices [1]. Necessary and sufficient conditions for the matrix D to be interpretable as a Euclidean squared distance matrix are that D is symmetric and that the Gram matrix1 derived from G = - 1 HDHT is semipositive definite, where H = I - 1 11T. The optimization can             2                                                          V thus be formulated as the semidefinite programming problem:
             Maximize ijDij subject to: (i) DT = D, (ii) - 1 HDH                0, and                                                                         2                     (iii) Dij = |pi - pj|2 for all neighboring vectors pi and pj.

   1Assuming without loss of generality that the vectors xi are centered on the origin, the dot prod- ucts Gij = xi  xj are related to the pairwise squared distances Dij = |xi - xj |2 as stated above."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/1175defd049d3301e047ce50d93e9c7a-Abstract.html,Discrete profile alignment via constrained information bottleneck,"Sean O'rourke, Gal Chechik, Robin Friedman, Eleazar Eskin","Amino acid profiles, which capture position-specific mutation prob-           abilities, are a richer encoding of biological sequences than the in-           dividual sequences themselves. However, profile comparisons are           much more computationally expensive than discrete symbol com-           parisons, making profiles impractical for many large datasets. Fur-           thermore, because they are such a rich representation, profiles can           be difficult to visualize. To overcome these problems, we propose a           discretization for profiles using an expanded alphabet representing           not just individual amino acids, but common profiles. By using an           extension of information bottleneck (IB) incorporating constraints           and priors on the class distributions, we find an informationally           optimal alphabet. This discretization yields a concise, informative           textual representation for profile sequences. Also alignments be-           tween these sequences, while nearly as accurate as the full profile-           profile alignments, can be computed almost as quickly as those           between individual or consensus sequences. A full pairwise align-           ment of SwissProt would take years using profiles, but less than           3 days using a discrete IB encoding, illustrating how discrete en-           coding can expand the range of sequence problems to which profile           information can be applied.
1     Introduction
One of the most powerful techniques in protein analysis is the comparison of a target amino acid sequence with phylogenetically related or homologous proteins. Such comparisons give insight into which portions of the protein are important by revealing the parts that were conserved through natural selection. While mutations in non-functional regions may be harmless, mutations in functional regions are often lethal. For this reason, functional regions of a protein tend to be conserved between organisms while non-functional regions diverge.
 Department of Computer Science and Engineering, University of California San Diego      Department of Computer Science, Stanford University

Many of the state-of-the-art protein analysis techniques incorporate homologous sequences by representing a set of homologous sequences as a probabilistic profile, a sequence of the marginal distributions of amino acids at each position in the sequence. For example, Yona et al.[10] uses profiles to align distant homologues from the SCOP database[3]; the resulting alignments are similar to results from structural alignments, and tend to reflect both secondary and tertiary protein structure. The PHD algorithm[5] uses profiles purely for structure prediction. PSIBLAST[6] uses them to refine database searches.
Although profiles provide a lot of information about the sequence, the use of pro- files comes at a steep price. While extremely efficient string algorithms exist for aligning protein sequences (Smith-Waterman[8]) and performing database queries (BLAST[6]), these algorithms operate on strings and are not immediately applica- ble to profile alignment or profile database queries. While profile-based methods can be substantially more accurate than sequence-based ones, they can require at least an order of magnitude more computation time, since substitution penalties must be calculated by computing distances between probability distributions. This makes profiles impractical for use with large bioinformatics databases like SwissProt, which recently passed 150,000 sequences. Another drawback of profile as compared to string representations is that it is much more difficult to visually interpret a sequence of 20 dimensional vectors than a sequence of letters.
Discretizing the profiles addresses both of these problems. First, once a profile is rep- resented using a discrete alphabet, alignment and database search can be performed using the efficient string algorithms developed for sequences. For example, when aligning sequences of 1000 elements, runtime decreases from 20 seconds for profiles to 2 for discrete sequences. Second, by representing each class as a letter, discretized profiles can be presented in plain text like the original or consensus sequences, while conveying more information about the underlying profiles. This makes them more accurate than consensus sequences, and more dense than sequence logos (see figure 1). To make this representation intuitive, we want the discretization not only to minimize information loss, but also to reflect biologically meaningful categories by forming a superset of the standard 20-character amino acid alphabet. For example, we use ""A"" and ""a"" for strongly- and weakly-conserved Alanine. This formulation demands two types of constraints: similarities of the centroids to predefined values, and specific structural similarities between strongly- and weakly-conserved variants. We show below how these constraints can be added to the original IB formalism.
In this paper, we present a new discrete representation of proteins that takes into account information from homologues. The main idea behind our approach is to compress the space of probabilistic profiles in a data-dependent manner by clustering the actual profiles and representing them by a small alphabet of distributions. Since this discretization removes some of the information carried by the full profiles, we cluster the distribution in a way that is directly targeted at minimizing the information loss. This is achieved using a variant of Information Bottleneck (IB)[9], a distributional clustering approach for informationally optimal discretization.
We apply our algorithm to a subset of MEROPS[4], a database of peptidases or- ganized structurally by family and clan, and analyze the results in terms of both information loss and alignment quality. We show that multivariate IB in particular preserves much of the information in the original profiles using a small number of classes. Furthermore, optimal alignments for profile sequences encoded with these classes are much closer to the original profile-profile alignments than are alignments between the seed proteins. IB discretization is therefore an attractive way to gain some of the additional sensitivity of profiles with less computational cost.
0.0       0.0    0.0    0.09    0.34    0.23    0.12    0.0    0.0    0.0  0.0       0.0    0.0    0.04    0.01    0.01    0.03    0.0    0.0    0.0  0.0       0.0    1.0    0.01    0.05    0.14    0.09    0.0    1.0    0.0  0.0       0.0    0.0    0.38    0.04    0.00    0.04    0.0    0.0    0.0  0.0       0.0    0.0    0.06    0.00    0.08    0.04    0.0    0.0    1.0  0.0       0.0    0.0    0.00    0.06    0.01    0.03    1.0    0.0    0.0  0.0       0.0    0.0    0.02    0.00    0.04    0.00    0.0    0.0    0.0    N  0.0       0.0    0.0    0.00    0.00    0.03    0.00    0.0    0.0    0.0       ND                        GDF  0.0       0.0    0.0    0.04    0.01    0.01    0.00    0.0    0.0    0.0    S           EAAS                                                                                           S                                                                                               V                                                                                                  PT                                                                                               S                                                                                           A            A                                                                                               T                                                                                                  D                                                                                           F      F                                                                                                        D                                                                                           N                                                                                               G                                                                                                  S                                                                                                        L                                                                                           K                                                                                               D                                                                                           Q
                                                                                                   E

                                                                                             T                                                                                           R                                                                                               N                                                                                                  H

                                                                                                   F

                                                                                             Y





                                                                                                   V  0.0       0.0    0.0    0.01    0.01    0.00    0.09    0.0    0.0    0.0                    Q                                                                                                        Y


                                                                                          E





                                                                             A                                                                                      A                     A                                                                                                               A                                                                                                                   A                                                                               A

0.0       0.0    0.0    0.00    0.00    0.03    0.00    0.0    0.0    0.0                     (b)  0.5       1.0    0.0    0.05    0.05    0.01    0.01    0.0    0.0    0.0  0.0       0.0    0.0    0.02    0.00    0.23    0.00    0.0    0.0    0.0    P00790 Seq.:            ---EAPT---  0.0       0.0    0.0    0.04    0.05    0.00    0.00    0.0    0.0    0.0    Consensus Seq.:         NNDEAASGDF  0.0       0.0    0.0    0.04    0.01    0.00    0.00    0.0    0.0    0.0  0.5       0.0    0.0    0.16    0.10    0.06    0.29    0.0    0.0    0.0    IB Seq.:                NNDeaptGDF  0.0       0.0    0.0    0.02    0.10    0.05    0.20    0.0    0.0    0.0  0.0       0.0    0.0    0.00    0.14    0.03    0.04    0.0    0.0    0.0                     (c)  0.0       0.0    0.0    0.00    0.00    0.00    0.00    0.0    0.0    0.0  0.0       0.0    0.0    0.01    0.00    0.04    0.04    0.0    0.0    0.0
                                     (a)

Figure 1: (a) Profile, (b) sequence logo[2], and (c) textual representations for part of an alignment of Pepsin A precursor P00790, showing IB's concision compared to profiles and logos, and its precision compared to single sequences.
2         Information Bottleneck
Information Bottleneck [9] is an information theoretic approach for distributional clustering. Given a joint distribution p(X, Y ) of two random variables X and Y , the goal is to obtain a compressed representation C of X, while preserving the informa- tion about Y . The two goals of compression and information preservation are quan- tified by the same measure of mutual information I(X; Y ) =                                            p(x, y) log p(x,y)                                                                                                x,y               p(x)p(y) and the problem is therefore defined as the constrained optimization problem minp(c|x):I(C;Y )>K I(C; X) where K is a constraint on the level of information preserved about Y , and the problem should also obey the constraints p(y|c) =           p(y|x)p(x|c) and p(y) =                               p(y|x)p(x). This constrained optimization can be      x                                                    x reformulated using Lagrange multipliers, and turned into a tradeoff optimization function with Lagrange multiplier :
              min L def                                  = I(C; X) - I(C; Y )                                                                (1)                   p(c|x)

As an unsupervised learning technique, IB aims to characterize the set of solutions for the complete spectrum of constraint values K. This set of solutions is identical to the set of solutions of the tradeoff optimization problem obtained for the spectrum of  values.
When X is discrete, its natural compression is fuzzy clustering. In this case, the problem is not convex and cannot be guaranteed to contain a single global minimum. Fortunately, its solutions can be characterized analytically by a set of self consistent equations. These self consistent equations can then be used in an iterative algorithm that is guaranteed to converge to a local minimum. While the optimal solutions of the IB functional are in general soft clusters, in practice, hard cluster solutions are sometimes more easily interpreted. A series of algorithms was developed for hard IB, including an algorithm that can be viewed as a one-step look-ahead sequential version of K-Means [7].
To apply IB to the problem of profiles discretization discussed here, X is a given set of probabilistic profiles obtained from a set of aligned sequences and Y is the set of 20 amino acids.
2.1     Constraints on centroids' semantics
The application studied in this paper differs from standard IB applications in that we are interested in obtaining a representation that is both efficient and biologi- cally meaningful. This requires that we add two kinds of constraints on clusters' distributions, discussed below.
First, some clusters' meanings are naturally determined by limiting them to corre- spond to the common 20-letter alphabet used to describe amino acids. From the point of view of distributions over amino acids, each of these symbols is used today as the delta function distribution which is fully concentrated on a single amino acid. For the goal of finding an efficient representation, we require the centroids to be close to these delta distributions. More generally, we require the centroids to be close to some predefined values ^                                               ci, thus adding constraints to the IB target function of the form DKL[p(y|^                               ci)||p(y|ci)] < Ki for each constrained centroid. While solving the constrained optimization problem is difficult, the corresponding tradeoff opti- mization problem can be made very similar to standard IB. With the additional constraints, the IB trade-off optimization problem becomes
      min L  I(C; X) - I(C; Y ) +                                    (ci)DKL[p(y|^                                                                                                ci)||p(y|ci)]      .    (2)          p(c|x)                                                                     ciC

We now use the following identity
                        p(x, c)DKL[p(y|x)||p(y|c)]                      x,c

           =            p(x)          p(y|x) log p(y|x) -                p(c)         log p(y|c)         p(y|x)p(x|c)

                  x              y                                 c             y                  x

           =     -H(Y |X) + H(Y |C) = I(X; Y ) - I(Y ; C)

to rewrite the IB functional of Eq. (1) as
          L = I(C; X) +                         p(x, c)DKL[p(y|x)||p(y|c)] - I(X; Y )                                           cC xX

When          (ci)  1 we can similarly rewrite Eq. (2) as
     L      =     I(C; X) +                   p(x)            p(ci|x)DKL[p(y|x)||p(y|ci)]                         (3)                                             xX            ciC

                  +             (ci)DKL[p(y|^                                                             ci)||p(y|ci)] - I(X; Y )                              ciC

            = I(C; X) +                       p(x )            p(ci|x )DKL[p(y|x )||p(y|ci)] - I(X; Y )                                            x X             ciC

The optimization problem therefore becomes equivalent to the original IB problem, but with a modified set of samples x  X , containing X plus additional ""pseudo- counts"" or biases. This is similar to the inclusion of priors in Bayesian estimation. Formulated this way, the biases can be easily incorporated in standard IB algorithms by adding additional pseudo-counts x with prior probability p(x ) = i(c).
2.2     Constraints on relations between centroids
We want our discretization to capture correlations between strongly- and weakly- conserved variants of the same symbol. This can be done with standard IB using
separate classes for the alternatives. However, since the distributions of other amino acids in these two variants are likely to be related, it is preferable to define a single shared prior for both variants, and to learn a model capturing their correlation.
Friedman et al.[1] describe multivariate information bottleneck (mIB), an extension of information bottleneck to joint distributions over several correlated input and cluster variables. For profile discretization, we define two compression variables connected as in Friedman's ""parallel IB"": an amino acid class C  {A, C, . . .} with an associated prior, and a strength S  {0, 1}. Since this model correlates strong and weak variants of each category, it requires fewer priors than simple IB. It also has fewer parameters: a multivariate model with ns strengths and nc classes has as many categories as a univariate one with nc = nsnc classes, but has only ns +nc -2 free parameters for each x, instead of nsnc - 1."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/148260a1ce4fe4907df4cd475c442e28-Abstract.html,Multiple Relational Embedding,"Roland Memisevic, Geoffrey E. Hinton","We describe a way of using multiple different types of similarity rela-          tionship to learn a low-dimensional embedding of a dataset. Our method          chooses different, possibly overlapping representations of similarity by          individually reweighting the dimensions of a common underlying latent          space. When applied to a single similarity relation that is based on Eu-          clidean distances between the input data points, the method reduces to          simple dimensionality reduction. If additional information is available          about the dataset or about subsets of it, we can use this information to          clean up or otherwise improve the embedding. We demonstrate the po-          tential usefulness of this form of semi-supervised dimensionality reduc-          tion on some simple examples."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/1680829293f2a8541efa2647a0290f88-Abstract.html,Conditional Models of Identity Uncertainty with Application to Noun Coreference,"Andrew McCallum, Ben Wellner","Coreference analysis, also known as record linkage or identity uncer-          tainty, is a difficult and important problem in natural language process-          ing, databases, citation matching and many other tasks. This paper intro-          duces several discriminative, conditional-probability models for coref-          erence analysis, all examples of undirected graphical models. Unlike          many historical approaches to coreference, the models presented here          are relational--they do not assume that pairwise coreference decisions          should be made independently from each other. Unlike other relational          models of coreference that are generative, the conditional model here can          incorporate a great variety of features of the input without having to be          concerned about their dependencies--paralleling the advantages of con-          ditional random fields over hidden Markov models. We present positive          results on noun phrase coreference in two standard text data sets."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/1680e9fa7b4dd5d62ece800239bb53bd-Abstract.html,Outlier Detection with One-class Kernel Fisher Discriminants,Volker Roth,"The problem of detecting ""atypical objects"" or ""outliers"" is one of the          classical topics in (robust) statistics. Recently, it has been proposed to          address this problem by means of one-class SVM classifiers. The main          conceptual shortcoming of most one-class approaches, however, is that in          a strict sense they are unable to detect outliers, since the expected fraction          of outliers has to be specified in advance. The method presented in this          paper overcomes this problem by relating kernelized one-class classifica-          tion to Gaussian density estimation in the induced feature space. Having          established this relation, it is possible to identify ""atypical objects"" by          quantifying their deviations from the Gaussian model. For RBF kernels          it is shown that the Gaussian model is ""rich enough"" in the sense that it          asymptotically provides an unbiased estimator for the true density. In or-          der to overcome the inherent model selection problem, a cross-validated          likelihood criterion for selecting all free model parameters is applied.
1     Introduction
A one-class-classifier attempts to find a separating boundary between a data set and the rest of the feature space. A natural application of such a classifier is estimating a contour line of the underlying data density for a certain quantile value. Such contour lines may be used to separate ""typical"" objects from ""atypical"" ones. Objects that look ""sufficiently atypical"" are often considered to be outliers, for which one rejects the hypothesis that they come from the same distribution as the majority of the objects. Thus, a useful application scenario would be to find a boundary which separates the jointly distributed objects from the outliers. Finding such a boundary defines a classification problem in which, however, usually only sufficiently many labeled samples from one class are available. Usually no labeled samples from the outlier class are available at all, and it is even unknown if there are any outliers present.
It is interesting to notice that the approach of directly estimating a boundary, as opposed to first estimating the whole density, follows one of the main ideas in learning theory which states that one should avoid solving a too hard intermediate problem. While this line of rea- soning seems to be appealing from a theoretical point of view, it leads to a severe problem in practical applications: when it comes to detecting outliers, the restriction to estimating only a boundary makes it impossible to derive a formal characterization of outliers with- out prior assumptions on the expected fraction of outliers or even on their distribution. In practice, however, any such prior assumptions can hardly be justified. The fundamental
problem of the one-class approach lies in the fact that outlier detection is a (partially) unsu- pervised task which has been ""squeezed"" into a classification framework. The missing part of information has been shifted to prior assumptions which can probably only be justified, if the solution of the original problem was known in advance.
This paper aims at overcoming this problem by linking kernel-based one-class classifiers to Gaussian density estimation in the induced feature space. Objects which have an ""unex- pected"" high Mahalanobis distance to the sample mean are considered as ""atypical objects"" or outliers. A particular Mahalanobis distance is considered to be unexpected, if it is very unlikely to observe an object that far away from the mean vector in a random sample of a certain size. We will formalize this concept in section 3 by way of fitting linear models in quantile-quantile plots. The main technical ingredient of our method is the one-class kernel Fisher discriminant classifier (OC-KFD), for which the relation to Gaussian density estimation is shown. From the classification side, the OC-KFD-based model inherits the simple complexity control mechanism by using regularization techniques. The explicit re- lation to Gaussian density estimation, on the other hand, makes it possible to formalize the notion of atypical objects by observing deviations from the Gaussian model. It is clear that these deviations will heavily depend on the chosen model parameters. In order to derive an objective characterization of atypical objects it is, thus, necessary to select a suitable model in advance. This model-selection problem is overcome by using a likelihood-based cross-validation framework for inferring the free parameters.
2         Gaussian density estimation and one-class LDA
Let X denote the n  d data matrix which contains the n input vectors xi  Rd as rows. It has been proposed to estimate a one-class decision boundary by separating the dataset from the origin [12], which effectively coincides with replicating all xi with opposite sign and separating X and -X. Typically, a -SVM classifier with RBF kernel function is used. The parameter  bounds the expected number of outliers and must be selected a priori. The method proposed here follows the same idea of separating the data from their negatively replicated counterparts. Instead of a SVM, however, a Kernel Fisher Discriminant (KFD) classifier is used [7, 10]. The latter has the advantage that is is closely related to Gaussian density estimation in the induced feature space. By making this relation explicit, outliers can be identified without specifying the expected fraction of outliers in advance. We start with a linear discriminant analysis (LDA) model, and then kernels will be introduced.
Let X = (X, -X)                   denote the augmented (2n  d) data matrix which also contains the negative samples -xi. Without loss of generality we assume that the sample mean +                x               i         i > 0, so that the sample means of the positive data and the negative data differ: + = -. Let us now assume that our data are realizations of a normally distributed random variable in d dimensions: X  Nd(, ). Denoting by Xc the centered data matrix, the estimator for  takes the form ^                                                      W = (1/n)Xc Xc.
The LDA solution                                                          B                                 maximizes the between-class scatter            with B = ++ +                                                                    W  = 1      -    - under the constraint on the within-class scatter                    . Note that in our special case with X = (X, -X) the usual pooled within-class matrix W simply reduces to the above defined W = (1/n)Xc Xc. Denoting by y = (2, . . . , 2, -2, . . . , -2)                 a 2n-indicator vector for class membership in class ""+"" or ""-"", it is well-known (see e.g. [1]) that the LDA solution (up to a scaling factor) can be found by minimizing a least-squares functional: ^                     = arg min y -X  2. In [3] a slightly more general form of the problem is described where the above functional is minimized under a constrained on , which in the simplest case amounts to adding a term   to the functional. Such a ridge regression model assumes a penalized total covariance of the form T = (1/(2n))  X                  X + I = (1/n)  X X + I. Defining an n-vector of ones y = (1, . . . , 1) , the solution vector ^                                                                                                    
reads                   ^                    = (X     X + I)-1X y = (X X + I)-1X y.                                   (1)
According to [3], an appropriate scaling factor is defied in terms of the quantity s2 = (1/n)  y ^            y = (1/n)  y X ^                                    , which leads us to the correctly scaled LDA vector  =                                                                                            s-1(1 - s2)-1/2 ^                    that fulfills the normalization condition  W  = 1                                                                            .
One further derives from [3] that the mean vector of X, projected onto the 1-dimensional LDA-subspace has the coordinate value m+ = s(1 - s2)-1/2, and that the Mahalanobis distance from a vector x to the sample mean + is the sum of the squared Euclidean distance in the projected space and an orthogonal distance term:
 D(x, +) = ( x - m                                                x)2 + x T -1x.                              +)2 + D with D = -(1 - s2)(                                  (2)

Note that it is the term D which makes the density estimation model essentially different from OC-classification: while the latter considers only distances in the direction of the projection vector , the true density model additionally takes into account the distances in the orthogonal subspace.
Since the assumption X  Nd(, ) is very restrictive, we propose to relax it by assuming that we have found a suitable transformation of our input data  : Rd  Rp, x  (x), such that the transformed data are Gaussian in p dimensions. If the transformation is carried out implicitly by introducing a Mercer kernel k(xi, xj), we arrive at an equivalent problem in terms of the kernel matrix K =           and the expansion coefficients :
                                    ^                                         = (K + I)-1y.                                        (3)

From [11] it follows that the mapped vectors can be represented in Rn as (x) = K-1/2k(x), where k(x) denotes the kernel vector k(x) = (k(x, x1), . . . , k(x, xn)) . Finally we derive the following form of the Mahalanobis distances which again consist of the Euclidean distance in the classification subspace plus an orthogonal term:
        D(x, +) = ( k(x) - m                                 k(x))2 + n(x),                                              +)2 - (1 - s2)(                                (4)

where (x)      =       (x)(  + I)-1(x)              =     k (x)(K + I)-1K-1k(x), m+ = s(1 - s2)-1/2, s2 = (1/n)  y ^                                              y = (1/n)  y K ^                                                                , and  = s-1(1 - s2)-1/2 ^                                                                                                .
Equation (4) establishes the desired link between OC-KFD and Gaussian density estima- tion, since for our outlier detection mechanism only Mahalanobis distances are needed. While it seems to be rather complicated to estimate a density by the above procedure, the main benefit over directly estimating the mean and the covariance lies in the inherent com- plexity regulation properties of ridge regression. Such a complexity control mechanism is of particular importance in highly nonlinear kernel models. Moreover, for ridge-regression models it is possible to analytically calculate the effective degrees of freedom, a quantity that will be of particular interest when it comes to detecting outliers.
3    Detecting outliers
Let us assume that the model is completely specified, i.e. both the kernel function k(, ) and the regularization parameter  are fixed. The central lemma that helps us to detect outliers can be found in most statistical textbooks:
Lemma 1. Let X be a Gaussian random variable X  Nd(, ). Then   (X - ) -1(X - ) follows a chi-square (2) distribution on d degrees of freedom.
For the penalized regression models, it might be more appropriate to use the effective de- grees of freedom df instead of d in the above lemma. In the case of one-class LDA with ridge penalties we can easily estimate it as df = trace(X(X X + I)-1X ), [8], which
for a kernel model translates into df = trace(K(K + I)-1). The intuitive interpretation of the quantity df is the following: denoting by V the matrix of eigenvectors of K and by {i}ni=1 the corresponding eigenvalues, the fitted values ^                                                               y read
                        ^                            y = V diag {i = i/(i + )} V y.                                 (5)

It follows that compared to the unpenalized case, where all eigenvectors vi are constantly weighted by 1, the contribution of the i-th eigenvector vi is down-weighted by a factor i/1 = i. If the ordered eigenvalues decrease rapidly, however, the values i are either close to zero or close to one, and df determines the number of terms that are ""essentially different"" from zero. The same is true for the orthogonal distance term in eq. (4): note that
(x) = k (x)(K + I)-1K-1k(x) = k V diag i = ((i + )i)-1 V k(x). (6)
Compared to the unpenalized case (the contribution of vi is weighted by -2                                                                                i    ), the contri- bution of vi is down-weighted by the same factor i/-2 =                                                           i        i .
From lemma 1 we conclude that if the data are well described by a Gaussian model in the kernel feature space, the observed Mahalanobis distances should look like a sample from a 2-distribution with df degrees of freedom. A graphical way to test this hypothesis is to plot the observed quantiles against the theoretical 2 quantiles, which in the ideal case gives a straight line. Such a quantile-quantile plot is constructed as follows: Let (i) denote the observed Mahalanobis distances ordered from lowest to highest, and pi the cumulative proportion before each (i) given by pi = (i - 1/2)/n. Let further zi = F -1pi denote the theoretical quantile at position pi, where F is the cumulative 2-distribution function. The quantile-quantile plot is then obtained by plotting (i) against zi. Deviations from linearity can be formalized by fitting a linear model on the observed quantiles and calculating confidence intervals around the fit. Observations falling outside the confidence interval are then treated as outliers. A potential problem of this approach is that the outliers themselves heavily influence the quantile-quantile fit. In order to overcome this problem, the use of robust fitting procedures has been proposed in the literature, see e.g. [4]. In the experiments below we use an M-estimator with Huber loss function. For estimating confidence intervals around the fit we use the standard formula (see [2, 5])
                    ((i)) = b  (2(zi))-1 (pi(1 - pi))/n,                              (7)

which can be intuitively understood as the product of the slope b and the standard error of the quantiles. A 100(1 - )% envelope around the fit is then defined as (i)  z/2((i)) where z/2 is the 1 - (1 - )/2 quantile of the standard normal distribution.
The choice of the confidence level  is somewhat arbitrary, and from a conceptual viewpoint one might even argue that the problem of specifying one free parameter (i.e. the expected fraction of outliers) has simply been transferred into the problem of specifying another one. In practice, however, selecting  is a much more intuitive procedure than guessing the fraction of outliers. Whereas the latter requires problem-specific prior knowledge which is hardly available in practice, the former depends only on the variance of a linear model fit. Thus,  can be specified in a problem independent way.
4    Model selection
In our model the data are first mapped into some feature space, in which then a Gaussian model is fitted. Mahalanobis distances to the mean of this Gaussian are computed by evaluating (4). The feature space mapping is implicitly defined by the kernel function, for which we assume that it is parametrized by a kernel parameter . For selecting all free parameters in (4), we are, thus, left with the problem of selecting  = (, ) .
The idea is now to select  by maximizing the cross-validated likelihood. From a theoret- ical viewpoint, the cross-validated (CV) likelihood framework is appealing, since in [13]
the CV likelihood selector has been shown to asymptotically perform as well as the opti- mal benchmark selector which characterizes the best possible model (in terms of Kullback- Leibler divergence to the true distribution) contained in the parametric family.
For kernels that map into a space with dimension p > n, however, two problems arise: (i) the subspace spanned by the mapped samples varies with different sample sizes; (ii) not the whole feature space is accessible for vectors in the input space. As a consequence, it is difficult to find a ""proper"" normalization of the Gaussian density in the induced feature space. We propose to avoid this problem by considering the likelihood in the input space rather than in the feature space, i.e. we are looking for a properly normalized density model p(x|) in Rd such that p(x|) has the same contour lines as the Gaussian model in the feature space: p(xi|) = p(xi|)  p((xi)|) = p((xj)|). Denoting by Xn = {xi}ni=1 a sample from p(x) from which the kernel matrix K is built, a natural input space model is
         pn(x|Xn, ) = Z-1 exp{- 1 D(x; X                                p                                       2          n, )}, with Z =      Rd         n(x|Xn, ) dx,            (8) where D(x; Xn, ) denotes the (parametrized) Mahalanobis distances (4) of a Gaussian model in the feature space. Note that this density model in the input space has the same form as our Gaussian model in the feature space, except for the different normalization constant Z. Computing this constant Z requires us to solve a normalization integral over the whole d-dimensional input space. Since in general this integral is not analytically tractable for nonlinear kernel models, we propose to approximate Z by a Monte Carlo sampling method. In our experiments, for instance, the VEGAS algorithm [6], which implements a mixed importance-stratified sampling approach, showed to be a reasonable method for up to 10 input dimensions.

By using the CV likelihood framework we are guaranteed to (asymptotically) perform as well as the best model in the parametrized family. Thus, the question arises whether the family of densities defined by a Gaussian model in a kernel-induced feature space is ""rich enough"" such that no systematic errors occur. For RBF kernels, the following lemma pro- vides a positive answer to this question. Lemma 2. Let k(x                                   2                         i, xj ) = exp(- xi - xj         /). As   0 , pn(x|Xn, ) converges to a Parzen window with vanishing kernel width: p                                    n                                                         n(x|Xn, )  1                   (x - x                                                                        n          i=1               i).
A formal proof is omitted due to space limitations. The basic ingredients of the proof are: (i) In the limit   0 the expansion coefficients approach ^                                                                           1/(1 + )1. Thus, ^ y = K ^                 1/(1 + )1 and s2  1/(1 + ). (ii) D(x; , )  C(x) < , if x  {x                                                                                 n       i}n        i=1, and D(x; , )  , else. Finally pn(x|Xn, , )  1                          (x - x                                                                         n          i=1               i).
Note that in the limit   0 a Parzen window becomes an unbiased estimator for any continuous density, which provides an asymptotic justification for our approach: the cross- validated likelihood framework guarantees us to convergence to a model that performs as well as the best model in our model class as n  . The latter, however, is ""rich enough"" in the sense that it contains models which in the limit   0 converge to an unbiased estimator for every continuous p(x). Since contour lines of pn(x) are contour lines of a Gaussian model in the feature space, the Mahalanobis distances are expected to follow a 2 distribution, and atypical objects can be detected by observing the distribution of the empirical Mahalanobis distances as described in the last section.
It remains to show that describing the data as a Gaussian in a kernel-induced feature space is a statistically sound model. This is actually the case, since there exist decay rates for the kernel width  such that n grows at a higher rate as the effective degrees of freedom df : Lemma 3. Let k(x                                   2                         i, xj ) = exp(- xi - xj         /) and pn(x|Xn, , ) defined by (8). If   1 decays like O(n-1/2), and for fixed   1, the ratio df /n  0 as n  .
A formal proof is omitted due to space limitations. The basic ingredients of the proof are: (i) the eigenvalues i of (1/n)K converge to i as n  , (ii) the eigenvalue spectrum of
a Gaussian RBF kernel decays at an exponential-quadratic rate:                                                                               i  exp(-i2), (iii) for n sufficiently large, it holds that        n      1/[1 + (/n) exp(n-1/2i2)]  n1/2-1 log(n/)                                           i=1 (proof by induction, using the fact that ln(n + 1) - ln(n)  1/(n2 + n) which follows from a Taylor expansion of the logarithm)  df (n)/n  0."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/17b3c7061788dbe82de5abe9f6fe22b3-Abstract.html,Kernel Projection Machine: a New Tool for Pattern Recognition,"Laurent Zwald, Gilles Blanchard, Pascal Massart, Régis Vert","This paper investigates the effect of Kernel Principal Component Analy-             sis (KPCA) within the classification framework, essentially the regular-             ization properties of this dimensionality reduction method. KPCA has             been previously used as a pre-processing step before applying an SVM             but we point out that this method is somewhat redundant from a reg-             ularization point of view and we propose a new algorithm called Ker-             nel Projection Machine to avoid this redundancy, based on an analogy             with the statistical framework of regression for a Gaussian white noise             model. Preliminary experimental results show that this algorithm reaches             the same performances as an SVM.
1      Introduction
Let (xi, yi)i=1...n be n given realizations of a random variable (X, Y ) living in X  {-1;1}. Let P denote the marginal distribution of X. The xi's are often referred to as inputs (or patterns), and the yi's as labels. Pattern recognition is concerned with finding a classifier, i.e. a function that assigns a label to any new input x  X and that makes as few prediction errors as possible. It is often the case with real world data that the dimension of the patterns is very large, and some of the components carry more noise than information. In such cases, reducing the dimension of the data before running a classification algorithm on it sounds reasonable. One of the most famous methods for this kind of pre-processing is PCA, and its kernelized version (KPCA), introduced in the pioneering work of Sch olkopf, Smola and M uller [8].
   This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-506778.

Now, whether the quality of a given classification algorithm can be significantly improved by using such pre-processed data still remains an open question. Some experiments have already been carried out to investigate the use of KPCA for classification purposes, and numerical results are reported in [8]. The authors considered the USPS handwritten digit database and reported the test error rates achieved by the linear SVM trained on the data pre-processed with KPCA: the conclusion was that the larger the number of principal com- ponents, the better the performance. In other words, the KPCA step was useless or even counterproductive. This conclusion might be explained by a redundancy arising in their experiments: there is actually a double regularization, the first corresponding to the dimensionality reduction achieved by KPCA, and the other to the regularization achieved by the SVM. With that in mind it does not seem so surprising that KPCA does not help in that case: whatever the dimensionality reduction, the SVM anyway achieves a (possibly strong) regularization. Still, de-noising the data using KPCA seems relevant. The aforementioned experiments suggest that KPCA should be used together with a classification algorithm that is not regu- larized (e.g. a simple empirical risk minimizer): in that case, it should be expected that the KPCA is by itself sufficient to achieve regularization, the choice of the dimension being guided by adequate model selection. In this paper, we propose a new algorithm, called the Kernel Projection Machine (KPM), that implements this idea: an optimal dimension is sought so as to minimize the test error of the resulting classifier. A nice property is that the training labels are used to select the optimal dimension  optimal means that the resulting D-dimensional representation of the data contains the right amount of information needed to classify the inputs. To sum up, the KPM can be seen as a dimensionality-reduction-based classification method that takes into account the labels for the dimensionality reduction step. This paper is organized as follows: Section 2 gives some statistical background on regular- ized method vs. projection methods. Its goal is to explain the motivation and the ""Gaussian intuition"" that lies behind the KPM algorithm from a statistical point of view. Section 3 explicitly gives the details of the algorithm; experiments and results, which should be con- sidered preliminary, are reported in Section 4.
2      Motivations for the Kernel Projection Machine
2.1    The Gaussian Intuition: a Statistician's Perspective
Regularization methods have been used for quite a long time in non parametric statistics since the pioneering works of Grace Wahba in the eighties (see [10] for a review). Even if the classification context has its own specificity and offers new challenges (especially when the explanatory variables live in a high dimensional Euclidean space), it is good to remember what is the essence of regularization in the simplest non parametric statistical framework: the Gaussian white noise. So let us assume that one observes a noisy signal dY (x) = s(x)dx + 1                                                                              dw(x) , Y (0) = 0                                                                              n on [0,1] where dw(x) denotes standard white noise. To the reader not familiar with this model, it should be considered as nothing more but an idealization of the well-known fixed design regression problem Yi = s(i/n) + i for i = 1, . . . , n, where i  N(0, 1), where the goal is to recover the regression function s. (The white noise model is actually simpler to study from a mathematical point of view). The least square criterion is defined as
                                                    1                              n(f ) = f 2 - 2                f (x)dY (x)                                                    0

for every f  L2([0, 1]). Given a Mercer kernel k on [0, 1][0, 1], the regularization least square procedure proposes"
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/17f98ddf040204eda0af36a108cbdea4-Abstract.html,Seeing through water,"Alexei Efros, Volkan Isler, Jianbo Shi, Mirkó Visontai","We consider the problem of recovering an underwater image distorted by            surface waves. A large amount of video data of the distorted image is            acquired. The problem is posed in terms of finding an undistorted im-            age patch at each spatial location. This challenging reconstruction task            can be formulated as a manifold learning problem, such that the center            of the manifold is the image of the undistorted patch. To compute the            center, we present a new technique to estimate global distances on the            manifold. Our technique achieves robustness through convex flow com-            putations and solves the ""leakage"" problem inherent in recent manifold            embedding techniques.
1     Introduction
Consider the following problem. A pool of water is observed by a stationary video camera mounted above the pool and looking straight down. There are waves on the surface of the water and all the camera sees is a series of distorted images of the bottom of the pool, e.g. Figure 1. The aim is to use these images to recover the undistorted image of the pool floor  as if the water was perfectly still. Besides obvious applications in ocean optics and underwater imaging [1], variants of this problem also arise in several other fields, including astronomy (overcoming atmospheric distortions) and structure-from-motion (learning the appearance of a deforming object). Most approaches to solve this problem try to model the distortions explicitly. In order to do this, it is critical not only to have a good parametric model of the distortion process, but also to be able to reliably extract features from the data to fit the parameters. As such, this approach is only feasible in well understood, highly controlled domains. On the opposite side of the spectrum is a very simple method used in underwater imaging: simply, average the data temporally. Although this method performs surprisingly well in many situations, it fails when the structure of the target image is too fine with respect to the amplitude of the wave (Figure 2).
In this paper we propose to look at this difficult problem from a more statistical angle. We will exploit a very simple observation: if we watch a particular spot on the image plane, most of the time the picture projected there will be distorted. But once in a while, when the water just happens to be locally flat at that point, we will be looking straight down and seeing exactly the right spot on the ground. If we can recognize when this happens
  Authors in alphabetical order.

Figure 1: Fifteen consecutive frames from the video. The experimental setup involved: a transparent bucket of water, the cover of a vision textbook ""Computer Vision/A Modern Approach"".
     Figure 2: Ground truth image and reconstruction results using mean and median

and snap the right picture at each spatial location, then recovering the desired ground truth image would be simply a matter of stitching these correct observations together. In other words, the question that we will be exploring in this paper is not where to look, but when!
2    Problem setup
Let us first examine the physical setup of our problem. There is a ""ground truth"" image G on the bottom of the pool. Overhead, a stationary camera pointing downwards is recording a video stream V . In the absence of any distortion V (x, y, t) = G(x, y) at any time t. However, the water surface refracts in accordance with Snell's Law. Let us consider what the camera is seeing at a particular point x on the CCD array, as shown in Figure 3(c) (assume 1D for simplicity). If the normal to the water surface directly underneath x is pointing straight up, there is no refraction and V (x) = G(x). However, if the normal is tilted by angle 1, light will bend by the amount 2 = 1 - sin-1 ( 1 sin                                                                                1.33      1 ), so the camera point V (x) will see the light projected from G(x + dx) on the ground plane. It is easy to see that the relationship between the tilt of the normal to the surface 1 and the displacement dx is approximately linear (dx  0.251h using small angle approximation, where h is the height of the water). This means that, in 2D, what the camera will be seeing over time at point V (x, y, t) are points on the ground plane sampled from a disk centered at G(x, y) and with radius related to the height of the water and the overall roughness of the water surface. A similar relationship holds in the inverse direction as well: a point G(x, y) will be imaged on a disk centered around V (x, y).
What about the distribution of these sample points? According to Cox-Munk Law [2], the surface normals of rough water are distributed approximately as a Gaussian centered around the vertical, assuming a large surface area and stationary waves. Our own experiments, conducted by hand-tracking (Figure 3b), confirm that the distribution, though not exactly Gaussian, is definitely unimodal and smooth.
Up to now, we only concerned ourselves with infinitesimally small points on the image or the ground plane. However, in practice, we must have something that we can compute with. Therefore, we will make an assumption that the surface of the water can be locally approximated by a planar patch. This means that everything that was true for points is now true for local image patches (up to a small affine distortion).
3    Tracking via embedding
From the description outlined above, one possible solution emerges. If the distribution of a particular ground point on the image plane is unimodal, then one could track feature points in the video sequence over time. Computing their mean positions over the entire video will give an estimate of their true positions on the ground plane. Unfortunately, tracking over long periods of time is difficult even under favorable conditions, whereas our data is so fast (undersampled) and noisy that reliable tracking is out of the question (Figure 3(c)).
However, since we have a lot of data, we can substitute smoothness in time with smoothness in similarity  for a given patch we are more likely to find a patch similar to it somewhere in time, and will have a better chance to track the transition between them. An alternative to tracking the patches directly (which amounts to holding the ground patch G(x, y) fixed and centering the image patches V (x + dxt, y + dyt) on top of it in each frame), is to fix the image patch V (x, y) in space and observe the patches from G(x + dxt, y + dyt) appearing in this window. We know that this set of patches comes from a disk on the ground plane centered around patch G(x, y)  our goal. If the disk was small enough compared to the size of the patch, we could just cluster the patches together, e.g. by using translational EM [3]. Unfortunately, the disk can be rather large, containing patches with no overlap at all, thus making only the local similarity comparisons possible. However, notice that our set of patches lies on a low-dimensional manifold; in fact we know precisely which manifold  it's the disk on the ground plane centered at G(x, y)! So, if we could use the local patch similarities to find an embedding of the patches in V (x, y, t) on this manifold, the center of the embedding will hold our desired patch G(x, y).
The problem of embedding the patches based on local similarity is related to the recent work in manifold learning [4, 5]. Basic ingredients of the embedding algorithms are: defin- ing a distance measure between points, and finding an energy function that optimally places them in the embedding space. The distance can be defined as all-pairs distance matrix, or as distance from a particular reference node. In both cases, we want the distance function to satisfy some constraints to model the underlying physical problem.
The local similarity measure for our problem turned out to be particularly unreliable, so none of the previous manifold learning techniques were adequate for our purposes. In the following section we will describe our own, robust method for computing a global distance function and finding the right embedding and eventually the center of it.
                     1                    N      Surface



 h                         2

      G(x)      G(x + dx)

              (a)                      (b)                              (c)

Figure 3: (a) Snell's Law (b)-(c) Tracking points of the bottom of the pool: (b) the tracked position forms a distribution close to a Gaussian, (c): a vertical line of the image shown at different time instances (horizontal axis). The discontinuity caused by rapid changes makes the tracking infeasible.
4    What is the right distance function?
Let I     = {I1, . . . , In} be the set of patches, where It = V (x, y, t) and x = [xmin, xmax], y = [ymin, ymax] are the patch pixel coordinates. Our goal is to find a center patch to represent the set I. To achieve this goal, we need a distance function
d : I  I  IR such that d(Ii, Ij) < d(Ii, Ik) implies that Ij is more similar to Ii than Ik. Once we have such a measure, the center can be found by computing:
                                    I = arg min                     d(Ii, Ij)                 (1)                                                       IiI      Ij I

Unfortunately, the measurable distance functions, such as Normalized Cross Correlation (N CC) are only local. A common approach is to design a global distance function using the measurable local distances and transitivity [6, 4]. This is equivalent to designing a global distance function of the form:
                                    d                  d(I                         local(Ii, Ij ),              if dlocal(Ii, Ij)                         i, Ij ) =                                                                    (2)                                         dtransitive(Ii, Ij),              otherwise.

where dlocal is a local distance function,  is a user-specified threshold and dtransitive is a global, transitive distance function which utilizes dlocal. The underlying assumption here is that the members of I lie on a constraint space (or manifold) S. Hence, a local similarity function such as N CC can be used to measure local distances on the manifold. An important research question in machine learning is to extend the local measurements into global ones, i.e. to design dtransitive above.
One method for designing such a transitive distance function is to build a graph G = (V, E) whose vertices correspond to the members of I. The local distance measure is used to place edges which connect only very similar members of I. Afterwards, the length of pairwise shortest paths are used to estimate the true distances on the manifold S. For example, this method forms the basis of the well-known Isomap method [4].
Unfortunately, estimating the distance dtransitive(, ) using shortest path computations is not robust to errors in the local distances  which are very common. Consider a patch that contains the letter A and another one that contains the letter B. Since they are different letters, we expect that these patches would be quite distant on the manifold S. However, among the A patches there will inevitably be a very blurry A that would look quite similar to a very blurry B producing an erroneous local distance measurement. When the transitive global distances are computed using shortest paths, a single erroneous edge will single- handedly cause all the A patches to be much closer to all the B patches, short-circuiting the graph and completely distorting all the distances.
Such errors lead to the leakage problem in estimating the global distances of patches. This problem is illustrated in Figure 4. In this example, our underlying manifold S is a triangle. Suppose our local distance function erroneously estimates an edge between the corners of the triangle as shown in the figure. After the erroneous edge is inserted, the shortest paths from the top of the triangle leak through this edge. Therefore, the shortest path distances will fail to reflect the true distance on the manifold.
5    Solving the leakage problem
Recall that our goal is to find the center of our data set as defined in Equation 1. Note that, in order to compute the center we do not need all pairwise distances. All we need is the quantity dI (Ii) =               d(I                         I          i, Ij ) for all Ii.                          j I
The leakage problem occurs when we compute the values dI (Ii) using the shortest path metric. In this case, even a single erroneous edge may reduce the shortest paths from many different patches to Ii  changing the value of dI(Ii) drastically. Intuitively, in order to prevent the leakage problem we must prevent edges from getting involved in many shortest path computations to the same node (i.e. leaking edges). We can formalize this notion by casting the computation as a network flow problem.
Let G = (V, E) be our graph representation such that for each patch Ii  I, there is a vertex vi  V . The edge set E is built as follows: there is an edge (vi, vj) if dlocal(Ii, Ij) is less than a threshold. The weight of the edge (vi, vj) is equal to dlocal(Ii, Ij).
To compute the value dI (Ii), we build a flow network whose vertex set is also V . All vertices in V - {vi} are sources, pushing unit flow into the network. The vertex vi is a sink with infinite capacity. The arcs of the flow network are chosen using the edge set E. For each edge (vj, vk)  E we add the arcs vj  vk and vk  vj. Both arcs have infinite capacity and the cost of pushing one unit of flow on either arc is equal to the weight of (vj, vk), as shown in Figure 4 left (top and bottom). It can easily be seen that the minimum cost flow in this network is equal to dI (Ii). Let us call this network which is used to compute dI (Ii) as N W (Ii).
The crucial factor in designing such a flow network is choosing the right cost and capacity. Computing the minimum cost flow on N W (Ii) not only gives us dI(Ii) but also allows us to compute how many times an edge is involved in the distance computation: the amount of flow through an edge is exactly the number of times that edge is used for the shortest path computations. This is illustrated in Figure 4 (box A) where d1 units of cost is charged for each unit of flow through the edge (u, w). Therefore, if we prevent too much flow going through an edge, we can prevent the leakage problem.
                                                                                                     d3/                                                         d1/                                            d                     Error                                                                              u 2/c2 w                                                         u       w              d1                v                    A: Shortest Path                                 B: Convex Flow                         c                                                                                                         d1/c1                 1 c1 + c2



                                                         u

                                                                                                    C: Shortest Path with Capacity                                                                           Error                                                                                      d/                                                                             d1/c1                        v                                                                                     u        w                                                                      v                                                           c1

                                                                      w

Figure 4: The leakage problem. Left: Equivalence of shortest path leakage and uncapacitated flow leakage problem. Bottom-middle: After the erroneous edge is inserted, the shortest paths from the top of the triangle to vertex v go through this edge. Boxes A-C:Alternatives for charging a unit of flow between nodes u and w. The horizontal axis of the plots is the amount of flow and the vertical axis is the cost. Box A: Linear flow. The cost of a unit of flow is d1 Box B: Convex flow. Multiple edges are introduced between two nodes, with fixed capacity, and convexly increasing costs. The cost of a unit of flow increases from d1 to d2 and then to d3 as the amount of flow from u to w increases. Box C: Linear flow with capacity. The cost is d1 until a capacity of c1 is achieved and becomes infinite afterwards.
One might think that the leakage problem can simply be avoided by imposing capacity constraints on the arcs of the flow network (Figure 4, box C). Unfortunately, this is not very easy. Observe that in the minimum cost flow solution of the network N W (Ii), the amount of flow on the arcs will increase as the arcs get closer to Ii. Therefore, when we are setting up the network N W (Ii), we must adaptively increase the capacities of arcs ""closer"" to the sink vi  otherwise, there will be no feasible solution. As the structure of the graph G gets complicated, specifying this notion of closeness becomes a subtle issue. Further, the structure of the underlying space S could be such that some arcs in G must indeed
carry a lot of flow. Therefore imposing capacities on the arcs requires understanding the underlying structure of the graph G as well as the space S  which is in fact the problem we are trying to solve!
Our proposed solution to the leakage problem uses the notion of a convex flow. We do not impose a capacity on the arcs. Instead, we impose a convex cost function on the arcs such that the cost of pushing unit flow on arc a increases as the total amount of flow through a increases. See Figure 4, box B.
This can be achieved by transforming the network N W (Ii) to a new network N W (Ii). The transformation is achieved by applying the following operation on each arc in N W (Ii): Let a be an arc from u to w in N W (Ii). In N W (Ii), we replace a by k arcs a1, . . . , ak. The costs of these arcs are chosen to be uniformly increasing so that cost(a1) < cost(a2) < . . . < cost(ak). The capacity of arc ak is infinite. The weights and capacities of the other arcs are chosen to reflect the steepness of the desired convexity (Figure 4, box B). The network shown in the figure yields the following function for the cost of pushing x units of flow through the arc:
                       d1x,                                     if 0  x  c1          cost(x) =         d1c1 + d2(x - c1),                       if c1  x  c2       (3)                            d1c1 + d2(c2 - c1) + d3(x - c1 - c2),    if c2  x

The advantage of this convex flow computation is twofold. It does not require putting thresholds on the arcs a-priori. It is always feasible to have as much flow on a single arc as required. However, the minimum cost flow will avoid the leakage problem because it will be costly to use an erroneous edge to carry the flow from many different patches.
5.1    Fixing the leakage in Isomap
As noted earlier, the Isomap method [4] uses the shortest path measurements to estimate a distance matrix M . Afterwards, M is used to find an embedding of the manifold S via MDS.
As expected, this method also suffers from the leakage problem as demonstrated in Fig- ure 5. The top-left image in Figure 5 shows our ground truth. In the middle row, we present an embedding of these graphs computed using Isomap which uses the shortest path length as the global distance measure. As illustrated in these figures, even though isomap does a good job in embedding the ground truth when there are no errors, the embedding (or manifold) collapses after we insert the erroneous edges. In contrast, when we use the convex-flow based technique to estimate the distances, we recover the true embedding  even in the presence of erroneous edges (Figure 5 bottom row)."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/18ad9899f3f21a5a1583584d5f11c0c0-Abstract.html,Generative Affine Localisation and Tracking,"John Winn, Andrew Blake","We present an extension to the Jojic and Frey (2001) layered sprite model          which allows for layers to undergo affine transformations. This extension          allows for affine object pose to be inferred whilst simultaneously learn-          ing the object shape and appearance. Learning is carried out by applying          an augmented variational inference algorithm which includes a global          search over a discretised transform space followed by a local optimisa-          tion. To aid correct convergence, we use bottom-up cues to restrict the          space of possible affine transformations. We present results on a number          of video sequences and show how the model can be extended to track an          object whose appearance changes throughout the sequence."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/18bb68e2b38e4a8ce7cf4f6b2625768c-Abstract.html,Schema Learning: Experience-Based Construction of Predictive Action Models,"Michael P. Holmes, Charles Jr.","Schema learning is a way to discover probabilistic, constructivist, pre- dictive action models (schemas) from experience. It includes meth- ods for ﬁnding and using hidden state to make predictions more accu- rate. We extend the original schema mechanism [1] to handle arbitrary discrete-valued sensors, improve the original learning criteria to handle POMDP domains, and better maintain hidden state by using schema pre- dictions. These extensions show large improvement over the original schema mechanism in several rewardless POMDPs, and achieve very low prediction error in a difﬁcult speech modeling task. Further, we compare extended schema learning to the recently introduced predictive state rep- resentations [2], and ﬁnd their predictions of next-step action effects to be approximately equal in accuracy. This work lays the foundation for a schema-based system of integrated learning and planning."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/1b113258af3968aaf3969ca67e744ff8-Abstract.html,Machine Learning Applied to Perception: Decision Images for Gender Classification,"Felix A. Wichmann, Arnulf B. Graf, Heinrich H. Bülthoff, Eero P. Simoncelli, Bernhard Schölkopf","We study gender discrimination of human faces using a combination of psychophysical classiﬁcation and discrimination experiments together with methods from machine learning. We reduce the dimensionality of a set of face images using principal component analysis, and then train a set of linear classiﬁers on this reduced representation (linear support vec- tor machines (SVMs), relevance vector machines (RVMs), Fisher linear discriminant (FLD), and prototype (prot) classiﬁers) using human clas- siﬁcation data. Because we combine a linear preprocessor with linear classiﬁers, the entire system acts as a linear classiﬁer, allowing us to visu- alise the decision-image corresponding to the normal vector of the separ- ating hyperplanes (SH) of each classiﬁer. We predict that the female-to- maleness transition along the normal vector for classiﬁers closely mim- icking human classiﬁcation (SVM and RVM [1]) should be faster than the transition along any other direction. A psychophysical discrimina- tion experiment using the decision images as stimuli is consistent with this prediction."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/1b9812b99fe2672af746cefda86be5f9-Abstract.html,An Information Maximization Model of Eye Movements,"Laura W. Renninger, James M. Coughlan, Preeti Verghese, Jitendra Malik","We propose a sequential information maximization model as a  general strategy for programming eye movements. The model  reconstructs high-resolution visual information from a sequence of  fixations, taking into account the fall-off in resolution from the  fovea to the periphery. From this framework we get a simple rule  for predicting fixation sequences: after each fixation, fixate next at  the location that minimizes uncertainty (maximizes information)  about the stimulus. By comparing our model performance to human  eye movement data and to predictions from a saliency and random  model, we demonstrate that our model is best at predicting fixation  locations. Modeling additional biological constraints will improve  the prediction of fixation sequences. Our results suggest that  information maximization is a useful principle for programming  eye movements."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/1d49780520898fe37f0cd6b41c5311bf-Abstract.html,Learning Hyper-Features for Visual Identification,"Andras D. Ferencz, Erik G. Learned-miller, Jitendra Malik","We address the problem of identifying specific instances of a class (cars)           from a set of images all belonging to that class. Although we cannot build           a model for any particular instance (as we may be provided with only one           ""training"" example of it), we can use information extracted from observ-           ing other members of the class. We pose this task as a learning problem,           in which the learner is given image pairs, labeled as matching or not, and           must discover which image features are most consistent for matching in-           stances and discriminative for mismatches. We explore a patch based           representation, where we model the distributions of similarity measure-           ments defined on the patches. Finally, we describe an algorithm that           selects the most salient patches based on a mutual information criterion.           This algorithm performs identification well for our challenging dataset           of car images, after matching only a few, well chosen patches."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/1d94108e907bb8311d8802b48fd54b4a-Abstract.html,Parametric Embedding for Class Visualization,"Tomoharu Iwata, Kazumi Saito, Naonori Ueda, Sean Stromsten, Thomas L. Griffiths, Joshua B. Tenenbaum","In this paper, we propose a new method, Parametric Embedding (PE), for visualizing the posteriors estimated over a mixture model. PE simultane- ously embeds both objects and their classes in a low-dimensional space. PE takes as input a set of class posterior vectors for given data points, and tries to preserve the posterior structure in an embedding space by minimizing a sum of Kullback-Leibler divergences, under the assump- tion that samples are generated by a Gaussian mixture with equal covari- ances in the embedding space. PE has many potential uses depending on the source of the input data, providing insight into the classiﬁer’s be- havior in supervised, semi-supervised and unsupervised settings. The PE algorithm has a computational advantage over conventional embedding methods based on pairwise object relations since its complexity scales with the product of the number of objects and the number of classes. We demonstrate PE by visualizing supervised categorization of web pages, semi-supervised categorization of digits, and the relations of words and latent topics found by an unsupervised algorithm, Latent Dirichlet Allo- cation."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/217e342fc01668b10cb1188d40d3370e-Abstract.html,A Topographic Support Vector Machine: Classification Using Local Label Configurations,"Johannes Mohr, Klaus Obermayer","The standard approach to the classification of objects is to consider the          examples as independent and identically distributed (iid). In many real          world settings, however, this assumption is not valid, because a topo-          graphical relationship exists between the objects. In this contribution we          consider the special case of image segmentation, where the objects are          pixels and where the underlying topography is a 2D regular rectangular          grid. We introduce a classification method which not only uses measured          vectorial feature information but also the label configuration within a to-          pographic neighborhood. Due to the resulting dependence between the          labels of neighboring pixels, a collective classification of a set of pixels          becomes necessary. We propose a new method called 'Topographic Sup-          port Vector Machine' (TSVM), which is based on a topographic kernel          and a self-consistent solution to the label assignment shown to be equiv-          alent to a recurrent neural network. The performance of the algorithm is          compared to a conventional SVM on a cell image segmentation task."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/220a7f49d42406598587a66f02584ac3-Abstract.html,New Criteria and a New Algorithm for Learning in Multi-Agent Systems,"Rob Powers, Yoav Shoham","We propose a new set of criteria for learning algorithms in multi-agent          systems, one that is more stringent and (we argue) better justified than          previous proposed criteria. Our criteria, which apply most straightfor-          wardly in repeated games with average rewards, consist of three require-          ments: (a) against a specified class of opponents (this class is a parameter          of the criterion) the algorithm yield a payoff that approaches the payoff          of the best response, (b) against other opponents the algorithm's payoff          at least approach (and possibly exceed) the security level payoff (or max-          imin value), and (c) subject to these requirements, the algorithm achieve          a close to optimal payoff in self-play. We furthermore require that these          average payoffs be achieved quickly. We then present a novel algorithm,          and show that it meets these new criteria for a particular parameter class,          the class of stationary opponents. Finally, we show that the algorithm          is effective not only in theory, but also empirically. Using a recently          introduced comprehensive game theoretic test suite, we show that the          algorithm almost universally outperforms previous learning algorithms."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/220c77af02f8ad8561b150d93000ddff-Abstract.html,The Cerebellum Chip: an Analog VLSI Implementation of a Cerebellar Model of Classical Conditioning,"Constanze Hofstoetter, Manuel Gil, Kynan Eng, Giacomo Indiveri, Matti Mintz, Jörg Kramer, Paul F. Verschure","We  present  a  biophysically  constrained  cerebellar  model  of                classical  conditioning,  implemented  using  a  neuromorphic  analog                VLSI (aVLSI) chip.  Like its biological counterpart, our cerebellar                model  is  able  to  control  adaptive  behavior  by  predicting  the                precise timing of events.  Here we describe the functionality of the                chip  and  present  its  learning  performance,  as  evaluated  in                simulated  conditioning  experiments  at  the  circuit  level  and  in                behavioral  experiments  using  a  mobile  robot.    We  show  that  this                aVLSI model supports the acquisition and extinction of adaptively                timed  conditioned  responses  under  real-world  conditions  with                ultra-low power consumption."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/2288f691b58edecadcc9a8691762b4fd-Abstract.html,Pictorial Structures for Molecular Modeling: Interpreting Density Maps,"Frank Dimaio, George Phillips, Jude W. Shavlik","X-ray crystallography is currently the most common way protein  structures are elucidated. One of the most time-consuming steps in  the crystallographic process is interpretation of the electron density  map, a task that involves finding patterns in a three-dimensional  picture of a protein. This paper describes DEFT (DEFormable  Template), an algorithm using pictorial structures to build a  flexible protein model from the protein's amino-acid sequence.  Matching this pictorial structure into the density map is a way of  automating density-map interpretation. Also described are several  extensions to the pictorial structure matching algorithm necessary  for this automated interpretation. DEFT is tested on a set of  density maps ranging from 2 to 4Å resolution, producing root- mean-squared errors ranging from 1.38 to 1.84Å."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/22b1f2e0983160db6f7bb9f62f4dbb39-Abstract.html,Support Vector Classification with Input Data Uncertainty,"Jinbo Bi, Tong Zhang","This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncer- tainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/23fc4cba066f390a8cc729c7592b6ee8-Abstract.html,Planning for Markov Decision Processes with Sparse Stochasticity,"Maxim Likhachev, Sebastian Thrun, Geoffrey J. Gordon","Planning algorithms designed for deterministic worlds, such as A*          search, usually run much faster than algorithms designed for worlds with          uncertain action outcomes, such as value iteration. Real-world planning          problems often exhibit uncertainty, which forces us to use the slower          algorithms to solve them. Many real-world planning problems exhibit          sparse uncertainty: there are long sequences of deterministic actions          which accomplish tasks like moving sensor platforms into place, inter-          spersed with a small number of sensing actions which have uncertain out-          comes. In this paper we describe a new planning algorithm, called MCP          (short for MDP Compression Planning), which combines A* search with          value iteration for solving Stochastic Shortest Path problem in MDPs          with sparse stochasticity. We present experiments which show that MCP          can run substantially faster than competing planners in domains with          sparse uncertainty; these experiments are based on a simulation of a          ground robot cooperating with a helicopter to fill in a partial map and          move to a goal location.
In deterministic planning problems, optimal paths are acyclic: no state is visited more than once. Because of this property, algorithms like A* search can guarantee that they visit each state in the state space no more than once. By visiting the states in an appropriate order, it is possible to ensure that we know the exact value of all of a state's possible successors before we visit that state; so, the first time we visit a state we can compute its correct value.    By contrast, if actions have uncertain outcomes, optimal paths may contain cycles: some states will be visited two or more times with positive probability. Because of these cycles, there is no way to order states so that we determine the values of a state's successors before we visit the state itself. Instead, the only way to compute state values is to solve a set of simultaneous equations.    In problems with sparse stochasticity, only a small fraction of all states have uncertain outcomes. It is these few states that cause all of the cycles: while a deterministic state s may participate in a cycle, the only way it can do so is if one of its successors has an action with a stochastic outcome (and only if this stochastic action can lead to a predecessor of s).    In such problems, we would like to build a smaller MDP which contains only states which are related to stochastic actions. We will call such an MDP a compressed MDP, and we will call its states distinguished states. We could then run fast algorithms like A* search to plan paths between distinguished states, and reserve slower algorithms like value iteration for deciding how to deal with stochastic outcomes.
      (a) Segbot         (b) Robotic helicopter





   (d) Planning map     (e) Execution simulation              (c) 3D Map                             Figure 1: Robot-Helicopter Coordination

 There are two problems with such a strategy. First, there can be a large number of states which are related to stochastic actions, and so it may be impractical to enumerate all of them and make them all distinguished states; we would prefer instead to distinguish only states which are likely to be encountered while executing some policy which we are considering. Second, there can be a large number of ways to get from one distinguished state to another: edges in the compressed MDP correspond to sequences of actions in the original MDP. If we knew the values of all of the distinguished states exactly, then we could use A* search to generate optimal paths between them, but since we do not we cannot.      In this paper, we will describe an algorithm which incrementally builds a compressed MDP using a sequence of deterministic searches. It adds states and edges to the compressed MDP only by encountering them along trajectories; so, it never adds irrelevant states or edges to the compressed MDP. Trajectories are generated by deterministic search, and so undistinguished states are treated only with fast algorithms. Bellman errors in the values for distinguished states show us where to try additional trajectories, and help us build the relevant parts of the compressed MDP as quickly as possible.

1    Robot-Helicopter Coordination Problem
The motivation for our research was the problem of coordinating a ground robot and a helicopter. The ground robot needs to plan a path from its current location to a goal, but has only partial knowledge of the surrounding terrain. The helicopter can aid the ground robot by flying to and sensing places in the map.      Figure 1(a) shows our ground robot, a converted Segway with a SICK laser rangefinder. Figure 1(b) shows the helicopter, also with a SICK. Figure 1(c) shows a 3D map of the environment in which the robot operates. The 3D map is post-processed to produce a discretized 2D environment (Figure 1(d)). Several places in the map are unknown, either because the robot has not visited them or because their status may have changed (e.g, a car may occupy a driveway). Such places are shown in Figure 1(d) as white squares. The elevation of each white square is proportional to the probability that there is an obstacle there; we assume independence between unknown squares.      The robot must take the unknown locations into account when planning for its route. It may plan a path through these locations, but it risks having to turn back if its way is blocked. Alternately, the robot can ask the helicopter to fly to any of these places and sense them. We assign a cost to running the robot, and a somewhat higher cost to running the helicopter. The planning task is to minimize the expected overall cost of running the robot and the helicopter while getting the robot to its destination and the helicopter back to its home base. Figure 1(e) shows a snapshot of the robot and helicopter executing a policy.      Designing a good policy for the robot and helicopter is a POMDP planning problem; unfortunately POMDPs are in general difficult to solve (PSPACE-complete [7]). In the POMDP representation, a state is the position of the robot, the current location of the helicopter (a point on a line segment from one of the unknown places to another unknown place or the home base), and the true status of each unknown location. The positions of the robot and the helicopter are observable, so that the only hidden variables are whether each
unknown place is occupied. The number of states is (# of robot locations)(# of helicopter locations)2# of unknown places. So, the number of states is exponential in the number of unknown places and therefore quickly becomes very large.        We approach the problem by planning in the belief state space, that is, the space of probability distributions over world states. This problem is a continuous-state MDP; in this belief MDP, our state consists of the ground robot's location, the helicopter's location, and a probability of occupancy for each unknown location. We will discretize the continuous probability variables by breaking the interval [0, 1] into several chunks; so, the number of belief states is exponential in the number of unknown places, and classical algorithms such as value iteration are infeasible even on small problems.        If sensors are perfect, this domain is acyclic: after we sense a square we know its true state forever after. On the other hand, imperfect sensors can lead to cycles: new sensor data can contradict older sensor data and lead to increased uncertainty. With or without sensor noise, our belief state MDP differs from general MDPs because its stochastic transitions are sparse: large portions of the policy (while the robot and helicopter are traveling be- tween unknown locations) are deterministic. The algorithm we propose in this paper takes advantage of this property of the problem, as we explain in the next section.
2      The Algorithm
Our algorithm can be broken into two levels. At a high level, it constructs a compressed MDP, denoted M c, which contains only the start, the goal, and some states which are out- comes of stochastic actions. At a lower level, it repeatedly runs deterministic searches to find new information to put into M c. This information includes newly-discovered stochas- tic actions and their outcomes; better deterministic paths from one place to another; and more accurate value estimates similar to Bellman backups. The deterministic searches can use an admissible heuristic h to focus their effort, so we can often avoid putting many irrelevant actions into M c.        Because M c will often be much smaller than M , we can afford to run stochastic plan- ning algorithms like value iteration on it. On the other hand, the information we get by planning in M c will improve the heuristic values that we use in our deterministic searches; so, the deterministic searches will tend to visit only relevant portions of the state space.
2.1     Constructing and Solving a Compressed MDP
Each action in the compressed MDP represents several consecutive actions in M : if we see a sequence of states and actions s1, a1, s2, a2, . . . , sk, ak where a1 through ak-1 are deterministic but ak is stochastic, then we can represent it in M c with a single action a, available at s1, whose outcome distribution is P (s | sk, ak) and whose cost is
                                       k-1

                     c(s1, a, s ) =           c(si, ai, si+1) + c(sk, ak, s )                                            i=1

(See Figure 2(a) for an example of such a compressed action.) In addition, if we see a se- quence of deterministic actions ending in sgoal, say s1, a1, s2, a2, . . . , sk, ak, sk+1 = sgoal, we can define a compressed action which goes from s1 to sgoal at cost                 k      c(s                                                                                      i=1      i, ai, si+1). We can label each compressed action that starts at s with (s, s , a) (where a = null if s = sgoal).        Among all compressed actions starting at s and ending at (s , a) there is (at least) one with lowest expected cost; we will call such an action an optimal compression of (s, s , a). Write Astoch for the set of all pairs (s, a) such that action a when taken from state s has more than one possible outcome, and include as well (sgoal, null). Write Sstoch for the states which are possible outcomes of the actions in Astoch, and include sstart as well. If we include in our compressed MDP an optimal compression of (s, s , a) for every s  Sstoch and every (s , a)  Astoch, the result is what we call the full compressed MDP; an example is in Figure 2(b).        If we solve the full compressed MDP, the value of each state will be the same as the value of the corresponding state in M . However, we do not need to do that much work:
                                                (a) action compression





                                              (b) full MDP compression





                                           (c) incremental MDP compression


                                             Figure 2: MDP compression

Main() 01 initialize M c with sstart and sgoal and set their v-values to 0; 02 while (s  M c s.t. RHS(s) - v(s) >  and s belongs to the current greedy policy) 03    select spivot to be any such state s; 04    [v; vlim] = Search(spivot); 05    v(spivot) = v; 06    set the cost c(spivot,                              a, sgoal) of the limit action                                                           a from spivot to vlim; 07    optionally run some algorithm satisfying req. A for a bounded amount of time to improve the value function in M c;
                                               Figure 3: MCP main loop

many states and actions in the full compressed MDP are irrelevant since they do not appear in the optimal policy from sstart to sgoal. So, the goal of the MCP algorithm will be to construct only the relevant part of the compressed MDP by building M c incrementally. Figure 2(c) shows the incremental construction of a compressed MDP which contains all of the stochastic states and actions along an optimal policy in M .       The pseudocode for MCP is given in Figure 3. It begins by initializing M c to contain only sstart and sgoal, and it sets v(sstart) = v(sgoal) = 0. It maintains the invariant that 0  v(s)  v(s) for all s. On each iteration, MCP looks at the Bellman error of each of the states in M c. The Bellman error is v(s) - RHS(s), where
      RHS(s) = min RHS(s, a)                        RHS(s, a) = Es succ(s,a)(c(s, a, s ) + v(s ))                          aA(s)

By convention the min of an empty set is , so an s which does not have any compressed actions yet is considered to have infinite RHS.       MCP selects a state with negative Bellman error, spivot, and starts a search at that state. (We note that there exist many possible ways to select spivot; for example, we can choose the state with the largest negative Bellman error, or the largest error when weighted by state visitation probabilities in the best policy in M c.) The goal of this search is to find a new compressed action a such that its RHS-value can provide a new lower bound on v(spivot). This action can either decrease the current RHS(spivot) (if a seems to be a better action in terms of the current v-values of action outcomes) or prove that the current RHS(spivot) is valid. Since v(s )  v(s ), one way to guarantee that RHS(spivot, a)  v(spivot) is
to compute an optimal compression of (spivot, s, a) for all s, a, then choose the one with the smallest RHS. A more sophisticated strategy is to use an A search with appropriate safeguards to make sure we never overestimate the value of a stochastic action. MCP, however, uses a modified A search which we will describe in the next section.        As the search finds new compressed actions, it adds them and their outcomes to M c. It is allowed to initialize newly-added states to any admissible values. When the search returns, MCP sets v(spivot) to the returned value. This value is at least as large as RHS(spivot). Consequently, Bellman error for spivot becomes non-negative.        In addition to the compressed action and the updated value, the search algorithm returns a ""limit value"" vlim(spivot). These limit values allow MCP to run a standard MDP planning algorithm on M c to improve its v(s) estimates. MCP can use any planning algorithm which guarantees that, for any s, it will not lower v(s) and will not increase v(s) beyond the smaller of vlim(s) and RHS(s) (Requirement A). For example, we could insert a fake ""limit action"" into M c which goes directly from spivot to sgoal at cost vlim(spivot) (as we do on line 06 in Figure 3), then run value iteration for a fixed amount of time, selecting for each backup a state with negative Bellman error.        After updating M c from the result of the search and any optional planning, MCP begins again by looking for another state with a negative Bellman error. It repeats this process until there are no negative Bellman errors larger than . For small enough , this property guarantees that we will be able to find a good policy (see section 2.3).
2.2     Searching the MDP Efficiently
The top level algorithm (Figure 3) repeatedly invokes a search method for finding trajec- tories from spivot to sgoal. In order for the overall algorithm to work correctly, there are several properties that the search must satisfy. First, the estimate v that search returns for the expected cost of spivot should always be admissible. That is, 0  v  v(spivot) (Property 1). Second, the estimate v should be no less than the one-step lookahead value of spivot in M c. That is, v  RHS(spivot) (Property 2). This property ensures that search either increases the value of spivot or finds additional (or improved) compressed actions. The third and final property is for the vlim value, and it is only important if MCP uses its optional planning step (line 07). The property is that v  vlim  v(spivot) (Property 3). Here v(spivot) denotes the minimum expected cost of starting at spivot, picking a com- pressed action not in M c, and acting optimally from then on. (Note that v can be larger than v if the optimal compressed action is already part of M c.) Property 3 uses v rather than v since the latter is not known while it is possible to compute a lower bound on the former efficiently (see below).        One could adapt A* search to satisfy at least Properties 1 and 2 by assuming that we can control the outcome of stochastic actions. However, this sort of search is highly optimistic and can bias the search towards improbable trajectories. Also, it can only use heuristics which are even more optimistic than it is: that is, h must be admissible with respect to the optimistic assumption of controlled outcomes.        We therefore present a version of A, called MCP-search (Figure 4), that is more effi- cient for our purposes. MCP-search finds the correct expected value for the first stochas- tic action it encounters on any given trajectory, and is therefore far less optimistic. And, MCP-search only requires heuristic values to be admissible with respect to v values, h(s)  v(s). Finally, MCP-search speeds up repetitive searches by improving heuris- tic values based on previous searches.        A maintains a priority queue, OPEN, of states which it plans to expand. The OPEN queue is sorted by f (s) = g(s)+h(s), so that A* always expands next a state which appears to be on the shortest path from start to goal. During each expansion a state s is removed from OPEN and all the g-values of s's successors are updated; if g(s ) is decreased for some state s , A* inserts s into OPEN. A* terminates as soon as the goal state is expanded. We use the variant of A* with pathmax [5] to use efficiently heuristics that do not satisfy the triangle inequality.        MCP is similar to A, but the OPEN list can also contain state-action pairs {s, a} where a is a stochastic action (line 31). Plain states are represented in OPEN as {s, null}. Just
ImproveHeuristic(s) 01 if s  M c then h(s) = max(h(s), v(s)); 02 improve heuristic h(s) further if possible using f best and g(s) from previous iterations;
procedure fvalue({s, a}) 03 if s = null return ; 04 else if a = null return g(s) + h(s); 05 else return g(s) + max(h(s), E                         {c(s, a, s ) + h(s )})                                           s Succ(s,a)                              ;
CheckInitialize(s) 06 if s was accessed last in some previous search iteration 07    ImproveHeuristic(s); 08 if s was not yet initialized in the current search iteration 09    g(s) = ;
InsertUpdateCompAction(spivot, s, a) 10 reconstruct the path from spivot to s; 11 insert compressed action (spivot, s, a) into A(spivot) (or update the cost if a cheaper path was found) 12 for each outcome u of a that was not in M c previously 13    set v(u) to h(u) or any other value less than or equal to v(u); 14    set the cost c(u,                             a, sgoal) of the limit action                                                          a from u to v(u);
procedure Search(spivot) 15 CheckInitialize(sgoal), CheckInitialize(spivot); 16 g(spivot) = 0; 17 OPEN = {{spivot, null}}; 18 {sbest, abest} = {null, null}, f best = ; 19 while(g(sgoal) > min{s,a}OPEN(fvalue({s, a})) AND f best +  > min{s,a}OPEN(fvalue({s, a}))) 20    remove {s, a} with the smallest fvalue({s, a}) from OPEN breaking ties towards the pairs with a = null; 21    if a = null              //expand state s 22      for each s  Succ(s) 23            CheckInitialize(s ); 24      for each deterministic a  A(s) 25            s = Succ(s, a ); 26            h(s ) = max(h(s ), h(s) - c(s, a , s )); 27            if g(s ) > g(s) + c(s, a , s ) 28              g(s ) = g(s) + c(s, a , s ); 29              insert/update {s , null} into OPEN with fvalue({s , null}); 30      for each stochastic a  A(s) 31            insert/update {s, a } into OPEN with fvalue({s, a }); 32    else              //encode stochastic action a from state s as a compressed action from spivot 33      InsertUpdateCompAction(spivot, s, a); 34      if f best > fvalue({s, a}) then {sbest, abest} = {s, a}, f best = fvalue({s, a}); 35 if (g(sgoal)  min{s,a}OPEN(fvalue({s, a})) AND OPEN = ) 36    reconstruct the path from spivot to sgoal; 37    update/insert into A(spivot) a deterministic action a leading to sgoal; 38    if f best  g(sgoal) then {sbest, abest} = {sgoal, null}, f best = g(sgoal); 39 return [f best; min{s,a}OPEN(fvalue({s, a}))];
                                            Figure 4: MCP-search Algorithm

like A*, MCP-search expands elements in the order of increasing f -values, but it breaks ties towards elements encoding plain states (line 20). The f -value of {s, a} is defined as g(s) + max[h(s), Es Succ(s,a)(c(s, a, s ) + h(s ))] (line 05). This f -value is a lower bound on the cost of a policy that goes from sstart to sgoal by first executing a series of deterministic actions until action a is executed from state s. This bound is as tight as possible given our heuristic values.       State expansion (lines 21-31) is very similar to A. When the search removes from OPEN a state-action pair {s, a} with a = null, it adds a compressed action to M c (line 33). It also adds a compressed action if there is an optimal deterministic path to sgoal (line 37). f best tracks the minimum f -value of all the compressed actions found. As a result, f best  v(spivot) and is used as a new estimate for v(spivot). The limit value vlim(spivot) is obtained by continuing the search until the minimum f -value of elements in OPEN approaches f best +  for some   0 (line 19). This minimum f -value then provides a lower bound on v(spivot).       To speed up repetitive searches, MCP-search improves the heuristic of every state that it encounters for the first time in the current search iteration (lines 01 and 02). Line 01 uses the fact that v(s) from M c is a lower bound on v(s). Line 02 uses the fact that f best - g(s) is a lower bound on v(s) at the end of each previous call to Search; for more details see [4].
2.3     Theoretical Properties of the Algorithm
We now present several theorems about our algorithm. The proofs of these and other theo- rems can be found in [4]. The first theorem states the main properties of MCP-search. Theorem 1 The search function terminates and the following holds for the values it re- turns:         (a) if sbest = null then v(spivot)  f best  E{c(spivot, abest, s ) + v(s )}         (b) if sbest = null then v(spivot) = f best =          (c) f best  min{s,a}OPEN(fvalue({s, a}))  v(spivot).        If neither sgoal nor any state-action pairs were expanded, then sbest = null and (b) says that there is no policy from spivot that has a finite expected cost. Using the above theorem it is easy to show that MCP-search satisfies Properties 1, 2 and 3, considering that f best is returned as variable v and min{s,a}OPEN(fvalue({s, a})) is returned as variable vlim in the main loop of the MCP algorithm (Figure 3). Property 1 follows directly from (a) and (b) and the fact that costs are strictly positive and v-values are non-negative. Property 2 also follows trivially from (a) and (b). Property 3 follows from (c). Given these properties the next theorem states the correctness of the outer MCP algorithm (in the theorem cgreedy denotes a greedy policy that always chooses an action that looks best based on its cost and the v-values of its immediate successors). Theorem 2 Given a deterministic search algorithm which satisfies Properties 13, the MCP algorithm will terminate. Upon termination, for every state s  M c  c                    we                                                                                       greedy have RHS(s) -   v(s)  v(s).        Given the above theorem one can show that for 0   < cmin (where cmin is the smallest expected action cost in our MDP) the expected cost of executing c                   from                                                                                     greedy sstart is at most cmin v(s                     c          start). Picking   cmin is not guaranteed to result in a proper                      min - policy, even though Theorem 2 continues to hold.
3      Experimental Study
We have evaluated the MCP algorithm on the robot-helicopter coordination problem de- scribed in section 1. To obtain an admissible heuristic, we first compute a value function for every possible configuration of obstacles. Then we weight the value functions by the probabilities of their obstacle configurations, sum them, and add the cost of moving the helicopter back to its base if it is not already there. This procedure results in optimistic cost estimates because it pretends that the robot will find out the obstacle locations immediately instead of having to wait to observe them.        The results of our experiments are shown in Figure 5. We have compared MCP against three algorithms: RTDP [1], LAO* [2] and value iteration on reachable states (VI). RTDP can cope with large size MDPs by focussing its planning efforts along simulated execu- tion trajectories. LAO* uses heuristics to prune away irrelevant states, then repeatedly performs dynamic programming on the states in its current partial policy. We have im- plemented LAO* so that it reduces to AO* [6] when environments are acyclic (e.g., the robot-helicopter problem with perfect sensing). VI was only able to run on the problems with perfect sensing since the number of reachable states was too large for the others.        The results support the claim that MCP can solve large problems with sparse stochas- ticity. For the problem with perfect sensing, on average MCP was able to plan 9.5 times faster than LAO, 7.5 times faster than RTDP, and 8.5 times faster than VI. On average for these problems, MCP computed values for 58633 states while M c grew to 396 states, and MCP encountered 3740 stochastic transitions (to give a sense of the degree of stochastic- ity). The main cost of MCP was in its deterministic search subroutine; this fact suggests that we might benefit from anytime search techniques such as ARA [3].        The results for the problems with imperfect sensing show that, as the number and den- sity of uncertain outcomes increases, the advantage of MCP decreases. For these problems MCP was able to solve environments 10.2 times faster than LAO* but only 2.2 times faster than RTDP. On average MCP computed values for 127,442 states, while the size of M c was 3,713 states, and 24,052 stochastic transitions were encountered.
Figure 5: Experimental results. The top row: the robot-helicopter coordination problem with perfect sensors. The bottom row: the robot-helicopter coordination problem with sensor noise. Left column: running times (in secs) for each algorithm grouped by environments. Middle column: the number of backups for each algorithm grouped by environments. Right column: an estimate of the expected cost of an optimal policy (v(sstart)) vs. running time (in secs) for experiment (k) in the top row and experiment (e) in the bottom row. Algorithms in the bar plots (left to right): MCP, LAO*, RTDP and VI (VI is only shown for problems with perfect sensing). The characteristics of the environments are given in the second and third rows under each of the bar plot. The second row indicates how many cells the 2D plane is discretized into, and the third row indicates the number of initially unknown cells in the environment. 4    Discussion
The MCP algorithm incrementally builds a compressed MDP using a sequence of deter- ministic searches. Our experimental results suggest that MCP is advantageous for problems with sparse stochasticity. In particular, MCP has allowed us to scale to larger environments than were otherwise possible for the robot-helicopter coordination problem.
Acknowledgements This research was supported by DARPA's MARS program. All conclusions are our own."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/2417dc8af8570f274e6775d4d60496da-Abstract.html,Expectation Consistent Free Energies for Approximate Inference,"Manfred Opper, Ole Winther","We propose a novel a framework for deriving approximations for in-          tractable probabilistic models. This framework is based on a free energy          (negative log marginal likelihood) and can be seen as a generalization          of adaptive TAP [1, 2, 3] and expectation propagation (EP) [4, 5]. The          free energy is constructed from two approximating distributions which          encode different aspects of the intractable model such a single node con-          straints and couplings and are by construction consistent on a chosen set          of moments. We test the framework on a difficult benchmark problem          with binary variables on fully connected graphs and 2D grid graphs. We          find good performance using sets of moments which either specify fac-          torized nodes or a spanning tree on the nodes (structured approximation).          Surprisingly, the Bethe approximation gives very inferior results even on          grids."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/283085d30e10513624c8cece7993f4de-Abstract.html,Using Machine Learning to Break Visual Human Interaction Proofs (HIPs),"Kumar Chellapilla, Patrice Y. Simard","Machine learning is often used to automatically solve human tasks. 
          In this paper, we look for tasks where machine learning algorithms 
          are not as good as humans with the hope of gaining insight into 
          their current limitations. We studied various Human Interactive 
          Proofs (HIPs) on the market, because they are systems designed to 
          tell computers and humans apart by posing challenges presumably 
          too hard for computers. We found that most HIPs are pure 
          recognition tasks which can easily be broken using machine 
          learning. The harder HIPs use a combination of segmentation and 
          recognition tasks. From this observation, we found that building 
          segmentation tasks is the most effective way to confuse machine 
          learning algorithms. This has enabled us to build effective HIPs 
          (which we deployed in MSN Passport), as well as design 
          challenging segmentation tasks for machine learning algorithms."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/2e74c2cf88f68a68c84e9509abc7ea56-Abstract.html,Making Latin Manuscripts Searchable using gHMM's,"Jaety Edwards, Yee W. Teh, Roger Bock, Michael Maire, Grace Vesom, David A. Forsyth","We describe a method that can make a scanned, handwritten mediaeval          latin manuscript accessible to full text search. A generalized HMM is          fitted, using transcribed latin to obtain a transition model and one exam-          ple each of 22 letters to obtain an emission model. We show results for          unigram, bigram and trigram models. Our method transcribes 25 pages          of a manuscript of Terence with fair accuracy (75% of letters correctly          transcribed). Search results are very strong; we use examples of vari-          ant spellings to demonstrate that the search respects the ink of the doc-          ument. Furthermore, our model produces fair searches on a document          from which we obtained no training data."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/2e7ceec8361275c4e31fee5fe422740b-Abstract.html,Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning,"Xiaojin Zhu, Jaz Kandola, Zoubin Ghahramani, John D. Lafferty","We present an algorithm based on convex optimization for constructing kernels for semi-supervised learning. The kernel matrices are derived from the spectral decomposition of graph Laplacians, and combine la- beled and unlabeled data in a systematic fashion. Unlike previous work using diffusion kernels and Gaussian random ﬁeld kernels, a nonpara- metric kernel approach is presented that incorporates order constraints during optimization. This results in ﬂexible kernels and avoids the need to choose among different parametric forms. Our approach relies on a quadratically constrained quadratic program (QCQP), and is compu- tationally feasible for large datasets. We evaluate the kernels on real datasets using support vector machines, with encouraging results."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/309a8e73b2cdb95fc1affa8845504e87-Abstract.html,Sparse Coding of Natural Images Using an Overcomplete Set of Limited Capacity Units,"Eizaburo Doi, Michael S. Lewicki","It has been suggested that the primary goal of the sensory system is to            represent input in such a way as to reduce the high degree of redun-            dancy. Given a noisy neural representation, however, solely reducing            redundancy is not desirable, since redundancy is the only clue to reduce            the effects of noise. Here we propose a model that best balances redun-            dancy reduction and redundant representation. Like previous models, our            model accounts for the localized and oriented structure of simple cells,            but it also predicts a different organization for the population. With noisy,            limited-capacity units, the optimal representation becomes an overcom-            plete, multi-scale representation, which, compared to previous models,            is in closer agreement with physiological data. These results offer a new            perspective on the expansion of the number of neurons from retina to V1            and provide a theoretical model of incorporating useful redundancy into            efficient neural representations.
1     Introduction
Efficient coding theory posits that one of the primary goals of sensory coding is to eliminate redundancy from raw sensory signals, ideally representing the input by a set of statistically independent features [1]. Models for learning efficient codes, such as sparse coding [2] or ICA [3], predict the localized, oriented, and band-pass characteristics of simple cells. In this framework, units are assumed to be non-redundant and so the number of units should be identical to the dimensionality of the data.
Redundancy, however, can be beneficial if it is used to compensate for inherent noise in the system [4]. The models above assume that the system noise is low and negligible so that redundancy in the representation is not necessary. This is equivalent to assuming that the representational capacity of individual units is unlimited. Real neurons, however, have limited capacity [5], and this should place constraints on how a neural population can best encode a sensory signal. In fact, there are important characteristics of simple cells, such as the multi-scale representation, that cannot be explained by efficient coding theory.
The aim of this study is to evaluate how the optimal representation changes when the system
is constrained by limited capacity units. We propose a model that best balances redundancy reduction and redundant representation given the limited capacity units. In contrast to the efficient coding models, it is possible to have a larger number of units than the intrinsic dimensionality of the data. This further allows to introduce redundancy in the population, enabling precise reconstruction using the imprecise representation of a single unit."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/30f8f6b940d1073d8b6a5eebc46dd6e5-Abstract.html,Optimal Information Decoding from Neuronal Populations with Specific Stimulus Selectivity,"Marcelo A. Montemurro, Stefano Panzeri","A typical neuron in visual cortex receives most inputs from other cortical            neurons with a roughly similar stimulus preference. Does this arrange-            ment of inputs allow efficient readout of sensory information by the tar-            get cortical neuron? We address this issue by using simple modelling of            neuronal population activity and information theoretic tools. We find that            efficient synaptic information transmission requires that the tuning curve            of the afferent neurons is approximately as wide as the spread of stim-            ulus preferences of the afferent neurons reaching the target neuron. By            meta analysis of neurophysiological data we found that this is the case            for cortico-cortical inputs to neurons in visual cortex. We suggest that            the organization of V1 cortico-cortical synaptic inputs allows optimal in-            formation transmission.
1     Introduction
A typical neuron in visual cortex receives most of its inputs from other visual cortical neu- rons. The majority of cortico-cortical inputs arise from afferent cortical neurons with a preference to stimuli which is similar to that of the target neuron [1, 2, 3]. For exam- ple, orientation selective neurons in superficial layers in ferret visual cortex receive more than 50% of their cortico-cortical excitatory inputs from neurons with orientation prefer- ence which is less than 30o apart. However, this input structure is rather broad in terms of stimulus-specificity: cortico-cortical connections between neurons tuned to dissimilar stimulus orientation also exist [4]. The structure and spread of the stimulus specificity of cortico-cortical connections has received a lot of attention because of its importance for understanding the mechanisms of generation of orientation tuning (see [4] for a review). However, little is still known on whether this structure of inputs allows efficient transmis- sion of sensory information across cortico-cortical synapses.
It is likely that efficiency of information transmission across cortico-cortical synapses also depends on the width of tuning curves of the afferent cortical neurons to stimuli. In fact, theoretical work on population coding has shown that the width of the tuning curves has
 Corresponding author

an important influence on the quality and the nature of the information encoding in cortical populations [5, 6, 7, 8]. Another factor that may influence the efficiency of cortico-cortical synaptic information transmission is the biophysical capability of the target neuron. To conserve all information during synaptic transmission, the target neuron must conserve the `label' of the spikes arriving from multiple input neurons at different places on its dendritic tree [9]. Because of biophysical limitations, a target neuron that e.g. can only sum inputs at the soma may lose a large part of the information present in the afferent activity. The optimal arrangement of cortico-cortical synapses may also depend on the capability of postsynaptic neurons in processing separately spikes from different neurons.
In this paper, we address the problem of whether cortico-cortical synaptic systems encode information efficiently. We introduce a simple model of neuronal information processing that takes into account both the selective distribution of stimulus preferences typical of cortico-cortical connections and the potential biophysical limitations of cortical neurons. We use this model and information theoretic tools to investigate whether there is an opti- mal trade-off between the spread of distribution of stimulus preference across the afferent neurons and the tuning width of the afferent neurons itself. We find that efficient synaptic information transmission requires that the tuning curve of the afferent neurons is approx- imately as wide as the spread of stimulus preferences of the afferent fibres reaching the target neuron. By reviewing anatomical and physiological data, we argue that this optimal trade-off is approximately reached in visual cortex. These results suggest that neurons in visual cortex are wired to decode optimally information from a stimulus-specific distribu- tion of synaptic inputs.
2      Model of the activity of the afferent neuronal population
We consider a simple model for the activity of the afferent neuronal population based on the known tuning properties and spatial and synaptic organisation of sensory areas.
2.1    Stimulus tuning of individual afferent neurons
We assume that the the population is made of a large number N of neurons (for a real cortical neuron, the number N of afferents is in the order of few thousands [10]). The response of each neuron rk(k = 1,    , N) is quantified as the number of spikes fired in a salient post-stimulus time window of a length  . Thus, the overall neuronal population response is represented as a spike count vector r = (r1,    , rN ).
We assume that the neurons are tuned to a small number D of relevant stimulus parameters [11, 12], such as e.g. orientation, speed or direction of motion of a visual object. The stimulus variable will thus be described as a vector s = (s1, . . . , sD) of dimension D. The number of stimulus features that are encoded by the neuron will be left as a free parameter to be varied within the range 1-5 for two reasons. First, although there is evidence that the number of stimulus features encoded by a single neuron is limited [11, 12], more research is still needed to determine exactly how many stimulus parameters are encoded in different areas. Second, a previous related study [8] has shown that, when considering large neuronal populations with a uniform distribution of stimulus preferences (such as an hypercolumn in V1 containing all stimulus orientations) the tuning width of individual neurons which is optimal for population coding depends crucially on the number of stimulus features being encoded. Thus, it is interesting to investigate how the optimal arrangement of cortico- cortical synaptic systems depends on the number of stimulus features being encoded.
The neuronal tuning function of the k - th neuron (k = 1,    , N ), which quantifies the mean spike count of the k - th neuron to the presented stimulus, is modelled as a Gaussian distribution, characterised by the following parameters: preferred stimulus s(k), tuning
width f , and response modulation m:
                                                   - (s-s(k))2                                     f (k)(s) = me            2f 2                        (1)

The Gaussian tuning curve is a good description of the tuning properties of e.g. V1 or MT neurons to variables such as stimulus orientation motion direction [13, 14, 15], and is hence widely used in models of sensory coding [16, 17]. Large values of f indicate coarse coding, whereas small values of f indicate sharp tuning.
Spike count responses of each neuron on each trial are assumed to follow a Poisson distri- bution whose mean is given by the above neuronal tuning function (Eq. 1). The Poisson model is widely used because it is the simplest model of neuronal firing that captures the salient property of neuronal firing that the variance of spike counts is proportional to its mean. The Poisson model neglects all correlations between spikes. This assumption is certainly a simplification but it is sufficient to account for the majority of the information transmitted by real cortical neurons [18, 19, 20] and, as we shall see later, it is mathemati- cally convenient because it makes our model tractable.
2.2       Distribution of stimulus preferences among the afferent population
Neurons in sensory cortex receive a large number of inputs from other neurons with a vari- ety of stimulus preferences. However, the majority of their inputs come from neurons with roughly similar stimulus preference [1, 2, 3]. To characterise correctly this type of spread of stimulus preference among the afferent population, we assume (unlike in previous stud- ies [8]), that the probability distribution of the preferred stimulus among afferent neurons follows a Gaussian distribution:
                                              1           - (^s-^s0)2                                 P (^s) =                              22                                                                         p                                             (2               e                            (2)                                              )D/2D                                                         p

In Eq. (2) the parameter ^                              s0 represents the the center of the distribution, thus being the most represented preferred stimulus in the population. (we set, without loss of general- ity, ^        s0 = 0.) The parameter p controls the spread of stimulus preferences of the afferent neuronal population: a small value of p indicates that a large fraction of the population have similar stimulus preferences, and a large value of p indicates that all stimuli are represented similarly. A Gaussian distribution of stimulus preferences of the afferent pop- ulation fits well empirical data on distribution of preferred orientations of synaptic inputs of neurons in both deep and superficial layers of ferret primary visual cortex [3].
3      Width of tuning and spread of stimulus preferences in visual cortex
To estimate the width of tuning f and the spread of stimulus preferences p of cortico- cortical afferent populations in visual cortex, we reviewed critically published anatomical and physiological data. We concentrated on excitatory synaptic inputs, which form the majority of inputs to a cortical pyramidal neuron [10]. We computed p by fitting (by a least square method) the published histograms of synaptic connections as function of stimulus preference of the input neuron to Gaussian distributions. Similarly, we determined f by fitting spike count histograms to Gaussian tuning curves.
When considering a target neuron in ferret primary visual cortex and using orientation as the stimulus parameters, the spread of stimulus preferences p of its inputs is  20o for layer 5/6 neurons [3], and 16o [3] to 23o [21] for layer 2/3 neurons. The orientation tuning width f of the cortical inputs to the V1 target neuron is that of other V1 neurons that project to it. This f is 17o for Layer 4 neurons [22], and it is similar for neurons in deep and superficial layers [3]. When considering a target neuron in Layer 4 of cat visual cortex
and orientation tuning, the spread of stimulus preference p is 20o [2] and f is  17o. When considering a target neuron in ferret visual cortex and motion direction tuning, the spread of tuning of its inputs p is  30 o [1]. Motion direction tuning widths of macaque neurons is  28o, and this width is similar across species (see [13]).
The most notable finding of our meta-analysis of published data is that p and f appear to be approximately of the same size and their ratio f /p is distributed around 1, in the range 0.7 to 1.1 for the above data. We will use our model to understand whether this range of f /p corresponds to an optimal way to transmit information across a synaptic system.
4      Information theoretic quantification of population decoding
To characterise how a target neuronal system can decode the information about sensory stimuli contained in the activity of its afferent neuronal population, we use mutual infor- mation [23]. The mutual information between a set of stimuli and the neuronal responses quantifies how well any decoder can discriminate among stimuli by observing the neuronal responses. This measure has the advantage of being independent of the decoding mecha- nism used, and thus puts precise constraints on the information that can be decoded by any biological system operating on the afferent activity.
Previous studies on the information content of an afferent neuronal population [7, 8] have assumed that the target neuronal decoding system can extract all the information during synaptic transmission. To do so, the target neuron must conserve the ""label"" of the spikes arriving from multiple neurons at different sites on its dendritic tree [9]. Given the poten- tial biophysical difficulty in processing each spike separately, a simple alternative to spike labelling has been proposed, - spike pooling [10, 24]. In this scheme, the target neuron simply sums up the afferent activity. To characterize how the decoding of afferent informa- tion would work in both cases, we compute both the information that can be decoded by either a system that processes separately spikes from different neurons (the ""labeled-line information"") and the information available to a decoder that sums all incoming spikes (the ""pooled information"") [9, 24]. In the next two subsections we define these quantities and we explain how we compute it in our model.
4.1    The information available to the the labeled-line decoder
The mutual information between the set of the stimuli and the labeled-line neuronal popu- lation activity is defined as follows [9, 24]:
                  ILL(S, R) =       dsP (s)         P (r|s) log P (r|s)                                                    r                P (r)                  (3)

where P (s) is the probability of stimulus occurrence (here taken for simplicity as a uni- form distribution over the hypersphere of D dimensions and `radius' s). P (r|s) is the probability of observing a neuronal population response r conditional to the occurrence of stimulus s, and P (r) =     dsP (s)P (r|s). Since the response vector r keeps separate the spike counts of each neuron, the amount of information in Eq. (3) is only accessible to a decoder than can keep the label of which neuron fired which spike [9, 24]. The probability P (r|s) is computed according to the Poisson distribution, which is entirely determined by the knowledge of the tuning curves [5]. The labeled-line mutual information is difficult to compute for large populations, because it requires the knowledge of the probability of the large-dimensional response vector r. However, since in our model we assume that we have a very large number of independent neurons in the population and that the total activity of the system is of the order of its size, then we can use the following simpler (but still exact)
expression[16, 25]:                                                               1                ILL(S, R) = H(S) - D ln (2                                                2     e) + 2 ds P(s) ln (|J (s)|)            (4) where H(S) is the entropy of the prior stimulus presentation distribution P (S), J (s) is the Fisher information matrix and | . . . | stands for the determinant. The Fisher information matrix is a D  D matrix who's elements i, j are defined as follows:                        Ji,j(s) = -             P (r|s)     2 log P(r|s) ,                   (5)                                           r               si sj Fisher information is a useful measure of the accuracy with which a particular stimulus can be reconstructed from a single trial observation of neuronal population activity. However, in this paper it is used only as a step to obtain a computationally tractable expression for the labeled-line mutual information. The Fisher information matrix can be computed by taking into account that for a population of Poisson neurons is just the sum of the Fisher informa- tion for individual neurons, and the latter has a simple expression in terms of tuning curves [16]. Since the neuronal population size N is is large, the sum over Fisher information of individual neurons can be replaced by an integral over the stimulus preferences of the neurons in the population, weighted by their probability density P (^s). After performing the integral over the distribution of preferred stimuli, we arrived at the following result for the elements of the Fisher information matrix:
         J                          D-2                                     -    2                i,j(s) = N  m                        i,j + 2 (i,j + ij) e 2(1+2)       (6)                            2p (1 + 2)D2 +2 where we have introduced the following short-hand notation f /p   and s/p  ; i,j stands for the Kroneker Delta. From Eq. (6) it is possible to compute explicitly the determinant |J (s)|, which has the following form:                                D                   |J (s)| =           i = ()D(1 + 2)D-1 1 + 2(1 + 2)                   (7)                                i=1 where () is given by:                                                      D-2            -    2                             () = N  m                            e 2(1+2)                (8)                                           2p (1 + 2)D2 +1 Inserting Eq. (7) into Eq. (4), one obtains a tractable but still exact expression for the mutual information , which has the advantage over Eq. (3) of requiring only an integral over a D-dimensional stimulus rather than a sum over an infinite population.

We have studied numerically the dependence of the labeled-line information on the pa- rameters f and p as a function of the number of encoded stimulus features D 1. We investigated this by fixing p and then varying the ration f /p over a wide range. Results (obtained for p = 1 but representative of a wide f range) are reported in Fig. 1. We found that, unlike the case of a uniform distribution of stimulus preferences [8], there is a finite value of the width of tuning f that maximizes the information for all D  2. Inter- estingly, for D  2 the range 0.7  f /p  1.1 found in visual cortex either contains the maximum or corresponds to near optimal values of information transmission. For D = 1, information is maximal for very narrow tuning curves. However, also in this case the in- formation values are still efficient in the cortical range f /p  1, in that the tail of the D = 1 information curve is avoided in that region. Thus, the range of values of f and p found in visual cortex allows efficient synaptic information transmission over a wide range of number of stimulus features encoded by the neuron.
1We found (data not shown) that other parameters such as m and , had a weak or null effect on the optimal configuration; see [17] for a D = 1 example in a different context.
                                           D=1




                           (S,R)                        LL I


                                                              D=5




                                    0          2               4            6               8                                                             /                                                              f     p

Figure 1: Mutual labeled-line information as a function of the ratio of tuning curve width and stimulus preference spread f /p. The curves for each stimulus dimensionality D were shifted by a constant factor to separate them for visual inspection (lower curves cor- respond to higher values of D). The y-axis is thus in arbitrary units. The position of the maximal information for each stimulus dimension falls either inside the range of values of f /p found in visual cortex, or very close to it (see text) . Parameters are as follows: s = 2, rmax = 50Hz,  = 10ms.
4.2    The information available to the the pooling decoder
We now consider the case in which the target neuron cannot process separately spikes from different neurons (for example, a neuron that just sums up post-synaptic potentials of approximately equal weight at the soma). In this case the label of the neuron that fired each spike is lost by the target neuron, and it can only operate on the pooled neuronal signal, in which the identity of each spike is lost. Pooling mechanisms have been proposed as simple information processing strategies for the nervous system. We now study how pooling changes the requirements for efficient decoding by the target neuron.
Mathematically speaking, pooling maps the vector r onto a scalar  equal to the sum of the individual activities:  =                   rk. Thus, the mutual information that can be extracted by any decoder that only pools it inputs is given by the following expression:
                  IP (S, R) =                       dsP (s)          P (|s) log P (|s)                                                                                     P ()             (9)

where P (|s) and P () are the the stimulus-conditional and stimulus-unconditional proba- bility of observing a pooled population response  on a single trial. The probability P (|s) can be computed by noting that a sum of Poisson-distributed responses is still a Poisson- distributed response whose tuning curve to stimuli is just the sum of the individual tuning curves. The pooled mutual information is thus a function of a single Poisson-distributed response variables and can be computed easily also for large populations.
The dependence of the pooled information on the parameters f and p as a function of the number of encoded stimulus features D is reported in Fig. 2. There is one important difference with respect to the labeled-line information transmission case. The difference is that, for the pooled information, there is a finite optimal value for information transmission also when the neurons are tuned to one-dimensional stimulus feature. For all cases of stim- ulus dimensionality considered, the efficient information transmission though the pooled
                                                D=1 





                          (S,R)         D=3                         P I





                                   0    1                 2       3           4                                                        /                                                        f    p

Figure 2: Pooled mutual information as a function of the ratio of tuning curve width and stimulus preference spread f /p. The maxima are inside the range of experimental values of f /p found in the visual cortex, or very close to it (see text). As for Fig. 1, the curves for each stimulus dimensionality D were shifted by a constant factor to separate them for visual inspection (lower curves correspond to higher values of D). The y-axis is thus in arbitrary units. Parameters are as follows: s = 2, rmax = 50 Hz, = 10ms.
neuronal decoder can still be reached in the visual cortical range 0.7  f p  1.1. This finding shows that the range of values of f and p found in visual cortex allows effi- cient synaptic information transmission even if the target neuron does not rely on complex dendritic processing to conserve the label of the neuron that fired the spike."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/321cf86b4c9f5ddd04881a44067c2a5a-Abstract.html,Implicit Wiener Series for Higher-Order Image Analysis,"Matthias O. Franz, Bernhard Schölkopf","The computation of classical higher-order statistics such as higher-order          moments or spectra is difficult for images due to the huge number of          terms to be estimated and interpreted. We propose an alternative ap-          proach in which multiplicative pixel interactions are described by a se-          ries of Wiener functionals. Since the functionals are estimated implicitly          via polynomial kernels, the combinatorial explosion associated with the          classical higher-order statistics is avoided. First results show that image          structures such as lines or corners can be predicted correctly, and that          pixel interactions up to the order of five play an important role in natural          images.
Most of the interesting structure in a natural image is characterized by its higher-order statistics. Arbitrarily oriented lines and edges, for instance, cannot be described by the usual pairwise statistics such as the power spectrum or the autocorrelation function: From knowing the intensity of one point on a line alone, we cannot predict its neighbouring intensities. This would require knowledge of a second point on the line, i.e., we have to consider some third-order statistics which describe the interactions between triplets of points. Analogously, the prediction of a corner neighbourhood needs at least fourth-order statistics, and so on.
In terms of Fourier analysis, higher-order image structures such as edges or corners are described by phase alignments, i.e. phase correlations between several Fourier components of the image. Classically, harmonic phase interactions are measured by higher-order spectra [4]. Unfortunately, the estimation of these spectra for high-dimensional signals such as images involves the estimation and interpretation of a huge number of terms. For instance, a sixth-order spectrum of a 1616 sized image contains roughly 1012 coefficients, about 1010 of which would have to be estimated independently if all symmetries in the spectrum are considered. First attempts at estimating the higher-order structure of natural images were therefore restricted to global measures such as skewness or kurtosis [8], or to submanifolds of fourth-order spectra [9].
Here, we propose an alternative approach that models the interactions of image points in a series of Wiener functionals. A Wiener functional of order n captures those image components that can be predicted from the multiplicative interaction of n image points. In contrast to higher-order spectra or moments, the estimation of a Wiener model does not require the estimation of an excessive number of terms since it can be computed implicitly
via polynomial kernels. This allows us to decompose an image into components that are characterized by interactions of a given order.
In the next section, we introduce the Wiener expansion and discuss its capability of model- ing higher-order pixel interactions. The implicit estimation method is described in Sect. 2, followed by some examples of use in Sect. 3. We conclude in Sect. 4 by briefly discussing the results and possible improvements.
1       Modeling pixel interactions with Wiener functionals
For our analysis, we adopt a prediction framework: Given a d  d neighbourhood of an image pixel, we want to predict its gray value from the gray values of the neighbours. We are particularly interested to which extent interactions of different orders contribute to the overall prediction. Our basic assumption is that the dependency of the central pixel value y on its neighbours xi, i = 1, . . . , m = d2 - 1 can be modeled as a series
                     y = H0[x] + H1[x] + H2[x] +    + Hn[x] +                                                          (1)

of discrete Volterra functionals
                                                     m                     m          H0[x] = h0 = const. and Hn[x] =                                            h(n)                 x . . . x .          (2)                                                                                         i                    i         i                                                          i                                   1 ...i               1                                                                                                        n                    n                                                               1 =1             i =1                                                                                n

Here, we have stacked the grayvalues of the neighbourhood into the vector x = (x1, . . . , xm)  Rm. The discrete nth-order Volterra functional is, accordingly, a linear combination of all ordered nth-order monomials of the elements of x with mn coefficients h(n)        . Volterra functionals provide a controlled way of introducing multiplicative inter-  i1...in actions of image points since a functional of order n contains all products of the input of order n. In terms of higher-order statistics, this means that we can control the order of the statistics used since an nth-order Volterra series leads to dependencies between maximally n + 1 pixels.
Unfortunately, Volterra functionals are not orthogonal to each other, i.e., depending on the input distribution, a functional of order n generally leads to additional lower-order interac- tions. As a result, the output of the functional will contain components that are proportional to that of some lower-order monomials. For instance, the output of a second-order Volterra functional for Gaussian input generally has a mean different from zero [5]. If one wants to estimate the zeroeth-order component of an image (i.e., the constant component created without pixel interactions) the constant component created by the second-order interactions needs to be subtracted. For general Volterra series, this correction can be achieved by de- composing it into a new series y = G0[x] + G1[x] +    + Gn[x] +    of functionals Gn[x] that are uncorrelated, i.e., orthogonal with respect to the input. The resulting Wiener functionals1 Gn[x] are linear combinations of Volterra functionals up to order n. They are computed from the original Volterra series by a procedure akin to Gram-Schmidt or- thogonalization [5]. It can be shown that any Wiener expansion of finite degree minimizes the mean squared error between the true system output and its Volterra series model [5]. The orthogonality condition ensures that a Wiener functional of order n captures only the component of the image created by the multiplicative interaction of n pixels. In contrast to general Volterra functionals, a Wiener functional is orthogonal to all monomials of lower order [5].
So far, we have not gained anything compared to classical estimation of higher-order mo- ments or spectra: an nth-order Volterra functional contains the same number of terms as
 1Strictly speaking, the term Wiener functional is reserved for orthogonal Volterra functionals with respect to Gaussian input. Here, the term will be used for orthogonalized Volterra functionals with arbitrary input distributions.

the corresponding n + 1-order spectrum, and a Wiener functional of the same order has an even higher number of coefficients as it consists also of lower-order Volterra functionals. In the next section, we will introduce an implicit representation of the Wiener series using polynomial kernels which allows for an efficient computation of the Wiener functionals.
2     Estimating Wiener series by regression in RKHS
Volterra series as linear functionals in RKHS.                 The nth-order Volterra functional is a weighted sum of all nth-order monomials of the input vector x. We can interpret the evaluation of this functional for a given input x as a map n defined for n = 0, 1, 2, . . . as
         0(x) = 1 and n(x) = (xn1, xn-1                                                      1      x2, . . . , x1xn-1                                                                          2        , xn                                                                                     2 , . . . , xn )                                                                                                   m             (3)

such that n maps the input x  Rm into a vector n(x)  Fn = Rmn containing all mn ordered monomials of degree n. Using n, we can write the nth-order Volterra functional in Eq. (2) as a scalar product in Fn,
                                    Hn[x] =                                                        n     n(x),                                                (4)

with the coefficients stacked into the vector n = (h(n)                                                                  1,1,..1, h(n)                                                                           1,2,..1, h(n)                                                                                           1,3,..1, . . . )     Fn. The same idea can be applied to the entire pth-order Volterra series. By stacking the maps n into a single map (p)(x) = (0(x), 1(x), . . . , p(x)) , one obtains a mapping from Rm into F(p) = R  Rm  Rm2  . . . Rmp = RM with dimensionality M = 1-mp+1 . The                                                                                                        1-m entire pth-order Volterra series can be written as a scalar product in F(p)
                               p                                           Hn[x] = ((p)) (p)(x)                                                (5)                                    n=0

with (p)  F(p). Below, we will show how we can express (p) as an expansion in terms of the training points. This will dramatically reduce the number of parameters we have to estimate.
This procedure works because the space Fn of nth-order monomials has a very special property: it has the structure of a reproducing kernel Hilbert space (RKHS). As a conse- quence, the dot product in Fn can be computed by evaluating a positive definite kernel function kn(x1, x2). For monomials, one can easily show that (e.g., [6])
                     n(x1) n(x2) = (x1 x2)n =: kn(x1, x2).                                                (6)

Since F(p) is generated as a direct sum of the single spaces Fn, the associated scalar product is simply the sum of the scalar products in the Fn:
                                            p                    (p)(x1) (p)(x2) =                    (x1 x2)n = k(p)(x1, x2).                              (7)                                                 n=0

Thus, we have shown that the discretized Volterra series can be expressed as a linear func- tional in a RKHS2.
Linear regression in RKHS.         For our prediction problem (1), the RKHS property of the Volterra series leads to an efficient solution which is in part due to the so called repre- senter theorem (e.g., [6]). It states the following: suppose we are given N observations
 2A similar approach has been taken by [1] using the inhomogeneous polynomial kernel k(p) (  inh x1, x2) = (1 + x1 x2)p. This kernel implies a map inh into the same space of monomi- als, but it weights the degrees of the monomials differently as can be seen by applying the binomial theorem.

(x1, y1), . . . , (xN , yN ) of the function (1) and an arbitrary cost function c,  is a nonde- creasing function on R>0 and . F is the norm of the RKHS associated with the kernel k. If we minimize an objective function
                     c((x1, y1, f (x1)), . . . , (xN , yN , f (xN ))) + ( f F),                                     (8)

over all functions in the RKHS, then an optimal solution3 can be expressed as
                                                  N                                   f (x) =                    ajk(x, xj),              aj  R.                            (9)                                                       j=1

In other words, although we optimized over the entire RKHS including functions which are defined for arbitrary input points, it turns out that we can always express the solution in terms of the observations xj only. Hence the optimization problem over the extremely large number of coefficients (p) in Eq. (5) is transformed into one over N variables aj.
Let us consider the special case where the cost function is the mean squared error, c((                                                                          N        x1, y1, f (x1)), . . . , (xN , yN , f (xN ))) = 1                            (f (x                                                                    N         j=1             j ) - yj )2, and the regularizer  is zero4. The solution for a = (a1, . . . , aN ) is readily computed by setting the derivative of (8) with respect to the vector a equal to zero; it takes the form a = K -1y with the Gram matrix defined as Kij = k(xi, xj), hence5
                               y = f (x) = a z(x) = y K-1z(x),                                                      (10)

where z(x) = (k(x, x1), k(x, x2), . . . k(x, xN ))  RN .
Implicit Wiener series estimation.                     As we stated above, the pth-degree Wiener expan- sion is the pth-order Volterra series that minimizes the squared error. This can be put into the regression framework: since any finite Volterra series can be represented as a linear functional in the corresponding RKHS, we can find the pth-order Volterra series that min- imizes the squared error by linear regression. This, by definition, must be the pth-degree Wiener series since no other Volterra series has this property6. From Eqn. (10), we obtain the following expressions for the implicit Wiener series                           1                    p                             p               G0[x] =          y 1,                   G                             H                          N                                 n[x] =                        n[x] = y    K-1                                                                                                         p    z(p)(x)    (11)                                                n=0                           n=0
where the Gram matrix Kp and the coefficient vector z(p)(x) are computed using the kernel from Eq. (7) and 1 = (1, 1, . . . )                  RN . Note that the Wiener series is represented only implicitly since we are using the RKHS representation as a sum of scalar products with the training points. Thus, we can avoid the ""curse of dimensionality"", i.e., there is no need to compute the possibly large number of coefficients explicitly.
The explicit Volterra and Wiener expansions can be recovered from Eq. (11) by collecting all terms containing monomials of the desired order and summing them up. The individual nth-order Volterra functionals in a Wiener series of degree p > 0 are given implicitly by
                                           Hn[x] = y K-1                                                                         p    zn(x)                                      (12)

with zn(x) = ((x1 x)n, (x2 x)n, . . . , (x x)n) . For p = 0 the only term is the                                                               N constant zero-order Volterra functional H0[x] = G0[x]. The coefficient vector n = (h(n)   1,1,...1, h(n)                 1,2,...1, h(n)                           1,3,...1, . . . )    of the explicit Volterra functional is obtained as
                                                 n =  K-1                                                                 n       p    y                                          (13)

   3for conditions on uniqueness of the solution, see [6]        4Note that this is different from the regularized approach used by [1]. If  is not zero, the resulting Volterra series are different from the Wiener series since they are not orthogonal with respect to the input.        5If K is not invertible, K-1 denotes the pseudo-inverse of K.        6assuming symmetrized Volterra kernels which can be obtained from any Volterra expanson.

using the design matrix n = (n(x1) , n(x1) , . . . , n(x1) ) . The individual Wiener functionals can only be recovered by applying the regression procedure twice. If we are interested in the nth-degree Wiener functional, we have to compute the solution for the kernels k(n)(x1, x2) and k(n-1)(x1, x2). The Wiener functional for n > 0 is then obtained from the difference of the two results as
            n                         n-1      Gn[x] =           Gi[x] -                   Gi[x] = y           K-1                                                                        n    z(n)(x) - K-1                                                                                               n-1 z(n-1)(x) .    (14)                 i=0                       i=0

The corresponding ith-order Volterra functionals of the nth-degree Wiener functional are computed analogously to Eqns. (12) and (13) [3].
Orthogonality.         The resulting Wiener functionals must fulfill the orthogonality condition which in its strictest form states that a pth-degree Wiener functional must be orthogonal to all monomials in the input of lower order. Formally, we will prove the following
Theorem 1 The functionals obtained from Eq. (14) fulfill the orthogonality condition
                                             E [m(x)Gp[x]] = 0                                               (15)

where E denotes the expectation over the input distribution and m(x) an arbitrary ith- order monomial with i < p.
We will show that this a consequence of the least squares fit of any linear expansion in a set of basis functions of the form y =                 M                                                          j=1         j j (x). In the case of the Wiener and Volterra expansions, the basis functions j(x) are monomials of the components of x.
We denote the error of the expansion as e(x) = y -                          M                                                                                   j=1         j j (xi). The minimum of the expected quadratic loss L with respect to the expansion coefficient k is given by
                      L                                                  =           E e(x) 2 = -2E [                                                                           k (x)e(x)] = 0.                      (16)                                k         k

This means that, for an expansion in a set of basis functions minimizing the squared error, the error is orthogonal to all basis functions used in the expansion.
Now let us assume we know the Wiener series expansion (which minimizes the mean squared error) of a system up to degree p - 1. The approximation error is given by the sum of the higher-order Wiener functionals e(x) =                                  G                                                                              n=p         n[x], so Gp[x] is part of the error. As a consequence of the linearity of the expectation, Eq. (16) implies
                                                                                          E [k(x)Gn[x]] = 0 and                                      E [k(x)Gn[x]] = 0            (17)                 n=p                                                    n=p+1

for any k of order less than p. The difference of both equations yields E [k(x)Gp[x]] = 0, so that Gp[x] must be orthogonal to any of the lower order basis functions, namely to all monomials with order smaller than p.
3      Experiments
Toy examples. In our first experiment, we check whether our intuitions about higher-order statistics described in the introduction are captured by the proposed method. In particular, we expect that arbitrarily oriented lines can only be predicted using third-order statistics. As a consequence, we should need at least a second-order Wiener functional to predict lines correctly.
Our first test image (size 80  110, upper row in Fig. 1) contains only lines of varying orientations. Choosing a 5  5 neighbourhood, we predicted the central pixel using (11).
original image      0th-order         1st-order       1st-order      2nd-order       2nd-order      3rd-order       3rd-order                    component/       reconstruction    component    reconstruction    component    reconstruction    component                   reconstruction
                                mse = 583.7                    mse = 0.006                      mse = 0





                                 mse = 1317                     mse = 37.4                    mse = 0.001





                                 mse = 1845                    mse = 334.9                     mse = 19.0

Figure 1: Higher-order components of toy images. The image components of different orders are created by the corresponding Wiener functionals. They are added up to obtain the different orders of reconstruction. Note that the constant 0-order component and reconstruction are identical. The reconstruction error (mse) is given as the mean squared error between the true grey values of the image and the reconstruction. Although the linear first-order model seems to reconstruct the lines, this is actually not true since the linear model just smoothes over the image (note its large reconstruction error). A correct prediction is only obtained by adding a second-order component to the model. The third-order component is only significant at crossings, corners and line endings.
Models of orders 0 . . . 3 were learned from the image by extracting the maximal training set of 76  106 patches of size 5  57. The corresponding image components of order 0 to 3 were computed according to (14). Note the different components generated by the Wiener functionals can also be negative. In Fig. 1, they are scaled to the gray values [0..255]. The behaviour of the models conforms to our intuition: the linear model cannot capture the line structure of the image thus leading to a large reconstruction error which drops to nearly zero when a second-order model is used. The additional small correction achieved by the third-order model is mainly due to discretization effects.
Similar to lines, we expect that we need at least a third-order model to predict crossings or corners correctly. This is confirmed by the second and third test image shown in the corresponding row in Fig. 1. Note that the third-order component is only significant at crossings, corners and line endings. The fourth- and fifth-order terms (not shown) have only negligible contributions. The fact that the reconstruction error does not drop to zero for the third image is caused by the line endings which cannot be predicted to a higher accuracy than one pixel.
Application to natural images. Are there further predictable structures in natural images that are not due to lines, crossings or corners? This can be investigated by applying our method to a set of natural images (an example of size 80  110 is depicted in Fig. 2). Our
7In contrast to the usual setting in machine learning, training and test set are identical in our case since we are not interested in generalization to other images, but in analyzing the higher-order components of the image at hand.
original image       0th-order         1st-order       1st-order      2nd-order       2nd-order                      component/       reconstruction    component    reconstruction    component                     reconstruction     mse = 1070                    mse = 957.4
  3rd-order       3rd-order         4th-order       4th-order      5th-order       5th-order  reconstruction      component        reconstruction    component    reconstruction    component      mse = 414.6                       mse = 98.5                     mse = 18.5





  6th-order       6th-order         7th-order       7th-order      8th-order       8th-order  reconstruction      component        reconstruction    component    reconstruction    component      mse = 4.98                        mse = 1.32                     mse = 0.41

Figure 2: Higher-order components and reconstructions of a photograph. Interactions up to the fifth order play an important role. Note that significant components become sparser with increasing model order.
results on a set of 10 natural images of size 50  70 show an an approximately exponential decay of the reconstruction error when more and more higher-order terms are added to the reconstruction (Fig. 3). Interestingly, terms up to order 5 still play a significant role, although the image regions with a significant component become sparser with increasing model order (see Fig. 2). Note that the nonlinear terms reduce the reconstruction error to almost 0. This suggests a high degree of higher-order redundancy in natural images that cannot be exploited by the usual linear prediction models.
4       Conclusion
The implicit estimation of Wiener functionals via polynomial kernels opens up new pos- sibilities for the estimation of higher-order image statistics. Compared to the classical methods such as higher-order spectra, moments or cumulants, our approach avoids the combinatorial explosion caused by the exponential increase of the number of terms to be estimated and interpreted. When put into a predictive framework, multiplicative pixel inter- actions of different orders are easily visualized and conform to the intuitive notions about image structures such as edges, lines, crossings or corners.
There is no one-to-one mapping between the classical higher-order statistics and multi- plicative pixel interactions. Any nonlinear Wiener functional, for instance, creates infinitely many correlations or cumulants of higher order, and often also of lower order. On the other
700                                                     Figure 3: Mean square reconstruction error of    600                                                     models of different order for a set of 10 natural                                                            images.    500
400
mse 300
200
100
   00    1      2      3    4        5    6       7                          model order

hand, a Wiener functional of order n produces only harmonic phase interactions up to order n + 1, but sometimes also of lower orders. Thus, when one analyzes a classical statistic of a given order, one often cannot determine by which order of pixel interaction it was created. In contrast, our method is able to isolate image components that are created by a single order of interaction.
Although of preliminary nature, our results on natural images suggest an important role of statistics up to the fifth order. Most of the currently used low-level feature detectors such as edge or corner detectors maximally use third-order interactions. The investigation of fourth- or higher-order features is a field that might lead to new insights into the nature and role of higher-order image structures.
As often observed in the literature (e.g. [2][7]), our results seem to confirm that a large proportion of the redundancy in natural images is contained in the higher-order pixel in- teractions. Before any further conclusions can be drawn, however, our study needs to be extended in several directions: 1. A representative image database has to be analyzed. The images must be carefully calibrated since nonlinear statistics can be highly calibration- sensitive. In addition, the contribution of image noise has to be investigated. 2. Currently, only images up to 9000 pixels can be analyzed due to the matrix inversion required by Eq. 11. To accomodate for larger images, our method has to be reformulated in an iterative algorithm. 3. So far, we only considered 5  5-patches. To systematically investigate patch size effects, the analysis has to be conducted in a multi-scale framework."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/32b991e5d77ad140559ffb95522992d0-Abstract.html,Following Curved Regularized Optimization Solution Paths,Saharon Rosset,"Regularization plays a central role in the analysis of modern data, where              non-regularized fitting is likely to lead to over-fitted models, useless for              both prediction and interpretation. We consider the design of incremen-              tal algorithms which follow paths of regularized solutions, as the regu-              larization varies. These approaches often result in methods which are              both efficient and highly flexible. We suggest a general path-following              algorithm based on second-order approximations, prove that under mild              conditions it remains ""very close"" to the path of optimal solutions and              illustrate it with examples.
1       Introduction
Given a data sample (xi, yi)n                                    i=1 (with xi  Rp and yi  R for regression, yi  {1} for classification), the generic regularized optimization problem calls for fitting models to the data while controlling complexity by solving a penalized fitting problem:
                        ^ (1)                         () = arg min               C(yi,  xi) + J()                                                    i

where C is a convex loss function and J is a convex model complexity penalty (typically taken to be the lq norm of , with q  1).1
Many commonly used supervised learning methods can be cast in this form, including regularized 1-norm and 2-norm support vector machines [13, 4], regularized linear and logistic regression (i.e. Ridge regression, lasso and their logistic equivalents) and more. In [8] we show that boosting can also be described as approximate regularized optimization, with an l1-norm penalty.
Detailed discussion of the considerations in selecting penalty and loss functions for regu- larized fitting is outside the scope of this paper. In general, there are two main areas we need to consider in this selection:

Statistical considerations: robustness (which affects selection of loss), sparsity (l1-norm penalty encourages sparse solutions) and identifiability are among the questions we should
1We assume a linear model in (1), but this is much less limiting than it seems, as the model can be linear in basis expansions of the original predictors, and so our approach covers Kernel methods, wavelets, boosting and more

keep in mind when selecting our formulation. 2. Computational considerations: we should be able to solve the problems we pose with the computational resources at our disposal. Kernel methods and boosting are examples of computational tricks that allow us to solve very high dimensional problems  exactly or approximately  with a relatively small cost. In this paper we suggest a new computational approach.
Once we have settled on a loss and penalty, we are still faced with the problem of select- ing a ""good"" regularization parameter , in terms of prediction performance. A common approach is to solve (1) for several values of , then use holdout data (or theoretical ap- proaches, like AIC or SRM) to select a good value. However, if we view the regularized optimization problem as a family of problems, parameterized by the regularization parame- ter , it allows us to define the ""path"" of optimal solutions { ^                                                               () : 0    }, which is a 1-dimensional curve through Rp. Path following methods attempt to utilize the mathemat- ical properties of this curve to devise efficient procedures for ""following"" it and generating the full set of regularized solutions with a (relatively) small computational cost.
As it turns out, there is a family of well known and interesting regularized problems for which efficient exact path following algorithms can be devised. These include the lasso [3], 1- and 2-norm support vector machines [13, 4] and many others [9]. The main property of these problems which makes them amenable to such methods is the piecewise linearity of the regularized solution path in Rp. See [9] for detailed exposition of these properties and the resulting algorithms.
However, the path following idea can stretch beyond these exact piecewise linear algo- rithms. The ""first order"" approach is to use gradient-based approaches. In [8] we have described boosting as an approximate gradient-based algorithm for following l1-norm reg- ularized solution paths. [6] suggest a gradient descent algorithm for finding an optimal so- lution for a fixed value of  and are seemingly unaware that the path they are going through is of independent interest as it consists of approximate (alas very approximate) solutions to l1-regularized problems. Gradient-based methods, however, can only follow regularized paths under strict and non-testable conditions, and theoretical ""closeness"" results to the optimal path are extremely difficult to prove for them (see [8] for details).
In this paper, we suggest a general second-order algorithm for following ""curved"" regu- larized solution paths (i.e. ones that cannot be followed exactly by piecewise-linear al- gorithms). It consists of iteratively changing the regularization parameter, while making a single Newton step at every iteration towards the optimal penalized solution, for the current value of . We prove that if both the loss and penalty are ""nice"" (in terms of bounds on their relevant derivatives in the relevant region), then the algorithm is guaranteed to stay ""very close"" to the true optimal path, where ""very close"" is defined as:
     If the change in the regularization parameter at every iteration is , then          the solution path we generate is guaranteed to be within O( 2) from the          true path of penalized optimal solutions

In section 2 we present the algorithm, and we then illustrate it on l1- and l2-regularized logistic regression in section 3. Section 4 is devoted to a formal statement and proof outline of our main result. We discuss possible extensions and future work in section 5.
2    Path following algorithm
We assume throughout that the loss function C is twice differentiable. Assume for now also that the penalty J is twice differentiable (this assumption does not apply to the l1- norm penalty which is of great interest and we address this point later). The key to our
method are the normal equations for (1):
(2)                                           C( ^                                                    ()) +  J( ^                                                                          ()) = 0
Our algorithm iteratively constructs an approximate solution ( )                                                                                                 t      by taking ""small"" Newton-Raphson steps trying to maintain (2) as the regularization changes. Our main result in this paper is to show, both empirically and theoretically, that for small , the dif-
ference ( )             t    - ^                        (0 +  t) is small, and thus that our method successfully tracks the path of optimal solutions to (1).
Algorithm 1 gives a formal description of our quadratic tracking method. We start from a solution to (1) for some fixed 0 (e.g. ^                                                    (0), the non-regularized solution). At each iteration we increase  by and take a single Newton-Raphson step towards the solution to (2) with the new  value in step 2(b).
Algorithm 1 Approximate incremental quadratic algorithm for regularized optimization
   1. Set ( )                  0      = ^                               (0), set t = 0.

   2. While (t < max)

       (a) t+1 = t +

       (b) ( )                       t+1 =                                                                               -1                       ( )                                2                        t      -    2C(( )                                          t    ) + t+1         J(( )                                                                   t      )          C(( )                                                                                            t         ) + t+1 J(( )                                                                                                                  t      )

       (c) t = t + 1

2.1    The l1-norm penalty
The l1-norm penalty, J() =  1, is of special interest because of its favorable statistical properties (e.g. [2]) and its widespread use in popular methods, such as the lasso [10] and 1-norm SVM [13]. However it is not differentiable and so our algorithm does not apply to l1-penalized problems directly.
To understand how we can generalize Algorithm 1 to this situation, we need to consider the Karush-Kuhn-Tucker (KKT) conditions for optimality of the optimization problem implied by (1). It is easy to verify that the normal equations (2) can be replaced by the following KKT-based condition for l1-norm penalty:
(3)                                     | C( ^                                                 ())j| <   ^                                                                               ()j = 0                                          ^ (4)                                     ()j = 0  | C( ^                                                                        ())j| = 
these conditions hold for any differentiable loss and tell us that at each point on the path we have a set A of non-0 coefficients which corresponds to the variables whose current ""gen- eralized correlation"" | C( ^                                      ())j| is maximal and equal to . All variables with smaller generalized correlation have 0 coefficient at the optimal penalized solution for this . Note that the l1-norm penalty is twice differentiable everywhere except at 0. So if we carefully manage the set of non-0 coefficients according to these KKT conditions, we can still apply our algorithm in the lower-dimensional subspace spanned by non-0 coefficients only.
Thus we get Algorithm 2, which employs the Newton approach of Algorithm 1 for twice differentiable penalty, limited to the sub-space of ""active"" coefficients denoted by A. It adds to Algorithm 1 updates for the ""add variable to active set"" and ""remove variable from
active set"" events, when a variable becomes ""highly correlated"" as defined in (4) and when a coefficient hits 0 , respectively. 2
Algorithm 2 Approximate incremental quadratic algorithm for regularized optimization with lasso penalty
     1. Set ( )                      0    = ^                             (0), set t = 0, set A = {j : ^                                                                      (0)j = 0}.

     2. While (t < max)

          (a) t+1 = t +               (b)

                                                                -1                           ( )                            t+1 = ( )                                    t     -      2C(( )                                                          t    )A              C(( )                                                                                   t      )A + t+1sgn(( )                                                                                                        t      )A

          (c) A = A  {j /                                     A :        C(( )                                                     t+1)j > t+1}

          (d) A = A - {j  A : |( )                 | < }                                                 t+1,j               (e) t = t + 1

2.2      Computational considerations
For a fixed 0 and max, Algorithms 1 and 2 take O(1/ ) steps. At each iteration they need to calculate the Hessians of both the loss and the penalty at a typical computational cost of O(n  p2); invert the resulting p  p matrix at a cost of O(p3); and perform the gradient calculation and multiplication, which are o(n  p2) and so do not affect the complexity calculation. Since we implicitly assume throughout that n  p, we get overall complexity of O(n  p2/ ). The choice of                 represents a tradeoff between computational complexity and accuracy (in section 4 we present theoretical results on the relationship between                               and the accuracy of the path approximation we get). In practice, our algorithm is practical for problems with up to several hundred predictors and several thousand observations. See the example in section 3.
It is interesting to compare this calculation to the obvious alternative, which is to solve O(1/ ) regularized problems (1) separately, using a Newton-Raphson approach, resulting in the same complexity (assuming the number of Newton-Raphson iterations for finding each solution is bounded). There are several reasons why our approach is preferable:
       The number of iterations until convergence of Newton-Raphson may be large even              if it does converge. Our algorithm guarantees we stay very close to the optimal              solution path with a single Newton step at each new value of .

       Empirically we observe that in some cases our algorithm is able to follow the path              while direct solution for some values of  fails to converge. We assume this is              related to various numeric properties of the specific problems being solved.

       For the interesting case of l1-norm penalty and a ""curved"" loss function (like logis-              tic log-likelihood), there is no direct Newton-Raphson algorithm. Re-formulating              the problem into differentiable form requires doubling the dimensionality. Using              our Algorithm 2, we can still utilize the same Newton method, with significant              computational savings when many coefficients are 0 and we work in a lower-              dimensional subspace.

   2When a coefficient hits 0 it not only hits a non-differentiability point in the penalty, it also ceases to be maximally correlated as defined in (4). A detailed proof of this fact and the rest of the ""accounting"" approach can be found in [9]

On the flip side, our results in section 4 below indicate that to guarantee successful tracking we require      to be small, meaning the number of steps we do in the algorithm may be significantly larger than the number of distinct problems we would typically solve to select  using a non-path approach.
2.3     Connection to path following methods from numerical analysis
There is extensive literature on path-following methods for solution paths of general para- metric problems. A good survey is given in [1]. In this context, our method can be described as a ""predictor-corrector"" method with a redundant first order predictor step. That is, the corrector step starts from the previous approximate solution. These methods are recognized as attractive options when the functions defining the path (in our case, the combination of loss and penalty) are ""smooth"" and ""far from linear"". These conditions for efficacy of our approach are reflected in the regularity conditions for the closeness result in Section 4.
3       Example: l2- and l1-penalized logistic regression
Regularized logistic regression has been successfully used as a classification and proba- bility estimation approach [11, 12]. We first illustrate applying our quadratic method to this regularized problem using a small subset of the ""spam"" data-set, available from the UCI repository (http://www.ics.uci.edu/~mlearn/MLRepository.html) which allows us to present some detailed diagnostics. Next, we apply it to the full ""spam"" data-set, to demonstrate its time complexity on bigger problems.
We first choose five variables and 300 observations and track the solution paths to two regularized logistic regression problems with the l2-norm and the l1-norm penalties:
                     ^ (5)                      () = arg min log(1 + exp{-yi xi}) +   22                                                                    ^ (6)                      () = arg min log(1 + exp{-yi xi}) +   1

Figure 1 shows the solution paths ( )(t) generated by running Algorithms 1 and 2 on this data using     = 0.02 and starting at  = 0, i.e. from the non-regularized logistic regression solution. The interesting graphs for our purpose are the ones on the right. They represent the ""optimality gap"":
                                          C(( )                                   e               t     )                                        t =                   +  t                                               J(( )                                                   t     )

where the division is done componentwise (and so the five curves in each plot correspond to the five variables we are using). Note that the optimal solution ^                                                                                (t ) is uniquely defined by the fact that (2) holds and therefore the ""optimality gap"" is equal to zero componentwise at ^      (t ). By convexity and regularity of the loss and the penalty, there is a correspondence between small values of e and small distance ( )(t) - ^                                                                       (t ) . In our example we observe that the components of e seem to be bounded in a small region around 0 for both paths (note the small scale of the y axis in both plots -- the maximal error is under 10-3). We conclude that on this simple example our method tracks the optimal solution paths well, both for the l1- and l2-regularized problems. The plots on the left show the actual coefficient paths -- the curve in R5 is shown as five coefficient traces in R, each corresponding to one variable, with the non-regularized solution (identical for both problems) on the extreme left.
Next, we run our algorithm on the full ""spam"" data-set, containing p = 57 predic- tors and n = 4601 observations.               For both the l1- and l2-penalized paths we used
                                                                                                                                            -4                                                                                                                                            x 10                                      2.5


                                  2                                                                                                                                       4                                      1.5                                           

                                  1                                                  J +                  (/)                                                                                                                2                                                                 0.5                                                           C /                                                                                                                                        0                                       0


                                -0.5                                                                                             -2                                             0    10    20      30           40                                                             0          10    20    30    40                                                                                                                                                            

                                                                                                                                            -4                                                                                                                                            x 10                                      2.5


                                  2                                                                                                                                       4                                      1.5                                                                    

                                  1                                                                           J +                  (/)                                                                                                                2                                                                      0.5                                                                                    C /                                                                                                                                        0                                       0


                                -0.5                                                                                             -2                                             0    10    20      30           40                                                             0          10    20    30    40

Figure 1: Solution paths (left) and optimality criterion (right) for l1 penalized logistic re- gression (top) and l2 penalized logistic regression (bottom). These result from running Algorithms 2 and 1, respectively, using                                      = 0.02 and starting from the non-regularized logistic regression solution (i.e.  = 0)
0 = 0, max = 50, = 0.02, and the whole path was generated in under 5 minutes using a Matlab implementation on an IBM T-30 Laptop. Like in the small scale example, the ""optimality criterion"" was uniformly small throughout the two paths, with none of its 57 components exceeding 10-3 at any point.
4      Theoretical closeness result
In this section we prove that our algorithm can track the path of true solutions to (1). We show that under regularity conditions on the loss and penalty (which hold for all the candidates we have examined), if we run Algorithm 1 with a specific step size , then we remain within O( 2) of the true path of optimal regularized solutions.
Theorem 1 Assume 0 > 0, then for                                      small enough and under regularity conditions on the derivatives of C and J ,
               0 < c < max - 0 ,                                     ( )(c/ ) - ^                                                                                                                                                 (0 + c) = O( 2)

So there is a uniform bound O( 2) on the error which does not depend on c.
Proof We give the details of the proof in Appendix A of [7]. Here we give a brief review of the main steps.
Similar to section 3 we define the ""optimality gap"":
                                                         C(( ) (7)                                                    (        t      ) )j + t = etj                                                              J(( )                                                                 t      ) Also define a ""regularity constant"" M , which depends on 0 and the first, second and third derivatives of the loss and penalty.

The proof is presented as a succession of lemmas:
                                                        Lemma 2 Let u1 = M  p  2, ut = M (ut-1 +                      p  )2, then: et 2  ut

This lemma gives a recursive expression bounding the error in the optimality gap (7) as the algorithm proceeds. The proof is based on separate Taylor expansions of the numerator and denominator of the ratio     C                              J in the optimality gap and some tedious algebra.                                                                                                                                                                                                                         1-4 p M Lemma 3 If          p M  1/4 then u       1                                       t          -         p  -                     = O( 2) , t                                            2M                             2M
This lemma shows that the recursive bound translates to a uniform O( 2) bound, if                    is small enough. The proof consists of analytically finding the fixed point of the increasing series ut.
Lemma 4 Under regularity conditions on the penalty and loss functions in the neighbor- hood of the solutions to (1), the O( 2) uniform bound of lemma 3 translates to an O( 2) uniform bound on ( )(c/ ) - ^                                     (0 + c)
Finally, this lemma translates the optimality gap bound to an actual closeness result. This is proven via a Lipschitz argument.
4.1    Required regularity conditions
Regularity in the loss and the penalty is required in the definition of the regularity constant M and in the translation of the O( 2) bound on the ""optimality gap"" into one on the distance from the path in lemma 4. The exact derivation of the regularity conditions is highly tech- nical and lengthy. They require us to bound the norm of third derivative ""hyper-matrices"" for the loss and the penalty as well as the norms of various functions of the gradients and Hessians of both (the boundedness is required only in the neighborhood of the optimal path where our approximate path can venture, obviously). We also need to have 0 > 0 and max < . Refer to Appendix A of [7] for details. Assuming that 0 > 0 and max <  these conditions hold for every interesting example we have encountered, including:
     Ridge regression and the lasso (that is, l2- and l1- regularized squared error loss).          l1- and l2-penalized logistic regression. Also Poisson regression and other expo-           nential family models.          l1- and l2-penalized exponential loss.

Note that in our practical examples above we have started from 0 = 0 and our method still worked well. We observe in figure 1 that the tracking algorithm indeed suffers the biggest inaccuracy for the small values of , but manages to ""self correct"" as  increases."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/33267e5dc58fad346e92471c43fcccdc-Abstract.html,"Learning, Regularization and Ill-Posed Inverse Problems","Lorenzo Rosasco, Andrea Caponnetto, Ernesto D. Vito, Francesca Odone, Umberto D. Giovannini","Many works have shown that strong connections relate learning from ex-           amples to regularization techniques for ill-posed inverse problems. Nev-           ertheless by now there was no formal evidence neither that learning from           examples could be seen as an inverse problem nor that theoretical results           in learning theory could be independently derived using tools from reg-           ularization theory. In this paper we provide a positive answer to both           questions. Indeed, considering the square loss, we translate the learning           problem in the language of regularization theory and show that consis-           tency results and optimal regularization parameter choice can be derived           by the discretization of the corresponding inverse problem.
1     Introduction
The main goal of learning from examples is to infer an estimator, given a finite sample of data drawn according to a fixed but unknown probabilistic input-output relation. The desired property of the selected estimator is to perform well on new data, i.e. it should gen- eralize. The fundamental works of Vapnik and further developments [16], [8], [5], show that the key to obtain a meaningful solution to the above problem is to control the complex- ity of the solution space. Interestingly, as noted by [12], [8], [2], this is the idea underlying regularization techniques for ill-posed inverse problems [15], [7]. In such a context to avoid undesired oscillating behavior of the solution we have to restrict the solution space.
Not surprisingly the form of the algorithms proposed in both theories is strikingly similar. Anyway a careful analysis shows that a rigorous connection between learning and regular- ization for inverse problem is not straightforward. In this paper we consider the square loss and show that the problem of learning can be translated into a convenient inverse problem and consistency results can be derived in a general setting. When a generic loss is consid- ered the analysis becomes immediately more complicated. Some previous works on this subject considered the special case in which the elements of the input space are fixed and not probabilistically drawn [11], [9]. Some weaker results in the same spirit of those presented in this paper can be found in [13] where anyway the connections with inverse problems is not discussed. Finally, our analysis is close to the idea of stochastic inverse problems discussed in [16]. It follows the plan of the paper. Af- ter recalling the main concepts and notation of learning and inverse problems, in section 4 we develop a formal connection between the two theories. In section 5 the main results are stated and discussed. Finally in section 6 we conclude with some remarks and open problems.
2    Learning from examples
We briefly recall some basic concepts of learning theory [16], [8]. In the framework of learning, there are two sets of variables: the input space X, compact subset of Rn, and the output space Y , compact subset of R. The relation between the input x  X and the output y  Y is described by a probability distribution (x, y) = (x)(y|x) on X  Y . The distribution  is known only through a sample z = (x, y) = ((x1, y1), . . . , (x , y )), called training set, drawn i.i.d. according to . The goal of learning is, given the sample z, to find a function fz : X  R such that fz(x) is an estimate of the output y when the new input x is given. The function fz is called estimator and the rule that, given a sample z, provides us with fz is called learning algorithm.
Given a measurable function f : X  R, the ability of f to describe the distribution  is measured by its expected risk defined as
                         I[f ] =               (f (x) - y)2 d(x,y).                                           XY

The regression function
                                     g(x) =            y d(y|x),                                                      Y is the minimizer of the expected risk over the set of all measurable functions and always exists since Y is compact. Usually, the regression function cannot be reconstructed exactly since we are given only a finite, possibly small, set of examples z. To overcome this problem, in the regularized least squares algorithm an hypothesis space H is fixed, and, given  > 0, an estimator f                                                        z     is defined as the solution of the regularized least squares problem,

                                1                             min{                (f (xi) - yi)2 +  f 2H}.                            (1)                             f H                                          i=1

The regularization parameter  has to be chosen depending on the available data,  = ( , z), in such a way that, for every > 0
                       lim P I[f ( ,z)                                            z         ] - inf I[f]  = 0.                             (2)                            +                              f H

We note that in general inffH I[f ] is larger that I[g] and represents a sort of irreducible error associated with the choice of the space H. The above convergence in probability is usually called consistency of the algorithm [16] [14].
3     Ill-Posed Inverse Problems and Regularization
In this section we give a very brief account of linear inverse problems and regularization theory [15], [7]. Let H and K be two Hilbert spaces and A : H  K a linear bounded operator. Consider the equation                                                    Af = g                                      (3)
where g, g  K and g - g K  . Here g represents the exact, unknown data and g the available, noisy data. Finding the function f satisfying the above equation, given A and g, is the linear inverse problem associated to Eq. (3). The above problem is, in general, ill- posed, that is, the Uniqueness can be restored introducing the Moore-Penrose generalized inverse f  = Ag defined as the minimum norm solution of the problem
                                       min Af - g 2 .                                       (4)                                                                   K                                            f H

However the operator A is usually not bounded so, in order to ensure a continuous de- pendence of the solution on the data, the following Tikhonov regularization scheme can be considered1                                  min{ Af - g 2                                                               +  f 2                                                           K                H},                  (5)                                  f H
whose unique minimizer is given by
                               f                                        = (AA + I )-1Ag ,                                    (6)

where A denotes the adjoint of A.
A crucial step in the above algorithm is the choice of the regularization parameter  = (, g), as a function of the noise level  and the data g, in such a way that
                                lim f (,g)                        = 0,                   (7)                                                          - f                                    0                              H

that is, the regularized solution f (,g) converges to the generalized solution f  = Ag                                        (f  exists if and only if P g  Range(A), where P is the projection on the closure of the range of A and, in that case, Af  = P g) when the noise  goes to zero.
The similarity between regularized least squares algorithm (1) and Tikhonov regulariza- tion (5) is apparent. However, several difficulties emerge. First, to treat the problem of learning in the setting of ill-posed inverse problems we have to define a direct problem by means of a suitable operator A. Second, in the context of learning, it is not clear the nature of the noise . Finally we have to clarify the relation between consistency (2) and the kind of convergence expressed by (7). In the following sections we will show a possible way to tackle these problems.
4     Learning as an Inverse Problem
We can now show how the problem of learning can be rephrased in a framework close to the one presented in the previous section. We assume that hypothesis space H is a reproducing kernel Hilbert space [1] with a contin- uous kernel K : X X  R. If x  X, we let Kx(s) = K(s, x), and, if  is the marginal distribution of  on X, we define the bounded linear operator A : H  L2(X, ) as                                   (Af )(x) = f, Kx                  = f (x),                                                                H
 1In the framework of inverse problems, many other regularization procedures are introduced [7]. For simplicity we only treat the Tikhonov regularization.

that is, A is the canonical injection of H in L2(X, ). In particular, for all f  H, the expected risk becomes,                                        I[f ] = Af - g 2                             + I[g],                                                                   L2(X,) where g is the regression function [2]. The above equation clarifies that if the expected risk admits a minimizer fH on the hypothesis space H, then it is exactly the generalized solution2 f  = Ag of the problem                                                         Af = g.                                                              (8) Moreover, given a training set z = (x, y), we get a discretized version Ax : H  E of A, that is                                         (Axf)i = f, Kx                      = f (x                                                                   i    H                   i), where E = R is the finite dimensional euclidean space endowed with the scalar product
                                                              1                                                 y, y         =                y                                                         E                          iyi.                                                                        i=1 It is straightforward to check that

                              1           (f (xi) - yi)2 = Axf - y 2 ,                                                                                                   E                                        i=1

so that the estimator f                            z     given by the regularized least squares algorithm is the regularized solution of the discrete problem                                                         Axf = y.                                                             (9) At this point it is useful to remark the following two facts. First, in learning from examples we are not interested into finding an approximation of the generalized solution of the dis- cretized problem (9), but we want to find a stable approximation of the solution of the exact problem (8) (compare with [9]). Second, we notice that in learning theory the consistency property (2) involves the control of the quantity
           I[f                     z ] - inf I[f] = Af - g 2                                                 Af                        .    (10)                                                              L2(X,) - inf                             - g 2L2(X,)                           f H                                                      f H

If P is the projection on the closure of the range of A, the definition of P gives                                                                                             2                                I[f                                                                        z ] - inf I[f] = Afz - P g                                                                (11)                                               f H                                          L2(X,)
(the above equality stronlgy depends on the fact that the loss function is the square loss). In the inverse problem setting, the square root of the above quantity is called the residue of the solution f              z . Hence, consistency is controlled by the residue of the estimator, instead of
the reconstruction error f                                    z     - f (as in inverse problems). In particular, consistency                                                   H is a weaker condition than the one required by (7) and does not require the existence of the generalized solution fH.
5     Regularization, Stochastic Noise and Consistency
To apply the framework of ill-posed inverse problems of Section 3 to the formulation of learning proposed above, we note that the operator Ax in the discretized problem (9) differs from the operator A in the exact problem (8) and a measure of the difference between Ax and A is required. Moreover, the noisy data y  E and the exact data g  L2(X, ) belong to different spaces, so that the notion of noise has to be modified. Given the above premise our derivation of consistency results is developed in two steps: we first study the residue of the solution by means of a measure of the noise due to discretization and then we show a possible way to give a probabilistic evaluation of the noise previously introduced.
 2The fact that fH is the minimal norm solution of (4) is ensured by the assumption that the support of the measure  is X, since in this case the operator A is injective.

5.1     Bounding the Residue of the Regularized Solution
We recall that the regularized solutions of problems (9) and (8) are given by
                                 f          =         (A A                                  y,                                       z                         x    x + I )-1A                                                                                              x                                      f          =         (AA + I)-1Ag.

The above equations show that f  and f  depend only on A A                                                  z                                                            x    x and AA which are operators from H into H and on Ay and Ag which are elements of                                                  x                                                                   H, so that the space E disappears. This observation suggests that noise levels could be A A                                                                                                                           x    x - AA L(H) and A y                  , where                          is the uniform operator norm. To this purpose, for           x    - Ag H                 L(H) every  = (1, 2)  R2+ we define the collection of training sets.        U = {z  (X  Y ) | Ay                                                         A                                      x      - Ag H  1, Ax x - AA L(H)  2,  N} and we let M = sup{|y| | y  Y }. The next theorem is the central result of the paper. Theorem 1 If  > 0, the following inequalities hold
     1. for any training set z  U                                                                                                                    M                                   Af                                                                                                 2 + 1                            z - P g L2(X,) - Af - P g L2(X,)  4                                                             2

     2. if P g  Range(A), for any training set z  U,                                                                                                         M                                                 f                                                                        2 + 1                                       z - f H - f - f H                                                   3                                                                                                         2 2

Moreover if we choose  = (, z) in such a way that                                lim0 sup (,z) = 0                                                                zU               2                                    lim                                       1                                            0        sup                                     =          0                                                                 zU                                                                       (12)                                                                            (,z)                                then                           lim                                          2                                            0        sup                                     =          0                                                                 zU        (,z)
                          lim sup Af (,z)                                                    = 0.                                   (13)                                                            z         - Pg                               0 zU                                         L2(X,)

We omit the complete proof and refer to [3]. Briefly, the idea is to note that
                        Af                                 z - P g L2(X,) - Af - P g L2(X,)                                                                                    1                     Af                                        = (AA) 2 (f                              z - Af L2(X,)                                                  z - f) H where the last equation follows by polar decomposition of the operator A. Moreover a simple algebraic computation gives

f                                                    A              A                            y+(AA+I)-1(A y  z -f = (AA+I)-1(AA-Ax x)(Ax x+I)-1Ax                                                                                         x -Ag) where the relevant quantities for definition of the noise appear. The first item in the above proposition quantifies the difference between the residues of the regularized solutions of the exact and discretized problems in terms of the noise level  = (1, 2). As mentioned before this is exactly the kind of result needed to derive consistency. On the other hand the last part of the proposition gives sufficient conditions on the parameter  to ensure convergence of the residue to zero as the level noise decreases. The above results were obtained introducing the collection U of training sets compatible with a certain noise level . It is left to quantify the noise level corresponding to a training set of cardinality . This will be achieved in a probabilistic setting in the next section.
5.2      Stochastic Evaluation of the Noise
In this section we estimate the discretization noise  = (1, 2).
Theorem 2 Let 1, 2 > 0 and  = supxX                         K(x, x), then
                                        M                                                                    2              P       Ag - A                                                                                              x y                                                              A                                          H   + 1, AA - Ax                                         x L(H)   + 2

                                                                        2                              2                                                                             1                              2                                                          1 - e-22M2 - e-24                                                            (14)

The proof is given in [3] and it is based on McDiarmid inequality [10] applied to the random variables                      F (z) = A                                                                                            x y - Ag            G(z) = A                          A                            .                                                    H                             x              x - AA L(H) Other estimates of the noise  can be given using, for example, union bounds and Hoeffd- ing's inequality. Anyway rather then providing a tight analysis our concern was to find an natural, explicit and easy to prove estimate of .
5.3      Consistency and Regularization Parameter Choice
Combining Theorems 1 and 2, we easily derive the following corollary.
Corollary 1 Given 0 <  < 1, with probability greater that 1 - ,
                      Af                                 z    - Pg                - Af - Pg                                             L2(X,)                                        L2(X,)

                                       M      1                                                 4                                                        +             1 + log                                                            (15)                                            2       2"
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/339a18def9898dd60a634b2ad8fbbd58-Abstract.html,Density Level Detection is Classification,"Ingo Steinwart, Don Hush, Clint Scovel","We show that anomaly detection can be interpreted as a binary classifi-          cation problem. Using this interpretation we propose a support vector          machine (SVM) for anomaly detection. We then present some theoret-          ical results which include consistency and learning rates. Finally, we          experimentally compare our SVM with the standard one-class SVM."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/358aee4cc897452c00244351e4d91f69-Abstract.html,Learning Syntactic Patterns for Automatic Hypernym Discovery,"Rion Snow, Daniel Jurafsky, Andrew Y. Ng","Semantic taxonomies such as WordNet provide a rich source of knowl-
         edge for natural language processing applications, but are expensive to
         build, maintain, and extend. Motivated by the problem of automatically
         constructing and extending such taxonomies, in this paper we present a
         new algorithm for automatically learning hypernym (is-a) relations from
         text. Our method generalizes earlier work that had relied on using small
         numbers of hand-crafted regular expression patterns to identify hyper-
         nym pairs. Using ""dependency path"" features extracted from parse trees,
         we introduce a general-purpose formalization and generalization of these
         patterns. Given a training set of text containing known hypernym pairs,
         our algorithm automatically extracts useful dependency paths and applies
         them to new corpora to identify novel pairs. On our evaluation task (de-
         termining whether two nouns in a news article participate in a hypernym
         relationship), our automatically extracted database of hypernyms attains
         both higher precision and higher recall than WordNet."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/361440528766bbaaaa1901845cf4152b-Abstract.html,"Joint Tracking of Pose, Expression, and Texture using Conditionally Gaussian Filters","Tim K. Marks, J. C. Roddey, Javier R. Movellan, John R. Hershey","We present a generative model and stochastic filtering algorithm for si-           multaneous tracking of 3D position and orientation, non-rigid motion,           object texture, and background texture using a single camera. We show           that the solution to this problem is formally equivalent to stochastic fil-           tering of conditionally Gaussian processes, a problem for which well           known approaches exist [3, 8]. We propose an approach based on Monte           Carlo sampling of the nonlinear component of the process (object mo-           tion) and exact filtering of the object and background textures given the           sampled motion. The smoothness of image sequences in time and space           is exploited by using Laplace's method to generate proposal distributions           for importance sampling [7]. The resulting inference algorithm encom-           passes both optic flow and template-based tracking as special cases, and           elucidates the conditions under which these methods are optimal. We           demonstrate an application of the system to 3D non-rigid face tracking."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/362387494f6be6613daea643a7706a42-Abstract.html,Multiple Alignment of Continuous Time Series,"Jennifer Listgarten, Radford M. Neal, Sam T. Roweis, Andrew Emili","Multiple realizations of continuous-valued time series from a stochastic            process often contain systematic variations in rate and amplitude. To            leverage the information contained in such noisy replicate sets, we need            to align them in an appropriate way (for example, to allow the data to be            properly combined by adaptive averaging). We present the Continuous            Profile Model (CPM), a generative model in which each observed time            series is a non-uniformly subsampled version of a single latent trace, to            which local rescaling and additive noise are applied. After unsupervised            training, the learned trace represents a canonical, high resolution fusion            of all the replicates. As well, an alignment in time and scale of each            observation to this trace can be found by inference in the model. We            apply CPM to successfully align speech signals from multiple speakers            and sets of Liquid Chromatography-Mass Spectrometry proteomic data.
1      A Profile Model for Continuous Data
When observing multiple time series generated by a noisy, stochastic process, large sys- tematic sources of variability are often present. For example, within a set of nominally replicate time series, the time axes can be variously shifted, compressed and expanded, in complex, non-linear ways. Additionally, in some circumstances, the scale of the mea- sured data can vary systematically from one replicate to the next, and even within a given replicate.
We propose a Continuous Profile Model (CPM) for simultaneously analyzing a set of such time series. In this model, each time series is generated as a noisy transformation of a single latent trace. The latent trace is an underlying, noiseless representation of the set of replicated, observable time series. Output time series are generated from this model by moving through a sequence of hidden states in a Markovian manner and emitting an observable value at each step, as in an HMM. Each hidden state corresponds to a particular location in the latent trace, and the emitted value from the state depends on the value of the latent trace at that position. To account for changes in the amplitude of the signals across and within replicates, the latent time states are augmented by a set of scale states, which control how the emission signal will be scaled relative to the value of the latent trace. During training, the latent trace is learned, as well as the transition probabilities controlling the Markovian evolution of the scale and time states and the overall noise level of the
observed data. After training, the latent trace learned by the model represents a higher resolution 'fusion' of the experimental replicates. Figure 1 illustrate the model in action.
                                    Unaligned, Linear Warp Alignment and CPM Alignment

                   40

                   30

                   20

  Amplitude 10

                       0                        50

                   40

                   30

                   20       Amplitude                        10

                       0                        30


                   20



               Amplitude 10



                       0                                  Time

                                                        a)                                                                b)

Figure 1: a) Top: ten replicated speech energy signals as described in Section 4), Middle: same signals, aligned using a linear warp with an offset, Bottom: aligned with CPM (the learned latent trace is also shown in cyan). b) Speech waveforms corresponding to energy signals in a), Top: unaligned originals, Bottom: aligned using CPM.
2                             Defining the Continuous Profile Model (CPM)
The CPM is generative model for a set of K time series, xk = (xk1, xk2, ..., xk ). The                                                                                                                                               N k temporal sampling rate within each xk need not be uniform, nor must it be the same across the different xk. Constraints on the variability of the sampling rate are discussed at the end of this section. For notational convenience, we henceforth assume N k = N for all k, but this is not a requirement of the model.
The CPM is set up as follows: We assume that there is a latent trace, z = (z1, z2, ..., zM ), a canonical representation of the set of noisy input replicate time series. Any given observed time series in the set is modeled as a non-uniformly subsampled version of the latent trace to which local scale transformations have been applied. Ideally, M would be infinite, or at least very large relative to N so that any experimental data could be mapped precisely to the correct underlying trace point. Aside from the computational impracticalities this would pose, great care to avoid overfitting would have to be taken. Thus in practice, we have used M = (2 + )N (double the resolution, plus some slack on each end) in our experiments and found this to be sufficient with                                                           < 0.2. Because the resolution of the latent trace is higher than that of the observed time series, experimental time can be made effectively to speed up or slow down by advancing along the latent trace in larger or smaller jumps.
The subsampling and local scaling used during the generation of each observed time se- ries are determined by a sequence of hidden state variables. Let the state sequence for observation k be k. Each state in the state sequence maps to a time state/scale state pair: k  { k, k}. Time states belong to the integer set (1..M ); scale states belong to an  i                                 i      i ordered set (1..Q). (In our experiments we have used Q=7, evenly spaced scales in logarithmic space). States, k, and observation values, xk, are related by the emission                                                                             i                                      i probability distribution: A                                                 (xk|z)  p(xk|k, z, , uk)  N (xk; z kuk, )                                                                       k                                                                      , where                                                                                   i                i    i                 i      k       i                                                                        i                                                            i
is the noise level of the observed data, N (a; b, c) denotes a Gaussian probability density for a with mean b and standard deviation c. The uk are real-valued scale parameters, one per observed time series, that correct for any overall scale difference between time series k and the latent trace.
To fully specify our model we also need to define the state transition probabilities. We define the transitions between time states and between scale states separately, so that T k              p(                         i|i-1) = p(i|i-1)pk (i|i-1).             The constraint that time must move       i-1,i forward, cannot stand still, and that it can jump ahead no more than J time states is en- forced. (In our experiments we used J = 3.) As well, we only allow scale state transitions between neighbouring scale states so that the local scale cannot jump arbitrarily. These constraints keep the number of legal transitions to a tractable computational size and work well in practice. Each observed time series has its own time transition probability dis- tribution to account for experiment-specific patterns. Both the time and scale transition probability distributions are given by multinomials:
                                                      dk                                                            1, ifa-b=1                                                           dk2, ifa-b=2                                                                .                              pk(                                                               i = a|i-1 = b) =                                                            ..dk, ifa-b=J                                                            J                                                                                                                                                   0, otherwise                                                          s0, ifD(a,b)=0                                                             s                              p(                                 1,    if D(a, b) = 1                                     i = a|i-1 = b) =                                                           s1, ifD(a,b)=-1                                                          0, otherwise where D(a, b) = 1 means that a is one scale state larger than b, and D(a, b) = -1 means that a is one scale state smaller than b, and D(a, b) = 0 means that a = b. The distributions are constrained by:           J     dk = 1 and 2s                               i=1     i               1 + s0 = 1.

J determines the maximum allowable instantaneous speedup of one portion of a time series relative to another portion, within the same series or across different series. However, the length of time for which any series can move so rapidly is constrained by the length of the latent trace; thus the maximum overall ratio in speeds achievable by the model between any two entire time series is given by min(J , M ).                                                             N
After training, one may examine either the latent trace or the alignment of each observable time series to the latent trace. Such alignments can be achieved by several methods, in- cluding use of the Viterbi algorithm to find the highest likelihood path through the hidden states [1], or sampling from the posterior over hidden state sequences. We found Viterbi alignments to work well in the experiments below; samples from the posterior looked quite similar.
3       Training with the Expectation-Maximization (EM) Algorithm
As with HMMs, training with the EM algorithm (often referred to as Baum-Welch in the context of HMMs [1]), is a natural choice. In our model the E-Step is computed exactly using the Forward-Backward algorithm [1], which provides the posterior probability over states for each time point of every observed time series, k(i)  p(                                                                               s               i = s|x) and also the pairwise state posteriors, s,t(i)  p(i-1 = s, i = t|xk). The algorithm is modified
only in that the emission probabilities depend on the latent trace as described in Section 2. The M-Step consists of a series of analytical updates to the various parameters as detailed below.
Given the latent trace (and the emission and state transition probabilities), the complete log likelihood of K observed time series, xk, is given by Lp  L + P. L is the likelihood term arising in a (conditional) HMM model, and can be obtained from the Forward-Backward algorithm. It is composed of the emission and state transition terms. P is the log prior (or penalty term), regularizing various aspects of the model parameters as explained below. These two terms are:                        K                                             N                                                    N
           L                  log p(1) +                                 log A (xk|z) +                                   log T k                                                (1)                                                                                         i       i                                             i-1,i                        k=1                                           i=1                                                  i=2

                           -1                                              K                P  -                   (zj+1 - zj)2 +                                 log D(dk|{k}) + log D(s                                                     }),                                                                                                                v          v                               v |{v                        (2)                               j=1                                               k=1

where p(1) are priors over the initial states. The first term in Equation 2 is a smoothing penalty on the latent trace, with  controlling the amount of smoothing. k                                                                                                                                                                         v and v are Dirichlet hyperprior parameters for the time and scale state transition probability distribu- tions respectively. These ensure that all non-zero transition probabilities remain non-zero. For the time state transitions, v  {1, J } and kv corresponds to the pseudo-count data for the parameters d1, d2 . . . dJ . For the scale state transitions, v  {0, 1} and k                                                                                                                                                                    v corresponds to the pseudo-count data for the parameters s0 and s1.
Letting S be the total number of possible states, that is, the number of elements in the cross-product of possible time states and possible scale states, the expected complete log likelihood is:
                               K        S                                                 K                S      N

      <Lp>=P +                                   k(1) log T k +                                                          k(i) log A                        |z) + . . .                                                            s                    0,s                                              s                        s(xk                                                                                                                                                              i                                    k=1 s=1                                                   k=1 s=1 i=1

                                   K         S              S     N

                   . . . +                                                 k (i) log T k                                                                                 s,s                            s,s                                        k=1 s=1 s =1 i=2

using the notation T k                               0         p(                                                                   (i)                           (i)                                  ,s                        1 = s), and where k                                                                                                           s           and k                        are the posteriors over                                                                                                                                       s,s states as defined above. Taking derivatives of this quantity with respect to each of the parameters and finding the critical points provides us with the M-Step update equations. In updating the latent trace z we obtain a system of M simultaneous equations, for j = 1..M :
                        K                                   N  <Lp>                                                                                       (xk - z                = 0 =                                                  k(i)                         i                j uk s) - (4z     z                                                                    s            suk                                                                j - 2zj-1 - 2zj+1)           j                                                                                                         2                          k=1 {s|s=j} i=1

For the cases j = 1, N , the terms 2zj-1 and 2zj+1, respectively, drop out. Considering all such equations we obtain a system of M equations in M unknowns. Each equation depends only linearly on three variables from the latent trace. Thus the solution is easily obtained numerically by solving a tridiagonal linear system.
Analytic updates for 2 and uk are given by:
              S           N         k(i)(xk - z uk                                                                         S           z             N       k(i)xk      2 =         s=1         i=1           s                   i         s           s)2 ,              uk =                   s=1          s     s      i=1         s          i                                                       N                                                                          S           (z                             k(i)                                                                                                                                  s=1          s     s)2           N                                                                                                                                                                    i=1        s

Lastly, updates for the scale and state transition probability distributions are given by:
                                k +        S                                    N       k           (i)                                          v      s=1        {s | -                  i=2 s,s                dk =                                                s     s =v}                 v            J     k +         J        S                                         N     k          (i)                              j=1         j       j=1       s=1          {s | -                     i=2 s,s                                                                             s       s =j}

                                 +         K         S                                    N          k          (i)                                          j      k=1       s=1           {s H(s,v)}            i=2 s,s                sv =          1       +         K         S                                                      N     k     (i)                              j=0         j      k=1       s=1           {s H(s,1),H(s,0)}                       i=2 s,s

where H(s, j)                     {s |s is exactly j scale states away from s}.                                        Note that we do not normalize the Dirichlets, and omit the traditional minus one in the exponent: D(dk|{k}) =           J         (dk)kv                          }) =      1      (s      v    v                                   and D(s                        v=1          v                    v |{v              v=0          v )v .
The M-Step updates uk, , and z are coupled. Thus we arbitrarily pick an order to update them and as one is updated, its new values are used in the updates for the coupled parameter updates that follow it. In our experiments we updated in the following order: , z, uk. The other two parameters, dkv and sv, are completely decoupled.
4    Experiments with Laboratory and Speech Data
We have applied the CPM model to analyze several Liquid Chromatography - Mass Spec- trometry (LC-MS) data sets from an experimental biology laboratory. Mass spectrometry technology is currently being developed to advance the field of proteomics [2, 3]. A mass spectrometer takes a sample as input, for example, human blood serum, and produces a measure of the abundance of molecules that have particular mass/charge ratios. In pro- teomics the molecules in question are small protein fragments. From the pattern of abun- dance values one can hope to infer which proteins are present and in what quantity. For protein mixtures that are very complex, such as blood serum, a sample preparation step is used to physically separate parts of the sample on the basis of some property of the molecules, for example, hydrophobicity. This separation spreads out the parts over time so that at each unique time point a less complex mixture is fed into the mass spectrometer. The result is a two-dimensional time series spectrum with mass/charge on one axis and time of input to the mass spectrometer on the other. In our experiments we collapsed the data at each time point to one dimension by summing together abundance values over all mass/charge values. This one-dimensional data is referred to as the Total Ion Count (TIC). We discuss alternatives to this in the last section. After alignment of the TICs, we assessed the alignment of the LC-MS data by looking at both the TIC alignments, and also the cor- responding two-dimensional alignments of the non-collapsed data, which is where the true information lies.
The first data set was a set of 13 replicates, each using protein extracted from lysed E. coli cells. Proteins were digested and subjected to capillary-scale LC-MS coupled on-line to an ion trap mass spectrometer. First we trained the model with no smoothing (i.e.,  = 0) on the 13 replicates. This provided nice alignments when viewed in both the TIC space and the full two-dimensional space. Next we used leave-one-out cross-validation on six of the replicates in order to choose a suitable value for . Because the uk and dkv are time series specific, we ran a restricted EM on the hold-out case to learn these parameters, holding the other parameters fixed at the values found from learning on the training set. Sixteen values of  over five orders of magnitude, and also zero, were used. Note that we did not include the regularization likelihood term in the calculations of hold-out likelihood. One of the non-zero values was found to be optimal (statistically significant at a p=0.05 level using a paired sample t-test to compare it to no smoothing). Visually, there did not appear to be a difference between no regularization and the optimal value of , in either the TIC space
or the full two-dimensional space. Figure 2 shows the alignments applied to the TICs and also the two-dimensional data, using the optimal value of .
                                              8                                                x 10                  Unaligned and Aligned Time Series                                                                                                                                              8                                                                                                                                           x 10                              Replicate 5

                                     10                                                                                          9                                                      Latent Trace                                                                                                                                              Original Time Series                           Aligned Experimental Time Series                                                                                                                                      8                                          8                                                                                                                                      7                                          6                                                                                                                                      6   Amplitude                              4                                                                                           5

                                     2                                                                                           4                                                                                                                         Amplitude

                                              8                                                                                  3                                          0 x 10                                                                                                                                      2

                                     6                                                                                           1

                                                                                                                                 0                                                                                                                                                   100      200       300    400      500     600            700    800                                                                                                                                           Residual                                          4

                                                                                                                                 3    Time Jump From Previous State                                                                                                                                      2                                          2                                                                                           1

           Latent Space Amplitude                                                                                                     Scale States

                                     0                                                        100    200      300      400        500    600     700    800                                       200              400              600                   800                                                                                    Time                                                                                     Latent Time

                                                                            a)                                                                                           b)





                                                                            c)                                                                                           d)

Figure 2: Figure 2: a) Top: 13 Replicate pre-processed TICs as described in Section 4), Bottom: same as top, but aligned with CPM (the learned latent trace is also shown). b) The fifth TIC replicate aligned to the learned latent trace (inset shows the original, unaligned). Below are three strips showing, from top-to-bottom, i) the error residual, ii) the number of time states moved between every two states in the Viterbi alignment, and iii) the local scaling applied at each point in the alignment. c) A portion of the two-dimensional LC-MS data from replicates two (in red) and four (in green). d) Same as c), but after alignment (the same one dimensional alignment was applied to every Mass/Charge value). Marker lines labeled A to F show how time in c) was mapped to latent time using the Viterbi alignment.
We also trained our model on five different sets of LC-MS data, each consisting of human blood serum. We used no smoothing and found the results visually similar in quality to the first data set.
To ensure convergence to a good local optimum and to speed up training, we pre-processed the LC-MS data set by coarsely aligning and scaling each time series as follows: We 1) translated each time series so that the center of mass of each time series was aligned to the median center of mass over all time series, 2) scaled the abundance values such that the sum of abundance values in each time series was equal to the median sum of abundance values over all time series.
We also used our model to align 10 speech signals, each an utterance of the same sentence
spoken by a different speaker. The short-time energy (using a 30ms Hanning window) was computed every 8ms for each utterance and the resulting vectors were used as the input to CPM for alignment. The smoothing parameter  was set to zero. For comparison, we also performed a linear warping of time with an offset. (i.e. each signal was translated so as to start at the same time, and the length of each signal was stretched or compressed so as to each occupy the same amount of time). Figure 1 shows the successful alignment of the speech signals by CPM and also the (unsuccessful) linear warp. Audio for this exam- ple can be heard at http://www.cs.toronto.edu/~jenn/alignmentStudy, which also contains some supplemental figures for the paper.
Initialization for EM training was performed as follows:  was set to 15% of the difference between the maximum and minimum values of the first time series. The latent trace was initialized to be the first observed time series, with Gaussian, zero-mean noise added, with standard deviation equal to . This was then upsampled by a factor of two by repeating every value twice in a row. The additional slack at either end of the latent trace was set to be the minimum value seen in the given time series. The uk were each set to one and the multinomial scale and state transition probabilities were set to be uniform.
5    Related Algorithms and Models
Our proposed CPM has many similarities to Input/Output HMMs (IOHMMs), also called Conditional HMMs [4]. IOHMMs extend standard HMMs [1] by conditioning the emission and transition probabilities on an observed input sequence. Each component of the output sequence corresponds to a particular component of the input. Training of an IOHMM is supervised -- a mapping from an observed input sequence to output target sequence is learned. Our CPM also requires input and thus is also a type of conditional HMM. However, the input is unobserved (but crucially it is shared between all replicates) and hence learning is unsupervised in the CPM model. One could also take the alternative view that the CPM is simply an HMM with an extra set of parameters, the latent trace, that affect the emission probabilities and which are learned by the model.
The CPM is similar in spirit to Profile HMMs which have been used with great success for discrete, multiple sequence alignment, modeling of protein families and their con- served structures, gene finding [5], among others. Profile HMM are HMMs augmented by constrained-transition 'Delete' and 'Insert' states, with the former emitting no observa- tions. Multiple sequences are provided to the Profile HMM during training and a summary of their shared statistical properties is contained in the resulting model. The development of Profile HMMs has provided a robust, statistical framework for reasoning about sets of related discrete sequence data. We put forth the CPM as a continuous data, conditional analogue.
Many algorithms currently used for aligning continuous time series data are variations of Dynamic Time Warping (DTW) [6], a dynamic programming based approach which origi- nated in the speech recognition community as a robust distance measure between two time series. DTW works on pairs of time series, aligning one time series to a specified reference time series. DTW does not take in to account systematic variations in the amplitude of the signal. Our CPM can be viewed as a rich and robust extension of DTW that can be applied to many time series in parallel and which automatically uncovers the underlying template of the data.
6    Discussion and Conclusion
We have introduced a generative model for sets of continuous, time series data. By training this model one can leverage information contained in noisy, replicated experimental data,
and obtain a single, superior resolution 'fusion' of the data. We demonstrated successful use of this model on real data, but note that it could be applied to a wide range of problems involving time signals, for example, alignment of gene expression time profiles, alignment of temporal physiological signals, alignment of motion capture data, to name but a few.
Certain assumptions of the model presented here may be violated under different ex- perimental conditions. For example, the Gaussian emission probabilities treat errors in large amplitudes in the same absolute terms as in smaller amplitudes, whereas in real- ity, it may be that the error scales with signal amplitude. Similarly, the penalty term -       -1(z         j=1       j+1 - zj )2 does not scale with the amplitude; this might result in the model arbitrarily preferring a lower amplitude latent trace. (However, in practice, we did not find this to be a problem.)
One immediate and straight-forward extension to the model would be to allow the data at each time point to be a multi-dimensional feature vector rather than a scalar value. This could easily be realized by allowing the emission probabilities to be multi-dimensional. In this way a richer set of information could be used: either the raw, multi-dimensional feature vector, or some transformation of the feature vectors, for example, Principal Components Analysis. The rest of the model would be unchanged and each feature vector would move as a coherent piece. However, it might also be useful to allow different dimensions of the feature vector to be aligned differently. For example, with the LC-MS data, this might mean allowing different mass/charge peptides to be aligned differently at each time point. However, in its full generality, such a task would be extremely computational intense.
A perhaps more interesting extension is to allow the model to work with non-replicate data. For example, suppose one had a set of LC-MS experiments from a set of cancer patients, and also a set from normal persons. It would be desirable to align the whole set of time series and also to have the model tease out the differences between them. One approach is to consider the model to be semi-supervised - the model is told the class membership of each training example. Then each class is assigned its own latent trace, and a penalty is introduced for any disagreements between the latent traces. Care needs to be taken to ensure that the penalty plateaus after a certain amount of disagreement between latent trace points, so that parts of the latent trace which are truly different are able to whole-heartedly disagree. Assuming that the time resolution in the observed time series is sufficiently high, one might also want to encourage the amount of disagreement over time to be Markovian. That is, if the previous time point disagreed with the other latent traces, then the current point should be more likely to disagree."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/36e729ec173b94133d8fa552e4029f8b-Abstract.html,Hierarchical Clustering of a Mixture Model,"Jacob Goldberger, Sam T. Roweis","In this paper we propose an efficient algorithm for reducing a large         mixture of Gaussians into a smaller mixture while still preserv-         ing the component structure of the original model; this is achieved         by clustering (grouping) the components. The method minimizes         a new, easily computed distance measure between two Gaussian         mixtures that can be motivated from a suitable stochastic model         and the iterations of the algorithm use only the model parameters,         avoiding the need for explicit resampling of datapoints. We demon-         strate the method by performing hierarchical clustering of scenery         images and handwritten digits."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/38181d991caac98be8fb2ecb8bd0f166-Abstract.html,Hierarchical Bayesian Inference in Networks of Spiking Neurons,Rajesh P. Rao,"There is growing evidence from psychophysical and neurophysiological          studies that the brain utilizes Bayesian principles for inference and de-          cision making. An important open question is how Bayesian inference          for arbitrary graphical models can be implemented in networks of spik-          ing neurons. In this paper, we show that recurrent networks of noisy          integrate-and-fire neurons can perform approximate Bayesian inference          for dynamic and hierarchical graphical models. The membrane potential          dynamics of neurons is used to implement belief propagation in the log          domain. The spiking probability of a neuron is shown to approximate the          posterior probability of the preferred state encoded by the neuron, given          past inputs. We illustrate the model using two examples: (1) a motion de-          tection network in which the spiking probability of a direction-selective          neuron becomes proportional to the posterior probability of motion in          a preferred direction, and (2) a two-level hierarchical network that pro-          duces attentional effects similar to those observed in visual cortical areas          V2 and V4. The hierarchical model offers a new Bayesian interpretation          of attentional modulation in V2 and V4."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html,Efficient Kernel Discriminant Analysis via QR Decomposition,"Tao Xiong, Jieping Ye, Qi Li, Ravi Janardan, Vladimir Cherkassky","Linear Discriminant Analysis (LDA) is a well-known method for fea-           ture extraction and dimension reduction. It has been used widely in           many applications such as face recognition. Recently, a novel LDA algo-           rithm based on QR Decomposition, namely LDA/QR, has been proposed,           which is competitive in terms of classification accuracy with other LDA           algorithms, but it has much lower costs in time and space. However,           LDA/QR is based on linear projection, which may not be suitable for data           with nonlinear structure. This paper first proposes an algorithm called           KDA/QR, which extends the LDA/QR algorithm to deal with nonlin-           ear data by using the kernel operator. Then an efficient approximation of           KDA/QR called AKDA/QR is proposed. Experiments on face image data           show that the classification accuracy of both KDA/QR and AKDA/QR           are competitive with Generalized Discriminant Analysis (GDA), a gen-           eral kernel discriminant analysis algorithm, while AKDA/QR has much           lower time and space costs.
1     Introduction
Linear Discriminant Analysis [3] is a wellknown method for dimension reduction. It has been used widely in many applications such as face recognition [2]. Classical LDA aims to find optimal transformation by minimizing the within-class distance and maximizing the between-class distance simultaneously, thus achieving maximum discrimination. The optimal transformation can be readily computed by computing the eigen-decomposition on the scatter matrices.
Although LDA works well for linear problems, it may be less effective when severe non- linearity is involved. To deal with such a limitation, nonlinear extensions through kernel functions have been proposed. The main idea of kernel-based methods is to map the input data to a feature space through a nonlinear mapping, where the inner products in the feature
space can be computed by a kernel function without knowing the nonlinear mapping explic- itly [9]. Kernel Principal Component Analysis (KPCA) [10], Kernel Fisher Discriminant Analysis (KFDA) [7] and Generalized Discriminant Analysis (GDA) [1] are, respectively, kernel-based nonlinear extensions of the well known PCA, FDA and LDA methods.
To our knowledge, there are few efficient algorithms for general kernel based discriminant algorithms -- most known algorithms effectively scale as O(n3) where n is the sample size. In [6, 8], S. Mika et al. made a first attempt to speed up KFDA through a greedy approximation technique. However the algorithm was developed to handle the binary clas- sification problem. For multi-class problem, the authors suggested the one against the rest scheme by considering all two-class problems.
Recently, an efficient variant of LDA, namely LDA/QR, was proposed in [11, 12]. The essence of LDA/QR is the utilization of QR-decomposition on a small size matrix. The time complexity of LDA/QR is linear in the size of the training data, as well as the number of dimensions of the data. Moreover, experiments in [11, 12] show that the classification accuracy of LDA/QR is competitive with other LDA algorithms.
In this paper, we first propose an algorithm, namely KDA/QR1, which is a nonlinear exten- sion of LDA/QR. Since KDA/QR involves the whole kernel matrix, which is not scalable for large datasets, we also propose an approximation of KDA/QR, namely AKDA/QR. A distinct property of AKDA/QR is that it scales as O(ndc), where n is the size of the data, d is the dimension of the data, and c is the number of classes.
We apply the proposed algorithms on face image datasets and compare them with LDA/QR, and Generalized Discriminant Analysis (GDA) [1], a general method for kernel discrim- inant analysis. Experiments show that: (1) AKDA/QR is competitive with KDA/QR and GDA in classification; (2) both KDA/QR and AKDA/QR outperform LDA/QR in classifi- cation; and (3) AKDA/QR has much lower costs in time and space than GDA."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/3bd4017318837e92a66298c7855f4427-Abstract.html,Semi-parametric Exponential Family PCA,"Sajama Sajama, Alon Orlitsky","We present a semi-parametric latent variable model based technique for            density modelling, dimensionality reduction and visualization. Unlike            previous methods, we estimate the latent distribution non-parametrically            which enables us to model data generated by an underlying low dimen-            sional, multimodal distribution. In addition, we allow the components            of latent variable models to be drawn from the exponential family which            makes the method suitable for special data types, for example binary or            count data. Simulations on real valued, binary and count data show fa-            vorable comparison to other related schemes both in terms of separating            different populations and generalization to unseen samples."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/40173ea48d9567f1f393b20c855bb40b-Abstract.html,Self-Tuning Spectral Clustering,"Lihi Zelnik-manor, Pietro Perona","We study a number of open issues in spectral clustering: (i) Selecting the appropriate scale of analysis, (ii) Handling multi-scale data, (iii) Cluster- ing with irregular background clutter, and, (iv) Finding automatically the number of groups. We ﬁrst propose that a ‘local’ scale should be used to compute the afﬁnity between each pair of points. This local scaling leads to better clustering especially when the data includes multiple scales and when the clusters are placed within a cluttered background. We further suggest exploiting the structure of the eigenvectors to infer automatically the number of groups. This leads to a new algorithm in which the ﬁnal randomly initialized k-means stage is eliminated."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/403ea2e851b9ab04a996beab4a480a30-Abstract.html,The Entire Regularization Path for the Support Vector Machine,"Saharon Rosset, Robert Tibshirani, Ji Zhu, Trevor J. Hastie","In this paper we argue that the choice of the SVM cost parameter can be          critical. We then derive an algorithm that can fit the entire path of SVM          solutions for every value of the cost parameter, with essentially the same          computational cost as fitting one SVM model."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/41ab1b1d6bf108f388dfb5cd282fb76c-Abstract.html,The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space,"Robert Jenssen, Deniz Erdogmus, Jose Principe, Torbjørn Eltoft","A new distance measure between probability density functions           (pdfs) is introduced, which we refer to as the Laplacian pdf dis-           tance. The Laplacian pdf distance exhibits a remarkable connec-           tion to Mercer kernel based learning theory via the Parzen window           technique for density estimation. In a kernel feature space defined           by the eigenspectrum of the Laplacian data matrix, this pdf dis-           tance is shown to measure the cosine of the angle between cluster           mean vectors. The Laplacian data matrix, and hence its eigenspec-           trum, can be obtained automatically based on the data at hand,           by optimal Parzen window selection. We show that the Laplacian           pdf distance has an interesting interpretation as a risk function           connected to the probability of error.
1      Introduction
In recent years, spectral clustering methods, i.e. data partitioning based on the eigenspectrum of kernel matrices, have received a lot of attention [1, 2]. Some unresolved questions associated with these methods are for example that it is not always clear which cost function that is being optimized and that is not clear how to construct a proper kernel matrix.
In this paper, we introduce a well-defined cost function for spectral clustering. This cost function is derived from a new information theoretic distance measure between cluster pdfs, named the Laplacian pdf distance. The information theoretic/spectral duality is established via the Parzen window methodology for density estimation. The resulting spectral clustering cost function measures the cosine of the angle between cluster mean vectors in a Mercer kernel feature space, where the feature space is determined by the eigenspectrum of the Laplacian matrix. A principled approach to spectral clustering would be to optimize this cost function in the feature space by assigning cluster memberships. Because of space limitations, we leave it to a future paper to present an actual clustering algorithm optimizing this cost function, and focus in this paper on the theoretical properties of the new measure.
 Corresponding author. Phone: (+47) 776 46493. Email: robertj@phys.uit.no

An important by-product of the theory presented is that a method for learning the Mercer kernel matrix via optimal Parzen windowing is provided. This means that the Laplacian matrix, its eigenspectrum and hence the feature space mapping can be determined automatically. We illustrate this property by an example.
We also show that the Laplacian pdf distance has an interesting relationship to the probability of error.
In section 2, we briefly review kernel feature space theory. In section 3, we utilize the Parzen window technique for function approximation, in order to introduce the new Laplacian pdf distance and discuss some properties in sections 4 and 5. Section 6 concludes the paper.
2         Kernel Feature Spaces
Mercer kernel-based learning algorithms [3] make use of the following idea: via a nonlinear mapping                                  : Rd  F, x  (x)                                                (1) the data x1, . . . , xN  Rd is mapped into a potentially much higher dimensional feature space F. For a given learning problem one now considers the same algorithm in F instead of in Rd, that is, one works with (x1),...,(xN)  F. Consider a symmetric kernel function k(x, y). If k : C  C  R is a continuous kernel of a positive integral operator in a Hilbert space L2(C) on a compact set C  Rd, i.e.                            L2(C) : k(x,y)(x)(y)dxdy  0,                                       (2)                                        C then there exists a space F and a mapping  : Rd  F, such that by Mercer's theorem [4]                                                     NF
                     k(x, y) = (x), (y) =            ii(x)i(y),                            (3)                                                     i=1

where , denotes an inner product, the i's are the orthonormal eigenfunctions of the kernel and NF   [3]. In this case                            (x) = [    11(x),    22(x), . . . ]T ,                              (4)
can potentially be realized.
In some cases, it may be desirable to realize this mapping. This issue has been addressed in [5]. Define the (N  N) Gram matrix, K, also called the affinity, or kernel matrix, with elements Kij = k(xi, xj), i, j = 1, . . . , N . This matrix can be diagonalized as ET KE = , where the columns of E contains the eigenvectors of K and  is a diagonal matrix containing the non-negative eigenvalues ~                                                                                1, . . . , ~                                                                                           N , ~                                                                                                 1    ~N. In [5], it was shown that the eigenfunctions and eigenvalues of (4) can                                                    ~ be approximated as                                j                           j (xi)  Neji, j            , where e                                                    N                  ji denotes the ith element of the jth eigenvector. Hence, the mapping (4), can be approximated as
                         (xi)  [ ~1e1i,..., ~NeNi]T.                                        (5) Thus, the mapping is based on the eigenspectrum of K. The feature space data set may be represented in matrix form as NN = [(x1), . . . , (xN )]. Hence,  =      1  2 ET . It may be desirable to truncate the mapping (5) to C-dimensions. Thus,



                                                                                                                                       T only the C first rows of  are kept, yielding ^                                                                               . It is well-known that ^                                                                                                                             K = ^                                                                                                                                            ^                                                                                                                                                  is the best rank-C approximation to K wrt. the Frobenius norm [6].

The most widely used Mercer kernel is the radial-basis-function (RBF)
                                       k(x, y) = exp -||x - y||2 .                                                                           (6)                                                                                    22

3          Function Approximation using Parzen Windowing
Parzen windowing is a kernel-based density estimation method, where the resulting density estimate is continuous and differentiable provided that the selected kernel is continuous and differentiable [7]. Given a set of iid samples {x1,...,xN} drawn from the true density f (x), the Parzen window estimate for this distribution is [7]                                                                       N                                                 ^              1                                                 f (x) =                     W                                                                N                 2 (x, xi),                                                     (7)                                                                      i=1
where W2 is the Parzen window, or kernel, and 2 controls the width of the kernel. The Parzen window must integrate to one, and is typically chosen to be a pdf itself with mean xi, such as the Gaussian kernel                                                                1                             W2 (x, xi) =                                   exp                                  ,                               (8)                                                                       d               -||x - xi||2                                                      (22) 2                               22 which we will assume in the rest of this paper. In the conclusion, we briefly discuss the use of other kernels.
Consider a function h(x) = v(x)f (x), for some function v(x). We propose to estimate h(x) by the following generalized Parzen estimator                                                                 N                                            ^              1                                            h(x) =                     v(xi)W                                                           N                           2 (x, xi).                                                (9)                                                                i=1 This estimator is asymptotically unbiased, which can be shown as follows
         1 N      Ef                 v(xi)W             N                     2 (x, xi)         =         v(z)f (z)W2 (x, z)dz = [v(x)f (x)]  W2(x),                  i=1                                                                                                                                                 (10) where Ef () denotes expectation with respect to the density f(x). In the limit as N   and (N)  0, we have                                      lim [v(x)f (x)]  W2(x) = v(x)f(x).                                                                       (11)                                     N                                    (N )0 Of course, if v(x) = 1 x, then (9) is nothing but the traditional Parzen estimator of h(x) = f (x). The estimator (9) is also asymptotically consistent provided that the kernel width (N ) is annealed at a sufficiently slow rate. The proof will be presented in another paper.

Many approaches have been proposed in order to optimally determine the size of the Parzen window, given a finite sample data set. A simple selection rule was proposed by Silverman [8], using the mean integrated square error (MISE) between the estimated and the actual pdf as the optimality metric:                                                                                                 1                                                                                             d+4                                            opt = X            4N -1(2d + 1)-1                            ,                                     (12) where d is the dimensionality of the data and 2 = d-1                                                               , where            are the                                                                                  X                   i         Xii                 Xii diagonal elements of the sample covariance matrix. More advanced approximations to the MISE solution also exist.
4         The Laplacian PDF Distance
Cost functions for clustering are often based on distance measures between pdfs. The goal is to assign memberships to the data patterns with respect to a set of clusters, such that the cost function is optimized.
Assume that a data set consists of two clusters. Associate the probability density function p(x) with one of the clusters, and the density q(x) with the other cluster. Let f (x) be the overall probability density function of the data set. Now define the f -1 weighted inner product between p(x) and q(x) as p, q f  p(x)q(x)f-1(x)dx. In such an inner product space, the Cauchy-Schwarz inequality holds, that is, p, q 2                     q, q         . Based on this discussion, an information theoretic distance           f  p, p f               f measure between the two pdfs can be expressed as
                                                                               p, q                                            D                                               f                                                 L = - log                                                    0.                                           (13)                                                                               p, p         q, q                                                                                       f               f

We refer to this measure as the Laplacian pdf distance, for reasons that we discuss next. It can be seen that the distance DL is zero if and only if the two densities are equal. It is non-negative, and increases as the overlap between the two pdfs decreases. However, it does not obey the triangle inequality, and is thus not a distance measure in the strict mathematical sense.
We will now show that the Laplacian pdf distance is also a cost function for clus- tering in a kernel feature space, using the generalized Parzen estimators discussed in the previous section. Since the logarithm is a monotonic function, we will derive the expression for the argument of the log in (13). This quantity will for simplicity be denoted by the letter ""L"" in equations.
Assume that we have available the iid data points {xi}, i = 1,...,N1, drawn from p(x), which is the density of cluster C1, and the iid {xj}, j = 1, . . ., N2, drawn from q(x), the density of C2. Let h(x) = f - 12 (x)p(x) and g(x) = f - 12 (x)q(x). Hence, we may write                                                                            h(x)g(x)dx                                                 L =                                                              .                                         (14)                                                                         h2(x)dx            g2(x)dx
We estimate h(x) and g(x) by the generalized Parzen kernel estimators, as follows
                      N1                                                                               N2      ^          1                                                                               1      h(x) =                      f - 12 (xi)W                                                                     f - 12 (xj )W                 N                                       2 (x, xi ),          ^                                                                               g(x) =                                                   2 (x, xj ).        (15)                      1                                                                          N2                           i=1                                                                         j=1

The approach taken, is to substitute these estimators into (14), to obtain
                                                      N                                                           N                                                    1           1                                            1              2           h(x)g(x)dx                                               f - 12 (xi)W                                                f - 12 (xj )W                                               N                                       2 (x, xi )                                                2 (x, xj )                                                    1                                                       N2                                                           i=1                                                         j=1

                                                      N                                               1                1 ,N2                                   =                                     f - 12 (xi)f - 12 (xj )                   W                                          N                                                                             2 (x, xi )W2 (x, xj )dx                                               1N2 i,j=1

                                                      N                                               1                1 ,N2                                   =                                     f - 12 (xi)f - 12 (xj )W                                          N                                                                       22 (xi, xj ),                            (16)                                               1N2 i,j=1

where in the last step, the convolution theorem for Gaussians has been employed. Similarly, we have
                                                                  N                                                             1              1 ,N1                                   h2(x)dx                                            f - 12 (xi)f - 12 (xi )W                                                            N 2                                                                  22 (xi, xi ),                  (17)                                                                 1 i,i =1

                                                                  N                                                            1               2 ,N2                                   g2(x)dx                                            f - 12 (xj)f - 12 (xj )W                                                        N 2                                                                      22 (xj , xj ).                 (18)                                                             2 j,j =1

Now we define the matrix Kf , such that
                              Kf = K                                      ij               f (xi, xj ) = f - 1                                                                                            2 (xi)f - 12 (xj )K(xi, xj ),                                        (19)

where K(xi, xj ) = W22 (xi, xj) for i, j = 1, . . . , N and N = N1 + N2. As a consequence, (14) can be re-written as follows
                                                                             N1,N2 Kf (xi, xj)                                    L =                                           i,j=1                                                                          (20)                                                        N1,N1 K                                                           K                                                        i,i =1                 f (xi, xi )                   N2,N2                                                                                                             j,j =1            f (xj , xj )

The key point of this paper, is to note that the matrix K = Kij = K(xi, xj), i, j = 1, . . . , N , is the data affinity matrix, and that K(xi, xj) is a Gaussian RBF kernel function. Hence, it is also a kernel function that satisfies Mercer's theorem. Since K(xi, xj) satisfies Mercer's theorem, the following by definition holds [4]. For any set of examples {x1,...,xN} and any set of real numbers 1,...,N                                                            N          N
                                                                          ijK(xi, xj)  0,                                                                (21)                                                            i=1 j=1

in analogy to (3). Moreover, this means that
  N          N                                                                                          N      N

                  ijf - 12 (xi)f - 12 (xj )K(xi, xj) =                                                             ijKf (xi, xj)  0,                   (22)      i=1 j=1                                                                                           i=1 j=1

hence Kf (xi, xj ) is also a Mercer kernel.
Now, it is readily observed that the Laplacian pdf distance can be analyzed in terms of inner products in a Mercer kernel-based Hilbert feature space, since Kf (xi, xj) = f (xi), f (xj) . Consequently, (20) can be written as follows
                                                                       N1,N2 f (xi), f (xj)                       L =                                                  i,j=1                                       N1,N1                                                                                                                    i,i =1                f (xi), f (xi )                                N2,N2                                                                                                             j,j =1             f (xj ), f (xj )

                                              1              N1                                             N2                                                   N               i=1             f (xi ), 1                                                                                              N                   j=1     f (xj ) =                                                     1                                           2            1           N1                                  N1                                                   N2                         N2              N                           f (xi), 1                                     f (xi )               1                                                                                                                                f (xj ), 1                  f (xj )             1          i=1                      N1          i =1                                  N2              j=1                   N2    j =1

                                                   m1 , m2                                            =                     f               f       = cos (m , m ),                                                        (23)                                                  ||m                                                               1f          2f                                                            1f ||||m2f || where m                             Ni             i         = 1                                    f      N                       f (xl), i = 1, 2, that is, the sample mean of the ith cluster                              i      l=1 in feature space.

This is a very interesting result. We started out with a distance measure between densities in the input space. By utilizing the Parzen window method, this distance measure turned out to have an equivalent expression as a measure of the distance between two clusters of data points in a Mercer kernel feature space. In the feature space, the distance that is measured is the cosine of the angle between the cluster mean vectors.
The actual mapping of a data point to the kernel feature space is given by the eigendecomposition of Kf , via (5). Let us examine this mapping in more detail.              1 Note that f 2 (xi) can be estimated from the data by the traditional Parzen pdf estimator as follows
                                                    N                             1                      1                         f 2 (xi) =                             W          (xi, xl) =              di.    (24)                                                    N                2                                                                      f                                                         l=1

Define the matrix D = diag(d1, . . . , dN ). Then Kf can be expressed as
                                          Kf = D- 12 KD- 12 .                                        (25)

Quite interestingly, for 2 = 22, this is in fact the Laplacian data matrix. 1                                  f
The above discussion explicitly connects the Parzen kernel and the Mercer kernel. Moreover, automatic procedures exist in the density estimation literature to opti- mally determine the Parzen kernel given a data set. Thus, the Mercer kernel is also determined by the same procedure. Therefore, the mapping by the Laplacian matrix to the kernel feature space can also be determined automatically. We regard this as a significant result in the kernel based learning theory.
As an example, consider Fig. 1 (a) which shows a data set consisting of a ring with a dense cluster in the middle. The MISE kernel size is opt = 0.16, and the Parzen pdf estimate is shown in Fig. 1 (b). The data mapping given by the corresponding Laplacian matrix is shown in Fig. 1 (c) (truncated to two dimensions for visualization purposes). It can be seen that the data is distributed along two lines radially from the origin, indicating that clustering based on the angular measure we have derived makes sense.
The above analysis can easily be extended to any number of pdfs/clusters. In the C-cluster case, we define the Laplacian pdf distance as
                                        C-1                     pi, pj                             L =                                                  f                .      (26)                                             i=1 j=i C          pi, pi            p                                                                             f         j , pj f

In the kernel feature space, (26), corresponds to all cluster mean vectors being pairwise as orthogonal to each other as possible, for all possible unique pairs.
4.1    Connection to the Ng et al. [2] algorithm
Recently, Ng et al. [2] proposed to map the input data to a feature space determined by the eigenvectors corresponding to the C largest eigenvalues of the Laplacian ma- trix. In that space, the data was normalized to unit norm and clustered by the C-means algorithm. We have shown that the Laplacian pdf distance provides a
1It is a bit imprecise to refer to Kf as the Laplacian matrix, as readers familiar with spectral graph theory may recognize, since the definition of the Laplacian matrix is L = I - Kf . However, replacing Kf by L does not change the eigenvectors, it only changes the eigenvalues from i to 1                                      -         i.
                                                                                        0





                                                                                                                         0




      (a) Data set                        (b) Parzen pdf estimate                             (c) Feature space data

Figure 1: The kernel size is automatically determined (MISE), yielding the Parzen estimate (b) with the corresponding feature space mapping (c).
clustering cost function, measuring the cosine of the angle between cluster means, in a related kernel feature space, which in our case can be determined automati- cally. A more principled approach to clustering than that taken by Ng et al. is to optimize (23) in the feature space, instead of using C-means. However, because of the normalization of the data in the feature space, C-means can be interpreted as clustering the data based on an angular measure. This may explain some of the success of the Ng et al. algorithm; it achieves more or less the same goal as cluster- ing based on the Laplacian distance would be expected to do. We will investigate this claim in our future work. Note that we in our framework may choose to use only the C largest eigenvalues/eigenvectors in the mapping, as discussed in section 2. Since we incorporate the eigenvalues in the mapping, in contrast to Ng et al., the actual mapping will in general be different in the two cases.
5    The Laplacian PDF distance as a risk function
We now give an analysis of the Laplacian pdf distance that may further motivate its use as a clustering cost function. Consider again the two cluster case. The overall data distribution can be expressed as f (x) = P1p(x) + P2q(x), were Pi, i = 1, 2, are the priors. Assume that the two clusters are well separated, such that for xi  C1, f (xi)  P1p(xi), while for xi  C2, f(xi)  P2q(xi). Let us examine the numerator of (14) in this case. It can be approximated as                                     p(x)q(x) dx                                                                                          f (x)
                p(x)q(x)                       p(x)q(x)                 1                         1                                dx +                              dx                    q(x)dx +                 p(x)dx.    (27)           C          f (x)                              f (x)              P1                         P2                1                             C2                                    C1                       C2

By performing a similar calculation for the denominator of (14), it can be shown to be approximately equal to                          1                                                           . Hence, the Laplacian pdf distance can be written                                                    P1P1 as a risk function, given by
                                                       1                        1                               L  P1P2                                 q(x)dx +                   p(x)dx .                   (28)                                                         P1 C                       P2                                                                   1                        C2

Note that if P1 = P2 = 1 , then L = 2P                                         2                              e, where Pe is the probability of error when assigning data points to the two clusters, that is
                               Pe = P1                       q(x)dx + P2              p(x)dx.                            (29)                                                            C1                       C2

Thus, in this case, minimizing L is equivalent to minimizing Pe. However, in the case that P1 = P2, (28) has an even more interesting interpretation. In that situation, it can be seen that the two integrals in the expressions (28) and (29) are weighted exactly oppositely. For example, if P1 is close to one, L                    p(x)dx, while P                                                                     C                            e                                                                           2        q(x)dx. Thus, the Laplacian pdf distance emphasizes to cluster the most un-  C1 likely data points correctly. In many real world applications, this property may be crucial. For example, in medical applications, the most important points to classify correctly are often the least probable, such as detecting some rare disease in a group of patients.
6       Conclusions
We have introduced a new pdf distance measure that we refer to as the Laplacian pdf distance, and we have shown that it is in fact a clustering cost function in a kernel feature space determined by the eigenspectrum of the Laplacian data matrix. In our exposition, the Mercer kernel and the Parzen kernel is equivalent, making it possible to determine the Mercer kernel based on automatic selection procedures for the Parzen kernel. Hence, the Laplacian data matrix and its eigenspectrum can be determined automatically too. We have shown that the new pdf distance has an interesting property as a risk function.
The results we have derived can only be obtained analytically using Gaussian ker- nels. The same results may be obtained using other Mercer kernels, but it requires an additional approximation wrt. the expectation operator. This discussion is left for future work.
Acknowledgments.               This work was partially supported by NSF grant ECS- 0300340."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/421b3ac5c24ee992edd6087611c60dbb-Abstract.html,Experts in a Markov Decision Process,"Eyal Even-dar, Sham M. Kakade, Yishay Mansour","We consider an MDP setting in which the reward function is allowed to             change during each time step of play (possibly in an adversarial manner),             yet the dynamics remain fixed. Similar to the experts setting, we address             the question of how well can an agent do when compared to the reward             achieved under the best stationary policy over time. We provide efficient             algorithms, which have regret bounds with no dependence on the size of             state space. Instead, these bounds depend only on a certain horizon time             of the process and logarithmically on the number of actions. We also             show that in the case that the dynamics change over time, the problem             becomes computationally hard.
1      Introduction
There is an inherent tension between the objectives in an expert setting and those in a re- inforcement learning setting. In the experts problem, during every round a learner chooses one of n decision making experts and incurs the loss of the chosen expert. The setting is typically an adversarial one, where Nature provides the examples to a learner. The stan- dard objective here is a myopic, backwards looking one -- in retrospect, we desire that our performance is not much worse than had we chosen any single expert on the sequence of examples provided by Nature. In contrast, a reinforcement learning setting typically makes the much stronger assumption of a fixed environment, typically a Markov decision pro- cess (MDP), and the forward looking objective is to maximize some measure of the future reward with respect to this fixed environment.
The motivation of this work is to understand how to efficiently incorporate the benefits of existing experts algorithms into a more adversarial reinforcement learning setting, where certain aspects of the environment could change over time. A naive way to implement an experts algorithm is to simply associate an expert with each fixed policy. The running time of such algorithms is polynomial in the number of experts and the regret (the difference from the optimal reward) is logarithmic in the number of experts. For our setting the num- ber of policies is huge, namely #actions#states, which renders the naive experts approach computationally infeasible.
Furthermore, straightforward applications of standard regret algorithms produce regret bounds which are logarithmic in the number of policies, so they have linear dependence
   This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-506778, by a grant from the Israel Science Foundation and an IBM faculty award. This publication only reflects the authors' views.

on the number of states. We might hope for a more effective regret bound which has no dependence on the size of state space (which is typically large).
The setting we consider is one in which the dynamics of the environment are known to the learner, but the reward function can change over time. We assume that after each time step the learner has complete knowledge of the previous reward functions (over the entire environment), but does not know the future reward functions.
As a motivating example one can consider taking a long road-trip over some period of time T . The dynamics, namely the roads, are fixed, but the road conditions may change frequently. By listening to the radio, one can get (effectively) instant updates of the road and traffic conditions. Here, the task is to minimize the cost during the period of time T . Note that at each time step we select one road segment, suffer a certain delay, and need to plan ahead with respect to our current position.
This example is similar to an adversarial shortest path problem considered in Kalai and Vempala [2003]. In fact Kalai and Vempala [2003], address the computational difficulty of handling a large number of experts under certain linear assumptions on the reward func- tions. However, their algorithm is not directly applicable to our setting, due to the fact that in our setting, decisions must be made with respect to the current state of the agent (and the reward could be changing frequently), while in their setting the decisions are only made with respect to a single state.
McMahan et al. [2003] also considered a similar setting -- they also assume that the reward function is chosen by an adversary and that the dynamics are fixed. However, they assume that the cost functions come from a finite set (but are not observable) and the goal is to find a min-max solution for the related stochastic game.
In this work, we provide efficient ways to incorporate existing best experts algorithms into the MDP setting. Furthermore, our loss bounds (compared to the best constant policy) have no dependence on the number of states and depend only on on a certain horizon time of the environment and log(#actions). There are two sensible extensions of our setting. The first is where we allow Nature to change the dynamics of the environment over time. Here, we show that it becomes NP-Hard to develop a low regret algorithm even for oblivious adversary. The second extension is to consider one in which the agent only observes the rewards for the states it actually visits (a generalization of the multi-arm bandits problem). We leave this interesting direction for future work."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/42fe880812925e520249e808937738d2-Abstract.html,Neighbourhood Components Analysis,"Jacob Goldberger, Geoffrey E. Hinton, Sam T. Roweis, Ruslan Salakhutdinov","In this paper we propose a novel method for learning a Mahalanobis              distance measure to be used in the KNN classification algorithm. The              algorithm directly maximizes a stochastic variant of the leave-one-out              KNN score on the training set. It can also learn a low-dimensional lin-              ear embedding of labeled data that can be used for data visualization              and fast classification. Unlike other methods, our classification model              is non-parametric, making no assumptions about the shape of the class              distributions or the boundaries between them. The performance of the              method is demonstrated on several data sets, both for metric learning and              linear dimensionality reduction."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/453fadbd8a1a3af50a9df4df899537b5-Abstract.html,Learning Gaussian Process Kernels via Hierarchical Bayes,"Anton Schwaighofer, Volker Tresp, Kai Yu","We present a novel method for learning with Gaussian process regres-            sion in a hierarchical Bayesian framework. In a first step, kernel matri-            ces on a fixed set of input points are learned from data using a simple            and efficient EM algorithm. This step is nonparametric, in that it does            not require a parametric form of covariance function. In a second step,            kernel functions are fitted to approximate the learned covariance matrix            using a generalized Nystrom method, which results in a complex, data            driven kernel. We evaluate our approach as a recommendation engine            for art images, where the proposed hierarchical Bayesian method leads            to excellent prediction performance.
1     Introduction
In many real-world application domains, the available training data sets are quite small, which makes learning and model selection difficult. For example, in the user preference modelling problem we will consider later, learning a preference model would amount to fitting a model based on only 20 samples of a user's preference data. Fortunately, there are situations where individual data sets are small, but data from similar scenarios can be obtained. Returning to the example of preference modelling, data for many different users are typically available. This data stems from clearly separate individuals, but we can expect that models can borrow strength from data of users with similar tastes. Typically, such problems have been handled by either mixed effects models or hierarchical Bayesian modelling.
In this paper we present a novel approach to hierarchical Bayesian modelling in the context of Gaussian process regression, with an application to recommender systems. Here, hier- archical Bayesian modelling essentially means to learn the mean and covariance function of the Gaussian process.
In a first step, a common collaborative kernel matrix is learned from the data via a simple and efficient EM algorithm. This circumvents the problem of kernel design, as no paramet- ric form of kernel function is required here. Thus, this form of learning a covariance matrix is also suited for problems with complex covariance structure (e.g. nonstationarity).
A portion of the learned covariance matrix can be explained by the input features and, thus,
generalized to new objects via a content-based kernel smoother. Thus, in a second step, we generalize the covariance matrix (learned by the EM-algorithm) to new items using a generalized Nystrom method. The result is a complex content-based kernel which itself is a weighted superposition of simple smoothing kernels. This second part could also be applied to other situations where one needs to extrapolate a covariance matrix on a finite set (e.g. a graph) to a continuous input space, as, for example, required in induction for semi-supervised learning [14].
The paper is organized as follows. Sec. 2 casts Gaussian process regression in a hierarchical Bayesian framework, and shows the EM updates to learn the covariance matrix in the first step. Extrapolating the covariance matrix is shown in Sec. 3. We illustrate the function of the EM-learning on a toy example in Sec. 4, before applying the proposed methods as a recommender system for images in Sec. 4.1.
1.1      Previous Work
In statistics, modelling data from related scenarios is typically done via mixed effects mod- els or hierarchical Bayesian (HB) modelling [6]. In HB, parameters of models for indi- vidual scenarios (e.g. users in recommender systems) are assumed to be drawn from a common (hyper)prior distribution, allowing the individual models to interact and regular- ize each other. Recent examples of HB modelling in machine learning include [1, 2]. In other contexts, this learning framework is called multi-task learning [4]. Multi-task learn- ing with Gaussian processes has been suggested by [8], yet with the rather stringent as- sumption that one has observations on the same set of points in each individual scenario. Based on sparse approximations of GPs, a more general GP multi-task learner with para- metric covariance functions has been presented in [7]. In contrast, the approach presented in this paper only considers covariance matrices (and is thus non-parametric) in the first step. Only in a second extrapolation step, kernel smoothing leads to predictions based on a covariance function that is a data-driven combination of simple kernel functions.
2       Learning GP Kernel Matrices via EM
The learning task we are concerned with can be stated as follows: The data are observations from M different scenarios. In the i.th scenario, we have observations yi = (yi , . . . , yi )                                                                                             1        N i on a total of N i points, Xi = {xi , . . . , xi }                                          1              . In order to analyze this data in a hierarchical                                                  N i Bayesian way, we assume that the data for each scenario is a noisy sample of a Gaussian process (GP) with unknown mean and covariance function. We assume that mean and covariance function are shared across different scenarios.1
In the first modelling step presented in this section, we consider transductive learning (""la- belling a partially labelled data set""), that is, we are interested in the model's behavior only on points X, with X =             M      Xi and cardinality N = |X|. This situation is relevant                                   i=1 for most collaborative filtering applications. Thus, test points are the unlabelled points in each scenario. This reduces the whole ""infinite dimensional"" Gaussian process to its finite dimensional projection on points X, which is an N -variate Gaussian distribution with co- variance matrix K and mean vector m. For the EM algorithm to work, we also require that there is some overlap between scenarios, that is, Xi  Xj =  for some i, j. Coming back to the user modelling problem mentioned above, this means that at least some items have been rated by more than one user.
Thus, our first modelling step focusses on directly learning the covariance matrix K and
   1Alternative HB approaches for collaborative filtering, like that discussed in [5], assume that model weights are drawn from a shared Gaussian distribution.

m from the data via an efficient EM algorithm. This may be of particular help in problems where one would need to specify a complex (e.g. nonstationary) covariance function.
Following the hierarchical Bayesian assumption, the data observed in each scenario is thus a partial sample from N (y | m, K + 21), where 1 denotes the unit matrix. The joint model is simply                                                 M
                             p(m, K)               p(yi | f i)p(f i | m, K),                            (1)

                                            i=1

where p(m, K) denotes the prior distribution for mean and covariance. We assume a Gaus- sian likelihood p(yi | f i) with diagonal covariance matrix 21.
2.1      EM Learning
For the above hierarchical Bayesian model, Eq. (1), the marginal likelihood becomes
                                     M

                          p(m, K)                p(yi | f i)p(f i | m, K) df i.                         (2)

                                     i=1

To obtain simple and stable solutions when estimating m and K from the data, we con- sider point estimates of the parameters m and K, based on a penalized likelihood approach with conjugate priors.2 The conjugate prior for mean m and covariance K of a multivari- ate Gaussian is the so-called Normal-Wishart distribution [6], which decomposes into the product of an inverse Wishart distribution for K and a Normal distribution for m,
                         p(m, K) = N (m | , -1K)Wi-1(K|, U ).                                        (3)

That is, the prior for the Gram matrix K is given by an inverse Wishart distribution with scalar parameter  > 1/2(N - 1) and U being a symmetric positive-definite matrix. Given the covariance matrix K, m is Gaussian distributed with mean  and covariance -1K, where  is a positive scalar. The parameters can be interpreted in terms of an equivalent data set for the mean (this data set has size A, with A = , and mean  = ) and a data set for the covariance that has size B, with  = (B + N )/2, and covariance S, U = (B/2)S.
In order to write down the EM algorithm in a compact way, we denote by I(i) the set of indices of those data points that have been observed in the i.th scenario, that is I(i) = {j | j  {1, . . . , N } and xj  Xi}. Keep in mind that in most applications of interest N i         N such that most targets are missing in training. KI(i),I(i) denotes the square submatrix of K that corresponds to points I(i), that is, the covariance matrix for points in the i.th scenario. By K,I(i) we denote the covariance matrix of all N points versus those in the i.th scenario."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/47fd3c87f42f55d4b233417d49c34783-Abstract.html,Nonlinear Blind Source Separation by Integrating Independent Component Analysis and Slow Feature Analysis,"Tobias Blaschke, Laurenz Wiskott","In contrast to the equivalence of linear blind source separation and linear independent component analysis it is not possible to recover the origi- nal source signal from some unknown nonlinear transformations of the sources using only the independence assumption. Integrating the ob- jectives of statistical independence and temporal slowness removes this indeterminacy leading to a new method for nonlinear blind source sepa- ration. The principle of temporal slowness is adopted from slow feature analysis, an unsupervised method to extract slowly varying features from a given observed vectorial signal. The performance of the algorithm is demonstrated on nonlinearly mixed speech data."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/48c3ec5c3a93a9e294a8a6392ccedeb4-Abstract.html,Solitaire: Man Versus Machine,"Xiang Yan, Persi Diaconis, Paat Rusmevichientong, Benjamin V. Roy","In this paper, we use the rollout method for policy improvement to an-           alyze a version of Klondike solitaire. This version, sometimes called           thoughtful solitaire, has all cards revealed to the player, but then follows           the usual Klondike rules. A strategy that we establish, using iterated roll-           outs, wins about twice as many games on average as an expert human           player does.
1     Introduction
Though proposed more than fifty years ago [1, 7], the effectiveness of the policy improve- ment algorithm remains a mystery. For discounted or average reward Markov decision problems with n states and two possible actions per state, the tightest known worst-case upper bound in terms of n on the number of iterations taken to find an optimal policy is O(2n/n) [9]. This is also the tightest known upper bound for deterministic Markov de- cision problems. It is surprising, however, that there are no known examples of Markov decision problems with two possible actions per state for which more than n + 2 iterations are required. A more intriguing fact is that even for problems with a large number of states  say, in the millions  an optimal policy is often delivered after only half a dozen or so iterations.
In problems where n is enormous  say, a googol  this may appear to be a moot point because each iteration requires (n) compute time. In particular, a policy is represented by a table with one action per state and each iteration improves the policy by updating each entry of this table. In such large problems, one might resort to a suboptimal heuris- tic policy, taking the form of an algorithm that accepts a state as input and generates an action as output. An interesting recent development in dynamic programming is the roll- out method. Pioneered by Tesauro and Galperin [13, 2], the rollout method leverages the policy improvement concept to amplify the performance of any given heuristic. Unlike the conventional policy improvement algorithm, which computes an optimal policy off-line so that it may later be used in decision-making, the rollout method performs its computations on-line at the time when a decision is to be made. When making a decision, rather than applying the heuristic policy directly, the rollout method computes an action that would result from an iteration of policy improvement applied to the heuristic policy. This does
not require (n) compute time since only one entry of the table is computed.
The way in which actions are generated by the rollout method may be considered an al- ternative heuristic that improves on the original. One might consider applying the rollout method to this new heuristic. Another heuristic would result, again with improved perfor- mance. Iterated a sufficient number of times, this process would lead to an optimal policy. However, iterating is usually not an option. Computational requirements grow exponen- tially in the number of iterations, and the first iteration, which improves on the original heuristic, is already computationally intensive. For this reason, prior applications of the rollout method have involved only one iteration [3, 4, 5, 6, 8, 11, 12, 13]. For example, in the interesting study of Backgammon by Tesauro and Galperin [13], moves were generated in five to ten seconds by the rollout method running on configurations of sixteen to thirty- two nodes in a network of IBM SP1 and SP2 parallel-RISC supercomputers with parallel speedup efficiencies of 90%. A second iteration of the rollout method would have been infeasible  requiring about six orders of magnitude more time per move.
In this paper, we apply the rollout method to a version of solitaire, modeled as a deter- ministic Markov decision problem with over 52! states. Determinism drastically reduces computational requirements, making it possible to consider iterated rollouts1. With five iterations, a game, implemented in Java, takes about one hour and forty-five minutes on average on a SUN Blade 2000 machine with two 900MHz CPUs, and the probability of winning exceeds that of a human expert by about a factor of two. Our study represents an important contribution both to the study of the rollout method and to the study of solitaire."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/4bbdcc0e821637155ac4217bdab70d2e-Abstract.html,A Machine Learning Approach to Conjoint Analysis,"Olivier Chapelle, Zaïd Harchaoui","Choice-based conjoint analysis builds models of consumer preferences            over products with answers gathered in questionnaires. Our main goal is            to bring tools from the machine learning community to solve this prob-            lem more efficiently. Thus, we propose two algorithms to quickly and            accurately estimate consumer preferences.
1     Introduction
Conjoint analysis (also called trade-off analysis) is one of the most popular marketing re- search technique used to determine which features a new product should have, by conjointly measuring consumers trade-offs between discretized1 attributes. In this paper, we will fo- cus on the choice-based conjoint analysis (CBC) framework [11] since it is both widely used and realistic: at each question in the survey, the consumer is asked to choose one product from several.
The preferences of a consumer are modeled via a utility function representing how much a consumer likes a given product. The utility u(x) of a product x is assumed to be the sum of the partial utilities (or partworths) for each attribute, i.e. linear: u(x) = w  x. However, instead of observing pairs (xl, yl), the training samples are of the form ({x1, . . . , xp}, y                                                                                        k    k     k ) indicating that among the p products {x1 , . . . , xp}, the yth was preferred. Without noise,                                                 k         k          k this is expressed mathematically by u(xyk )  u(xb ),                b = y                                                 k              k               k .
Let us settle down the general framework of a regular conjoint analysis survey. We have a population of n consumers available for the survey. The survey consists of a questionnaire of q questions for each consumer, each asking to choose one product from a basket of p. Each product profile is described through a attributes with l1, ..., la levels each, via a vector of length m =           a     l                                s=1 s, with 1 at positions of levels taken by each attribute and 0 elsewhere.
Marketing researchers are interested in estimating individual partworths in order to per- form for instance a segmentation of the population afterwards. But traditional conjoint estimation techniques are not reliable for this task since the number of parameters m to be estimated is usually larger than the number of answers q available for each consumer. They estimate instead the partworths on the whole population (aggregated partworths). Here we
 1e.g. if the discretized attribute is weight, the levels would be light/heavy.

aim to investigate this issue, for which machine learning can provide efficient tools. We also address adaptive questionnaire design with active learning heuristics.
2       Hierarchical Bayes Analysis
The main idea of HB2 is to estimate the individual utility functions under the constraint that their variance should not be too small. By doing so, the estimation problem is not ill-posed and the lack of information for a consumer can be completed by the other ones.
2.1      Probabilistic model
In this section, we follow [11] for the description of the HB model and its implementation. This method aims at estimating the individual linear utility functions ui(x) = wi  x, for 1  i  n. The probabilistic model is the following:
     1. The individual partworths wi are drawn from a Gaussian distribution with mean               (representing the aggregated partworths) and covariance  (encoding population's              heterogeneity),

     2. The covariance matrix  has an invert Wishart prior, and  has an (improper) flat              prior.

     3. Given a set of products (x1, . . . xp), the probability that the consumer i chooses              the product x is given by

                                                      exp(wi  x)                                        P (x|wi) =       p                      .              (1)                                                                 exp(w                                                          b=1         i  xb)

2.2      Model estimation
We describe now the standard way of estimating , w  (w1, . . . , wn) and  based on Gibbs sampling and then propose a much faster algorithm that approximates the maximum a posteriori (MAP) solution.
Gibbs sampling          As far as we know, all implementations of HB rely on a variant of the Gibbs sampling [11]. During one iteration, each of the three sets of variables (, w and ) is drawn in turn from its posterior distribution the two others being fixed. Sampling for  and  is straightforward, whereas sampling from P (w|, , Y )  P (Y |w). P (w|, ) is achieved with the Metropolis-Hastings algorithm.
When convergence is reached, the sampling goes on and finally outputs the empirical ex- pectation of , w and . Although the results of this sampling-based implementation of HB3 are impressive, practitioners complain about its computational burden.
Approximate MAP solution              So far HB implementations make predictions by evaluating (1) at the empirical mean of the samples, in contrast with the standard bayesian approach, which would average the rhs of (1) over the different samples, given samples w from the posterior. In order to alleviate the computational issues associated with Gibbs sampling, we suggest to consider the maximum of the posterior distribution (maximum a posteriori, MAP) rather than its mean.
   2Technical papers of Sawtooth software [11], the world leading company for conjoint analysis softwares, provide very useful and extensive references.        3that we will call HB-Sampled or HB-S in the rest of the paper.

To find , w and  which maximize P (, w, |Y ), let us use Bayes' rule,
            P (, w, |Y )             P (Y |, w, )  P (w|, )  P (|)  P ()

                                       P (Y |w)  P (w|, )  P ()                            (2)

Maximizing (2) with respect to  yields MAP = I+Cw , with C                                                                  n+d          w being the ""covariance"" matrix of the wi centered at : Cw =                   (wi - )(wi - ) . Putting back this value in (2), we get
           - log P (, w, MAP|Y ) = - log P (Y |w) + log |I + Cw()| + C,                       (3)

where C is an irrelevant constant. Using the model (1), the first term in the rhs of (3) is convex in w, but not the second term. For this reason, we propose to change log |I + Cw| by trace(Cw) =           ||wi - ||2 (this would be a valid approximation if trace(Cw)                1). With this new prior on w, the rhs of (3) becomes
                                          n

                      W (, w) =                - log P (Yi|wi) + ||wi - ||2.                   (4)                                              i=1

As in equation (3), this objective function is minimized with respect to  when  is equal to the empirical mean of the wi. We thus suggest the following iterative scheme to minimize the convex functional (4):
     1. For a given , minimize (4) with respect to each of the wi independently.

     2. For a given w, set  to the empirical mean4 of the w.

Thanks to the convexity, this optimization problem can be solved very efficiently. A New- ton approach in step 1, as well as in step 2 to speed-up the global convergence to a fixed point , has been implemented. Only couple of steps in both cases are necessary to reach convergence.
Remark         The approximation from equation (3) to (4) might be too crude. After all it boils down to setting  to the identity matrix. One might instead consider  as an hyperparam- eter and optimize it by maximizing the marginalized likelihood [14].
3       Conjoint Analysis with Support Vector Machines
Similarly to what has recently been proposed in [3], we are now investigating the use of Support Vector Machines (SVM) [1, 12] to solve the conjoint estimation problem.
3.1      Soft margin formulation of conjoint estimation
Let us recall the learning problem. At the k-th question, the consumer chooses the yth                                                                                                       k product from the basket {x1 , . . . , xp}: w  xyk  w  xb ,  b = y                                   k           k            k        k         k . Our goal is to estimate the individual partworths w, with the individual utility function now being u(x) = w  x. With a reordering of the products, we can actually suppose that yk = 1. Then the above inequalities can be rewritten as a set of p - 1 constraints:
                                   w  (x1 - xb )  0,      2  b  p.                                              k        k                                              (5)

Eq. (5) shows that the conjoint estimation problem can be cast as a classification problem in the product-profiles differences space. From this point of view, it seems quite natural to use state-of-the-art classifiers such as SVMs for this purpose.
   4which is consistent with the L2-loss measuring deviations of wi-s from .

More specifically, we propose to train a L2-soft margin classifier (see also [3] for a similar approach) with only positive examples and with a hyperplane passing through the origin (no bias), modelling the noise in the answers with slack variables kb:
                                        Minimize w2 + C                      q        p         2                                                                                  k=1      b=2       kb                                             subject to w  (x1 - xb )  1 -                                                                       k           k                  kb.

3.2        Estimation of individual utilities
It was proposed in [3] to train one SVM per consumer to get wi and to compute the individual partworths by regularizing with the aggregated partworths w = 1                                                          n      w                                                                                                                                n    i=1    i: w = wi+w .      i        2
Instead, to estimate the individual utility partworths wi, we suggest the following opti- mization problem (the set Qi contains the indices j such that the consumer i was asked to choose between products x1 , . . . , xp) :                                              k          k
                                                                                     ~                         Minimize w2 + C                         p         2 +           C                        p      2                                        i          qi    kQi    b=2       kb                   q                  b=2    kb                                                                                         j=i    j          k /                                                                                                            Qi                         subject to wi  (x1 - xb )  1 -                                                    k      k                kb,         k, b  2 .

Here the ratio C determines the trade-off between the individual scale and the aggregated                            ~                           C one.5 For C = 1, the population is modeled as if it were homogeneous, i.e. all partworths                    ~                    C wi are equal. For C                    1, the individual partworths are computed independently, without                                 ~                                 C taking into account aggregated partworths.
4         Related work
Ordinal regression                   Very recently [2] explores the so-called ordinal regression task for ranking, and derive two techniques for hyperparameters learning and model selection in a hierarchical bayesian framework, Laplace approximation and Expectation Propagation respectively. Ordinal regression is similar yet distinct from conjoint estimation since train- ing data are supposed to be rankings or ratings in contrast with conjoint estimation where training data are choice-based. See [4] for more extensive bibliography.
Large margin classifiers                       Casting the preference problem in a classification framework, leading to learning by convex optimization, was known for a long time in the psycho- metrics community. [5] pioneered the use of large margin classifiers for ranking tasks. [3] introduced the kernel methods machinery for conjoint analysis on the individual scale. Very recently [10] proposes an alternate method for dealing with heterogeneity in conjoint analysis, which boils down to a very similar optimization to our HB-MAP approximation objective function, but with large margin regularization and with minimum deviation from the aggregated partworths.
Collaborative filtering                      Collaborative filtering exploits similarity between ratings across a population. The goal is to predict a person's rating on new products given the person's past ratings on similar products and the ratings of other people on all the products. Again collaborative is designed for overlapping training samples for each consumer, and usually rating/ranking training data, whereas conjoint estimation usually deals with different ques- tionnaires for each consumer and choice-based training data.
      5C  ~                C In this way, directions for which the xj , j  Qi contain information are estimated accurately, whereas the others directions are estimated thanks to the answers of the other consumers.

5       Experiments
Artificial experiments           We tested our algorithms on the same benchmarking artificial ex- perimental setup used in [3, 16]. The simulated product profiles consist of 4 attributes, each of them being discretized through 4 levels. A random design was used for the question- naire. For each question, the consumer was asked to choose one product from a basket of 4. A population of 100 consumers was simulated, each of them having to answer 4 questions. Finally, the results presented below are averaged over 5 trials.
The 100 true consumer partworths were generated from a Gaussian distribution with mean (-, -/3, /3, ) (for each attribute) and with a diagonal covariance matrix 2I. Each answer is a choice from the basket of products, sampled from the discrete logit-type distri- bution (1). Hence when  (called the magnitude6) is large, the consumer will choose with high probability the product with the highest utility, whereas when  is small, the answers will be less reliable. The ratio 2/ controls the heterogeneity7 of the population.
Finally, as in [3], the performances are computed using the mean of the L2 distances be- tween the true and estimated individual partworths (also called RMSE). Beforehand the partworths are translated such that the mean on each attribute is 0 and normalized to 1.
Real experiments           We tested our algorithms on disguised industrial datasets kindly pro- vided by Sawtooth Software Inc., the world leading company in conjoint analysis soft- wares.
11 one-choice-based8 conjoint surveys datasets9 were used for real experiments below. The number of attributes ranged from 3 to 6 (hence total number of levels from 13 to 28), the size of the baskets, to pick one product from at each question, ranged from 2 to 5, and the number of questions ranged from 6 to 15. The numbers of respondents ranged roughly from 50 to 1200. Since here we did not address the issue of no choice options in question answering, we removed10 questions where customers refused to choose a product from the basket and chose the no-choice-option as an answer11.
Finally, as in [16], the performances are computed using the hit rate, i.e. the misprediction rate of the preferred product.
5.1      Analysis of HB-MAP
We compare in this section our implementation of the HB method described in Section 2, that we call HB-MAP, to HB-S, the standard HB implementation.
The average training time for HB-S was 19 minutes (with 12000 iterations as suggested in [11]), whereas our implementation based on the approximation of the MAP solution took in average only 1.8 seconds. So our primary goal, i.e. to alleviate the sampling phase complexity, was achieved since we got a speed-up factor of the order of 1000.
The accuracy does not seem to be significantly weakened by this new implementation. Indeed, as shown in both Table 1 and Table 2, the performances achieved by HB-MAP were surprisingly often as good as HB-S's, and sometimes even a bit better. This might be
   6as in [3], we tested High Magnitude ( = 3) and Low Magnitude ( = 0.5).        7It was either set to 2 = 3 or 2 = 0.5, respectively High and Low Heterogeneity cases.        8We limited ourselves to datasets in which respondents were asked to choose 1 product among a basket at each question.        9see [4] for more details on the numerical features of the datasets.      10One could use EM-based methods to deal with such missing training choice data.      11When this procedure boiled down to unreasonable number of questions for hold-out evaluation of our algorithms, we simply removed the corresponding individuals.

explained by the fact that assuming that the covariance matrix is quasi-diagonal is a reason- able approximation, and that the mode of the posterior distribution is actually roughly close to the mean, for the real datasets considered. Additionally it is likely that HB-S may have demanded much more iterations for convergence to systematically behave more accurately than HB-MAP as one would have normally expected.
5.2       Analysis of SVMs
We now turn to the SVM approach presented in section 3.2 that we call Im.SV12. We did not use a non-linear kernel in our experiments. Hence it was possible to minimize (3.2) directly in the primal, instead of using the dual formulation as done usually. This turned out to be faster since the number of constraints was, for our problem, larger than the number of variables. The resulting mean training time was 4.7 seconds. The so-called chapspan, span estimate of leave-one-out prediction error [17], was used to select a suitable value of C13, since it gave a quasi-convex estimation on the regularization path.
The performances of Im.SV in Table 2, compared to the HB methods and logistic regression [3] are very satisfactory in case of artificial experiments. In real experiments, Im.SV gives overall quite satisfactory results, but sometimes disappointing ones in Table 2. One reason might be that hyperparameters (C, ~                                                C) were optimized once for the whole population. This may also be due to the lack of robustness14 of Im.SV to heterogeneity in the number of training samples for each consumer.
        Table 1: Average RMSE between estimated and true individual partworths

                      Mag        Het     HB-S     HB-MAP            Logistic    Im.SV                                L      L       0.90       0.83            0.84       0.86                                L     H        0.95       0.91            1.16       0.90                                H      L       0.44       0.40            0.43       0.41                                H     H        0.72       0.68            0.82       0.67




                            Table 2: Hit rate performances on real datasets.

                   Im.SV        HB-MAP      HB-S                      Im.SV      HB-MAP     HB-S           Dat12        0.16          0.16        0.17         Dat15         0.52       0.45      0.48           Dat22        0.15          0.13        0.15         Dat25         0.58       0.47      0.51                        Im.SV        HB-MAP      HB-S                      Im.SV      HB-MAP     HB-S           Dat13        0.37          0.24        0.25         Dat1           Dat2                                                     4        0.33       0.36      0.35                   3    0.34          0.33        0.33         Dat2           Dat3                                                     4        0.33       0.36      0.28                   3    0.35          0.28        0.24         Dat3           Dat4                                                     4        0.45       0.40      0.25                   3    0.35          0.31        0.28

Legend of Tables 1 and 2                     The first two columns indicate the Magnitude and the Heterogeneity (High or Low). p in Datmp is the number of products respondents are asked to choose one from at each question.
12since individual choice data are Immersed in the rest of the population choice data, via the optimization objective   13We observed that the value of the constant ~                                                          C was irrelevant, and that only the ratio C/ ~                                                                                                      C mat- tered.   14Indeed the no-choice data cleaning step might have lead to a strong unbalance to which Im.SV is maybe much more sensitive than HB-MAP or HB-S.
6      Active learning
Motivation        Traditional experimental designs are built by minimizing the variance of an estimator (e.g. orthogonal designs [6]). However, they are sub-optimal because they do not take into account the previous answers of the consumer. Therefore adaptive conjoint analysis was proposed [11, 16] for adaptively designing questionnaires.
The adaptive design concept is often called active learning in machine learning, as the algorithm can actively select questions whose responses are likely to be informative. In the SVM context, a common and intuitive strategy is to select, as the next point to be labeled, the nearest one from the decision boundary (see for instance [15]).
Experiments         We implemented this heuristic for conjoint analysis by selecting for each question a set of products whose estimated utilities are as close as possible15. To compare the different designs, we used the same artificial simulations as in section 5, but with 16 questions per consumer in order to fairly compare to the orthogonal design.
           Table 3: Comparison of the RMSE achieved by different designs.

                     Mag      Het    Random       Orthogonal       Adaptive                            L       L        0.66          0.61           0.66                            L       H        0.62          0.56           0.56                            H       L        0.31          0.29           0.24                            H       H        0.49          0.45           0.34

Results in Table 3 show that active learning produced an adaptive design which seems efficient, especially in the case of high magnitude, i.e. when the answers are not noisy16."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/4be5a36cbaca8ab9d2066debfe4e65c1-Abstract.html,Intrinsically Motivated Reinforcement Learning,"Nuttapong Chentanez, Andrew G. Barto, Satinder P. Singh","Psychologists call behavior intrinsically motivated when it is engaged in             for its own sake rather than as a step toward solving a specific problem             of clear practical value. But what we learn during intrinsically motivated             behavior is essential for our development as competent autonomous en-             tities able to efficiently solve a wide range of practical problems as they             arise. In this paper we present initial results from a computational study             of intrinsically motivated reinforcement learning aimed at allowing arti-             ficial agents to construct and extend hierarchies of reusable skills that are             needed for competent autonomy.
1      Introduction
Psychologists distinguish between extrinsic motivation, which means being moved to do something because of some specific rewarding outcome, and intrinsic motivation, which refers to being moved to do something because it is inherently enjoyable. Intrinsic motiva- tion leads organisms to engage in exploration, play, and other behavior driven by curiosity in the absence of explicit reward. These activities favor the development of broad com- petence rather than being directed to more externally-directed goals (e.g., ref. [14]). In contrast, machine learning algorithms are typically applied to single problems and so do not cope flexibly with new problems as they arise over extended periods of time.
Although the acquisition of competence may not be driven by specific problems, this com- petence is routinely enlisted to solve many different specific problems over the agent's lifetime. The skills making up general competence act as the ""building blocks"" out of which an agent can form solutions to new problems as they arise. Instead of facing each new challenge by trying to create a solution out of low-level primitives, it can focus on combining and adjusting its higher-level skills. In animals, this greatly increases the effi- ciency of learning to solve new problems, and our main objective is to achieve a similar efficiency in our machine learning algorithms and architectures.
This paper presents an elaboration of the reinforcement learning (RL) framework [11] that encompasses the autonomous development of skill hierarchies through intrinsically mo- tivated reinforcement learning. We illustrate its ability to allow an agent to learn broad competence in a simple ""playroom"" environment. In a related paper [1], we provide more extensive background for this approach, whereas here the focus is more on algorithmic details.
Lack of space prevents us from providing a comprehensive background to the many ideas to which our approach is connected. Many researchers have argued for this kind of devel-
opmental approach in which an agent undergoes an extended developmental period dur- ing which collections of reusable skills are autonomously learned that will be useful for a wide range of later challenges (e.g., [4, 13]). The previous machine learning research most closely related is that of Schmidhuber (e.g., [8]) on confidence-based curiosity and the ideas of exploration and shaping bonuses [6, 10], although our definition of intrinsic re- ward differs from these. The most direct inspiration behind the experiment reported in this paper, comes from neuroscience. The neuromodulator dopamine has long been associated with reward learning [9]. Recent studies [2, 3] have focused on the idea that dopamine not only plays a critical role in the extrinsic motivational control of behaviors aimed at harvest- ing explicit rewards, but also in the intrinsic motivational control of behaviors associated with novelty and exploration. For instance, salient, novel sensory stimuli inspire the same sort of phasic activity of dopamine cells as unpredicted rewards. However, this activation extinguishes more or less quickly as the stimuli become familiar. This may underlie the fact that novelty itself has rewarding characteristics [7]. These connections are key components of our approach to intrinsically motivated RL.
2         Reinforcement Learning of Skills
According to the ""standard"" view of RL (e.g., [11]) the agent-environment interaction is envisioned as the interaction between a controller (the agent) and the controlled system (the environment), with a specialized reward signal coming from a ""critic"" in the environment that evaluates (usually with a scalar reward value) the agent's behavior (Fig. 1A). The agent learns to improve its skill in controlling the environment in the sense of learning how to increase the total amount of reward it receives over time from the critic.
                                                                     External Environment                         Environment                          Actions                              Sensations


                        Critic                                       Internal Environment


                                                                            Critic                          Rewards           Actions                           States                            Rewards

                                                        Decisions                               States




                                                                            Agent                             Agent

                                                                                             ""Organism""      A                                                 B

 Figure 1: Agent-Environment Interaction in RL. A: The usual view. B: An elaboration.

Sutton and Barto [11] point out that one should not identify this RL agent with an entire animal or robot. An an animal's reward signals are determined by processes within its brain that monitor not only external state but also the animal's internal state. The critic is in an animal's head. Fig. 1B makes this more explicit by ""factoring"" the environment of Fig. 1A into an external environment and an internal environment, the latter of which contains the critic which determines primary reward. This scheme still includes cases in which reward is essentially an external stimulus (e.g., a pat on the head or a word of praise). These are simply stimuli transduced by the internal environment so as to generate the appropriate level of primary reward.
The usual practice in applying RL algorithms is to formulate the problem one wants the agent to learn how to solve (e.g., win at backgammon) and define a reward function spe- cially tailored for this problem (e.g., reward = 1 on a win, reward = 0 on a loss). Sometimes considerable ingenuity is required to craft an appropriate reward function. The point of departure for our approach is to note that the internal environment contains, among other things, the organism's motivational system, which needs to be a sophisticated system that
should not have to be redesigned for different problems. Handcrafting a different special- purpose motivational system (as in the usual RL practice) should be largely unnecessary.
Skills--Autonomous mental development should result in a collection of reusable skills. But what do we mean by a skill? Our approach to skills builds on the theory of options [12]. Briefly, an option is something like a subroutine. It consists of 1) an option policy that directs the agent's behavior for a subset of the environment states, 2) an initiation set con- sisting of all the states in which the option can be initiated, and 3) a termination condition, which specifies the conditions under which the option terminates. It is important to note that an option is not a sequence of actions; it is a closed-loop control rule, meaning that it is responsive to on-going state changes. Furthermore, because options can invoke other options as actions, hierarchical skills and algorithms for learning them naturally emerge from the conception of skills as options. Theoretically, when options are added to the set of admissible agent actions, the usual Markov decision process (MDP) formulation of RL extends to semi-Markov decision processes (SMDPs), with the one-step actions now be- coming the ""primitive actions."" All of the theory and algorithms applicable to SMDPs can be appropriated for decision making and learning with options [12].
Two components of the the options framework are especially important for our approach: 1. Option Models: An option model is a probabilistic description of the effects of executing an option. As a function of an environment state where the option is initiated, it gives the probability with which the option will terminate at any other state, and it gives the total amount of reward expected over the option's execution. Option models can be learned from experience (usually only approximately) using standard methods. Option models allow stochastic planning methods to be extended to handle planning at higher levels of abstraction. 2. Intra-option Learning Methods: These methods allow the policies of many options to be updated simultaneously during an agent's interaction with the environment. If an option could have produced a primitive action in a given state, its policy can be updated on the basis of the observed consequences even though it was not directing the agent's behavior at the time.
In most of the work with options, the set of options must be provided by the system designer. While an option's policy can be improved through learning, each option has to be prede- fined by providing its initiation set, termination condition, and the reward function that evaluates its performance. Many researchers have recognized the desirability of automati- cally creating options, and several approaches have recently been proposed (e.g., [5]). For the most part, these methods extract options from the learning system's attempts to solve a particular problem, whereas our approach creates options outside of the context of solving any particular problem.
Developing Hierarchical Collections of Skills--Children accumulate skills while they engage in intrinsically motivated behavior, e.g., while at play. When they notice that some- thing they can do reliably results in an interesting consequence, they remember this in a form that will allow them to bring this consequence about if they wish to do so at a future time when they think it might contribute to a specific goal. Moreover, they improve the efficiency with which they bring about this interesting consequence with repetition, before they become bored and move on to something else. We claim that the concepts of an option and an option model are exactly appropriate to model this type of behavior. Indeed, one of our main contributions is a (preliminary) demonstration of this claim.
3    Intrinsically Motivated RL
Our main departure from the usual application of RL is that our agent maintains a knowl- edge base of skills that it learns using intrinsic rewards. In most other regards, our ex- tended RL framework is based on putting together learning and planning algorithms for
Loop forever       Current state st, current primitive action at, current option ot,       extrinsic reward ret, intrinsic reward rit
  Obtain next state st+1

  //-- Deal with special case if next state is salient       If st+1 is a salient event e           If option for e, oe, does not exist in O (skill-KB)                 Create option oe in skill-KB;                 Add st to Ioe // initialize initiation set                 Set oe (st+1) = 1 // set termination probability           //-- set intrinsic reward value           ri         =  [1 - P oe (s               t+1                                     t+1|st)] //  is a constant multiplier       else           ri         = 0               t+1


  //-- Update all option models       For each option o = oe in skill-KB (O)           If st+1  Io, then add st to Io // grow initiation set           If at is greedy action for o in state st                 //-- update option transition probability model                                                         P o(x|st)  [(1 - o(st+1)P o(x|st+1) + o(st+1)st+1x]                 //-- update option reward model                                               Ro(st)  [re + (1 - o(s                                             t                          t+1))Ro(st+1)]


  //-- Q-learning update of behavior action-value function                                QB(st, at)  [re + ri +  max                                   t              t               aAO QB (st+1, a)]


  //-- SMDP-planning update of behavior action-value function       For each option o in skill-KB                                         QB(st, o)  [Ro(st) +                                        P o(x|s                                                                 xS                t) maxaAO QB (x, a)]


  //-- Update option action-value functions       For each option o  O such that st  Io                                         Qo(st, at)  [re +  (o(s                                             t                   t+1)  terminal value for option o)                                                                 +(1 - o(st+1))  maxaAO Qo(st+1, a)]           For each option o  O such that st  Io and o = o                                                         Qo(st, o )  Ro (st) +                                     P o (x|s                                                                    xS                t)[o(x)  terminal val for option o                                                                            +((1 - o(x))  maxaAO Qo(x, a))]

  Choose at+1 using -greedy policy w.r.to QB // -- Choose next action       //-- Determine next extrinsic reward       Set ret+1 to the extrinsic reward for transition st, at  st+1

  Set st  st+1; at  at+1; re  re                                             ri                                                            t           t+1; rit        t+1

Figure 2: Learning Algorithm. Extrinsic reward is denoted re while intrinsic reward is denoted ri.                                    Equations of the form x  [y] are short for x  (1-)x+[y]. The behavior action value function QB is updated using a combination of Q-learning and SMDP planning. Throughout  is a discount factor and  is the step-size. The option action value functions Qo are updated using intra-option Q-learning. Note that the intrinsic reward is only used in updating QB and not any of the Qo.
options [12]. Behavior The agent behaves in its environment according to an -greedy policy with re- spect to an action-value function QB that is learned using a mix of Q-learning and SMDP planning as described in Fig. 2. Initially only the primitive actions are available to the agent. Over time, skills represented internally as options and their models also become available to the agent as action choices. Thus, QB maps states s and actions a (both primitive and options) to the expected long-term utility of taking that action a in state s. Salient Events In our current implementation we assume that the agent has intrinsic or hardwired notions of interesting or ""salient"" events in its environment. For example, in the playroom environment we present shortly, the agent finds changes in light and sound intensity to be salient. These are intended to be independent of any specific task and likely to be applicable to many environments. Reward In addition to the usual extrinsic rewards there are occasional intrinsic rewards generated by the agent's critic (see Fig. 1B). In this implementation, the agent's intrinsic reward is generated in a way suggested by the novelty response of dopamine neurons. The intrinsic reward for each salient event is proportional to the error in the prediction of the salient event according to the learned option model for that event (see Fig. 2 for detail). Skill-KB The agent maintains a knowledge base of skills that it has learned in its environ- ment. Initially this may be empty. The first time a salient event occurs, say light turned on, structures to learn an option that achieves that salient event (turn-light-on option) are created in the skill-KB. In addition, structures to learn an option model are also created. So for option o, Qo maps states s and actions a (again, both primitive and options) to the long-term utility of taking action a in state s. The option for a salient event terminates with probability one in any state that achieves that event and never terminates in any other state. The initiation set, Io, for an option o is incrementally expanded to includes states that lead to states in the current initiation set. Learning The details of the learning algorithm are presented in Fig. 2.
4    Playroom Domain: Empirical Results
We implemented intrinsically motivated RL (of Fig. 2) in a simple artificial ""playroom"" domain shown in Fig. 3A. In the playroom are a number of objects: a light switch, a ball, a bell, two movable blocks that are also buttons for turning music on and off, as well as a toy monkey that can make sounds. The agent has an eye, a hand, and a visual marker (seen as a cross hair in the figure). The agent's sensors tell it what objects (if any) are under the eye, hand and marker. At any time step, the agent has the following actions available to it: 1) move eye to hand, 2) move eye to marker, 3) move eye one step north, south, east or west, 4) move eye to random object, 5) move hand to eye, and 6) move marker to eye. In addition, if both the eye and and hand are on some object, then natural operations suggested by the object become available, e.g., if both the hand and the eye are on the light switch, then the action of flicking the light switch becomes available, and if both the hand and eye are on the ball, then the action of kicking the ball becomes available (which when pushed, moves in a straight line to the marker).
The objects in the playroom all have potentially interesting characteristics. The bell rings once and moves to a random adjacent square if the ball is kicked into it. The light switch controls the lighting in the room. The colors of any of the blocks in the room are only visible if the light is on, otherwise they appear similarly gray. The blue block if pressed turns music on, while the red block if pressed turns music off. Either block can be pushed and as a result moves to a random adjacent square. The toy monkey makes frightened sounds if simultaneously the room is dark and the music is on and the bell is rung. These objects were designed to have varying degrees of difficulty to engage. For example, to get the monkey to cry out requires the agent to do the following sequence of actions: 1) get its eye to the light switch, 2) move hand to eye, 3) push the light switch to turn the light on, 4) find the blue block with its eye, 5) move the hand to the eye, 6) press the blue block to turn
music on, 7) find the light switch with its eye, 8) move hand to eye, 9) press light switch to turn light off, 10) find the bell with its eye, 11) move the marker to the eye, 12) find the ball with its eye, 13) move its hand to the ball, and 14) kick the ball to make the bell ring. Notice that if the agent has already learned how to turn the light on and off, how to turn music on, and how to make the bell ring, then those learned skills would be of obvious use in simplifying this process of engaging the toy monkey.
A                      B                                                                                           C                                                                         Performance of Learned Options                                                                   Effect of Intrinsically Motivated Learning                                                                                                                                                                 10000                                                                      120
                                                                 100                                                                                         8000

                                                                 80      Sound On                                                                                                Extrinsic Reward Only                                                                                                                                                                   6000

                                                                 60      Light On                                                                                      Music On                                                                     4000                                                                      40                   Toy Monkey On                                                                              Intrinsic & Extrinsic Rewards 

                                                                 20                                                                                          2000

                                                                  0                               Average # of Actions to Salient Event       0    0.5     1        1.5    2      2.5                                                     0                                                                                                                    Number of steps between extrinsic rewards        0         100      200    300    400    500    600                                                                                                             7                                                                               Number of Actions x 10                                                                           Number of extrinsic rewards

Figure 3: A. Playroom domain. B. Speed of learning of various skills. C. The effect of intrinsically motivated learning when extrinsic reward is present. See text for details
For this simple example, changes in light and sound intensity are considered salient by the playroom agent. Because the initial action value function, QB, is uninformative, the agent starts by exploring its environment randomly. Each first encounter with a salient event initiates the learning of an option and an option model for that salient event. For example, the first time the agent happens to turn the light on, it initiates the data structures necessary for learning and storing the light-on option. As the agent moves around the environment, all the options (initiated so far) and their models are simultaneously updated using intra-option learning.
As shown in Fig. 2, the intrinsic reward is used to update QB. As a result, when the agent encounters an unpredicted salient event a few times, its updated action value function drives it to repeatedly attempt to achieve that salient event. There are two interesting side effects of this: 1) as the agent tries to repeatedly achieve the salient event, learning improves both its policy for doing so and its option-model that predicts the salient event, and 2) as its option policy and option model improve, the intrinsic reward diminishes and the agent gets ""bored"" with the associated salient event and moves on. Of course, the option policy and model become accurate in states the agent encounters frequently. Occasionally, the agent encounters the salient event in a state (set of sensor readings) that it has not encountered before, and it generates intrinsic reward again (it is ""surprised"").
A summary of results is presented in Fig. 4. Each panel of the figure is for a distinct salient event. The graph in each panel shows both the time steps at which the event occurs as well as the intrinsic reward associated by the agent to each occurrence. Each occurrence is denoted by a vertical bar whose height denotes the amount of associated intrinsic reward. Note that as one goes from top to bottom in this figure, the salient events become harder to achieve and, in fact, become more hierarchical. Indeed, the lowest one for turning on the monkey noise (Non) needs light on, music on, light off, sound on in sequence. A number of interesting results can be observed in this figure. First note that the salient events that are simpler to achieve occur earlier in time. For example, Lon (light turning on) and Loff (light turning off) are the simplest salient events, and the agent makes these happen quite early. The agent tries them a large number of times before getting bored and moving on to other salient events. The reward obtained for each of these events diminishes after repeated exposure to the event. Thus, automatically, the skill of achieving the simpler events are learned before those for the more complex events.
Figure 4: Results from the playroom domain. Each panel depicts the occurrences of salient events as well as the associated intrinsic rewards. See text for details.
Of course, the events keep happening despite their diminished capacity to reward because they are needed to achieve the more complex events. Consequently, the agent continues to turn the light on and off even after it has learned this skill because this is a step along the way toward turning on the music, as well as along the way toward turning on the monkey noise. Finally note that the more complex skills are learned relatively quickly once the required sub-skills are in place, as one can see by the few rewards the agent receives for them. The agent is able to bootstrap and build upon the options it has already learned for the simpler events. We confirmed the hierarchical nature of the learned options by inspecting the greedy policies for the more complex options like Non and Noff. The fact that all the options are successfully learned is also seen in Fig. 3B in which we show how long it takes to bring about the events at different points in the agent's experience (there is an upper cutoff of 120 steps). This figure also shows that the simpler skills are learned earlier than the more complex ones.
An agent having a collection of skills learned through intrinsic reward can learn a wide variety of extrinsically rewarded tasks more easily than an agent lacking these skills. To illustrate, we looked at a playroom task in which extrinsic reward was available only if the agent succeeded in making the monkey cry out. This requires the 14 steps described above. This is difficult for an agent to learn if only the extrinsic reward is available, but much easier if the agent can use intrinsic reward to learn a collection of skills, some of which are relevant to the overall task. Fig. 3C compares the performance of two agents in this task. Each starts out with no knowledge of task, but one employs the intrinsic reward mechanism we have discussed above. The extrinsic reward is always available, but only when the monkey cries out. The figure, which shows the average of 100 repetitions of the experiment, clearly shows the advantage of learning with intrinsic reward.
Discussion      One of the key aspects of the Playroom example was that intrinsic reward was generated only by unexpected salient events. But this is only one of the simplest
possibilities and has many limitations. It cannot account for what makes many forms of exploration and manipulation ""interesting."" In the future, we intend to implement compu- tational analogs of other forms of intrinsic motivation as suggested in the psychological, statistical, and neuroscience literatures.
Despite the ""toy"" nature of this domain, these results are among the most sophisticated we have seen involving intrinsically motivated learning. Moreover, they were achieved quite directly by combining a collection of existing RL algorithms for learning options and option-models with a simple notion of intrinsic reward. The idea of intrinsic motivation for artificial agents is certainly not new, but we hope to have shown that the elaboration of the formal RL framework in the direction we have pursued, together with the use of recently- developed hierarchical RL algorithms, provides a fruitful basis for developing competently autonomous agents.
Acknowledgement Satinder Singh and Nuttapong Chentanez were funded by NSF grant CCF 0432027 and by a grant from DARPA's IPTO program. Andrew Barto was funded by NSF grant CCF 0432143 and by a grant from DARPA's IPTO program."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/4de81d9105c85bca6e6e4666e6dd536a-Abstract.html,Joint Probabilistic Curve Clustering and Alignment,"Scott J. Gaffney, Padhraic Smyth","Clustering and prediction of sets of curves is an important problem in          many areas of science and engineering. It is often the case that curves          tend to be misaligned from each other in a continuous manner, either in          space (across the measurements) or in time. We develop a probabilistic          framework that allows for joint clustering and continuous alignment of          sets of curves in curve space (as opposed to a fixed-dimensional feature-          vector space). The proposed methodology integrates new probabilistic          alignment models with model-based curve clustering algorithms. The          probabilistic approach allows for the derivation of consistent EM learn-          ing algorithms for the joint clustering-alignment problem. Experimental          results are shown for alignment of human growth data, and joint cluster-          ing and alignment of gene expression time-course data."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/4ebccfb3e317c7789f04f7a558df4537-Abstract.html,Triangle Fixing Algorithms for the Metric Nearness Problem,"Suvrit Sra, Joel Tropp, Inderjit S. Dhillon","Various problems in machine learning, databases, and statistics involve          pairwise distances among a set of objects. It is often desirable for these          distances to satisfy the properties of a metric, especially the triangle in-          equality. Applications where metric data is useful include clustering,          classification, metric-based indexing, and approximation algorithms for          various graph problems. This paper presents the Metric Nearness Prob-          lem: Given a dissimilarity matrix, find the ""nearest"" matrix of distances          that satisfy the triangle inequalities. For p nearness measures, this pa-          per develops efficient triangle fixing algorithms that compute globally          optimal solutions by exploiting the inherent structure of the problem.          Empirically, the algorithms have time and storage costs that are linear          in the number of triangle constraints. The methods can also be easily          parallelized for additional speed.
1     Introduction
Imagine that a lazy graduate student has been asked to measure the pairwise distances among a group of objects in a metric space. He does not complete the experiment, and he must figure out the remaining numbers before his adviser returns from her conference. Obviously, all the distances need to be consistent, but the student does not know very much about the space in which the objects are embedded. One way to solve his problem is to find the ""nearest"" complete set of distances that satisfy the triangle inequalities. This procedure respects the measurements that have already been taken while forcing the missing numbers to behave like distances.
More charitably, suppose that the student has finished the experiment, but--measurements being what they are--the numbers do not satisfy the triangle inequality. The student knows that they must represent distances, so he would like to massage the data so that it corre- sponds with his a priori knowledge. Once again, the solution seems to require the ""nearest"" set of distances that satisfy the triangle inequalities.
Matrix nearness problems [6] offer a natural framework for developing this idea. If there are n points, we may collect the measurements into an n  n symmetric matrix whose (j, k) entry represents the dissimilarity between the j-th and k-th points. Then, we seek to approximate this matrix by another whose entries satisfy the triangle inequalities. That is,
mik  mij + mjk for every triple (i, j, k). Any such matrix will represent the distances among n points in some metric space. We calculate approximation error with a distortion measure that depends on how the corrected matrix should relate to the input matrix. For example, one might prefer to change a few entries significantly or to change all the entries a little.
We call the problem of approximating general dissimilarity data by metric data the Metric Nearness (MN) Problem. This simply stated problem has not previously been studied, al- though the literature does contain some related topics (see Section 1.1). This paper presents a formulation of the Metric Nearness Problem (Section 2), and it shows that every locally optimal solution is globally optimal. To solve the problem we present triangle-fixing al- gorithms that take advantage of its structure to produce globally optimal solutions. It can be computationally prohibitive, both in time and storage, to solve the MN problem without these efficiencies.
1.1    Related Work
The Metric Nearness (MN) problem is novel, but the literature contains some related work.
The most relevant research appears in a recent paper of Roth et al. [11]. They observe that machine learning applications often require metric data, and they propose a technique for metrizing dissimilarity data. Their method, constant-shift embedding, increases all the dissimilarities by an equal amount to produce a set of Euclidean distances (i.e., a set of numbers that can be realized as the pairwise distances among an ensemble of points in a Euclidean space). The size of the translation depends on the data, so the relative and ab- solute changes to the dissimilarity values can be large. Our approach to metrizing data is completely different. We seek a consistent set of distances that deviates as little as pos- sible from the original measurements. In our approach, the resulting set of distances can arise from an arbitrary metric space; we do not restrict our attention to obtaining Euclidean distances. In consequence, we expect metric nearness to provide superior denoising. More- over, our techniques can also learn distances that are missing entirely.
There is at least one other method for inferring a metric. An article of Xing et al. [12] proposes a technique for learning a Mahalanobis distance for data in Rs. That is, a metric dist(x, y) =     (x - y)T G(x - y), where G is an s  s positive semi-definite matrix. The user specifies that various pairs of points are similar or dissimilar. Then the matrix G is computed by minimizing the total squared distances between similar points while forcing the total distances between dissimilar points to exceed one. The article provides explicit algorithms for the cases where G is diagonal and where G is an arbitrary positive semi-definite matrix. In comparison, the metric nearness problem is not restricted to Ma- halanobis distances; it can learn a general discrete metric. It also allows us to use specific distance measurements and to indicate our confidence in those measurements (by means of a weight matrix), rather than forcing a binary choice of ""similar"" or ""dissimilar.""
The Metric Nearness Problem may appear similar to metric Multi-Dimensional Scaling (MDS) [8], but we emphasize that the two problems are distinct. The MDS problem en- deavors to find an ensemble of points in a prescribed metric space (usually a Euclidean space) such that the distances between these points are close to the set of input distances. In contrast, the MN problem does not seek to find an embedding. In fact MN does not impose any hypotheses on the underlying space other than requiring it to be a metric space.
The outline of rest of the paper is as follows. Section 2 formally describes the MN problem. In Section 3, we present algorithms that allow us to solve MN problems with p nearness measures. Some applications and experimental results follow in Section 4. Section 5 dis- cusses our results, some interesting connections, and possibilities for future research.
2    The Metric Nearness Problem
We begin with some basic definitions. We define a dissimilarity matrix to be a nonnegative, symmetric matrix with zero diagonal. Meanwhile, a distance matrix is defined to be a dissimilarity matrix whose entries satisfy the triangle inequalities. That is, M is a distance matrix if and only if it is a dissimilarity matrix and mik  mij + mjk for every triple of distinct indices (i, j, k). Distance matrices arise from measuring the distances among n points in a pseudo-metric space (i.e., two distinct points can lie at zero distance from each other). A distance matrix contains N = n (n - 1)/2 free parameters, so we denote the collection of all distance matrices by MN . The set MN is a closed, convex cone.
The metric nearness problem requests a distance matrix M that is closest to a given dis- similarity matrix D with respect to some measure of ""closeness."" In this work, we restrict our attention to closeness measures that arise from norms. Specifically, we seek a distance matrix M so that,
                        M       argmin W             X - D         ,                   (2.1)                                      XMN where        is a norm, W is a symmetric non-negative weight matrix, and ` ' denotes the elementwise (Hadamard) product of two matrices. The weight matrix reflects our confi- dence in the entries of D. When each dij represents a measurement with variance 2ij, we might set wij = 1/2ij. If an entry of D is missing, one can set the corresponding weight to zero.

Theorem 2.1. The function X  W              X - D             always attains its minimum on MN . Moreover, every local minimum is a global minimum. If, in addition, the norm is strictly convex and the weight matrix has no zeros or infinities off its diagonal, then there is a unique global minimum.
Proof. The main task is to show that the objective function has no directions of recession, so it must attain a finite minimum on MN . Details appear in [4].
It is possible to use any norm in the metric nearness problem. We further restrict our attention to the p norms. The associated Metric Nearness Problems are
                                                    1/p                                                    p                   min           wjk (xjk - djk)                 for 1  p < , and          (2.2)                  XMN    j=k

              min    max wjk (xjk - djk)                    for p = .                  (2.3)                  XMN j=k

Note that the p norms are strictly convex for 1 < p < , and therefore the solution to (2.2) is unique. There is a basic intuition for choosing p. The 1 norm gives the absolute sum of the (weighted) changes to the input matrix, while the  only reflects the maximum absolute change. The other p norms interpolate between these extremes. Therefore, a small value of p typically results in a solution that makes a few large changes to the original data, while a large value of p typically yields a solution with many small changes."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/50abc3e730e36b387ca8e02c26dc0a22-Abstract.html,Economic Properties of Social Networks,"Sham M. Kakade, Michael Kearns, Luis E. Ortiz, Robin Pemantle, Siddharth Suri","We examine the marriage of recent probabilistic generative models           for social networks with classical frameworks from mathematical eco-           nomics. We are particularly interested in how the statistical structure of           such networks influences global economic quantities such as price vari-           ation. Our findings are a mixture of formal analysis, simulation, and           experiments on an international trade data set from the United Nations."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/531db99cb00833bcd414459069dc7387-Abstract.html,Beat Tracking the Graphical Model Way,"Dustin Lang, Nando D. Freitas","We present a graphical model for beat tracking in recorded music. Using          a probabilistic graphical model allows us to incorporate local information          and global smoothness constraints in a principled manner. We evaluate          our model on a set of varied and difficult examples, and achieve impres-          sive results. By using a fast dual-tree algorithm for graphical model in-          ference, our system runs in less time than the duration of the music being          processed."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/53c6de78244e9f528eb3e1cda69699bb-Abstract.html,Large-Scale Prediction of Disulphide Bond Connectivity,"Jianlin Cheng, Alessandro Vullo, Pierre F. Baldi","The formation of disulphide bridges among cysteines is an important fea-          ture of protein structures. Here we develop new methods for the predic-          tion of disulphide bond connectivity. We first build a large curated data          set of proteins containing disulphide bridges and then use 2-Dimensional          Recursive Neural Networks to predict bonding probabilities between cys-          teine pairs. These probabilities in turn lead to a weighted graph matching          problem that can be addressed efficiently. We show how the method con-          sistently achieves better results than previous approaches on the same          validation data. In addition, the method can easily cope with chains with          arbitrary numbers of bonded cysteines. Therefore, it overcomes one of          the major limitations of previous approaches restricting predictions to          chains containing no more than 10 oxidized cysteines. The method can          be applied both to situations where the bonded state of each cysteine is          known or unknown, in which case bonded state can be predicted with          85% precision and 90% recall. The method also yields an estimate for          the total number of disulphide bridges in each chain."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/56584778d5a8ab88d6393cc4cd11e090-Abstract.html,An Application of Boosting to Graph Classification,"Taku Kudo, Eisaku Maeda, Yuji Matsumoto","This paper presents an application of Boosting for classifying labeled            graphs, general structures for modeling a number of real-world data, such            as chemical compounds, natural language texts, and bio sequences. The            proposal consists of i) decision stumps that use subgraph as features,            and ii) a Boosting algorithm in which subgraph-based decision stumps            are used as weak learners. We also discuss the relation between our al-            gorithm and SVMs with convolution kernels. Two experiments using            natural language data and chemical compounds show that our method            achieves comparable or even better performance than SVMs with convo-            lution kernels as well as improves the testing efficiency.
1     Introduction
Most machine learning (ML) algorithms assume that given instances are represented in numerical vectors. However, much real-world data is not represented as numerical vectors, but as more complicated structures, such as sequences, trees, or graphs. Examples include biological sequences (e.g., DNA and RNA), chemical compounds, natural language texts, and semi-structured data (e.g., XML and HTML documents).
Kernel methods, such as support vector machines (SVMs) [11], provide an elegant solution to handling such structured data. In this approach, instances are implicitly mapped into a high-dimensional space, where information about their similarities (inner-products) is only used for constructing a hyperplane for classification. Recently, a number of kernels have been proposed for such structured data, such as sequences [7], trees [2, 5], and graphs [6]. Most are based on the idea that a feature vector is implicitly composed of the counts of substructures (e.g., subsequences, subtrees, subpaths, or subgraphs).
Although kernel methods show remarkable performance, their implicit definitions of fea- ture space make it difficult to know what kind of features (substructures) are relevant or which features are used in classifications. To use ML algorithms for data mining or as knowledge discovery tools, they must output a list of relevant features (substructures). This information may be useful not only for a detailed analysis of individual data but for the hu- man decision-making process.
In this paper, we present a new machine learning algorithm for classifying labeled graphs that has the following characteristics: 1) It performs learning and classification using the
            Figure 1: Labeled connected graphs and subgraph relation

structural information of a given graph. 2) It uses a set of all subgraphs (bag-of-subgraphs) as a feature set without any constraints, which is essentially the same idea as a convolution kernel [4]. 3) Even though the size of the candidate feature set becomes quite large, it automatically selects a compact and relevant feature set based on Boosting.
2      Classifier for Graphs
We first assume that an instance is represented in a labeled graph. The focused problem can be formalized as a general problem called the graph classification problem. The graph classification problem is to induce a mapping f (x) : X  {1}, from given training examples T = { xi, yi }L                           i=1, where xi  X is a labeled graph and yi  {1} is a class label associated with the training data. We here focus on the problem of binary classifica- tion. The important characteristic is that input example xi is represented not as a numerical feature vector but as a labeled graph.
2.1    Preliminaries
In this paper we focus on undirected, labeled, and connected graphs, since we can easily extend our algorithm to directed or unlabeled graphs with minor modifications. Let us in- troduce a labeled connected graph (or simply a labeled graph), its definitions and notations.
Definition 1 Labeled Connected Graph A labeled graph is represented in a 4-tuple G = (V, E, L, l), where V is a set of vertices, E  V  V is a set of edges, L is a set of labels, and l : V  E  L is a mapping that assigns labels to the vertices and the edges. A labeled connected graph is a labeled graph such that there is a path between any pair of verticies.
Definition 2 Subgraph Let G = (V , E , L , l ) and G = (V, E, L, l) be labeled connected graphs. G matches G, or G is a subgraph of G (G  G) if the following conditions are satisfied: (1) V  V , (2) E  E, (3) L  L, and (4) l = l. If G is a subgraph of G, then G is a supergraph of G .
Figure 1 shows an example of a labeled graph and its subgraph and non-subgraph.
2.2    Decision Stumps
Decision stumps are simple classifiers in which the final decision is made by a single hy- pothesis or feature. Boostexter [10] uses word-based decision stumps for text classification. To classify graphs, we define the subgraph-based decision stumps as follows.
Definition 3 Decision Stumps for Graphs Let t and x be labeled graphs and y be a class label (y  {1}). A decision stump classifier for graphs is given by
                                            y    t  x                             h t,y (x) def                                           =    -y    otherwise.

The parameter for classification is a tuple t, y , hereafter referred to as a rule of decision stumps. The decision stumps are trained to find a rule ^                                                                                       t, ^                                                                                             y that minimizes the error rate for the given training data T = { xi, yi }L                                                              i=1:
                                L                                                                   L       ^                       1                                                              1       t, ^            y =      argmin                 I(yi = h t,y (xi)) =            argmin                             (1 - yih t,y (xi)),               (1)                    tF ,y{1} L                                         tF ,y{1} 2L                                     i=1                                                                i=1

where F is a set of candidate graphs or a feature set (i.e., F =                                              L      {t|t  x                                                                                                               i=1                i}) and I () is the indicator function. The gain function for a rule t, y is defined as
                                                          L

                                      gain( t, y ) def                                                        =             yih t,y (xi).                                                              (2)                                                               i=1

Using the gain, the search problem (1) becomes equivalent to the problem:                                                             ^                                                                                                                                       t, ^                                                                                                                                            y    = argmaxtF,y{1} gain( t, y ). In this paper, we use gain instead of error rate for clarity.
2.3        Applying Boosting
The decision stump classifiers are too inaccurate to be applied to real applications, since the final decision relies on the existence of a single graph. However, accuracies can be boosted by the Boosting algorithm [3, 10]. Boosting repeatedly calls a given weak learner and finally produces a hypothesis f , which is a linear combination of K hypotheses produced by the weak learners, i,e.: f (x) = sgn(                            K                       (x)). A weak learner is built                                                                     k=1         k h tk,yk
at each iteration k with different distributions or weights d(k) = (d(k), . . . , d(k)) on the                                                                                                                      i           L training data, where                L      d(k) = 1, d(k)  0. The weights are calculated to concentrate                                     i=1     i          i more on hard examples than easy examples. To use decision stumps as the weak learner of Boosting, we redefine the gain function (2) as:
                                                          L

                                     gain( t, y ) def                                                       =             yidih t,y (xi).                                                             (3)                                                              i=1

In this paper, we use the AdaBoost algorithm, the original and the best known algorithm among many variants of Boosting. However, it is trivial to fit our decision stumps to other boosting algorithms, such as Arc-GV [1] and Boosting with soft margins [8].
3          Efficient Computation
In this section, we introduce an efficient and practical algorithm to find the optimal rule  ^  t, ^       y from given training data. This problem is formally defined as follows.
Problem 1 Find Optimal Rule Let T = { x1, y1, d1 , . . . , xL, yL, dL } be training data where xi is a labeled graph, yi  {1} is a class label associated with xi and di (                                 L          d                                                                                        i=1             i = 1, di  0) is a normal- ized weight assigned to xi. Given T , find the optimal rule ^                                                                                               t, ^                                                                                                        y that maximizes the gain, i.e., ^            t, ^               y = argmax                                                                          {t|t  x                               tF ,y{1} diyih t,y , where F =                         L                                                                                         i=1                          i}.
The most naive and exhaustive method in which we first enumerate all subgraphs F and then calculate the gains for all subgraphs is usually impractical, since the number of sub- graphs is exponential to its size. We thus adopt an alternative strategy to avoid such ex- haustive enumerations. The method to find the optimal rule is modeled as a variant of branch-and-bound algorithm and will be summarized as the following strategies: 1) Define
                      Figure 2: Example of DFS Code Tree for a graph

a canonical search space in which a whole set of subgraphs can be enumerated. 2) Find the optimal rule by traversing this search space. 3) Prune the search space by proposing a criteria for the upper bound of the gain. We will describe these steps more precisely in the next subsections.
3.1    Efficient Enumeration of Graphs
Yan et al. proposed an efficient depth-first search algorithm to enumerate all subgraphs from a given graph [12]. The key idea of their algorithm is a DFS (depth first search) code, a lexicographic order to the sequence of edges. The search tree given by the DFS code is called a DFS Code Tree. Leaving the details to [12], the order of the DFS code is defined by the lexicographic order of labels as well as the topology of graphs. Figure 2 illustrates an example of a DFS Code Tree. Each node in this tree is represented in a 5-tuple [i, j, vi, eij, vj], where eij, vi and vj are the labels of i-j edge, i-th vertex, and j-th vertex respectively. By performing a pre-order search of the DFS Code Tree, we can obtain all the subgraphs of a graph in order of their DFS code. However, one cannot avoid isomorphic enumerations even giving pre-order traverse, since one graph can have several DFS codes in a DFS Code Tree. So, canonical DFS code (minimum DFS code) is defined as its first code in the pre-order search of the DFS Code Tree. Yan et al. show that two graphs G and G are isomorphic if and only if minimum DFS codes for the two graphs min(G) and min(G ) are the same. We can thus ignore non-minimum DFS codes in subgraph enumerations. In other words, in depth-first traverse, we can prune a node with DFS code c, if c is not minimum. The isomorphic graph represented in minimum code has already been enumerated in the depth-first traverse. For example, in Figure 2, if G1 is identical to G0, G0 has been discovered before the node for G1 is reached. This property allows us to avoid an explicit isomorphic test of the two graphs.
3.2    Upper bound of gain
DFS Code Tree defines a canonical search space in which one can enumerate all subgraphs from a given set of graphs. We consider an upper bound of the gain that allows pruning of subspace in this canonical search space. The following lemma gives a convenient method of computing a tight upper bound on gain( t , y ) for any supergraph t of t.
Lemma 1 Upper bound of the gain: (t) For any t  t and y  {1}, the gain of                t , y is bounded by (t) (i.e., gain( t y )  (t)), where (t) is given by
                                                   L                                     L                    def            (t)    =      max 2                di -           yi  di, 2             di +           yi  di .

                           {i|yi=+1,txi}          i=1               {i|yi=-1,txi}      i=1 Proof 1

                           L                              L

        gain( t , y ) =           diyih t ,y (xi) =              diyi  y  (2I(t  xi) - 1),                                i=1                            i=1

where I() is the indicator function. If we focus on the case y = +1, then
                                                                   L                                             L

        gain( t , +1 )     =    2                  yidi -                yi  di  2                     di -           yi  di

                                      {i|t xi}                   i=1                {i|yi=+1,t xi}             i=1

                                                              L

                               2                  di -             yi  di,

                                {i|yi=+1,txi}               i=1

since |{i|yi = +1, t  xi}|  |{i|yi = +1, t  xi}|                             for any t  t. Similarly,
                                                                                        L

                          gain( t , -1 )                    2               di +            yi  di.

                                                             {i|yi=-1,txi}          i=1

Thus, for any t  t and y  {1}, gain( t , y )  (t). 2
We can efficiently prune the DFS Code Tree using the upper bound of gain u(t). During pre-order traverse in a DFS Code Tree, we always maintain the temporally suboptimal gain  among all the gains calculated previously. If (t) <  , the gain of any supergraph t  t is no greater than  , and therefore we can safely prune the search space spanned from the subgraph t. If (t)   , then we cannot prune this space since a supergraph t  t might exist such that gain(t )   .
3.3      Efficient Computation in Boosting
At each Boosting iteration, the suboptimal value  is reset to 0. However, if we can calcu- late a tighter upper bound in advance, the search space can be pruned more effectively. For this purpose, a cache is used to maintain all rules found in the previous iterations. Subop- timal value  is calculated by selecting one rule from the cache that maximizes the gain of the current distribution. This idea is based on our observation that a rule in the cache tends to be reused as the number of Boosting iterations increases. Furthermore, we also maintain the search space built by a DFS Code Tree as long as memory allows. This cache reduces duplicated constructions of a DFS Code Tree at each Boosting iteration.
4       Connection to Convolution Kernel
Recent studies [1, 9, 8] have shown that both Boosting and SVMs [11] work according to similar strategies: constructing an optimal hypothesis that maximizes the smallest margin between positive and negative examples. The difference between the two algorithms is the metric of margin; the margin of Boosting is measured in l1-norm, while that of SVMs is measured in l2-norm. We describe how maximum margin properties are translated in the two algorithms.
AdaBoost and Arc-GV asymptotically solve the following linear program, [1, 9, 8],
                                                        J

                    max         ;      s.t. yi              wjhj(xi)  , ||w||1 = 1                                              (4)                     wIRJ ,IR+                        j=1

where J is the number of hypotheses. Note that in the case of decision stumps for graphs, J = |{1}  F | = 2|F |.
SVMs, on the other hand, solve the following quadratic optimization problem [11]: 1
                    max         ; s.t. yi  (w  (xi))  , ||w||2 = 1.                                                          (5)                     wIRJ ,IR+

   1For simplicity, we omit the bias term (b) and the extension of Soft Margin.

The function (x) maps the original input example x into a J -dimensional feature vector (i.e., (x)  IRJ ). The l2-norm margin gives the separating hyperplane expressed by dot- products in feature space. The feature space in SVMs is thus expressed implicitly by using a Marcer kernel function, which is a generalized dot-product between two objects, (i.e., K(x1, x2) = (x1)  (x2)).
The best known kernel for modeling structured data is a convolution kernel [4] (e.g., string kernel [7] and tree kernel [2, 5]), which argues that a feature vector is implicitly composed of the counts of substructures. 2 The implicit mapping defined by the convolution kernel is given as: (x) = (#(t1  x), . . . , #(t|F|  x)), where tj  F and #(u) is the cardinality of u. Noticing that a decision stump can be expressed as h t,y (x) = y  (2I(t  x) - 1), we see that the constraints or feature space of Boosting with substructure-based decision stumps are essentially the same as those of SVMs with the convolution kernel 3. The critical difference is the definition of margin: Boosting uses l1-norm, and SVMs use l2-norm. The difference between them can be explained by sparseness.
It is well known that the solution or separating hyperplane of SVMs is expressed in a linear combination of training examples using coefficients , (i.e., w =                    L                                                                                           i=1         i(xi)) [11]. Maximizing l2-norm margin gives a sparse solution in the example space, (i.e., most of i becomes 0). Examples having non-zero coefficients are called support vectors that form the final solution. Boosting, in contrast, performs the computation explicitly in feature space. The concept behind Boosting is that only a few hypotheses are needed to express the final solution. l1-norm margin realizes such a property [8]. Boosting thus finds a sparse solution in the feature space. The accuracies of these two methods depend on the given training data. However, we argue that Boosting has the following practical advantages. First, sparse hypotheses allow the construction of an efficient classification algorithm. The complexity of SVMs with tree kernel is O(l|n1||n2|), where n1 and n2 are trees, and l is the number of support vectors, which is too heavy to be applied to real applications. Boosting, in contrast, performs faster since the complexity depends only on a small number of decision stumps. Second, sparse hypotheses are useful in practice as they provide ""transparent"" models with which we can analyze how the model performs or what kind of features are useful. It is difficult to give such analysis with kernel methods since they define feature space implicitly.
5     Experiments and Discussion
To evaluate our algorithm, we employed two experiments using two real-world data.
(1) Cellphone review classification (REV) The goal of this task is to classify reviews for cellphones as positive or negative. 5,741 sen- tences were collected from an Web-BBS discussion about cellphones in which users were directed to submit positive reviews separately from negative reviews. Each sentence is rep- resented in a word-based dependency tree using a Japanese dependency parser CaboCha4.
(2) Toxicology prediction of chemical compounds (PTC) The task is to classify chemical compounds by carcinogenicity. We used the PTC data set5 consisting of 417 compounds with 4 types of test animals: male mouse (MM), female
 2Strictly speaking, graph kernel [6] is not a convolution kernel because it is not based on the count of subgraphs, but on random walks in a graph.      3The difference between decision stumps and the convolution kernels is that the former uses a binary feature denoting the existence (or absence) of each substructure, whereas the latter uses the cardinality of each substructure. However, it makes little difference since a given graph is often sparse and the cardinality of substructures will be approximated by their existence.      4http://chasen.naist.jp/~ taku/software/cabocha/      5http://www.predictive-toxicology.org/ptc/



              Table 1: Classification F-scores of the REV and PTC tasks

                                                 REV                 PTC                                                               MM       FM       MR     FR       Boosting     BOL-based Decision Stumps         76.6    47.0    52.9    42.7     26.9                    Subgraph-based Decision Stumps    79.0    48.9    52.5    55.1     48.5       SVMs         BOL Kernel                        77.2    40.9    39.9    43.9     21.8                    Tree/Graph Kernel                 79.4    42.3    34.1    53.2     25.9

mouse (FM), male rat (MR) and female rat (FR). Each compound is assigned one of the following labels: {EE,IS,E,CE,SE,P,NE,N}. We here assume that CE,SE, and P are ""posi- tive"" and that NE and NN are ""negative"", which is exactly the same setting as [6]. We thus have four binary classifiers (MM/FM/MR/FR) in this data set.
We compared the performance of our Boosting algorithm and support vector machines with tree kernel [2, 5] (for REV) and graph kernel [6] (for PTC) according to their F-score in 5-fold cross validation.
Table 1 summarizes the best results of REV and PCT task, varying the hyperparameters of Boosting and SVMs (e.g., maximum iteration of Boosting, soft margin parameter of SVMs, and termination probability of random walks in graph kernel [6]). We also show the results with bag-of-label (BOL) features as a baseline. In most tasks and categories, ML algorithms with structural features outperform the baseline systems (BOL). These re- sults support our first intuition that structural features are important for the classification of structured data, such as natural language texts and chemical compounds.
Comparing our Boosting algorithm with SVMs using tree kernel, no significant difference can be found the REV data set. However, in the PTC task, our method outperforms SVMs using graph kernel on the categories MM, FM, and FR at a statistically significant level. Furthermore, the number of active features (subgraphs) used in Boosting is much smaller than those of SVMs. With our methods, about 1800 and 50 features (subgraphs) are used in the REV and PTC tasks respectively, while the potential number of features is quite large. Even giving all subgraphs as feature candidates, Boosting selects a small and highly relevant subset of features.
Figure 3 show an example of extracted support features (subgraphs) in the REV and PTC task respectively. In the REV task, features reflecting the domain knowledge (cellphone reviews) are extracted: 1) ""want to use "" positive, 2) ""hard to use"" negative, 3) ""recharging time is short""  positive, 4) ""recharging time is long""  negative. These features are interesting because we cannot determine the correct label (positive/negative) only using such bag-of-label features as ""charging,"" ""short,"" or ""long."" In the PTC task, similar structures show different behavior. For instance, Trihalomethanes (TTHMs), well- known carcinogenic substances (e.g., chloroform, bromodichloromethane, and chlorodi- bromomethane), contain the common substructure H-C-Cl (Fig. 3(a)). However, TTHMs do not contain the similar but different structure H-C(C)-Cl (Fig. 3(b)). Such structural information is useful for analyzing how the system classifies the input data in a category and what kind of features are used in the classification. We cannot examine such analysis in kernel methods, since they define their feature space implicitly.
The reason why graph kernel shows poor performance on the PTC data set is that it cannot identify subtle difference between two graphs because it is based on a random walks in a graph. For example, kernel dot-product between the similar but different structures 3(c) and 3(d) becomes quite large, although they show different behavior. To classify chemical compounds by their functions, the system must be capable of capturing subtle differences among given graphs.
The testing speed of our Boosting algorithm is also much faster than SVMs with tree/graph
                     Figure 3: Support features and their weights

kernels. In the REV task, the speed of Boosting and SVMs are 0.135 sec./1,149 instances and 57.91 sec./1,149 instances respectively6. Our method is significantly faster than SVMs with tree/graph kernels without a discernible loss of accuracy."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/573eec40e4ef4f2089531dd5cbf629f8-Abstract.html,Multi-agent Cooperation in Diverse Population Games,"K. Wong, S. W. Lim, Z. Gao",We consider multi-agent systems whose agents compete for resources by striving to be in the minority group. The agents adapt to the environment by reinforcement learning of the preferences of the policies they hold. Diversity of preferences of policies is introduced by adding random bi- ases to the initial cumulative payoffs of their policies. We explain and provide evidence that agent cooperation becomes increasingly important when diversity increases. Analyses of these mechanisms yield excellent agreement with simulations over nine decades of data.
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/577fd60255d4bb0f466464849ffe6d8e-Abstract.html,Modeling Nonlinear Dependencies in Natural Images using Mixture of Laplacian Distribution,"Hyun J. Park, Te W. Lee",Capturing dependencies in images in an unsupervised manner is  important for many image processing applications. We propose a  new method for capturing nonlinear dependencies in images of  natural scenes. This method is an extension of the linear Independent  Component Analysis (ICA) method by building a hierarchical model  based on ICA and mixture of Laplacian distribution. The model  parameters are learned via an EM algorithm and it can accurately  capture variance correlation and other high order structures in a  simple manner. We visualize the learned variance structure and  demonstrate applications to image segmentation and denoising.
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/59eb5dd36914c29b299c84b7ddaf08ec-Abstract.html,Dependent Gaussian Processes,"Phillip Boyle, Marcus Frean","Gaussian processes are usually parameterised in terms of their covari-          ance functions. However, this makes it difficult to deal with multiple          outputs, because ensuring that the covariance matrix is positive definite          is problematic. An alternative formulation is to treat Gaussian processes          as white noise sources convolved with smoothing kernels, and to param-          eterise the kernel instead. Using this, we extend Gaussian processes to          handle multiple, coupled outputs."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/5a45828dead8c065099cb653a2185df1-Abstract.html,Synchronization of neural networks by mutual learning and its application to cryptography,"Einat Klein, Rachel Mislovaty, Ido Kanter, Andreas Ruttor, Wolfgang Kinzel","Two neural networks that are trained on their mutual output synchronize          to an identical time dependant weight vector. This novel phenomenon          can be used for creation of a secure cryptographic secret-key using a          public channel. Several models for this cryptographic system have been          suggested, and have been tested for their security under different sophis-          ticated attack strategies. The most promising models are networks that          involve chaos synchronization. The synchronization process of mutual          learning is described analytically using statistical physics methods."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/5a99158e0c52f9e7d290906c9d08268d-Abstract.html,Semigroup Kernels on Finite Sets,"Marco Cuturi, Jean-philippe Vert","Complex objects can often be conveniently represented by finite sets of          simpler components, such as images by sets of patches or texts by bags          of words. We study the class of positive definite (p.d.) kernels for two          such objects that can be expressed as a function of the merger of their          respective sets of components. We prove a general integral representa-          tion of such kernels and present two particular examples. One of them          leads to a kernel for sets of points living in a space endowed itself with a          positive definite kernel. We provide experimental results on a benchmark          experiment of handwritten digits image classification which illustrate the          validity of the approach."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/5ac8bb8a7d745102a978c5f8ccdb61b8-Abstract.html,A Hidden Markov Model for de Novo Peptide Sequencing,"Bernd Fischer, Volker Roth, Jonas Grossmann, Sacha Baginsky, Wilhelm Gruissem, Franz Roos, Peter Widmayer, Joachim M. Buhmann","De novo Sequencing of peptides is a challenging task in proteome re-          search. While there exist reliable DNA-sequencing methods, the high-          throughput de novo sequencing of proteins by mass spectrometry is still          an open problem. Current approaches suffer from a lack in precision          to detect mass peaks in the spectrograms. In this paper we present a          novel method for de novo peptide sequencing based on a hidden Markov          model. Experiments effectively demonstrate that this new method signif-          icantly outperforms standard approaches in matching quality."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/5acdc9ca5d99ae66afdfe1eea0e3b26b-Abstract.html,Instance-Based Relevance Feedback for Image Retrieval,"Giorgio Gia\-cin\-to, Fabio Roli","High  retrieval  precision  in  content-based  image  retrieval  can  be           attained  by  adopting  relevance  feedback  mechanisms.  These           mechanisms require that the user judges the quality of the results of           the  query  by  marking  all  the  retrieved  images  as  being  either           relevant or not. Then, the search engine exploits this information to           adapt  the  search  to  better  meet  user's  needs.  At  present,  the  vast           majority  of  proposed  relevance  feedback  mechanisms  are           formulated in terms of search model that has to be optimized. Such           an  optimization  involves  the  modification  of  some  search           parameters so that the nearest neighbor of the query vector contains           the  largest  number  of  relevant  images.  In  this  paper,  a  different           approach  to  relevance  feedback  is  proposed.  After  the  user           provides the first feedback, following retrievals are not based on k-          nn  search,  but  on  the  computation  of  a  relevance  score  for  each           image of the database. This score is computed as a function of two           distances, namely the distance from the nearest non-relevant image           and  the  distance  from  the  nearest  relevant  one.  Images  are  then           ranked  according  to  this  score  and  the  top  k  images  are  displayed.           Reported  results  on  three  image  data  sets  show  that  the  proposed           mechanism  outperforms  other  state-of-the-art  relevance  feedback           mechanisms."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/5b168fdba5ee5ea262cc2d4c0b457697-Abstract.html,Learning Preferences for Multiclass Problems,"Fabio Aiolli, Alessandro Sperduti","Many interesting multiclass problems can be cast in the general frame-          work of label ranking defined on a given set of classes. The evaluation          for such a ranking is generally given in terms of the number of violated          order constraints between classes. In this paper, we propose the Prefer-          ence Learning Model as a unifying framework to model and solve a large          class of multiclass problems in a large margin perspective. In addition,          an original kernel-based method is proposed and evaluated on a ranking          dataset with state-of-the-art results."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/5df07ecf4cea616e3eb384a9be3511bb-Abstract.html,Mass Meta-analysis in Talairach Space,Finn \. Nielsen,"We provide a method for mass meta-analysis in a neuroinformatics           database containing stereotaxic Talairach coordinates from neu-           roimaging experiments. Database labels are used to group the in-           dividual experiments, e.g., according to cognitive function, and the           consistent pattern of the experiments within the groups are de-           termined. The method voxelizes each group of experiments via           a kernel density estimation, forming probability density volumes.           The values in the probability density volumes are compared to           null-hypothesis distributions generated by resamplings from the           entire unlabeled set of experiments, and the distances to the null-           hypotheses are used to sort the voxels across groups of experi-           ments. This allows for mass meta-analysis, with the construction           of a list with the most prominent associations between brain ar-           eas and group labels. Furthermore, the method can be used for           functional labeling of voxels.
1     Introduction
Neuroimaging experimenters usually report their results in the form of 3- dimensional coordinates in the standardized stereotaxic Talairach system [1]. Auto- mated meta-analytic and information retrieval methods are enabled when such data are represented in databases such as the BrainMap DBJ ([2], www.brainmapdbj.org) or the Brede database [3]. Example methods include outlier detection [4] and iden- tification of similar volumes [5].
Apart from the stereotaxic coordinates, the databases record details of the exper- imental situation, e.g., the behavioral domain and the scanning modality. In the Brede database the main annotation is the so-called ""external components""1 which are heuristically organized in a simple ontology: A directed graph (more specifically, a causal network) with the most general components as the roots of the graph, e.g.,
 1External components might be thought of as ""cognitive components"" or simply ""brain functions"", but they are more general, e.g., they also incorporate neuroreceptors. The components are called ""external"" since they are external variables to the brain image.



                                                                      WOEXT: 41                                                                            Cold pain


          WOEXT: 40                  WOEXT: 261                   Pain                   Thermal pain


                                                                      WOEXT: 69                                                                             Hot pain

Figure 1: The external components around ""thermal pain"" with ""pain"" as the parent of ""thermal pain"" and ""cold pain"" and ""hot pain"" as children.
""hot pain"" is a child of ""thermal pain"" that in turn is a child of ""pain"" (see Figure 1). The simple ontology is setup from information typically found in the introduction
section of scientific articles, and it is compared with the Medical Subject Headings ontology of the National Library of Medicine. The ontology is stored in a simple XML file.
The Brede database is organized, like the BrainMap DBJ, on different levels with scientific papers on the highest level. Each scientific paper contains one or more ""experiments"", which each in turn contains one or more locations. The individual experiments are typically labeled with an external component. The experiments that are labeled with the same external component form a group, and the distribu- tion of locations within the group become relevant: If a specific external component is localized to a specific brain region, then the locations associated with the external component should cluster in Talairach space.
We will describe a meta-analytic method that identifies important associations be- tween external components and clustered Talairach coordinates. We have previously modeled the relation between Talairach coordinates and neuroanatomical terms [4, 6] and the method that we propose here can be seen as an extension describing the relationship between Talairach coordinates and, e.g., cognitive components."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/5e751896e527c862bf67251a474b3819-Abstract.html,Result Analysis of the NIPS 2003 Feature Selection Challenge,"Isabelle Guyon, Steve Gunn, Asa Ben-Hur, Gideon Dror","The NIPS 2003 workshops included a feature selection competi-           tion organized by the authors. We provided participants with five           datasets from different application domains and called for classifica-           tion results using a minimal number of features. The competition           took place over a period of 13 weeks and attracted 78 research           groups. Participants were asked to make on-line submissions on           the validation and test sets, with performance on the validation set           being presented immediately to the participant and performance           on the test set presented to the participants at the workshop. In           total 1863 entries were made on the validation sets during the           development period and 135 entries on all test sets for the final           competition. The winners used a combination of Bayesian neu-           ral networks with ARD priors and Dirichlet diffusion trees. Other           top entries used a variety of methods for feature selection, which           combined filters and/or wrapper or embedded methods using Ran-           dom Forests, kernel methods, or neural networks as a classification           engine. The results of the benchmark (including the predictions           made by the participants and the features they selected) and the           scoring software are publicly available. The benchmark is available           at www.nipsfsc.ecs.soton.ac.uk for post-challenge submissions           to stimulate further research.
1      Introduction
Recently, the quality of research in Machine Learning has been raised by the sus- tained data sharing efforts of the community. Data repositories include the well known UCI Machine Learning repository [13], and dozens of other sites [10]. Yet, this has not diminished the importance of organized competitions. In fact, the proliferation of datasets combined with the creativity of researchers in designing
experiments makes it hardly possible to compare one paper with another [12]. A number of large conferences have regularly organized competitions (e.g. KDD, CAMDA, ICDAR, TREC, ICPR, and CASP). The NIPS workshops offer an ideal forum for organizing such competitions. In 2003, we organized a competition on the theme of feature selection, the results of which were presented at a workshop on feature extraction, which attracted 98 participants. We are presently preparing a book combining tutorial chapters and papers from the proceedings of that work- shop [9]. In this paper, we present to the NIPS community a concise summary of our challenge design and the findings of the result analysis.
2     Benchmark design
We formatted five datasets (Table 1) from various application domains. All datasets are two-class classification problems. The data were split into three subsets: a training set, a validation set, and a test set. All three subsets were made available at the beginning of the benchmark, on September 8, 2003. The class labels for the validation set and the test set were withheld. The identity of the datasets and of the features (some of which were random features artificially generated) were kept secret. The participants could submit prediction results on the validation set and get their performance results and ranking on-line for a period of 12 weeks. By December 1st, 2003, which marked the end of the development period, the participants had to turn in their results on the test set. Immediately after that, the validation set labels were revealed. On December 8th, 2003, the participants could make submissions of test set predictions, after having trained on both the training and the validation set. Some details on the benchmark design are provided in this Section."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/60519c3dd22587d6de04d5f1e28bd41d-Abstract.html,Sub-Microwatt Analog VLSI Support Vector Machine for Pattern Classification and Sequence Estimation,"Shantanu Chakrabartty, Gert Cauwenberghs","An analog system-on-chip for kernel-based pattern classification and se-              quence estimation is presented. State transition probabilities conditioned              on input data are generated by an integrated support vector machine. Dot              product based kernels and support vector coefficients are implemented              in analog programmable floating gate translinear circuits, and probabil-              ities are propagated and normalized using sub-threshold current-mode              circuits. A 14-input, 24-state, and 720-support vector forward decod-              ing kernel machine is integrated on a 3mm3mm chip in 0.5m CMOS              technology. Experiments with the processor trained for speaker verifica-              tion and phoneme sequence estimation demonstrate real-time recognition              accuracy at par with floating-point software, at sub-microwatt power.
1     Introduction
The key to attaining autonomy in wireless sensory systems is to embed pattern recognition intelligence directly at the sensor interface. Severe power constraints in wireless integrated systems incur design optimization across device, circuit, architecture and system levels [1]. Although system-on-chip methodologies have been primarily digital, analog integrated sys- tems are emerging as promising alternatives with higher energy efficiency and integration density, exploiting the analog sensory interface and computational primitives inherent in device physics [2]. Analog VLSI has been chosen, for instance, to implement Viterbi [3] and HMM-based [4] sequence decoding in communications and speech processing. Forward-Decoding Kernel Machines (FDKM) [5] provide an adaptive framework for gen- eral maximum a posteriori (MAP) sequence decoding, that avoid the need for backward recursion over the data in Viterbi and HMM-based sequence decoding [6]. At the core of FDKM is a support vector machine (SVM) [7] for large-margin trainable pattern classifi- cation, performing noise-robust regression of transition probabilities in forward sequence estimation. The achievable limits of FDKM power-consumption are determined by the number of support vectors (i.e., regression templates), which in turn are determined by the complexity of the discrimination task and the signal-to-noise ratio of the sensor inter- face [8].
                                   MVM                                                                              MVM                         24


               2                1


                          SUPPORT VECTORS               KERNEL

                                                                                                              s               x                                                                                                                     s                                                                                              i1                                                    30x24                    30x24                                                                                   K(x,x


                                                                       s )

                              x                                                                      f (x)                                         14                                                                               24x24                                                                                                          i1

                               INPUT

                                                                                                  NORMALIZATION

                                                                                       P                                      P                                                                                                 i1                                i24                                                                                                                        24x24                                                                                                                                        24                                                                                                       FORWARD DECODING                 j[n-1]                                                                                                           24      i[n]

                                   Figure 1: FDKM system architecture.

In this paper we describe an implementation of FDKM in silicon, for use in adaptive se- quence detection and pattern recognition. The chip is fully configurable with parameters directly downloadable onto an array of floating-gate CMOS computational memory cells. By means of calibration and chip-in-loop training, the effect of mismatch and non-linearity in the analog implementation is significantly reduced. Section 2 reviews FDKM formulation and notations. Section 3 describes the schematic details of hardware implementation of FDKM. Section 4 presents results from experiments conducted with the fabricated chip and Section 5 concludes with future directions.
2    FDKM Sequence Decoding
FDKM recognition and sequence decoding are formulated in the framework of MAP (max- imum a posteriori) estimation, combining Markovian dynamics with kernel machines. The MAP forward decoder receives the sequence X[n] = {x[1], x[2], . . . , x[n]} and pro- duces an estimate of conditional probability measure of state variables q[n] over all classes i  1, .., S, i[n] = P (q[n] = i | X[n]). Unlike hidden Markov models, the states directly encode the symbols, and the observations x modulate transition probabilities be- tween states [6]. Estimates of the posterior probability i[n] are obtained from estimates of local transition probabilities using the forward-decoding procedure [6]
                                                        S                                                                      P                                                i[n] =                      ij [n] j [n - 1]                                                      (1)                                                          j=1

where Pij[n] = P (q[n] = i | q[n - 1] = j, x[n]) denotes the probability of making a transition from class j at time n - 1 to class i at time n, given the current observation vector x[n]. Forward decoding (1) expresses first order Markovian sequential dependence of state probabilities conditioned on the data. The transition probabilities Pij[n] in (1) attached to each outgoing state j are obtained by normalizing the SVM regression outputs fij(x):
                                          Pij[n] = [fij(x[n]) - zj[n]]+                                                                       (2)



                                                              Vdd


                                                               M4

                                                                      A                                          V                                                         V                                               g ref                                                 g                                V                       M1                             V                      M2                                     c                                                      c

                                                               M3                                                        C                                                            B                                          V                                                         V                     I                                               tunn                                                  tunn                 out

                   Iin                                                                                (a)

                                                                                             (x.x )2                                                                                    Vdd                  s                               x                                                              M7                       M9

                                                                                               M10

                                                         M8                                                                           Vbias                               M5               M6

                                                                           (b)                            sK(x, x )                                                                                                               ij              s

Figure 2: Schematic of the SVM stage. (a) Multiply accumulate cell and reference cell for the MVM blocks in Figure 1. (b) Combined input, kernel and MVM modules.
where [.]+ = max(., 0). The normalization mechanism is subtractive rather than divisive, with normalization offset factor zj[n] obtained using a reverse-waterfilling criterion with respect to a probability margin  [10],
                                                [fij(x[n]) - zj[n]]+ = .                                                       (3)                                                i

Besides improved robustness [8], the advantage of the subtractive normalization (3) is its amenability to current mode implementation as opposed to logistic normalization [11] which requires exponentiation of currents. The SVM outputs (margin variables) fij(x) are given by:                                                              N                                          f                         s K                                          ij (x) =                               (x, x                                                                          ij                     s) + bij                            (4)                                                              s
where K(, ) denotes a symmetric positive-definite kernel1 satisfying the Mercer condi- tion, such as a Gaussian radial basis function or a polynomial spline [7], and xs[m], m = 1, .., N denote the support vectors. The parameters s in (4) and the support vectors x                                                                                            ij                                      s[m] are determined by training on a labeled training set using a recursive FDKM procedure de- scribed in [5].
3     Hardware Implementation
A second order polynomial kernel K(x, y) = (x.y)2 was chosen for convenience of im- plementation. This inner-product based architecture directly maps onto an analog compu- tational array, where storage and computation share common circuit elements. The FDKM
 1K(x, y) = (x).(y). The map () need not be computed explicitly, as it only appears in inner-product form.



                          f [n]                                        Vdd     Vdd    Vdd    Vdd                                                                                        ij                             i[n]

                                                                 M6                            M9                  Aij                   P [n]                                              ij                                   M7       M8                                                                                                                               M4


                          M2       M3                                                                                                         M5                         M1                                                                              Vref                                                                              j[n-1]

                Figure 3: Schematic of the margin propagation block.

system architecture is shown in Figure 1. It consists of several SVM stages that generates state transition probabilities Pij[n] modulated by input data x[n], and a forward decoding block that performs maximum a posteriori (MAP) estimation of the state sequence i[n]."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/6403675579f6114559c90de0014cd3d6-Abstract.html,Maximum Margin Clustering,"Linli Xu, James Neufeld, Bryce Larson, Dale Schuurmans","We propose a new method for clustering based on finding maximum mar-          gin hyperplanes through data. By reformulating the problem in terms          of the implied equivalence relation matrix, we can pose the problem as          a convex integer program. Although this still yields a difficult com-          putational problem, the hard-clustering constraints can be relaxed to a          soft-clustering formulation which can be feasibly solved with a semidef-          inite program. Since our clustering technique only depends on the data          through the kernel matrix, we can easily achieve nonlinear clusterings in          the same manner as spectral clustering. Experimental results show that          our maximum margin clustering technique often obtains more accurate          results than conventional clustering methods. The real benefit of our ap-          proach, however, is that it leads naturally to a semi-supervised training          method for support vector machines. By maximizing the margin simul-          taneously on labeled and unlabeled training data, we achieve state of the          art performance by using a single, integrated learning principle."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/6412fef87392ae8c987b0ecc79da1902-Abstract.html,On the Adaptive Properties of Decision Trees,"Clayton Scott, Robert Nowak",Decision trees are surprisingly adaptive in three important respects: They          automatically (1) adapt to favorable conditions near the Bayes decision          boundary; (2) focus on data distributed on lower dimensional manifolds;          (3) reject irrelevant features. In this paper we examine a decision tree          based on dyadic splits that adapts to each of these conditions to achieve          minimax optimal rates of convergence. The proposed classifier is the          first known to achieve these optimal rates while being practical and im-          plementable.
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/64a08e5f1e6c39faeb90108c430eb120-Abstract.html,Kernel Methods for Implicit Surface Modeling,"Joachim Giesen, Simon Spalinger, Bernhard Schölkopf","We describe methods for computing an implicit model of a hypersurface            that is given only by a finite sampling. The methods work by mapping            the sample points into a reproducing kernel Hilbert space and then deter-            mining regions in terms of hyperplanes.
1     Introduction
Suppose we are given a finite sampling (in machine learning terms, training data) x1, . . . , xm  X , where the domain X is some hypersurface in Euclidean space Rd. The case d = 3 is especially interesting since these days there are many devices, e.g., laser range scanners, that allow the acquisition of point data from the boundary surfaces of solids. For further processing it is often necessary to transform this data into a continu- ous model. Today the most popular approach is to add connectivity information to the data by transforming them into a triangle mesh (see [4] for an example of such a transformation algorithm). But recently also implicit models, where the surface is modeled as the zero set of some sufficiently smooth function, gained some popularity [1]. They bear resemblance to level set methods used in computer vision [6]. One advantage of implicit models is that they easily allow the derivation of higher order differential quantities such as curvatures. Another advantage is that an inside-outside test, i.e., testing whether a query point lies on the bounded or unbounded side of the surface, boils down to determining the sign of a function-evaluation at the query point. Inside-outside tests are important when one wants to intersect two solids.
The goal of this paper is, loosely speaking, to find a function which takes the value zero on a surface which
  (1) contains the training data and

  (2) is a ""reasonable"" implicit model of X .

To capture properties of its shape even in the above general case, we need to exploit some structure on X . In line with a sizeable amount of recent work on kernel methods [11], we assume that this structure is given by a (positive definite) kernel, i.e., a real valued function
 Partially supported by the Swiss National Science Foundation under the project ""Non-linear manifold learning"".



                                                                              Figure 1: In the 2-D toy example depicted,                       o                                                           o                       the hyperplane w, (x) =  separates all            o                           o                                          but one of the points from the origin. The out-                  .         o                                             o                                     lier (x) is associated with a slack variable ,                                                                 o                                                                                   which is penalized in the objective function                 /||w||                                                           (4). The distance from the outlier to the hy-                                                      o      w                                                               o                                        /||w||                                    perplane is / w ; the distance between hy-                                   o                        x                            ( )                                                    perplane and origin is / w . The latter im-                                                                                   plies that a small w corresponds to a large                                                                                   margin of separation from the origin.

k on X  X which can be expressed as
                                                                 k(x, x ) = (x), (x )                                   (1)

for some map  into a Hilbert space H. The space H is the reproducing kernel Hilbert space (RKHS) associated with k, and  is called its feature map. A popular example, in the case where X is a normed space, is the Gaussian (where  > 0)
                                                                                  x - x 2                                                           k(x, x ) = exp -                              .                     (2)                                                                                           2 2

The advantage of using a positive definite kernel as a similarity measure is that it allows us to construct geometric algorithms in Hilbert spaces.
2     Single-Class SVMs
Single-class SVMs were introduced [8, 10] to estimate quantiles C  {x  X |f (x)  [, [} of an unknown distribution P on X using kernel expansions. Here,
                                                            f (x) =          ik(xi, x) - ,                              (3)                                                                             i

where x1, . . . , xm  X are unlabeled data generated i.i.d. according to P . The single-class SVM approximately computes the smallest set C  C containing a specified fraction of all training examples, where smallness is measured in terms of the norm in the RKHS H associated with k, and C is the family of sets corresponding to half-spaces in H. Depending on the kernel, this notion of smallness will coincide with the intuitive idea that the quantile estimate should not only contain a specified fraction of the training points, but it should also be sufficiently smooth so that the same is approximately true for previously unseen points sampled from P .
Let us briefly describe the main ideas of the approach. The training points are mapped into H using the feature map  associated with k, and then it is attempted to separate them from the origin with a large margin by solving the following quadratic program: for   (0, 1],1
                                                                        1             1                                                  minimize                        w 2 +                 i -                  (4)                                        wH,R                              2             m                                                                m ,R                             i

                                             subject to                 w, (xi)   - i, i  0.                        (5)

Since non-zero slack variables i are penalized in the objective function, we can expect that if w and  solve this problem, then the decision function, f (x) = sgn ( w, (x) - ) will
 1Here and below, bold face greek character denote vectors, e.g.,  = (1, . . . , m) , and indices i, j by default run over 1, . . . , m.

Figure 2: Models computed with a single class SVM using a Gaussian kernel (2). The three examples differ in the value chosen for  in the kernel - a large value (0.224 times the diameter of the hemisphere) in the left figure and a small value (0.062 times the diameter of the hemisphere) in the middle and right figure. In the right figure also non-zero slack variables (outliers) were allowed. Note that that the outliers in the right figure correspond to a sharp feature (non-smoothness) in the original surface.
equal 1 for most examples xi contained in the training set,2 while the regularization term  w will still be small. For an illustration, see Figure 1. The trade-off between these two goals is controlled by a parameter .
One can show that the solution takes the form
                          f (x) = sgn              ik(xi, x) -  ,                     (6)                                                  i

where the i are computed by solving the dual problem,
                                     1                       minimize                                                                         ij k(xi, xj )                        (7)                              Rm         2 ij                                                           1                        subject to     0  i                  and                                                                     m                       i = 1.    (8)                                                                         i

Note that according to (8), the training examples contribute with nonnegative weights i  0 to the solution (6). One can show that asymptotically, a fraction  of all training examples will have strictly positive weights, and the rest will be zero (the ""-property"").
In our application we are not primarily interested in a decision function itself but in the boundaries of the regions in input space defined by the decision function. That is, we are interested in f -1(0), where f is the kernel expansion (3) and the points x1, . . . , xm  X are sampled from some unknown hypersurface X  Rd. We want to consider f -1(0) as a model for X . In the following we focus on the case d = 3. If we assume that the xi are sampled without noise from X  which for example is a reasonable assumption for data obtained with a state of the art 3d laser scanning device  we should set the slack variables in (4) and (5) to zero. In the dual problem this results in removing the upper constraints on the i in (8). Note that sample points with non-zero slack variable cannot be contained in f -1(0). But also sample points whose image in feature space lies above the optimal hyperplane are not contained in f -1(0) (see Figure 1) -- we will address this in the next section. It turns out that it is useful in practice to allow non-zero slack variables, because they prevent f -1(0) from decomposing into many connected components (see Figure 2 for an illustration).
In our experience, one can ensure that the images of all sample points in feature space lie close to (or on) the optimal hyperplane can be achieved by choosing  in the Gaussian
2We use the convention that sgn (z) equals 1 for z  0 and -1 otherwise.
                                                                                                              Figure 3: Two parallel hy-                                                x                                                    ( * )o                                                         perplanes w, (x) =  +                       o                                   o                      /*                                                      () enclosing all but two                             .                                 ||w||                                        o                                                                          of the points.    The outlier                                                    o                                                                                                                   (x()) is associated with (+  * ||w||       )/              (+)/||w||                                 o                                               a slack variable (), which                  w                                                         o                                                    /||w||                                               o                                    x                                        ( )                                           o                            is penalized in the objective                                                                                                                   function (9).

kernel (2) such that the Gaussians in the kernel expansion (3) are highly localized. How- ever, highly localized Gaussians are not well suited for interpolation -- the implicit surface decomposes into several components. Allowing outliers mitigates the situation to a certain extent. Another way to deal with the problem is to further restrict the optimal region in feature space. In the following we will pursue the latter approach."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/65f2a94c8c2d56d5b43a1a3d9d811102-Abstract.html,Neural Network Computation by In Vitro Transcriptional Circuits,"Jongmin Kim, John Hopfield, Erik Winfree","The structural similarity of neural networks and genetic regulatory net-          works to digital circuits, and hence to each other, was noted from the          very beginning of their study [1, 2]. In this work, we propose a simple          biochemical system whose architecture mimics that of genetic regula-          tion and whose components allow for in vitro implementation of arbi-          trary circuits. We use only two enzymes in addition to DNA and RNA          molecules: RNA polymerase (RNAP) and ribonuclease (RNase). We          develop a rate equation for in vitro transcriptional networks, and de-          rive a correspondence with general neural network rate equations [3].          As proof-of-principle demonstrations, an associative memory task and a          feedforward network computation are shown by simulation. A difference          between the neural network and biochemical models is also highlighted:          global coupling of rate equations through enzyme saturation can lead          to global feedback regulation, thus allowing a simple network without          explicit mutual inhibition to perform the winner-take-all computation.          Thus, the full complexity of the cell is not necessary for biochemical          computation: a wide range of functional behaviors can be achieved with          a small set of biochemical components.
1     Introduction
Biological organisms possess an enormous repertoire of genetic responses to everchang- ing combinations of cellular and environmental signals. Characterizing and decoding the connectivity of the genetic regulatory networks that govern these responses is a major chal- lenge of the post-genome era [4]. Understanding the operation of biological networks is in- tricately intertwined with the ability to create sophisticated biochemical networks de novo. Recent work developing synthetic genetic regulatory networks has focused on engineered circuits in bacteria wherein protein signals are produced and degraded [5, 6]. Although remarkable, such network implementations in bacteria have many unknown and uncontrol- lable parameters.
We propose a biochemical model system  a simplified analog of genetic regulatory circuits  that provides well-defined connectivity and uses nucleic acid species as fuel and signals that control the network. Our goal is to establish an explicit model to guide the laboratory construction of synthetic biomolecular systems in which every component is known and
    RNAP                       transcript



                                                   RNase



   inhibitor

(A)                 activator    DNA switch                     (B)
Figure 1: (A) The components of an in vitro circuit. The switch template (blue) is shown with the activator (red) attached. The dotted box indicates the promoter sequence and the downstream direction. (B) The correspondence between a neural network and an in vitro biochemical network. Neuron activity corresponds to RNA transcript concentration, while synaptic connections correspond to DNA switches with specified input and output.
where quantitative predictions can be tested. Only two enzymes are used in addition to syn- thetic DNA templates: RNA polymerase, which recognizes a specific promoter sequence in double-stranded DNA and transcribes the downstream DNA to produce an RNA tran- script, and ribonuclease, which degrades RNA but not DNA. In this system, RNA transcript concentrations are taken as signals. Synthetic DNA templates may assume two different conformations with different transcription efficiency: ON or OFF. Upon interaction with a RNA transcript of the appropriate sequence, the DNA template switches between differ- ent conformations like a gene regulated by transcription factors. The connectivity  which RNA transcripts regulate which DNA templates  is dictated by WatsonCrick base-pairing rules and is easy to program. The network computation is powered by rNTP that drives the synthesis of RNA signals by RNAP, while RNase forces transient signals to decay. With a few assumptions, we find that this stripped-down analog of genetic regulatory networks is mathematically equivalent to recurrent neural networks, confirming that a wide range of programmable dynamical behaviors is attainable.
2      Construction of the transcriptional network
The DNA transcriptional switch. The elementary unit of our networks will be a DNA switch, which serves the role of a gene in a genetic regulatory circuit. The basic require- ments for a DNA switch are to have separate input and output domains, to transcribe poorly by itself [7], and to transcribe efficiently when an activator is bound to it. A possible mech- anism of activation is the complementation of an incomplete promoter region, allowing more favorable binding of RNAP to the DNA template. Figure 1A illustrates our proposed design for DNA transcriptional switches and circuits. We model a single DNA switch with the following binding reactions:
                                              A  +  I        AI

                OFF                           D  + A         DA            ON

                ON                           DA  +  I        D +  AI       OFF

where D (blue) is a DNA template with an incomplete promoter region, A (red) is an activator that complements the incomplete promoter region, and I (green) is an inhibitor complementary to A. Thus, I can bind free A. Furthermore, activator A contains a ""toe- hold"" region [8] that overhangs past the end of D, allowing inhibitor I to strip off A from the DA complex. D is considered OFF and DA is considered ON, based on their efficiency as templates for transcription. This set of binding reactions provides a means to choose the threshold of the sigmoidal activation function, as will be explained later.
RNAP and RNase drive changes in RNA transcript concentration; their activity is modeled using a first-order approximation for enzyme kinetics. For the moment, we assume that the input species (activator and inhibitor) are held at constant levels by external control.
               By RNA polymerase                                                                                  By RNase                            k                                                                                                   k                    DA p                             DA + R                                                                                   R d                           k                                                                                                                        D            p                            D + R where 0 <  < 1 due to lack of activation and  represents the complete degradation of RNA products by RNase. kd and kp are set by the concentration of enzymes.

In general, a set of chemical reactions obeying mass action have dynamics described by
                                               d[Xi] =                                k                    [X                              )                                                         dt                                                          j ]r                                                                                                                           j (p                                                                                                                                     i - ri                                                                                                         j

where k is the rate constant, ri is the stoichiometry of species Xi as a reactant (typically 0 or 1), and pi is the stoichiometry of Xi as a product in reaction . Analysis of our system is greatly simplified by the assumption that the binding reactions are fast and go to completion. We define Dtot as the sum of free and bound species:Dtot = [D] + [DA]. Similarly, Itot = [I]+[AI] and Atot = [A]+[DA]+[AI]. Then, [DA] depends on Dtot and , where  = Atot - Itot. Because I can scavenge A whether the latter is free or bound to D, A can activate D only when  > 0. The amount of [DA] is proportional to  when 0 <  < Dtot, as shown in Figure 2A. It is convenient to represent this nonlinearity using a piecewise-linear approximation of a sigmoidal function, specifically, (x) = |x+1|-|x-1| .                                                                                                                                                                                      2 Thus, we can represent [DA] using  and a rescaled : [DA] = 1 Dtot(1 + ( ^                                                                                                                                                                                   )), where                                                                                                                                                             2 ^  = 2          Dtot - 1 is called the signal activity. At steady-state, kd[R] = kp[DA] + kp[D]; thus,                                                               1 k                                                  [R] =                p Dtot((1                                                               2 k                                        - )(^) + 1 + ) .                                                                       d If we consider the activator concentration as an input and the steady-state transcript con- centration as an output, then the (presumed constant) inhibitor concentration, I tot, sets the threshold, and the function assumes a sigmoidal shape (Fig. 2D). Adjusting the amount of template, Dtot, sets the magnitude of the output signal and the width of the transition re- gion (Fig. 2C). We can adjust the width of the transition region independent of the threshold such that a step function would be achieved in the limit. Thus, we have a sigmoidal func- tion with an adjustable threshold, without reliance on cooperative binding of transcription factors as is common in biological systems [9].
Networks of transcriptional switches. The input domain of a DNA switch is upstream of the promoter region; the output domain is downstream of the promoter region. This separation of domains allows us to design DNA switches that have any desired connectivity.
         [ DA ]                                               (x)                    tot               D                                                            1                                          [ R ]                                              [ R ]



                                                            -1                   1              x                                      tot                                 D                                                                                  -1                                                                                                                                                      tot                                    tot      (A)                                         (B)                                                          (C)                               A                (D)                   A

Figure 2: (A) [DA] as a function of . (B) The sigmoid (x). (C,D) [R] as a function of Atot for three values of Dtot and Itot, respectively.
We assume that distinct signals in the network are represented as distinct RNA sequences that have negligible crosstalk (undesired binding of two molecules representing different signals). The set of legitimate binding reactions is as follows:
                                                                            j                                                                           A  +  Ij                                     j                                                                                                                 A  I j

             OFF                                                      D ij                  j               D ij             j                                                                                                                             A                                                            ON                                                                                                                                                                                   + A

             ON                                                       ij                                                                      D               j                                                                                 A + I j                         D ij +                j                                                                                                                                  A  I j                                                  OFF

where Dij is the DNA template that has the jth input domain and ith output domain, the activator Aj complements the incomplete promoter region of Dij, and the inhibitor Ij is complementary to Aj. Note that Ij can strip off Aj from the DijAj complex, thus imposing a sharp threshold as before. Again, we assume fast and complete binding reactions.
The set of enzyme reactions for the transcriptional network is as follows:
                By RNA polymerase                                                                                                           By RNase                                    k                                                                                                                  k                     D                   p                                                                                                                  d                           ij Aj                             k  Dij Aj + Ai                                               if sij = 1                                           Ij                                                                                                                                                              k                     D              p                                                                                                                            d                           ij  Dij + Ai                                                                                                         Aj                                    k                                                                                                                                                                                                                                                                                              k                     D                   p                                                                                                                                 d                           ij Aj                             k  Dij Aj + Ii                                               if sij = -1                                          AjIj                                                                                                                                                                             k                     D              p                                                                                                                                           d                           ij  Dij + Ii                                                                                                         DijAj  Dij where sij  {+1, -1} indicates whether switch ij will produce an activator or an inhibitor. This notation reflects that the production of Ii is equivalent to the consumption of Ai. The change of RNA concentrations over time is easy to express with i = Atot                                                                                                                                                                                     i     - Itot                                                                                                                                                                                                  i     :

                                    di =                                                                   s                                         dt              -kd  i + kp                                                ij ([Dij Aj ] + [Dij ]) .                                                             (1)                                                                                                       j

Network equivalence. We show next that the time evolution of this biochemical network model is equivalent to that of a general Hopfield neural network model [3]:
                                                         dx                                                                   i =                                          w                                                               dt            -xi +                                     ij (xj ) + i .                                                                      (2)                                                                                                            j

Equation 1 can be rewritten to use the same nonlinear activation function  defined earlier. Let ^        i = 2i                 Dtot - 1 be a rescaled difference between activator and inhibitor concentrations,                     i where Dtot                i is the load on Ai, i.e., the total concentration of all switches that bind to Ai: Dtot   i =                Dtot                  j         ji      and Dtot                                                   ij          = [DijAj] + [Dij]. Then, we can derive the following rate equation, where ^                              i plays the role of unit i's activity xi:
1 d ^          i                                             k                                 Dtot                                                   k                                             Dtot                =                                             p (1                                    ij         ( ^                                                                                                                                                      p (1 + )s                                ij  k                  -^i +                                           - )sij                                                     j )+                                                    ij            - 1 .       d dt                                         k                                      Dtot                                                   k                                             Dtot                                              j          d                                            i                                    j          d                                         i                                                                                                                                                                                                             (3) Given the set of constants describing an arbitrary transcriptional network, the constants for an equivalent neural network can be obtained immediately by comparing Equations 2 and 3. The time constant  is the inverse of the RNase degradation rate: fast turnover of RNA molecules leads to fast response of the network. The synaptic weight wij is proportional to the concentration of switch template ij, attenuated by the load on Ai. However, the thresh- old i is dependent on the weights, perhaps implying a lack of generality. To implement an arbitrary neural network, we must introduce two new types of switches to the transcrip- tional network. To achieve arbitrary thresholds, we introduce bias switches DiB which
have no input domain and thus produce outputs constitutively; this adds an adjustable con- stant to the right hand side of Equation 3. To balance the load on Ai, we add null switches D0i which bind to Ai but have no output domain; this allows us to ensure that all Dtot                                                                                                                                                           i are equal. Consequently, given any neural network with weights wij and thresholds i, we can specify concentrations Dtot                                          ij    such that the biochemical network has identical dynamics, for some  .
MichaelisMenten enzyme reactions. Next, we explore the validity of our assumption that enzyme kinetics are first-order reactions. A basic but more realistic model is the MichaelisMenten mechanism [10], in which the enzyme and substrate bind to form an enzyme-substrate complex. For example, if E is RNAP,
                                                k+                    k                                   E + D                                        cat                                            ij Aj          EDijAj  E + DijAj + Ii/Ai .                                                     k- An important ramification of MichaelisMenten reactions is that there is competition for the enzyme by the substrates, because the concentration of available enzymes is reduced as they bind to substrates, leading to saturation when the enzyme concentration is limit- ing. Using the steady-state assumption for MichaelisMenten reactions, we establish the following relations to the rate constants of first-order reactions:

                 Etot    k                                     Etot               k                                  Etot               k      k                            cat                                                      cat                            d                       d,cat           p =                                                                                              k                                                 (4)                  1 + L  K                                kp =                                                  d =                                                     M                                1 + L  K                                            1 + L               K                                                                                              M                                   d                d,M

where kcat and KM = (k- + kcat)/k+ are the catalytic constant (enzyme's speed) and Michaelis constant (enzyme's affinity to target) of RNAP for the ON state switch, kcat and KM are for the OFF state switch, and kd,cat and Kd,M are the constants of RNase. Etot                                                                                                                                                    [D and Etot                                                                                                                                                 ij Aj ] +            d         are the concentrations of RNAP and RNase, respectively. L =                                                           i,j      KM            [Dij ] is the load on RNAP and L                                                  [Aj ]+[Ij ]+[Aj Ij ]+[Dij Aj ] is the load on      i,j K                                                         d =                i,j              K                 M                                                                                           d,M RNase (i.e., the total concentration of binding targets divided by the Michaelis constants of the enzymes), both of which may be time varying. To make the first-order approximation valid, we must keep L and Ld constant. Introduction of a new type of switch with different Michaelis constants can make L constant by balancing the load on the enzyme. A scheme to keep Ld constant is not obvious, so we set reaction conditions such that Ld                                                                     1.
3         Example computations by transcriptional networks
Feed-forward networks.                          We first consider a feed-forward network to compute f (x, y, z) =                          xyz +                                    yz + x. From the Boolean circuit shown in Figure 3A, we can construct an equivalent neural network. We label units 1 through 6: units 1, 2, 3 correspond to inputs x, y, z whereas units 4, 5, 6 are computation units. Using the conversion rule discussed in the network equivalence section, we can calculate the parameters of the transcriptional network. Under the first-order approximation of Equation 3, the simulation result is exact (Fig. 3C). For comparison, we also explicitly simulated mass action dynamics for the full set of chemical equations with the MichaelisMenten enzyme reactions, using biologically plausible rate constants and with Etot and Etot                                                                      d           calculated from Equation 4 using estimated values of L and Ld. The full model performs the correct calculation of f for all eight 3-bit inputs, although the magnitude of signals is exaggerated due to an underestimate of RNase load (Fig. 3C).
Associative memories. Figure 4A shows three 4-by-4 patterns to be memorized in a con- tinuous neural network [3]. We chose orthogonal patterns because a 16 neuron network has limited capacity. Our training algorithm is gradient descent combined with the perceptron learning rule. After training, the parameters of the neural network are converted to the parameters of the transcriptional network as previously described. Starting from a random
                                                                                                                                              6

                                                              x           x                                                                                        1                                                                                  -1                                                               4                                                                                               1                                                                                         -2                                                        2                                                       f                     1                                                                   y                                                1                            i^ 0           y                                                                 -1                                           f                                                                                               1               2                                                                                                                                                 -2                                                                        1                -1                                                                                                                                                 -4   (A) z                                                    (B) z            1                                                 (C)                   0    200       400       600      800      1000                                                                                                                                                                    time(sec)

Figure 3: (A,B) A Boolean circuit and a neural network to compute f (x, y, z) =                                                                                                                                                                               xyz+                                                                                                                                                                                                yz+ x. (C) The activity of computation units (first-order approximation: solid lines; Michaelis- Menten reaction: dotted lines) for x=True=1, y=False=-1, z=True=1.                                                                                                                                         3
                                                                                                                                    2

                                                                                                                                    1

                                                                                                                                  i^ 0

                                                                                                                                  -1

                                                                                                                                  -2    (A)                                                                                                                  (B)           -30                200              400                600                                                                                                                                                                 time(sec)

Figure 4: (A) The three patterns to be memorized. (B) Time-course for the transcriptional network recovery of the third pattern. (odd columns: blue lines, even columns: red lines)
initial state, a typical response of the transcriptional network (with the first-order approx- imation of Equation 3) is shown in Figure 4B. Thus, our in vitro transcriptional networks can support complex sets of stable steady-states.
A winner-take-all network. Instead of trying to compensate for the saturation phenomena of MichaelisMenten reactions, we can make use of it for computation. As an example, consider the winner-take-all computation [11], which is commonly implemented as a neu- ral network with O(N 2) mutually inhibitory connections (Fig. 5A), but which can also be implemented as an electrical circuit with O(N ) interconnections by using a single global inhibitory feedback gate [12]. In a biochemical system, a limited global resource, such as RNAP, can act to regulate all the DNA switches and thus similarly produce global inhibi- tion. This effect is exploited by the simple transcriptional network shown in Figure 5B, in which the output from each DNA switch activates the same DNA switch itself, and mutual inhibition is achieved by competition for RNAP. Specifically, we have switch templates Dii with fixed thresholds set by Ii, and Dii produces Ai as its output RNA. With the instant binding assumption, we then derive the following equation:
           dAtot                  i                Etot         k                                  Etot                  k                               k                            =       d                d,cat Atot                                               cat [D                            cat [D                 dt              -1 + L                      i         +                                                iiAi] +                                  ii]             .                  (5)                                           d    Kd,M                              1 + L                  KM                             KM

The production rate of Ai depends on Atot                                                                        i               and on L, while the degradation rate of Ai depends on Atot                       i    and on Ld, as shown in Figure 6A. For a winner-take-all network, an ON state switch draws more RNAP than an OFF state switch (because of the smaller Michaelis constant for the ON state). Thus, if the other switches are turned OFF, the load on RNAP (L) becomes small, leading to faster production of the remaining ON switches. When the production rate curve and the degradation rate curve have three intersections, bistability is achieved such that the switches remain ON or OFF, depending on their current state.
Consider n equivalent switches starting with initial activator concentrations above the threshold, and with the highest concentration at least  above the rest (as a percentage). Analysis indicates that a less leaky system (small ) and sufficient differences in initial activator concentrations (large ) can guarantee the existence of a unique winner. Simula- tions of a 10-switch winner-take-all network confirm this analysis, although we do not see perfect behavior (Fig. 6B). Figure 6C shows a time-course of a unique winner situation. Switches get turned OFF one by one whenever the activator level approaches the threshold, until only one switch remains ON.
                                                                  -1

                                                                  -1                                                                 -1           -1                                                       0.5             0.5                     0.5                      0.5                0.5           0.5                                                                 -1           -1


                                       (A)             1           1                            1         (B)       1                  1             1

Figure 5: (A) A 3-unit WTA network with explicit mutual inhibition. (B) An equivalent biochemical network.
                                                                                          30                                   1                                                                                                                                                                                                                                                                                                                                                                                                                                    3                     tot                     i             d A                                                                               25                                   0.8                                         2.5

          dt                                       L : low                                20                                                                                2                                                                                                                                   0.6                        /   M]                                                                                               15                                                                                         1/                                                                                    i 1.5                                                                                                                                    0.4                                  [A                                                       L : high                                10                                                                                 1                                                                                                                                    0.2                                                                                                5                                                                               0.5

                                                                                                                               0                             tot     tot     tot       tot                                                5       10          15                                                  0      (A)                    i                             I       i +D                                     I       ii      A i                (B)                                    (%)                               (C)                              0              5000     10000    15000                                                                                                                                                                                                    time(sec)

Figure 6: For WTA networks: (A) Production rates (solid lines) for two different L's, compared to a linear degradation rate (dotted line). (B) Empirical probability of correct output as a function of  and . (C) Time-course with  = 0.33% and  = 0.04.
Similarly, we can consider a k-WTA network where k winners persist. If we set the pa- rameters appropriately such that k winners are stable but k + 1 winners are unstable, the simulation result recovered k winners most of the time. Even a single k-WTA gate can provide impressive computational power [13]."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/65fc9fb4897a89789352e211ca2d398f-Abstract.html,Similarity and Discrimination in Classical Conditioning: A Latent Variable Account,"Aaron C. Courville, Nathaniel D. Daw, David S. Touretzky","We propose a probabilistic, generative account of configural learning          phenomena in classical conditioning. Configural learning experiments          probe how animals discriminate and generalize between patterns of si-          multaneously presented stimuli (such as tones and lights) that are dif-          ferentially predictive of reinforcement. Previous models of these issues          have been successful more on a phenomenological than an explanatory          level: they reproduce experimental findings but, lacking formal founda-          tions, provide scant basis for understanding why animals behave as they          do. We present a theory that clarifies seemingly arbitrary aspects of pre-          vious models while also capturing a broader set of data. Key patterns          of data, e.g. concerning animals' readiness to distinguish patterns with          varying degrees of overlap, are shown to follow from statistical inference."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/6aed000af86a084f9cb0264161e29dd3-Abstract.html,Boosting on Manifolds: Adaptive Regularization of Base Classifiers,"Ligen Wang, Balázs Kégl","In this paper we propose to combine two powerful ideas, boosting and           manifold learning. On the one hand, we improve ADABOOST by incor-           porating knowledge on the structure of the data into base classifier design           and selection. On the other hand, we use ADABOOST's efficient learn-           ing mechanism to significantly improve supervised and semi-supervised           algorithms proposed in the context of manifold learning. Beside the spe-           cific manifold-based penalization, the resulting algorithm also accommo-           dates the boosting of a large family of regularized learning algorithms.
1     Introduction
ADABOOST [1] is one of the machine learning algorithms that have revolutionized pattern recognition technology in the last decade. The algorithm constructs a weighted linear com- bination of simple base classifiers in an iterative fashion. One of the remarkable properties of ADABOOST is that it is relatively immune to overfitting even after the training error has been driven to zero. However, it is now a common knowledge that ADABOOST can overfit if it is run long enough. The phenomenon is particularly pronounced on noisy data, so most of the effort to regularize ADABOOST has been devoted to make it tolerant to outliers by either ""softening"" the exponential cost function (e.g., [2]) or by explicitly detecting outliers and limiting their influence on the final classifier [3].
In this paper we propose a different approach based on complexity regularization. Rather than focusing on possibly noisy data points, we attempt to achieve regularization by fa- voring base classifiers that are smooth in a certain sense. The situation that motivated the algorithm is not when the data is noisy, rather when it has a certain structure that is ignored by ordinary ADABOOST. Consider, for example, the case when the data set is em- bedded in a high-dimensional space but concentrated around a low dimensional manifold (Figure 1(a)). ADABOOST will compare base classifiers based on solely their weighted errors so, implicitly, it will consider every base classifier having the same (usually low) complexity. On the other hand, intuitively, we may hope to achieve better generalization if we prefer base classifiers that ""cut through"" sparse regions to base classifiers that cut into ""natural"" clusters or cut the manifold several times. To formalize this intuition, we use the graph Laplacian regularizer proposed in connection to manifold learning [4] and spectral clustering [5] (Section 3). For binary base classifiers, this penalty is proportional to the number of edges of the neighborhood graph that the classifier cuts (Figure 1(b)).
                        (a)                                                (b)

Figure 1: (a) Given the data, the vertical stump has a lower ""effective"" complexity than the horizontal stump. (b) The graph Laplacian penalty is proportional to the number of separated neighbors.
To incorporate this adaptive penalization of base classifiers into ADABOOST, we will turn to the marginal ADABOOST algorithm [6] also known as arc-gv [7]. This algorithm can be interpreted as ADABOOST with an L1 weight decay on the base classifier coefficients with a weight decay coefficient . The algorithm has been used to maximize the hard margin on the data [7, 6] and also for regularization [3]. The coefficient  is adaptive in all these applications: in [7] and [6] it depends on the hard margin and the weighted error, respectively, whereas in [3] it is different for every training point and it quantifies the ""noisiness"" of the points. The idea of this paper is to make  dependent on the individual base classifiers, in particular, to set  to the regularization penalty of the base classifier. First, with this choice, the objective of base learning becomes standard regularized error minimization so the proposed algorithm accommodates the boosting of a large family of regularized learning algorithms. Second, the coefficients of the base classifiers are lowered proportionally with their complexity, which can be interpreted as an adaptive weight decay. The formulation can be also justified by theoretical arguments which are sketched after the formal description of the algorithm in Section 2.
Experimental results (Section 4) show that the regularized algorithm can improve general- ization. Even when the improvement is not significant, the difference between the training error and the test error decreases significantly and the final classifier is much sparser than ADABOOST's solution, both of which indicate reduced overfitting. Since the Laplacian penalty can be computed without knowing the labels, the algorithm can also be used for semi-supervised learning. Experiments in this context show that algorithm besignificantly the semi-supervised algorithm proposed in [4].
2     The REGBOOST algorithm
For the formal description, let the training data be Dn = (x1, y1), . . . , (xn, yn) where data points (xi, yi) are from the set Rd  {-1, 1}. The algorithm maintains a weight distri- bution w(t) = w(t)                      1 , . . . , w(t)                                    n     over the data points. The weights are initialized uniformly in line 1 (Figure 2), and are updated in each iteration in line 10. We suppose that we are given a base learner algorithm BASE Dn, w, P () that, in each iteration t, returns a base classifier h(t) coming from a subset of H =                   h : Rd  {-1, 1} . In ADABOOST, the goal of the base classifier is to minimize the weighted error
                                             n

                           = (t)(h) =              w(t)I {h(x                                                         i            i) = yi} , 12                                                 i=1

 1The indicator function I{A} is 1 if its argument A is true and 0 otherwise.      2We will omit the iteration index (t) and the argument (h) where it does not cause confusion.

REGBOOST Dn, BASE(, , ), P (), , T
1        w  (1/n, . . . , 1/n)

2        for t  1 to T

3             h(t)  BASE Dn, w(t), P ()

                             n

4             (t)                w(t)h(t)(x                                             i                 i)yi                             edge                             i=1


5             (t)  2P (h(t))                                           edge offset

                             1               1 + (t)            1 - (t)     6             (t)               ln                                                                 base coefficient                                  2               1 - (t)            1 + (t)

7             if (t)  0                                    base error  (1 - (t))/2

8                    return f (t-1)() =                           t-1 (j)h(j)()                                                                        j=1

9             for i  1 to n

                                                                   exp -(t)h(t)(xi)yi   10                     w(t+1)  w(t)                             i                         i        n       w(t) exp - (t)h(t)(x                                                                j=1       j                                     j )yj

11         return f (T )() =                  T         (t)h(t)()                                                  t=1
Figure 2: The pseudocode of the REGBOOST algorithm with binary base classifiers. Dn is the training data, BASE is the base learner, P is the penalty functional,  is the penalty coefficient, and T is the number of iterations.
which is equivalent to maximizing the edge  = 1 - 2 =                                            n      w(t)h(x                                                                                                   i=1     i             i)yi. The goal of REGBOOST's base learner is to minimize the penalized cost
                                                                               1      1                          R1(h) = (h) + P (h) =                                         - ( - ),                                    (1)                                                                                    2      2

where P : H  R is an arbitrary penalty functional or regularization operator, provided to REGBOOST and to the base learner,  is the penalty coefficient, and  = 2P (h) is the edge offset. Intuitively, the edge  quantifies by how much h is better than a random guess, while the edge offset  indicates by how much h(t) must be better than a random guess. This means that for complex base classifiers (with large penalties), we require a better base classification than for simple classifiers. The main advantage of R1 is that it has the form of conventional regularized error minimization, so it accommodates the boosting of all learning algorithms that minimize an error functional of this form (e.g., neural networks with weight decay). However, the minimization of R1 is suboptimal from boosting's point of view.3 If computationally possible, the base learner should minimize
                    1 -                 1+                       1-                 1 +  1+             1 -  1-           R2(h) = 2                                                           =                                                 .     (2)                         1 +                               1 -                           1 +                   1 -

3This statement along with the formulae for R1, R2, and (t) are explained formally after Theo- rem 1.
After computing the edge and the edge offset in lines 4 and 5, the algorithm sets the coef- ficient (t) of the base classifier h(t) to                                                   1                    1 + (t)                             1                1 + (t)                                 (t) =                 ln                                             -          ln                           .                      (3)                                                   2                    1 - (t)                             2                1 - (t)
In line 11, the algorithm returns the weighted average of the base classifiers f (T )() =   T      (t)h(t)() as the combined classifier, and uses the sign of f (T )(x) to classify x.   t=1 The algorithm must terminate if (t)  0 which is equivalent to (t)  (t) and to (t)  (1-(t))/2.4 In this case, the algorithm returns the actual combined classifier in line 8. This means that either the capacity of the set of base classifiers is too small ((t) is small), or the penalty is too high ((t) is high), so we cannot find a new base classifier that would improve the combined classifier. Note that the algorithm is formally equivalent to ADABOOST if (t)  0 and to marginal ADABOOST if (t)   is constant.
For the analysis of the algorithm, we first define the unnormalized margin achieved by f (T ) on (xi, yi) as                                                                        i = f (T )(xi)yi, and the (normalized) margin as                                                                                                T         (t)h(t)(x                                                                  i                             t=1                            i)yi                                              i =                            =                                                          ,                             (4)                                                                                                             T                                                                       1                                              (t)                                                                                                             t=1 where  1 =              T      (t) is the L                          t=1                                 1 norm of the coefficient vector. Let the average penalty or margin offset be defined as the average edge offset                                                                                            T                                                                                                     (t)(t)                                                               =                           t=1                         .                                             (5)                                                                                                 T      (t)                                                                                                 t=1 The following theorem upper bounds the marginal training error                                                                                                      n                                                                                             1                                              L()(f (T )) =                                                I                                                                                             n                           i <                                                                                                                                                                      (6)                                                                                                      i=1
achieved by the combined classifier f (T ) that REGBOOST outputs.
Theorem 1 Let (t) = 2P (h(t)), let                                                                                    and L()(f (T )) be as defined in (5) and (6), re- spectively. Let w(t) be the weight of training point (x                     i                                                                                                 i, yi) after the tth iteration (updated in line 10 in Figure 2), and let (t) be the weight of the base regressor h(t)() (computed in line 6 in Figure 2). Then                                  T                            n                                                                      T
      L()(f (T ))               e(t)(t)                            w(t)e-(t)h(t)(xi)yi =                                          E(t) (t), h(t) .        (7)                                                                                  i                                 t=1                          i=1                                                                    t=1

Proof. The proof is an extension of the proof of Theorem 5 in [8].                                                        n                              T                          T                                              1             L()(f (T ))        =                           I                                                                                                              (t) -                      (t)h(t)(x                                              n                                                                                               i)yi  0                (8)                                                    i=1                           t=1                             t=1                                                        n                                              1                                                                                 e PT (t)-PT (t)h(t)(                                                                              t=1                            t=1                     xi)yi                            (9)                                              n i=1                                                                                       T         n                                                  n                                                                                 =           e PT (t)                                                        t=1                                             w(t)e-(t)h(t)(xj)yj                             w(T +1).    (10)                                                                                                             j                                            i                                                                                  t=1 j=1                                                       i=1
4Strictly speaking, (t) = 0 could be allowed but in this case the (t) would remain 0 forever so it makes no sense to continue.
In (8) we used the definitions (6) and (4), the inequality (9) holds since ex  I{x  0}, and we obtained (10) by recursively applying line 10 in Figure 2. The theorem follows by the definition (5) and since      n         w(T +1) = 1.                                  i=1        i
First note that Theorem 1 explains the base objectives (1) and (2) and the base coefficient (3). The goal of REGBOOST is the greedy minimization of the exponential bound in (7), that is, in each iteration we attempt to minimize E(t) (, h). Given h(t), E(t) , h(t) is minimized by (3), and with this choice for (t), R2(h) = E(t) (t), h , so the base learner should attempt to minimize R2(h). If this is computationally impossible, we follow Mason et al.'s functional gradient descent approach [2], that is, we find h(t) by maximizing the
negative gradient - E(t)(,h) in  = 0. Since - E(t)(,h)                       =  - , this criterion is                                                                        =0 equivalent to the minimization of R1(h).5
Theorem 1 also suggests various interpretations of REGBOOST which indicate why it would indeed achieve regularization. First, by (9) it can be seen that REGBOOST directly minimizes                                  n                                       1           exp -                                       n                     i +                                                                   1 ,                                            i=1 which can be interpreted as an exponential cost on the unnormalized margin with an L1 weight decay. The weight decay coefficient                                                          is proportional to the average complexity of the base classifiers. Second, Theorem 1 also indicates that REGBOOST indirectly min- imizes the marginal error L()(f (T )) (6) where the margin parameter                                                                                         , again, is moving adaptively with the average complexity of the base classifiers. This explanation is sup- ported by theoretical results that bound the generalization error in terms of the marginal error (e.g., Theorem 2 in [8]). The third explanation is based on results that show that the difference between the marginal error and the generalization error can be upper bounded in terms of the complexity of the base classifier class H (e.g., Theorem 4 in [9]). By imposing a non-zero penalty on the base classifiers, we can reduce the pool of admissible functions to those of which the edge  is larger than the edge offset . Although the theoretical results do not apply directly, they support the empirical evidence (Section 4) that indicate that the reduction of the pool of admissible base classifiers and the sparsity of the combined classifier play an important role in decreasing the generalization error.
Finally note that the algorithm can be easily extended to real-valued base classifiers along the lines of [10] and to regression by using the algorithm proposed in [11]. If base clas- sifiers come from the set {h : Rd  R}, we can only use the base objective R1(h) (1), and the analytical solution (3) for the base coefficients (t) must be replaced by a simple numerical minimization (line search) of E(t) , h(t) .6 In the case of regression, the bi- nary cost function I {h(x) = y} should be replaced by an appropriate regression cost (e.g., quadratic), and the final regressor should be the weighted median of the base regressors instead of their weighted average.
3        The graph Laplacian regularizer
The algorithm can be used with any regularized base learner that optimizes a penalized cost of the form (1). In this paper we apply a smoothness functional based on the graph
 5Note that if  is constant (ADABOOST or marginal ADABOOST), the minimization of R1(h) and R2(h) leads to the same solution, namely, to the base classifier that minimizes the weighted error . This is no more the case if  depends on h.      6As a side remark, note that applying a non-zero (even constant) penalty  would provide an alternative solution to the singularity problem ((t) = ) in the abstaining base classifier model of [10].

Laplacian operator, proposed in a similar context by [4]. The advantage of this penalty is that it is relatively simple to compute for enumerable base classifiers (e.g., decision stumps or decision trees) and that it suits applications where the data exhibits a low dimensional manifold structure.
Formally, let G = (V, E) be the neighborhood graph of the training set where the vertex set V = {x1, . . . , xn} is identical to the set of observations, and the edge set E contains pairs of ""neighboring"" vertices (xi, xj) such that either xi - xj < r or xi (xj) is among the k nearest neighbors of xj (xi) where r or k is fixed. This graph plays a crucial role in several recently developed dimensionality reduction methods since it approximates the natural topology of the data if it is confined to a low-dimensional smooth manifold in the embedding space. To penalize base classifiers that cut through dense regions, we use the smoothness functional
                                                 n    n                                                 1                                    2                                PL(h) =                         h(x                        W                                           2|                          i) - h(xj )              ij ,                                            W| i=1 j=i+1

where W is the adjacency matrix of G, that is, Wij = I (xi, xj)  E , and 2|W| = 2     n      n         W       i=1    j=1            ij is a normalizing factor so that 0  PL(h)  1.7                         For binary base classifiers, PL(h) is proportional to the number of separated neighbors, that is, the number of connected pairs that are classified differently by h. Let the diagonal matrix D defined by Dii =          n      W                   j=1         ij , and let L = D - W be the graph Laplacian of G. Then it is easy to see that
                                                                       n

                          2|W|PL(h) = hLhT = h, Lh =                         i h, ei ,                                                                           j=1

where h = h(x1), . . . , h(xn) , and ei and i are the (normalized) eigenvectors and eigen- values of L, that is, Lei = iei, ei = 1. Since L is positive definite, all the eigenvalues are non-negative. The eigenvectors with the smallest eigenvalues can be considered as the ""smoothest"" functions on the neighborhood graph. Based on this observation, [4] proposed to learn a linear combination of a small number of the eigenvectors with the smallest eigen- values. One problem of this approach is that the out-of-sample extension of the obtained classifier is non-trivial since the base functions are only known at the data points that par- ticipated in forming the neighborhood graph, so it can only be used in a semi-supervised settings (when unlabeled test points are known before the learning). Our approach is based on the same intuition, but instead of looking for a linear combination of the eigenvectors, we form a linear combination of known base functions and penalize them according to their smoothness on the underlying manifold. So, beside semi-supervised learning (explored in Section 4), our algorithm can also be used to classify out-of-sample test observations.
The penalty functional can also be justified from the point of view of spectral clustering [5]. The eigenvectors of L with the smallest eigenvalues8 represent ""natural"" clusters in the data set, so PL(h) is small if h is aligned with these eigenvectors, and PL(h) is large if h splits the corresponding clusters.
 7Another variant (that we did not explore in this paper) is to weight edges decreasingly with their lengths.      8Starting from the second smallest; the smallest is 0 and it corresponds to the constant func- tion. Also note that spectral clustering usually uses the eigenvectors of the normalized Laplacian e L = D-1/2LD-1/2. Nevertheless, if the neighborhood graph is constructed by connecting a fixed number of nearest neighbors, Dii is approximately constant, so the eigenvectors of L and e                                                                                                                  L are approximately equal.

4                 Experiments
In this section we present experimental results on four UCI benchmark datasets. The re- sults are preliminary in the sense that we only validated the penalty coefficient , and did not optimize the number of neighbors (set to k = 8) and the weighting scheme of the edges of the neighborhood graph (Wij = 0 or 1). We used decision stumps as base classifiers, 10-fold cross validation for estimating errors, and 5-fold cross validation for determining . The results (Figure 3(a)-(d) and Table 1) show that the REGBOOST consistently improves generalization. Although the improvement is within the standard deviation, the difference between the test and the training error decreases significantly in two of the four experi- ments, which indicates reduced overfitting. The final classifier is also significantly sparser after 1000 iterations (last two columns of Table 1). To measure how the penalty affects the base classifier pool, in each iteration we calculated the number of admissible base classi- fiers relative to the total number of stumps considered by ADABOOST. Figure 3(e) shows that, as expected, REGBOOST traverses only a (sometimes quite small) subset of the base classifier space.
                                       (a)                                                                       (b)                                                                 (c)                                          ionosphere                                                              breast cancer                                                          sonar

0.25                                                                                   0.09                                                                       0.6                                                        training error (AdaBoost)                                                  training error (AdaBoost)                                          training error (AdaBoost)                                                              test error (AdaBoost)                                                      test error (AdaBoost)                                           test error (AdaBoost)                                                        training error (RegBoost)        0.08                                      training error (RegBoost)                                          training error (RegBoost)                                                              test error (RegBoost)                                                      test error (RegBoost)      0.5                                  test error (RegBoost)   0.2                                                                                   0.07
                                                                                    0.06                                                                       0.4

0.15                                                                                         0.05                                                                                                                                                                    0.3                                                                                         0.04   0.1
                                                                                    0.03                                                                       0.2


                                                                                    0.02  0.05                                                                                                                                                                    0.1                                                                                         0.01


  0                                                                                     0                                                                        0             1                 10                              100                      1000  1       10                                  100                      1000  1     10                         100                       1000

                                          t                                                                             t                                                              t

                                pima indians diabetes                                                  rate of admissible stumps                                           semi-supervised ionosphere

0.35                                                                                     1                                                                          0.2                                                        training error (AdaBoost)                                                                  ionosphere                                         training error (AdaBoost)                                                              test error (AdaBoost)                                                              breast cancer                                           test error (AdaBoost)                                                        training error (RegBoost)        0.9                                                            sonar       0.18                              training error (RegBoost)                                                              test error (RegBoost)                                                   pima indians diabetes                                              test error (RegBoost)   0.3                                                                                                                                                              0.16                                                                                         0.8
                                                                                                                                                               0.14                                                                                         0.7

0.25                                                                                                                                                              0.12                                                                                         0.6                                                                                                                                                                      0.1                                                                                         0.5   0.2                                                                                                                                                              0.08
                                                                                    0.4                                                                                                                                                                    0.06

                                                                                    0.3  0.15                                                                                                                                                              0.04

                                                                                    0.2                                                                        0.02

0.1                                                                                   0.1                                                                            0             1                 10                              100                      1000  1       10                               100                         1000  1     10                         100                       1000
                                          t                                                                        t                                                                        t                                            (d)                                                                       (e)                                                                 (f) Figure 3: Learning curves. Test and training errors for the (a) ionosphere, (b) breast cancer, (c) sonar, and (d) Pima Indians diabetes data sets. (e) Rate of admissible stumps. (f) Test and training errors for the ionosphere data set with 100 labeled and 251 unlabeled data points.


             data set                                    training error                                                       test error                                        # of stumps                                                        ADAB                           REGB        ADAB                                                REGB                    ADAB                           REGB                  ionosphere                            0%                             0%          9.14% (7.1)                                         7.7% (6.0)              182                            114                  breast cancer                         0%                             2.44%       5.29% (3.5)                                         3.82% (3.7)             58                             30                  sonar                                 0%                             0%          32.5% (19.8)                                        29.8% (18.8)            234                            199                  Pima Indians                          10.9%                          16.0%       25.3% (5.3)                                         23.3% (6.8)             175                            91

                      Table 1: Errors rates and number of base classifiers after 1000 iterations.

Since the Laplacian penalty can be computed without knowing the labels, the algorithm can also be used for semi-supervised learning. Figure 3(f) shows the results when only a subset of the training points are labeled. In this case, REGBOOST can use the combined data set to calculate the penalty, whereas both algorithms can use only the labeled points
to determine the base errors. Figure 3(f) indicates that REGBOOST has a clear advantage here. REGBOOST is also far better than the semi-supervised algorithm proposed in [12] (their best test error using the same settings is 18%)."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/7180cffd6a8e829dacfc2a31b3f72ece-Abstract.html,Surface Reconstruction using Learned Shape Models,"Jan E. Solem, Fredrik Kahl","We consider the problem of geometrical surface reconstruction from one          or several images using learned shape models. While humans can effort-          lessly retrieve 3D shape information, this inverse problem has turned out          to be difficult to perform automatically. We introduce a framework based          on level set surface reconstruction and shape models for achieving this          goal. Through this merging, we obtain an efficient and robust method for          reconstructing surfaces of an object category of interest.          The shape model includes surface cues such as point, curve and silhou-          ette features. Based on ideas from Active Shape Models, we show how          both the geometry and the appearance of these features can be modelled          consistently in a multi-view context. The complete surface is obtained by          evolving a level set driven by a PDE, which tries to fit the surface to the          inferred 3D features. In addition, an a priori 3D surface model is used to          regularize the solution, in particular, where surface features are sparse.          Experiments are demonstrated on a database of real face images."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/743c41a921516b04afde48bb48e28ce6-Abstract.html,Reducing Spike Train Variability: A Computational Theory Of Spike-Timing Dependent Plasticity,"Sander M. Bohte, Michael Mozer","Experimental studies have observed synaptic potentiation when a           presynaptic neuron fires shortly before a postsynaptic neuron, and           synaptic depression when the presynaptic neuron fires shortly af-           ter. The dependence of synaptic modulation on the precise tim-           ing of the two action potentials is known as spike-timing depen-           dent plasticity or STDP. We derive STDP from a simple compu-           tational principle: synapses adapt so as to minimize the postsy-           naptic neuron's variability to a given presynaptic input, causing           the neuron's output to become more reliable in the face of noise.           Using an entropy-minimization objective function and the biophys-           ically realistic spike-response model of Gerstner (2001), we simu-           late neurophysiological experiments and obtain the characteristic           STDP curve along with other phenomena including the reduction in           synaptic plasticity as synaptic efficacy increases. We compare our           account to other efforts to derive STDP from computational princi-           ples, and argue that our account provides the most comprehensive           coverage of the phenomena. Thus, reliability of neural response in           the face of noise may be a key goal of cortical adaptation.
1     Introduction
Experimental studies have observed synaptic potentiation when a presynaptic neu- ron fires shortly before a postsynaptic neuron, and synaptic depression when the presynaptic neuron fires shortly after. The dependence of synaptic modulation on the precise timing of the two action potentials, known as spike-timing dependent plasticity or STDP, is depicted in Figure 1. Typically, plasticity is observed only when the presynaptic and postsynaptic spikes (hereafter, pre and post) occur within a 2030 ms time window, and the transition from potentiation to depression is very rapid. Another important observation is that synaptic plasticity decreases with in- creased synaptic efficacy. The effects are long lasting, and are therefore referred to as long-term potentiation (LTP) and depression (LTD). For detailed reviews of the evidence for STDP, see [1, 2]. Because these intriguing findings appear to describe a fundamental learning mech- anism in the brain, a flurry of models have been developed that focus on different aspects of STDP, from biochemical models that explain the underlying mechanisms giving rise to STDP [3], to models that explore the consequences of a STDP-like learning rules in an ensemble of spiking neurons [4, 5, 6, 7], to models that pro- pose fundamental computational justifications for STDP. Most commonly, STDP
Figure 1: (a) Measuring STDP experimentally: pre-post spike pairs are repeatedly in- duced at a fixed interval tpre-post, and the resulting change to the strength of the synapse is assessed; (b) change in synaptic strength after repeated spike pairing as a function of the difference in time between the pre and post spikes (data from Zhang et al., 1998). We have superimposed an exponential fit of LTP and LTD.
is viewed as a type of asymmetric Hebbian learning with a temporal dimension. However, this perspective is hardly a fundamental computational rationale, and one would hope that such an intuitively sensible learning rule would emerge from a first-principle computational justification. Several researchers have tried to derive a learning rule yielding STDP from first principles. Rao and Sejnowski [8] show that STDP emerges when a neuron attempts to predict its membrane potential at some time t from the potential at time t - t. However, STDP emerges only for a narrow range of t values, and the qualitative nature of the modeling makes it unclear whether a quantitative fit can be obtained. Dayan and H               ausser [9] show that STDP can be viewed as an optimal noise-removal filter for certain noise distributions. However, even small variation from these noise distributions yield quite different learning rules, and the noise statistics of biological neurons are unknown. Eisele (private communication) has shown that an STDP-like learning rule can be derived from the goal of maintaining the relevant connections in a network. Chechik [10] is most closely related to the present work. He relates STDP to information theory via maximization of mutual information between input and output spike trains. This approach derives the LTP portion of STDP, but fails to yield the LTD portion. The computational approach of Chechik (as well as Dayan and H                                                                        ausser) is premised on a rate-coding neuron model that disregards the relative timing of spikes. It seems quite odd to argue for STDP using rate codes: if spike timing is irrelevant to information transmission, then STDP is likely an artifact and is not central to understanding mechanisms of neural computation. Further, as noted in [9], because STDP is not quite additive in the case of multiple input or output spikes that are near in time [11], one should consider interpretations that are based on individual spikes, not aggregates over spike trains. Here, we present an alternative computational motivation for STDP. We conjecture that a fundamental objective of cortical computation is to achieve reliable neural re- sponses, that is, neurons should produce the identical response--both in the number and timing of spikes--given a fixed input spike train. Reliability is an issue if neu- rons are affected by noise influences, because noise leads to variability in a neuron's dynamics and therefore in its response. Minimizing this variability will reduce the effect of noise and will therefore increase the informativeness of the neuron's output signal. The source of the noise is not important; it could be intrinsic to a neuron (e.g., a noisy threshold) or it could originate in unmodeled external sources causing fluctuations in the membrane potential uncorrelated with a particular input. We are not suggesting that increasing neural reliability is the only learning objective.
If it were, a neuron would do well to give no response regardless of the input. Rather, reliability is but one of many objectives that learning tries to achieve. This form of unsupervised learning must, of course, be complemented by supervised and reinforcement learning that allow an organism to achieve its goals and satisfy drives. We derive STDP from the following computational principle: synapses adapt so as to minimize the entropy of the postsynaptic neuron's output in response to a given presynaptic input. In our simulations, we follow the methodology of neurophysiolog- ical experiments. This approach leads to a detailed fit to key experimental results. We model not only the shape (sign and time course) of the STDP curve, but also the fact that potentiation of a synapse depends on the efficacy of the synapse--it decreases with increased efficacy. In addition to fitting these key STDP phenom- ena, the model allows us to make predictions regarding the relationship between properties of the neuron and the shape of the STDP curve. Before delving into the details of our approach, we attempt to give a basic intu- ition about the approach. Noise in spiking neuron dynamics leads to variability in the number and timing of spikes. Given a particular input, one spike train might be more likely than others, but the output is nondeterministic. By the entropy- minimization principle, adaptation should reduce the likelihood of these other pos- sibilities. To be concrete, consider a particular experimental paradigm. In [12], a pre neuron is identified with a weak synapse to a post neuron, such that the pre is unlikely to cause the post to fire. However, the post can be induced to fire via a second presynaptic connection. In a typical trial, the pre is induced to fire a single spike, and with a variable delay, the post is also induced to fire (typically) a single spike. To increase the likelihood of the observed post response, other response pos- sibilities must be suppressed. With presynaptic input preceding the postsynaptic spike, the most likely alternative response is no output spikes at all. Increasing the synaptic connection weight should then reduce the possibility of this alternative response. With presynaptic input following the postsynaptic spike, the most likely alternative response is a second output spike. Decreasing the synaptic connection weight should reduce the possibility of this alternative response. Because both of these alternatives become less likely as the lag between pre and post spikes is in- creased, one would expect that the magnitude of synaptic plasticity diminishes with the lag, as is observed in the STDP curve. Our approach to reducing response variability given a particular input pattern in- volves computing the gradient of synaptic weights with respect to a differentiable model of spiking neuron behavior. We use the Spike Response Model (SRM) of [13] with a stochastic threshold, where the stochastic threshold models fluctuations of the membrane potential or the threshold outside of experimental control. For the stochastic SRM, the response probability is differentiable with respect to the synap- tic weights, allowing us to calculate the entropy gradient with respect to the weights conditional on the presented input. Learning is presumed to take a gradient step to reduce this conditional entropy. In modeling neurophysiological experiments, we demonstrate that this learning rule yields the typical STDP curve. We can predict the relationship between the exact shape of the STDP curve and physiologically measurable parameters, and we show that our results are robust to the choice of the few free parameters of the model. Two papers in these proceedings are closely related to our work. They also find STDP-like curves when attempting to maximize an information-theoretic measure-- the mutual information between input and output--for a Spike Response Model [14, 15]. Bell & Parra [14] use a deterministic SRM model which does not model the LTD component of STDP properly. The derivation by Toyoizumi et al. [15] is valid only for an essentially constant membrane potential with small fluctuations. Neither of these approaches has succeeded in quantitatively modeling specific experimental
data with neurobiologically-realistic timing parameters, and neither explains the saturation of LTD/LTP with increasing weights as we do. Nonetheless, these models make an interesting contrast to ours by suggesting a computational principle of optimization of information transmission, as contrasted with our principle of neural noise reduction. Perhaps experimental tests can be devised to distinguish between these competing theories.
2     The Stochastic Spike Response Model
The Spike Response Model (SRM), defined by Gerstner [13], is a generic integrate- and-fire model of a spiking neuron that closely corresponds to the behavior of a biological spiking neuron and is characterized in terms of a small set of easily inter- pretable parameters [16]. The standard SRM formulation describes the temporal evolution of the membrane potential based on past neuronal events, specifically as a weighted sum of postsynaptic potentials (PSPs) modulated by reset and thresh- old effects of previous postsynaptic spiking events. Following [13], the membrane potential of cell i at time t, ui(t), is defined as:
                      ui(t) = (t - ^                                               fi) +               wij               (t - ^                                                                                                          fi, t - fj),                     (1)                                                        ji              fj F t                                                                               j

where i is the set of inputs connected to neuron i, Ft is the set of times prior to                                                                                                     j t that neuron j has spiked, ^                                             fi is the time of the last spike of neuron i, wij is the synaptic weight from neuron j to neuron i, (t - ^                                                                                     fi, t - fj) is the PSP in neuron i due to an input spike from neuron j at time fj, and (t - ^                                                                                                                   fi) is the refractory response due to the postsynaptic spike at time ^                                                                             fi. Neuron i fires when the potential ui(t) exceeds a threshold () from below. The postsynaptic potential  is modeled as the differential alpha function in [13], defined with respect to two variables: the time since the most recent postsynaptic spike, x, and the time since the presynaptic spike, s:                                   1                         s                            s                    (x, s) =                  exp -                - exp -                               H(s)H(x - s)+                    (2)                                 1 - s                                                                                                               m                                s                                        m                                 s - x                  x                            x                     +exp -                  exp -                 - exp -                          H(x)H(s - x) ,                                  s                    m                          s where s and m are the rise and decay time-constants of the PSP, and H is the Heaviside function. The refractory reset function is defined to be [13]:                                                                                     x +                                      x        (x) = u                                                                                           abs                       absH(abs - x)H(-x) + uabsexp -                                                            + usexp -           ,    (3)                                                                                                                     r                                                                                                     f                         s                                                                                                     r                          r where uabs is a large negative contribution to the potential to model the absolute refractory period, with duration abs. We smooth this refractory response by a fast decaying exponential with time constant  f . The third term in the sum represents                                                                    r the slow decaying exponential recovery of an elevated threshold, us, with time                                                                                                                          r constant  s. (Graphs of these  and  functions can be found in [13].) We made               r a minor modification to the SRM described in [13] by relaxing the constraint that  s =   r         m; smoothing the absolute refractory function is mentioned in [13] but not explicitly defined as we do here. In all simulations presented, abs = 2ms,  s = 4                                                                                                                                 r         m, and  f = 0.1       r             m. The SRM we just described is deterministic. Gerstner [13] introduces a stochas- tic variant of the SRM (sSRM) by incorporating the notion of a stochastic firing threshold: given membrane potential ui(t), the probability density of the neuron firing at time t is specified by (ui(t)). Herrmann & Gerstner [17] find that then for a realistic escape-rate noise model the firing probability density as a function of the potential is initially small and constant, transitioning to asymptotically linear
increasing around threshold . In our simulations, we use such a function:                                                                                              (v) =         (ln[1 + exp(( - v))] - ( - v)),                                                    (4)                                                     where  is the firing threshold in the absence of noise,  determines the abruptness of the constant-to-linear probability density transition around , and  determines the slope of the increasing part. Experiments with sigmoidal and exponential density functions were found to not qualitatively affect the results.
3         Minimizing Conditional Entropy
We now derive the rule for adjusting the weight from a presynaptic neuron j to a postsynaptic sSRM neuron i, so as to minimize the entropy of i's response given a particular spike sequence from j. A spike sequence is described by the set of all times at which spikes have occurred within some interval between 0 and T , denoted F T for neuron j. We assume the interval is wide enough that spikes outside the      j interval do not influence the state of the neuron within the interval (e.g., through threshold reset effects). We can then treat intervals as independent of each other. Let the postsynaptic neuron i produce a response   i, where i is the set of all possible responses given the input,   FT , and g() is the probability density over                                                                                      i responses. The differential conditional entropy h(i) of neuron i's response is then defined as:
                                               h(i) = -                         g()log g() d.                                          (5)                                                                                i

To minimize the differential conditional entropy by adjusting the neuron's weights, we compute the gradient of the conditional entropy with respect to the weights:                                   h(i)                                         log(g())                                                    = -                   g()                             log(g()) + 1 d.                    (6)                                    wij                                                  wij                                                                     i
For a differentiable neuron model, log(g())/wij can be expressed as follows when neuron i fires once at time ^                                                     fi [18]:                  log(g())                              T     (u                       u             (t - ^                                                                                                                 fi) - (ui(t))                                               =                               i(t))             i(t)                                dt,        (7)                            wij                         t=0     ui(t)                    wij                  (ui(t)) where (.) is the Dirac delta, and (ui(t)) is the firing probability-density of neuron i at time t. (See [18] for the generalization to multiple postsynaptic spikes.) With the sSRM we can compute the partial derivatives (ui(t))/ui(t) and ui(t)/wij. Given the density function (4),                     (ui(t))                                                                     u                                          =                                                 ,             i(t) = (t - ^                                                                                                                        f                            u                                                                                               i, t - fj ).                                  i(t)         1 + exp(( - ui(t))                                      wij To perform gradient descent in the conditional entropy, we use the weight update                     h( w                                i)           ij  -                                                                                                                               (8)                       wij
                                                                     T     (t - ^                                                                                                 fi, t - fj) (t - ^                                                                                                                       fi) - (ui(t))                           g() log(g()) + 1                                                                                               dt d.                                                                                          (1 + exp(( - ui(t)))(ui(t))                       i                                             t=0 We can use numerical methods to evaluate Equation (8). However, it seems bio- logically unrealistic to suppose a neuron can integrate over all possible responses . This dilemma can be circumvented in two ways. First, the resulting learning rule might be cached in some form through evolution so that the full computation is not necessary (e.g., in an STDP curve). Second, the specific response produced by a neuron on a single trial might be considered to be a sample from the distribution g(), and the integration is performed by a sampling process over repeated trials;

Figure 2: (a) Experimental setup of Zhang et al. and (b) their experimental STDP curve (small squares) vs. our model (solid line). Model parameters: s = 1.5ms, m = 12.25ms.
each trial would produce a stochastic gradient step.
4     Simulation Methodology
We model in detail the experiment of Zhang et al. [12] (Figure 2a). In this exper- iment, a post neuron is identified that has two neurons projecting to it, call them the pre and the driver. The pre is subthreshold: it produces depolarization but no spike. The driver is suprathreshold: it induces a spike in the post. Plasticity of the pre-post synapse is measured as a function of the timing between pre and post spikes (tpre-post) by varying the timing between induced spikes in the pre and the driver (tpre-driver). This measurement yields the well-known STDP curve (Figure 1b).1 The experiment imposes several constraints on a simulation: The driver alone causes spiking > 70% of the time, the pre alone causes spiking < 10% of the time, synchronous firing of driver and pre cause LTP if and only if the post fires, and the time constants of the EPSPs--s and m in the sSRM--are in the range of 13ms and 1015ms respectively. These constraints remove many free parameters from our simulation. We do not explicitly model the two input cells; instead, we model the EPSPs they produce. The magnitude of these EPSPs are picked to satisfy the experimental constraints: the driver EPSP alone causes a spike in the post on 77.4% of trials, and the pre EPSP alone causes a spike on fewer than 0.1% of trials. Free parameters of the simulation are  and  in the spike-probability function ( can be folded into ), and the magnitude (us, u                                          ,  f ,                                        r     abs) and reset time constants ( s                                                                             r       r     abs). The dependent variable of the simulation is tpre-driver, and we measure the time of the post spike to determine tpre-post. We estimate the weight update for a given tpre-driver using Equation 8, approximating the integral by a summation over all time-discretized output responses consisting of 0, 1, or 2 spikes. Three or more spikes have a probability that is vanishingly small."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/74934548253bcab8490ebd74afed7031-Abstract.html,Maximum Likelihood Estimation of Intrinsic Dimension,"Elizaveta Levina, Peter J. Bickel","We propose a new method for estimating intrinsic dimension of a dataset derived by applying the principle of maximum likelihood to the distances between close neighbors. We derive the estimator by a Poisson process approximation, assess its bias and variance theo- retically and by simulations, and apply it to a number of simulated and real datasets. We also show it has the best overall performance compared with two other intrinsic dimension estimators."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/75455e062929d32a333868084286bb68-Abstract.html,Distributed Information Regularization on Graphs,"Adrian Corduneanu, Tommi S. Jaakkola","We provide a principle for semi-supervised learning based on optimizing          the rate of communicating labels for unlabeled points with side informa-          tion. The side information is expressed in terms of identities of sets of          points or regions with the purpose of biasing the labels in each region          to be the same. The resulting regularization objective is convex, has a          unique solution, and the solution can be found with a pair of local prop-          agation operations on graphs induced by the regions. We analyze the          properties of the algorithm and demonstrate its performance on docu-          ment classification tasks."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/75e33da9b103b7b91dcd8da0abe1354b-Abstract.html,The Convergence of Contrastive Divergences,Alan L. Yuille,This paper analyses the Contrastive Divergence algorithm for learning          statistical parameters. We relate the algorithm to the stochastic approxi-          mation literature. This enables us to specify conditions under which the          algorithm is guaranteed to converge to the optimal solution (with proba-          bility 1). This includes necessary and sufficient conditions for the solu-          tion to be unbiased.
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/7876acb66640bad41f1e1371ef30c180-Abstract.html,Log-concavity Results on Gaussian Process Methods for Supervised and Unsupervised Learning,Liam Paninski,"Log-concavity is an important property in the context of optimization,          Laplace approximation, and sampling; Bayesian methods based on Gaus-          sian process priors have become quite popular recently for classification,          regression, density estimation, and point process intensity estimation.          Here we prove that the predictive densities corresponding to each of these          applications are log-concave, given any observed data. We also prove          that the likelihood is log-concave in the hyperparameters controlling the          mean function of the Gaussian prior in the density and point process in-          tensity estimation cases, and the mean, covariance, and observation noise          parameters in the classification and regression cases; this result leads to          a useful parameterization of these hyperparameters, indicating a suitably          large class of priors for which the corresponding maximum a posteriori          problem is log-concave."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/78f7d96ea21ccae89a7b581295f34135-Abstract.html,Incremental Algorithms for Hierarchical Classification,"Nicolò Cesa-bianchi, Claudio Gentile, Andrea Tironi, Luca Zaniboni","We study the problem of hierarchical classification when labels corre-            sponding to partial and/or multiple paths in the underlying taxonomy are            allowed. We introduce a new hierarchical loss function, the H-loss, im-            plementing the simple intuition that additional mistakes in the subtree of            a mistaken class should not be charged for. Based on a probabilistic data            model introduced in earlier work, we derive the Bayes-optimal classifier            for the H-loss. We then empirically compare two incremental approx-            imations of the Bayes-optimal classifier with a flat SVM classifier and            with classifiers obtained by using hierarchical versions of the Perceptron            and SVM algorithms. The experiments show that our simplest incremen-            tal approximation of the Bayes-optimal classifier performs, after just one            training epoch, nearly as well as the hierarchical SVM classifier (which            performs best). For the same incremental algorithm we also derive an            H-loss bound showing, when data are generated by our probabilistic data            model, exponentially fast convergence to the H-loss of the hierarchical            classifier based on the true model parameters.
1      Introduction and basic definitions
We study the problem of classifying data in a given taxonomy of labels, where the tax- onomy is specified as a tree forest. We assume that every data instance is labelled with a (possibly empty) set of class labels called multilabel, with the only requirement that mul- tilabels including some node i in the taxonony must also include all ancestors of i. Thus, each multilabel corresponds to the union of one or more paths in the forest, where each path must start from a root but it can terminate on an internal node (rather than a leaf).
Learning algorithms for hierarchical classification have been investigated in, e.g., [8, 9, 10, 11, 12, 14, 15, 17, 20]. However, the scenario where labelling includes multiple and partial paths has received very little attention. The analysis in [5], which is mainly theoretical, shows in the multiple and partial path case a 0/1-loss bound for a hierarchical learning algorithm based on regularized least-squares estimates. In this work we extend [5] in several ways. First, we introduce a new hierarchical loss func- tion, the H-loss, which is better suited than the 0/1-loss to analyze hierarchical classification tasks, and we derive the corresponding Bayes-optimal classifier under the parametric data model introduced in [5]. Second, considering various loss functions, including the H-loss, we empirically compare the performance of the following three incremental kernel-based
  This work was supported in part by the PASCAL Network of Excellence under EC grant no. 506778. This publication only reflects the authors' views.

algorithms: 1) a hierarchical version of the classical Perceptron algorithm [16]; 2) an ap- proximation to the Bayes-optimal classifier; 3) a simplified variant of this approximation. Finally, we show that, assuming data are indeed generated according to the parametric model mentioned before, the H-loss of the algorithm in 3) converges to the H-loss of the classifier based on the true model parameters. Our incremental algorithms are based on training linear-threshold classifiers in each node of the taxonomy. A similar approach has been studied in [8], though their model does not consider multiple-path classifications as we do. Incremental algorithms are the main focus of this research, since we strongly believe that they are a key tool for coping with tasks where large quantities of data items are generated and the classification system needs to be frequently adjusted to keep up with new items. However, we found it useful to provide a reference point for our empirical results. Thus we have also included in our experiments the results achieved by nonincremental algorithms. In particular, we have chosen a flat and a hierarchical version of SVM [21, 7, 19], which are known to perform well on the textual datasets considered here. We assume data elements are encoded as real vectors x  Rd which we call instances. A multilabel for an instance x is any subset of the set {1, . . . , N } of all labels/classes, including the empty set. We denote the multilabel associated with x by a vector y = (y1, . . . , yN )  {0, 1}N , where i belongs to the multilabel of x if and only if yi = 1. A taxonomy G is a forest whose trees are defined over the set of labels. A multilabel y  {0, 1}N is said to respect a taxonomy G if and only if y is the union of one or more paths in G, where each path starts from a root but need not terminate on a leaf. See Figure 1. We assume the data-generating mechanism produces examples (x, y) such that y respects some fixed underlying taxonomy G with N nodes. The set of roots in G is denoted by root(G). We use par(i) to denote the unique parent of node i, anc(i) to denote the set of ancestors of i, and sub(i) to denote the set of nodes in the subtree rooted at i (including i). Finally, given a predicate  over a set , we will use {} to denote both the subset of  where  is true and the indicator function of this subset."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/7aee26c309def8c5a2a076eb250b8f36-Abstract.html,On Semi-Supervised Classification,"Balaji Krishnapuram, David Williams, Ya Xue, Lawrence Carin, Mário Figueiredo, Alexander J. Hartemink","A graph-based prior is proposed for parametric semi-supervised classi-            fication. The prior utilizes both labelled and unlabelled data; it also in-            tegrates features from multiple views of a given sample (e.g., multiple            sensors), thus implementing a Bayesian form of co-training. An EM            algorithm for training the classifier automatically adjusts the tradeoff be-            tween the contributions of: (a) the labelled data; (b) the unlabelled data;            and (c) the co-training information. Active label query selection is per-            formed using a mutual information based criterion that explicitly uses the            unlabelled data and the co-training information. Encouraging results are            presented on public benchmarks and on measured data from single and            multiple sensors.
1      Introduction
In many pattern classification problems, the acquisition of labelled training data is costly and/or time consuming, whereas unlabelled samples can be obtained easily.                 Semi- supervised algorithms that learn from both labelled and unlabelled samples have been the focus of much research in the last few years; a comprehensive review up to 2001 can be found in [13], while more recent references include [1, 2, 6, 7, 1618].
Most recent semi-supervised learning algorithms work by formulating the assumption that ""nearby"" points, and points in the same structure (e.g., cluster), should have similar labels [6, 7, 16]. This can be seen as a form of regularization, pushing the class boundaries toward regions of low data density. This regularization is often implemented by associating the vertices of a graph to all the (labelled and unlabelled) samples, and then formulating the problem on the vertices of the graph [6, 1618].
While current graph-based algorithms are inherently transductive -- i.e., they cannot be used directly to classify samples not present when training -- our classifier is paramet- ric and the learned classifier can be used directly on new samples. Furthermore, our al- gorithm is trained discriminatively by maximizing a concave objective function; thus we avoid thorny local maxima issues that plague many earlier methods.
Unlike existing methods, our algorithm automatically learns the relative importance of the labelled and unlabelled data. When multiple views of the same sample are provided (e.g. features from different sensors), we develop a new Bayesian form of co-training [4]. In
addition, we also show how to exploit the unlabelled data and the redundant views of the sample (from co-training) in order to improve active label query selection [15].
The paper is organized as follows. Sec. 2 briefly reviews multinomial logistic regression. Sec. 3 describes the priors for semi-supervised learning and co-training. The EM algorithm derived to learn the classifiers is presented in Sec. 4. Active label selection is discussed in Sec. 5. Experimental results are shown in Sec. 6, followed by conclusions in Sec. 7.
2           Multinomial Logistic Regression
In an m-class supervised learning problem, one is given a labelled training set DL = {(x                                                                               d             1, y ), . . . , (x              )}                 1                  L, yL          , where xi  R is a feature vector and yi the corresponding                                                                                               (1)                      (m) class label. In ""1-of-m"" encoding, y                                            = [y                    , . . . , y           ]                                                                            i                                                       is a binary vector, such that                                                                                               i                        i      (c)                 (j) y           = 1 and y              = 0, for j = c, indicates that sample i belongs to class c. In multinomial      i                   i logistic regression [5], the posterior class probabilities are modelled as
          log P (y(c) = 1|x) = xT w(c) - log                                            m           exp(xT w(k)),                        for c = 1, . . . , m,    (1)                                                                                             k=1

where w(c)                   d                          R is the class-c weight vector. Notice that since                                                                     m      P (y(c)= 1|x) = 1,                                                                                                                                                c=1 one of the weight vectors is redundant; we arbitrarily choose to set w(m) = 0, and consider the (d (m-1))-dimensional vector w = [(w(1))T , ..., (w(m-1))T ]T . Estimation of w may be achieved by maximizing the log-likelihood (with Y  {y , ..., y }                                                                                                                                    1           L ) [5]
                                                                                   (c)             (w)  log P (Y|w) =                           L                m      y           xT w(c) - log                             m      exp(xT w(j)) .         (2)                                                           i=1              c=1         i           i                                    j=1             i

In the presence of a prior p(w), we seek a maximum a posteriori (MAP) estimate, w = arg max { (w) + log p(w)}                               w                                          . Actually, if the training data is separable, (w) is unbounded, and a prior is crucial.
Although we focus on linear classifiers, we may see the d-dimensional feature vectors x as having resulted from some deterministic, maybe nonlinear, transformation of an input raw feature vector r; e.g., in a kernel classifier, xi = [1, K(ri, r1), ..., K(ri, rL)] (d = L + 1).
3           Graph-Based Data-Dependent Priors
3.1           Graph Laplacians and Regularization for Semi-Supervised Learning
Consider a scalar function f = [f1, ..., f|V |]T , defined on the set V = {1, 2, ..., |V |} of vertices of an undirected graph (V, E). Each edge of the graph, joining vertices i and j, is given a weight kij = kji  0, and we collect all the weights in a |V |  |V | matrix K. A natural way to measure how much f varies across the graph is by the quantity
                                                                     kij(fi - fj)2 = 2 f T  f ,                                                                  (3)                                                      i         j

where  = diag{                            k                        k                                       j     1j , ...,          j    |V |j } - K is the so-called graph Laplacian [2]. Notice that kij  0 (for all i, j) guarantees that  is positive semi-definite and also that  has (at least) one null eigenvalue (1T1 = 0, where 1 has all elements equal to one).
In semi-supervised learning, in addition to DL, we are given U unlabelled samples DU = {xL+1, . . . , xL+U }. To use (3) for semi-supervised learning, the usual choice is to assign one vertex of the graph to each sample in X = [x1, . . . , xL+U ]T (thus |V | = L + U ), and to let kij represent some (non-negative) measure of ""similarity"" between xi and xj. A Gaussian random field (GRF) is defined on the vertices of V (with inverse variance )
                                                      p(f )  exp{- f T f /2},

in which configurations that vary more (according to (3)) are less probable. Most graph- based approaches estimate the values of f , given the labels, using p(f ) (or some modifica- tion thereof) as a prior. Accordingly, they work in a strictly transductive manner.
3.2         Non-Transductive Semi-Supervised Learning
We first consider two-class problems (m = 2, thus w                                                  d                                                                                                    R ). In contrast to previous uses of graph-based priors, we define f as the real function f (defined over the entire observation space) evaluated at the graph nodes. Specifically, f is defined as a linear function of x, and at the graph node i, fi  f (xi) = wT xi. Then, f = [f1, ..., f|V |]T = Xw, and p(f ) induces a Gaussian prior on w, with precision matrix A = XT X,
                  p(w)  exp{-(/2) wT XT Xw} = exp{-(/2) wT Aw}.                                                                                  (4)

Notice that since  is singular, A may also be singular, and the corresponding prior may therefore be improper. This is no problem for MAP estimation of w because (as is well known) the normalization factor of the prior plays no role in this estimate. If we include extra regularization, by adding a non-negative diagonal matrix to A, the prior becomes
                             p(w)  exp -(1/2) wT (0A + ) w ,                                                                                      (5)

where we may choose  = diag{1, ..., d},  = 1I, or even  = 0.
For m > 2, we define (m-1) identical independent priors, one for each w(c), c = 1, ..., m. The joint prior on w = [(w(1))T , ..., (w(m-1))T ]T is then
                  m-1              1                            (c)                                              1   p(w|)                     exp{-         (w(c))T                       A + (c) w(c)} = exp{-                           wT ()w}, (6)                                        2                            0                                                2                        c=1

                                                                (c)                                              (c)                (c) where  is a vector containing all the                                    parameters, (c) = diag{                        , ...,            }, and                                                                     i                                                1                  d

                                   (1)                (m-1)                     () = diag{             , ...,                    }  A + block-diag{(1), ..., (m-1)}.                                          (7)                                        0                  0

Finally, since all the 's are inverses of variances, the conjugate priors are Gamma [3]:        (c)                             (c)                                     (c)                              (c) p(           |                              |                                      |                                  |        0       0, 0)    = Ga(0                0, 0), and p(i                       1, 1)             = Ga(i            1, 1), for c               = 1, ..., m - 1 and i = 1, ..., d. Usually, 0, 0, 1, and 1 are given small values indicating diffuse priors. In the zero limit, we obtain scale-invariant (improper) Jeffreys hyper-priors.
Summarizing, our model for semi-supervised learning includes the log-likelihood (2), a prior (6), and Gamma hyper-priors. In Section 4, we present a simple and computationally efficient expectation-maximization (EM) algorithm for obtaining the MAP estimate of w.
3.3         Exploiting Features from Multiple Sensors: The Co-Training Prior
In some applications several sensors are available, each providing a different set of features. For simplicity, we assume two sensors s  {1, 2}, but everything discussed here is easily                                                                                                                                                          (s) extended to any number of sensors. Denote the features from sensor s, for sample i, as x ,                                                                                                                                                          i and Ss as the set of sample indices for which we have features from sensor s (S1  S2 = {1, ..., L + U }). Let O = S1  S2 be the indices for which both sensors are available, and OU = O  {L + 1, ..., L + U } the unlabelled subset of O.
By using the samples in S1 and S2 as two independent training sets, we may obtain two sep- arate classifiers (denoted w1 and w2). However, we can coordinate the information from both sensors by using an idea known as co-training [4]: on the OU samples, classifiers w1 and w2 should agree as much as possible. Notice that, in a logistic regression framework, the disagreement between the two classifiers on the OU samples can be measured by                                                                (1)                          (2)                                                [(w1)T x                   - (w2)T x                ]2 = T C ,                                          (8)                                   iOU                         i                            i
where  = [(w1)T (w2)T ]T and C =                                       [(x1)T (-x2)T ]T [(x1)T (-x2)T ]. This                                                                 iOU       i                i      i           i suggests the ""co-training prior"" (where co is an inverse variance):                                p(w1, w2) = p()  exp -(co/2) TC  .                                               (9)
This Gaussian prior can be combined with two smoothness Gaussian priors on w1 and w2 (obtained as described in Section 3.2); this leads to a prior which is still Gaussian,
   p(w1, w2) = p()  exp -(1/2) T coC + block-diag{1, 2}  ,                                               (10)

where 1 and 2 are the two graph-based precision matrices (see (7)) for w1 and w2. We can again adopt a Gamma hyper-prior for co. Under this prior, and with a logistic regression likelihood as above, estimates of w1 and w2 can easily be found using minor modifications to the EM algorithm described in Section 4. Computationally, this is only slightly more expensive than separately training the two classifiers.
4     Learning Via EM
To find the MAP estimate w, we use the EM algorithm, with  as missing data, which is equivalent to integrating out  from the full posterior before maximization [8]. For simplicity, we will only describe the single sensor case (no co-training).
E-step: We compute the expected value of the complete log-posterior, given Y and the current parameter estimate w: Q(w|w)  E[log p(w, |Y)|w]. Since
                    log p(w, |Y) = log p(Y|w) - (1/2)wT ()w + K,                                             (11)

(where K collects all terms independent of w) is linear w.r.t. all the  parameters (see (6) and (7)), we just have to plug their conditional expectations into (11):
 Q(w|w) = log p(Y|w) - (1/2)wT E[()|w] w = (w) - (1/2)wT (w) w. (12) We consider several different choices for the structure of the  matrix. The necessary expectations have well-known closed forms, due to the use of conjugate Gamma hyper-                                                 (c) priors [3]. For example, if the                       are m - 1 free non-negative parameters, we have                                                 0                         (c)             (c)                                E[           |w] = (2                          0               0                   0 + d) [2 0 + (w(c))T Aw(c)]-1.                                                 (c) for c = 1, ..., m - 1. For                             =                                                  0              0, we still have a simple closed-form expres-                                                                                (c) sion for E[0|w], and the same is true for the                                       parameters, for i > 0. Finally,                                                                                i (w)  E[()|w] results from replacing the 's in (7) by the corresponding conditional expectations.

M-step: Given matrix (w), the M-step reduces to a logistic regression problem with a quadratic regularizer, i.e., maximizing (12). To this end, we adopt the bound optimization approach (see details in [5, 11]). Let B be a positive definite matrix such that -B bounds below (in the matrix sense) the Hessian of (w), which is negative definite, and g(w) is the gradient of (w). Then, we have the following lower bound on Q(w|w):
  Q(w|w)  l(w) + (w - w)T g(w) - [(w - w)T B(w - w) + wT (w)w]/2.                                                                                        - The maximizer of this lower bound, wnew = (B + (w)) 1 (Bw + g(w)), is guaranteed to increase the Q-function, Q(wnew|w)  Q(w|w), and we thus obtain a monotonic gen- eralized EM algorithm [5, 11]. This (maybe costly) matrix inversion can be avoided by a sequential approach where we only maximize w.r.t. one element of w at a time, preserving the monotonicity of the procedure. The sequential algorithm visits one particular element of w, say wu, and updates its estimate by maximizing the bound derived above, while keeping all other variables fixed at their previous values. This leads to                                                                                                    -                    wnew = w                                                    ] [(B + (w))            1 ,                         u           u + [gu(w) - ((w)w)                                                            (13)                                                                           u                      uu] and wnew = w         v        v ,         for v = u. The total time required by a full sweep for all u = 1, ..., d is O(md(L + d)); this may be much better than the O((dm)3) of the matrix inversion.

5     Active Label Selection
If we are allowed to obtain the label for one of the unlabelled samples, the following ques- tion arises: which sample, if labelled, would provide the most information?
Consider the MAP estimate w provided by EM. Our approach uses a Laplace approxima- tion of the posterior p(w|Y)     N (w|w, H-1), where H is the posterior precision matrix, i.e., the Hessian of minus the log-posterior H =    2(- log p(w|Y)). This approximation is known to be accurate for logistic regression under a Gaussian prior [14]. By treating (w) (the expectation of ()) as deterministic, we obtain an evidence-type approximation [14]
 H =    2[- log(p(Y|w)p(w|(w)))] = (w) + L (diag{p } - p pT )  x                     ,                                                         i=1         i      i    i    ixT                                                                                        i

where pi is the (m - 1)-dimensional vector computed from (1), the c-th element of which indicates the probability that sample xi belongs to class c.
Now let x  DU be an unlabelled sample and y its label. Assume that the MAP esti- mate w remains unchanged after including y. In Sec. 7 we will discuss the merits and shortcomings of this assumption, which is only strictly valid when L  . Accepting it implies that after labeling x, and regardless of y, the posterior precision changes to                            H = H + (diag{p} - ppT )  xxT .                            (14) Since the entropy of a Gaussian with precision H is (-1/2) log |H| (up to an additive constant), the mutual information (MI) between y and w (i.e., the expected decrease in entropy of w when y is observed) is I(w; y) = (1/2) log {|H |/|H|}. Our criterion is then: the best sample to label is the one that maximizes I(w; y). Further insight into I(w; y) can be obtained in the binary case (where p is a scalar); here, the matrix identity |H + p(1 - p)xxT |                           = |H|(1 + p(1 - p)xT H-1x) yields                     I(w; y) = (1/2) log(1 + p(1 - p)xT H-1x).                          (15) This MI is larger when p  0.5, i.e., for samples with uncertain classifications. On the other hand, with p fixed, I(w; y) grows with xT H-1x, i.e., it is large for samples with high variance of the corresponding class probability estimate. Summarizing, (15) favors samples with uncertain class labels and high uncertainty in the class probability estimate.
6     Experimental Results
We begin by presenting two-dimensional synthetic examples to visually illustrate our semi- supervised classifier. Fig. 1 shows the utility of using unlabelled data to improve the deci-
Figure 1: Synthetic two-dimensional examples. (a) Comparison of the supervised logistic linear classifier (boundary shown as dashed line) learned only from the labelled data (shown in color) with the proposed semi-supervised classifier (boundary shown as solid line) which also uses the unlabelled samples (shown as dots). (b) A RBF kernel classifier obtained by our algorithm, using two labelled samples (shaded circles) and many unlabelled samples.
Figure 2: (a)-(c) Accuracy (on UCI datasets) of the proposed method, the supervised SVM, and the other semi-supervised classifiers mentioned in the text; a subset of samples is la- belled and the others are treated as unlabelled samples. In (d), a separate holdout set is used to evaluate the accuracy of our method versus the amount of labelled and unlabelled data.
sion boundary in linear and non-linear (kernel) classifiers (see figure caption for details).
Next we show results with linear classifiers on three UCI benchmark datasets. Results with nonlinear kernels are similar, and therefore omitted to save space. We compare our method against state-of-the-art semi-supervised classifiers: the GRF method of [18], the SGT method of [10], and the transductive SVM (TSVM) of [9]. For reference, we also present results for a standard SVM. To avoid unduly helping our method, we always use a k=5 nearest neighbors graph, though our algorithm is not very sensitive to k. To avoid disadvantaging other methods that do depend on such parameters, we use their best settings. Since these adjustments cannot be made in practice, the difference between our algorithm and the others is under-represented. Each point on the plots in Fig. 2(a)-(c) is an average of 20 trials: we randomly select 20 labelled sets which are used by every method. All remaining samples are used as unlabelled by the semi-supervised algorithms.
Figs. 2(a)-(c) are transductive, in the sense that the unlabelled and test data are the same. Our logistic GRF is non-transductive: after being trained, it may be applied to classify new data without re-training. In Fig. 2(d) we present non-transductive results for the Ionosphere data. Training took place using labelled and unlabelled data, and testing was performed on 200 new unseen samples. The results suggest that semi-supervised classifiers are most relevant when the labelled set is small relative to the unlabelled set (as is often the case).
Our final set of results address co-training (Sec. 3.3) and active learning (Sec. 5), applied to airborne sensing data for the detection of surface and subsurface land mines. Two sensors were used: (1) a 70-band hyper-spectral electro-optic (EOIR) sensor; (2) an X-band syn- thetic aperture radar (SAR). A simple (energy) ""prescreener"" detected potential targets; for each of these, two feature vectors were extracted, of sizes 420 and 9, for the EOIR and SAR sensors, respectively. 123 samples have features from the EOIR sensor alone, 398 from the
Figure 3: (a) Land mine detection ROC curves of classifiers designed using only hyper- spectral (EOIR) features, only SAR features, and both. (b) Number of landmines detected during the active querying process (dotted lines), for active training and random selection (for the latter the bars reflect one standard deviation about the mean). ROC curves (solid) are for the learned classifier as applied to the remaining samples.
SAR sensor alone, and 316 from both. This data will be made available upon request.
We first consider supervised and semi-supervised classification. For the purely supervised case, a sparseness prior is used (as in [14]). In both cases a linear classifier is employed. For the data for which only one sensor is available, 20% of it is labelled (selected randomly). For the data for which both sensors are available, 80% is labelled (again selected randomly). The results presented in Fig. 3(a) show that, in general, the semi-supervised classifiers outperform the corresponding supervised ones, and the classifier learned from both sensors is markedly superior to classifiers learned from either sensor alone.
In a second illustration, we use the active-learning algorithm (Sec. 5) to only acquire the 100 most informative labels. For comparison, we also show average results over 100 in- dependent realizations for random label query selection (error bars indicate one standard deviation). The results in Fig. 3(b) are plotted in two stages: first, mines and clutter are se- lected during the labeling process (dashed curves); then, the 100 labelled examples are used to build the final semi-supervised classifier, for which the ROC curve is obtained using the remaining unlabelled data (solid curves). Interestingly, the active-learning algorithm finds almost half of the mines while querying for labels. Due to physical limitations of the sen- sors, the rate at which mines are detected drops precipitously after approximately 90 mines are detected -- i.e., the remaining mines are poorly matched to the sensor physics."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/7c4bf50b715509a963ce81b168ca674b-Abstract.html,Joint MRI Bias Removal Using Entropy Minimization Across Images,"Erik G. Learned-miller, Parvez Ahammad","The correction of bias in magnetic resonance images is an important          problem in medical image processing. Most previous approaches have          used a maximum likelihood method to increase the likelihood of the pix-          els in a single image by adaptively estimating a correction to the unknown          image bias field. The pixel likelihoods are defined either in terms of a          pre-existing tissue model, or non-parametrically in terms of the image's          own pixel values. In both cases, the specific location of a pixel in the im-          age is not used to calculate the likelihoods. We suggest a new approach          in which we simultaneously eliminate the bias from a set of images of          the same anatomy, but from different patients. We use the statistics from          the same location across different images, rather than within an image, to          eliminate bias fields from all of the images simultaneously. The method          builds a ""multi-resolution"" non-parametric tissue model conditioned on          image location while eliminating the bias fields associated with the orig-          inal image set. We present experiments on both synthetic and real MR          data sets, and present comparisons with other methods."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/7e83722522e8aeb7512b7075311316b7-Abstract.html,Chemosensory Processing in a Spiking Model of the Olfactory Bulb: Chemotopic Convergence and Center Surround Inhibition,"Baranidharan Raman, Ricardo Gutierrez-osuna","This paper presents a neuromorphic model of two olfactory signal-          processing primitives: chemotopic convergence of olfactory           receptor neurons, and center on-off surround lateral inhibition in           the olfactory bulb. A self-organizing model of receptor           convergence onto glomeruli is used to generate a spatially           organized map, an olfactory image. This map serves as input to a           lattice of spiking neurons with lateral connections. The dynamics           of this recurrent network transforms the initial olfactory image into           a spatio-temporal pattern that evolves and stabilizes into odor- and           intensity-coding attractors. The model is validated using           experimental data from an array of temperature-modulated gas           sensors.  Our results are consistent with recent neurobiological           findings on the antennal lobe of the honeybee and the locust."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/7edccc661418aeb5761dbcdc06ad490c-Abstract.html,Using Random Forests in the Structured Language Model,"Peng Xu, Frederick Jelinek","In this paper, we explore the use of Random Forests (RFs) in the struc- tured language model (SLM), which uses rich syntactic information in predicting the next word based on words already seen. The goal in this work is to construct RFs by randomly growing Decision Trees (DTs) us- ing syntactic information and investigate the performance of the SLM modeled by the RFs in automatic speech recognition. RFs, which were originally developed as classiﬁers, are a combination of decision tree classiﬁers. Each tree is grown based on random training data sampled independently and with the same distribution for all trees in the forest, and a random selection of possible questions at each node of the decision tree. Our approach extends the original idea of RFs to deal with the data sparseness problem encountered in language modeling. RFs have been studied in the context of n-gram language modeling and have been shown to generalize well to unseen data. We show in this paper that RFs using syntactic information can also achieve better performance in both perplexity (PPL) and word error rate (WER) in a large vocabulary speech recognition system, compared to a baseline that uses Kneser-Ney smoothing."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/7f7c351ee977c765aa8cd5c7020bc38f-Abstract.html,Exploration-Exploitation Tradeoffs for Experts Algorithms in Reactive Environments,"Daniela D. Farias, Nimrod Megiddo","A reactive environment is one that responds to the actions of an agent rather than           evolving obliviously. In reactive environments, experts algorithms must balance           exploration and exploitation of experts more carefully than in oblivious ones. In           addition, a more subtle definition of a learnable value of an expert is required. A           general exploration-exploitation experts method is presented along with a proper           definition of value. The method is shown to asymptotically perform as well as           the best available expert. Several variants are analyzed from the viewpoint of the           exploration-exploitation tradeoff, including explore-then-exploit, polynomially           vanishing exploration, constant-frequency exploration, and constant-size explo-           ration phases. Complexity and performance bounds are proven."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/8169e05e2a0debcb15458f2cc1eff0ea-Abstract.html,Unsupervised Variational Bayesian Learning of Nonlinear Models,"Antti Honkela, Harri Valpola","In this paper we present a framework for using multi-layer per- ceptron (MLP) networks in nonlinear generative models trained by variational Bayesian learning. The nonlinearity is handled by linearizing it using a Gauss–Hermite quadrature at the hidden neu- rons. This yields an accurate approximation for cases of large pos- terior variance. The method can be used to derive nonlinear coun- terparts for linear algorithms such as factor analysis, independent component/factor analysis and state-space models. This is demon- strated with a nonlinear factor analysis experiment in which even 20 sources can be estimated from a real world speech data set."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/81c2f886f91e18fe16d6f4e865877cb6-Abstract.html,VDCBPI: an Approximate Scalable Algorithm for Large POMDPs,"Pascal Poupart, Craig Boutilier","Existing algorithms for discrete partially observable Markov decision          processes can at best solve problems of a few thousand states due to          two important sources of intractability: the curse of dimensionality and          the policy space complexity.     This paper describes a new algorithm          (VDCBPI) that mitigates both sources of intractability by combining the          Value Directed Compression (VDC) technique [13] with Bounded Pol-          icy Iteration (BPI) [14]. The scalability of VDCBPI is demonstrated on          synthetic network management problems with up to 33 million states.
1     Introduction
Partially observable Markov decision processes (POMDPs) provide a natural and expres- sive framework for decision making, but their use in practice has been limited by the lack of scalable solution algorithms. Two important sources of intractability plague discrete model-based POMDPs: high dimensionality of belief space, and the complexity of policy or value function (VF) space. Classic solution algorithms [4, 10, 7], for example, compute value functions represented by exponentially many value vectors, each of exponential size. As a result, they can only solve POMDPs with on the order of 100 states. Consequently, much research has been devoted to mitigating these two sources of intractability.
The complexity of policy/VF space has been addressed by observing that there are often very good policies whose value functions are representable by a small number of vectors. Various algorithms such as approximate vector pruning [9], point-based value iteration (PBVI) [12, 16], bounded policy iteration (BPI) [14], gradient ascent (GA) [11, 1] and stochastic local search (SLS) [3] exploit this fact to produce (often near-optimal) policies of low complexity (i.e., few vectors) allowing larger POMDPs to be solved. Still these scale to problems of only roughly 1000 states, since each value vector may still have ex- ponential dimensionality. Conversely, it has been observed that belief states often carry more information than necessary. Hence, one can often reduce vector dimensionality by using compact representations such as decision trees (DTs) [2], algebraic decision dia- grams (ADDs) [8, 9], or linear combinations of small basis functions (LCBFs) [6], or by indirectly compressing the belief space into a small subspace by a value-directed compres- sion (VDC) [14] or exponential PCA [15]. Once compressed, classic solution methods can be used. However, since none of these approaches address the exponential complexity of
policy/VF space, they can only solve slightly larger POMDPs (up to 8250 states [15]).
Scalable POMDP algorithms can only be realized when both sources of intractability are tackled simultaneously. While Hansen and Feng [9] implemented such an algorithm by combining approximate state abstraction with approximate vector pruning, they didn't demonstrate the scalability of the approach on large problems. In this paper, we describe how to combine value directed compression (VDC) with bounded policy iteration (BPI) and demonstrate the scalability of the resulting algorithm (VDCBPI) on synthetic network management problems of up to 33 million states. Among the techniques that deal with the curse of dimensionality, VDC offers the advantage that the compressed POMDP can be di- rectly fed into existing POMDP algorithms with no (or only slight) adjustments. This is not the case for exponential-PCA, nor compact representations (DTs, ADDs, LCBFs). Among algorithms that mitigate policy space complexity, BPI distinguishes itself by its ability to avoid local optima (cf. GA), its efficiency (cf. SLS) and the fact that belief state monitoring is not required (cf. PBVI, approximate vector pruning). Beyond the combination of VDC with BPI, we offer two other contributions. We propose a new simple heuristic to compute good lossy value directed compressions. We also augment BPI with the ability to bias its policy search to reachable belief states. As a result, BPI can often find a much smaller policy of similar quality for a given initial belief state.
2      POMDP Background
A POMDP is defined by: states S; actions A; observations Z; transition function T , where T (s, a, s ) denotes Pr(s |s, a); observation function Z, where Z(s, z) is the probability Pr(z|s, a) of observation z in state s after executing a; and reward function R, where R(s, a) is the immediate reward associated with s when executing a. We assume discrete state, action and observation sets and focus on discounted, infinite horizon POMDPs with discount factor 0   < 1.
Policies and value functions for POMDPs are typically defined over belief space B, where a belief state b is a distribution over S capturing an agent's knowledge about the current state of the world. Belief state b can be updated in response to a specific action-observation pair a, z using Bayes rule. We denote the (unnormalized) belief update mapping by T a,z, where T a,z = Pr(s            ij              j |a, si) Pr(z|sj ). A factored POMDP, with exponentially many states, thus gives rise to a belief space of exponential dimensionality.
Policies represented by finite state controllers (FSCs) are defined by a (possibly cyclic) di- rected graph  = N , E , where nodes n  N correspond to stochastic action choices and edges e  E to stochastic transitions. An FSC can be viewed as a policy  = ,  , where action strategy  associates each node n with a distribution over actions (n) = Pr(a|n), and observation strategy  associates each node n and observation z with a distribution over successor nodes (n, z) = Pr(n |n, z) (corresponding to the edge from n labeled with z). The value function V  of FSC  is given by:
 V (n, s) =         Pr(a|n)R(s, a) +          Pr(s |s, a) Pr(z|s , a)         Pr(n |n, z)V (n , s ) (1)

                a                          z                               n

The value V (n, b) of each node n is thus linear w.r.t the belief state; hence the value function of the controller is piecewise-linear and convex. The optimal value function V  often has a large (if not infinite) number of vectors, each corresponding to a different node. The optimal value function V  satisfies Bellman's equation:
                          V (b) = max R(b, a) +             Pr(z|b, a)V (ba)                         (2)                                                                                     z                                         a                                                              z



 max       s.t.    V (n, s) +                             [Pr(a|n)R(s, a) +                    Pr(s |s, a) Pr(z|s , a) Pr(a, n |n, z)V (n , s )], s                       a                                  s ,z                       Pr(a|n) = 1;                    Pr(a, n |n, z) = Pr(a|n), a                  a                               n               Pr(a|n)  0, a;                 Pr(a, n |n, z)  0, a, z


                  Table 1: LP to uniformly improve the value function of a node.



 max                o(s, n) s,n                  s,n       s.t.    V (n, s) +                                                s,n                            [Pr(a|n)R(s, a) +                    Pr(s |s, a) Pr(z|s , a) Pr(a, n |n, z)V (n , s )], s                       a                                  s ,z                       Pr(a|n) = 1;                    Pr(a, n |n, z) = Pr(a|n), a                  a                               n               Pr(a|n)  0, a;                 Pr(a, n |n, z)  0, a, z

Table 2: LP to improve the value function of a node in a non-uniform way according to the steady state occupancy o(s, n).
3      Bounded Policy Iteration
We briefly review the bounded policy iteration (BPI) algorithm (see [14] for details) and describe a simple extension to bias its search toward reachable belief states. BPI incre- mentally constructs an FSC by alternating policy improvement and policy evaluation. Un- like policy iteration [7], this is done by slowly increasing the number of nodes (and value vectors). The policy improvement step greedily improves each node n by optimizing its action and observation strategies by solving the linear program (LP) in Table 1. This LP uniformly maximizes the improvement                              in the value function by optimizing n's distribu- tions Pr(a, n |n, z). The policy evaluation step computes the value function of the current controller by solving Eq. 1. The algorithm monotonically improves the policy until con- vergence to a local optimum, at which point new nodes are introduced to escape the local optimum. BPI is guaranteed to converge to a policy that is optimal at the ""tangent"" belief states while slowly growing the size of the controller [14].
In practice, we often wish to find a policy suitable for a given initial belief state. Since only a small subset of belief space is often reachable, it is generally possible to construct much smaller policies tailored to the reachable region. We now describe a simple way to bias BPI's efforts toward the reachable region. Recall that the LP in Table 1 optimizes the parameters of a node to uniformly improve its value at all belief states. We propose a new LP (Table 2) that weighs the improvement by the (unnormalized) discounted occupancy distribution induced by the current policy. This accounts for belief states reachable for the node by aggregating them together. The (unnormalized) discounted occupancy distribution is given by:
          o(s , n ) = b0(s , n ) +                      o(s, n) Pr(a|n) Pr(z|a, s) Pr(n |n, z)     s , n

                                              s,a,z,n

The LP in Table 2 is obtained by introducing variables s,n for each s, replacing the ob- jective       by               o(s, n)                         s,n                 s,n and replacing         in each constraint by the corresponding s,n. When using the modified LP, BPI naturally tries to improve the policy at the reachable be- lief states before the others. Since the modification ensures that the value function doesn't decrease at any belief state, focusing the efforts on reachable belief states won't decrease policy value at other belief states. Furthermore, though the policy is initially biased toward reachable states, BPI will eventually improve the policy for all belief states.
                                                                                                                                T         ~                             ~                                                                             T                                                                                  T                              T


                                          f    ~                                     b                                        ~                                                    b         b'              b'                                                    ~                        ~                                                    R                        R                                          R                         R

                                               r                         r'

Figure 1: Functional flow of a POMDP (dotted arrows) and a compressed POMDP (solid arrows).
4        Value-Directed Compression
We briefly review the sufficient conditions for a lossless compression of POMDPs [13] and describe a simple new algorithm to obtain good lossy compressions. Belief states constitute a sufficient statistic summarizing all information available to the decision maker (i.e., past actions and observations). However, as long as enough information is available to evaluate the value of each policy, one can still choose the best policy. Since belief states often contain information irrelevant to the estimation of future rewards, one can often compress belief states into some lower-dimensional representation. Let f be a compression function that maps each belief state b into some lower dimensional compressed belief state ~                                                                                                                     b (see Figure 1). Here ~                      b can be viewed as a bottleneck that filters the information contained in b before it is used to estimate future rewards. We desire a compression f such that ~                                                                                                                          b corresponds to the smallest statistic sufficient for accurately predicting the current reward r as well as the next compressed belief state ~                                                                         b (since it captures all the information in b necessary to accurately predict subsequent rewards). Such a compression f exists if we can also find compressed transition dynamics ~                                                              T a,z and a compressed reward function ~                                                                                                                    R such that:                      R = ~                                   R  f and f  T a,z = ~                                                                           T a,z  f a  A, z  Z                      (3)
Given an f , ~                R and ~                           T a,z satisfying Eq. 3, we can evaluate any policy  using the compressed POMDP dynamics to obtain ~                                               V . Since V  = ~                                                                         V f , the compressed POMDP is equivalent to the original.
When restricting f to be linear (represented by matrix F ), we can rewrite Eq. 3
                            R = F ~                                               R and T a,zF = F ~                                                                            T a,z a  A, z  Z                         (4)

That is, the column space of F spans R and is invariant w.r.t. each T a,z. Hence, the columns of the best linear lossless compression mapping F form a basis for the smallest invariant subspace (w.r.t. each T a,z) that spans R, i.e., the Krylov subspace. We can find the columns of F by Krylov iteration: multiplying R by each T a,z until the newly generated vectors are linear combinations of previous ones.1 The dimensionality of the compressed space is equal to the number of columns of F , which is necessarily smaller than or equal to the dimensionality of the original belief space. Once F is found, we can compute ~                                                                                                                     R and each ~          T a,z by solving the system in Eq. 4.
Since linear lossless compressions are not always possible, we can extend the technique of [13] to find good lossy compressions with early stopping of the Krylov iteration. We retain only the vectors that are ""far"" from being linear combinations of prior vectors. For instance, if v is a linear combination of v1, v2, . . . , vn, then there are coefficients c1, c2, . . . , cn s.t. the error ||v -           c                      i         i vi ||2 is zero. Given a threshold                    or some upper bound k on the desired number of columns in F , we run Krylov iteration, retaining only the vectors with an error greater than , or the k vectors with largest error. When F is computed by approximate
 1For numerical stability, one must orthogonalize each vector before multiplying by T a,z.

Krylov iteration, we cannot compute ~                                        R and ~                                               T a,z by solving the linear system in Eq. 4-- due to the lossy nature of the compression, the system is overconstrained. But we can find suitable ~         R and ~                  T a,z by computing a least square approximation, solving:
          F R = F F ~                              R and F T a,zF = F F ~                                                            T a,z a  A, z  Z

While compression is required when the dimensionality of belief space is too large, unfortu- nately, the columns of F have the same dimensionality. Factored POMDPs of exponential dimension can, however, admit practical Krylov iteration if carried out using a compact representation (e.g., DTs or ADDs) to efficiently compute F , ~                                                                 R and each ~                                                                               T a,z.
5    Bounded Policy Iteration with Value-Directed Compression
In principle, any POMDP algorithm can be used to solve the compressed POMDPs pro- duced by VDC. If the compression is lossless and the POMDP algorithm exact, the com- puted policy will be optimal for the original POMDP. In practice, POMDP algorithms are usually approximate and lossless compressions are not always possible, so care must be taken to ensure numerical stability and a policy of high quality for the original POMDP. We now discuss some of the integration issues that arise when combining VDC with BPI.
Since V = F ~               V , maximizing the compressed value vector ~                                                             V of some node n automatically maximizes the value V of n w.r.t. the original POMDP when F is nonnegative; hence it is essential that F be nonnegative. Otherwise, the optimal policy of the compressed POMDP may not be optimal for the original POMDP. Fortunately, when R is nonnegative then F is guaranteed to be nonnegative by the nature of Krylov iteration. If some rewards are negative, we can add a sufficiently large constant to R to make it nonnegative without changing the decision problem.
Since most algorithms, including BPI, compute approximately optimal policies it is also critical to normalize the columns of F . Suppose F has two columns f1 and f2 with L1- lengths 1 and 100, respectively. Since V = F ~                                                   V = ~                                                         v1f1 + ~v2f2, changes in ~v2 have a much greater impact on V than changes in ~                                             v1. Such a difference in sensitivity may bias the search for a good policy to an undesirable region of the belief space, or may even cause the algorithm to return a policy that is far from optimal for the original POMDP despite the fact that it is -optimal for the compressed POMDP.
We note that it is ""safer"" to evaluate policies iteratively by successive approximation rather than solving the system in Eq. 1. By definition, the transition matrices T a,z have eigen- values with magnitude  1. In contrast, lossy compressed transition matrices ~                                                                                    T a,z are not guaranteed to have this property. Hence, solving the system in Eq. 1 may not correspond to policy evaluation. It is thus safer to evaluate policies by successive approximation for lossy compressions.
Finally several algorithms including BPI compute witness belief states to verify the domi- nance of a value vector. Since the compressed belief space ~                                                              B is different from the original belief space B, this must be approached with care. B is a simplex corresponding to the convex hull of the state points. In contrast, since each row vector of F is the compressed version of some state point, ~                                B corresponds to the convex hull of the row vectors of F . When F is non-negative, it is often possible to ignore this difference. For instance, when verifying the dominance of a value vector, if there is a compressed witness ~                                                                                  b, there is al- ways an uncompressed witness b, but not vice-versa. This means that we            can properly identify all dominating value vectors, but we may erroneously classify a dominated vector as dominating. In practice, this doesn't impact the correctness of algorithms such as policy iteration, bounded policy iteration, incremental pruning, witness algorithm, etc. but it will slow them down since they won't be able to prune as many value vectors as possible.
                                                                                               cycle16                                                                                                                    cycle19                                                                                                                          cycle22




                                                         105                                                                                                                        120

                                                         100                                                                                                                                                                                                                                                      130                                                                                                                                                                                         115

                                                          95                                                                                                                                                                                                                                                      125                                                                                                                                                                                         110

                                                          90                      Expected Rewards                                                                                                                               Expected Rewards 105                                                                                                          Expected Rewards 120


                                                         250                                                                                                                        250                                                                                                                           250                                                                       200                                                                120                                                     200                                                                120                                                        200                                                                      120                                                                                                                                   100                                                                              150                                                                                                                                                                             100                                                                                                                                 100                                                                                                                            80                                                                           150                                           80                                                                               150                                                80                                                                                                                      60                                                                                       100                                                                                                                                                       60                                                                                                                                  60                                                                                                               40                                                                                                 100                     40                                                                                                      100                        40                                                                                                         20                                                                                              50                                                                                                                                    20                                                                                                                                20                                                                     # of basis fns                                                                                                                                      50                                                                                                                               50                                                                                                                     # of nodes                                                                 # of basis fns                                  # of nodes                                                                     # of basis fns                                       # of nodes

                                                                                               cycle25                                                                                                                    3legs16                                                                                                                          3legs19




                                                                                                                                                                                    120                                                                                                                           135                                                              150                                                                                                                        115                                                                                                                           130

                                                         145                                                                                                                        110                                                                                                                           125

                                                                                                                                                                                    105                                                                                                                           120                                                              140                                                                                                                                                                                         100                                                                                                                           115                                          Expected Rewards                                                                                                           Expected Rewards                                                                                                              Expected Rewards                                                              135                                                                                                                         95                                                                                                                           110

                                                         250                                                                                                                        250                                                                                                                           250                                                                       200                                                                120                                                     200                                                                120                                                        200                                                                      120                                                                                                                                   100                                                                              150                                                                                                                                                                             100                                                                                                                                 100                                                                                                                            80                                                                           150                                           80                                                                               150                                                80                                                                                                                      60                                                                                       100                                                                                                                                                       60                                                                                                                                  60                                                                                                               40                                                                                                 100                     40                                                                                                      100                        40                                                                                                         20                                                                                              50                                                                                                                                    20                                                                                                                                20                                                                     # of basis fns                                                                                                                                      50                                                                                                                               50                                                                                                                     # of nodes                                                                 # of basis fns                                  # of nodes                                                                     # of basis fns                                       # of nodes

                                                                                               3legs22                                                                                                                    3legs25                                                                                                                         cycle25




                                                         150                                                                                                                                                                                                                                                      12

                                                         145                                                                                                                        160                                                                                                                           10

                                                                                                                                                                                                                                                                                                                   8                                                              140                                                                                                                        155                                                                                                                                                                                                                                                                                                                        6                                                              135                                                                                                                        150                                                                                                                                                                                                                                                                                                                        4                                                              130                                                                                                                        145  Expected Rewards                                                                                                                               Expected Rewards                                                                                                                                                       2                                                                                                                                                                                                                                                                            Time (1000 seconds)                                                              125                                                                                                                        140                                                              250                                                                                                                        250                                                                                250                                                                      200                                                                 120                                                    200                                                                 120                                                      200                                                                        120                                                                                                                                   100                                                                              150                                                                                                                                                                             100                                                                                                                                 100                                                                                                                            80                                                                           150                                           80                                                                              150                                                 80                                                                                                                      60                                                                                       100                                                                                                                                                       60                                                                                                                                 60                                                                                                               40                                                                                                 100                     40                                                                                                     100                        40                                                                                                        20                                                                                              50                                                                                                                                   20                                                                                                                              20                                                                     # of basis fns                                                                                                                                      50                                                                                                                              50                                                                                                                     # of nodes                                                                 # of basis fns                                  # of nodes                                                                   # of basis fns                                        # of nodes

Figure 2: Experimental results for cycle and 3legs network configurations of 16, 19, 22 and 25 machines. The bottom right graph shows the running time of BPI on compressed versions of a cycle network of 25 machines.
                                                                                                                                                                                     3legs                                                                                                                                         cycle                                                                                                                       16                        19                                                               22                      25                    16                                                                   19                         22                         25                                                                     VDCBPI                                          120.9                 137.0                                                          151.0                    164.8                      103.9                                                           121.3                      134.3                       151.4                                                                     heuristic                                       100.6                 118.3                                                          138.3                    152.3                      102.5                                                           117.9                      130.2                       152.3                                                                     doNothing                                       98.4                  112.9                                                          133.5                    147.1                      91.6                                                            105.4                      122.0                       140.1

Table 3: Comparison of the best policies achieved by VDCBPI to the doNothing and heuristic policies.
The above tips work well when VDC is integrated with BPI. We believe they are sufficient to ensure proper integration of VDC with other POMDP algorithms, though we haven't verified this empirically."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/825f9cd5f0390bc77c1fed3c94885c87-Abstract.html,A Generalized Bradley-Terry Model: From Group Competition to Individual Skill,"Tzu-kuo Huang, Chih-jen Lin, Ruby C. Weng",The Bradley-Terry model for paired comparison has been popular in many areas. We propose a generalized version in which paired individual comparisons are extended to paired team comparisons. We introduce a simple algorithm with convergence proofs to solve the model and obtain individual skill. A useful application to multi-class probability estimates using error-correcting codes is demonstrated.
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/8420d359404024567b5aefda1231af24-Abstract.html,Rate- and Phase-coded Autoassociative Memory,"Máté Lengyel, Peter Dayan","Areas of the brain involved in various forms of memory exhibit patterns           of neural activity quite unlike those in canonical computational models.           We show how to use well-founded Bayesian probabilistic autoassociative           recall to derive biologically reasonable neuronal dynamics in recurrently           coupled models, together with appropriate values for parameters such as           the membrane time constant and inhibition. We explicitly treat two cases.           One arises from a standard Hebbian learning rule, and involves activity           patterns that are coded by graded firing rates. The other arises from a           spike timing dependent learning rule, and involves patterns coded by the           phase of spike times relative to a coherent local field potential oscillation.           Our model offers a new and more complete understanding of how neural           dynamics may support autoassociation."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/852c44ddce7e0c7e4c64d86147300831-Abstract.html,Constraining a Bayesian Model of Human Visual Speed Perception,"Alan Stocker, Eero P. Simoncelli","It has been demonstrated that basic aspects of human visual motion per-          ception are qualitatively consistent with a Bayesian estimation frame-          work, where the prior probability distribution on velocity favors slow          speeds. Here, we present a refined probabilistic model that can account          for the typical trial-to-trial variabilities observed in psychophysical speed          perception experiments. We also show that data from such experiments          can be used to constrain both the likelihood and prior functions of the          model. Specifically, we measured matching speeds and thresholds in a          two-alternative forced choice speed discrimination task. Parametric fits          to the data reveal that the likelihood function is well approximated by          a LogNormal distribution with a characteristic contrast-dependent vari-          ance, and that the prior distribution on velocity exhibits significantly          heavier tails than a Gaussian, and approximately follows a power-law          function.
Humans do not perceive visual motion veridically. Various psychophysical experiments have shown that the perceived speed of visual stimuli is affected by stimulus contrast, with low contrast stimuli being perceived to move slower than high contrast ones [1, 2]. Computational models have been suggested that can qualitatively explain these perceptual effects. Commonly, they assume the perception of visual motion to be optimal either within a deterministic framework with a regularization constraint that biases the solution toward zero motion [3, 4], or within a probabilistic framework of Bayesian estimation with a prior that favors slow velocities [5, 6].
The solutions resulting from these two frameworks are similar (and in some cases identi- cal), but the probabilistic framework provides a more principled formulation of the problem in terms of meaningful probabilistic components. Specifically, Bayesian approaches rely on a likelihood function that expresses the relationship between the noisy measurements and the quantity to be estimated, and a prior distribution that expresses the probability of encountering any particular value of that quantity. A probabilistic model can also provide a richer description, by defining a full probability density over the set of possible ""percepts"", rather than just a single value. Numerous analyses of psychophysical experiments have made use of such distributions within the framework of signal detection theory in order to model perceptual behavior [7].
Previous work has shown that an ideal Bayesian observer model based on Gaussian forms
                                            high contrast                                                                           low contrast

         y                                                                                        y                                                        posterior                                                                                                                                                                     likelihood                   y densit                                                                                 y densit                        posterior

                                                                                                                                                              likelihood                               obabilit             prior                                                                                 prior                                                                                                                        obabilit                                           pr                                                                                       pr

                                                                 v^                                                                            v^        a                                                         visual speed                    b                                                visual speed

Figure 1: Bayesian model of visual speed perception. a) For a high contrast stimulus, the likelihood has a narrow width (a high signal-to-noise ratio) and the prior induces only a small shift  of the mean ^                                                                      v of the posterior. b) For a low contrast stimuli, the measurement is noisy, leading to a wider likelihood. The shift  is much larger and the perceived speed lower than under condition (a).
for both likelihood and prior is sufficient to capture the basic qualitative features of global translational motion perception [5, 6]. But the behavior of the resulting model deviates systematically from human perceptual data, most importantly with regard to trial-to-trial variability and the precise form of interaction between contrast and perceived speed. A recent article achieved better fits for the model under the assumption that human contrast perception saturates [8]. In order to advance the theory of Bayesian perception and provide significant constraints on models of neural implementation, it seems essential to constrain quantitatively both the likelihood function and the prior probability distribution. In previous work, the proposed likelihood functions were derived from the brightness constancy con- straint [5, 6] or other generative principles [9]. Also, previous approaches defined the prior distribution based on general assumptions and computational convenience, typically choos- ing a Gaussian with zero mean, although a Laplacian prior has also been suggested [4]. In this paper, we develop a more general form of Bayesian model for speed perception that can account for trial-to-trial variability. We use psychophysical speed discrimination data in order to constrain both the likelihood and the prior function.
1       Probabilistic Model of Visual Speed Perception
1.1         Ideal Bayesian Observer
Assume that an observer wants to obtain an estimate for a variable v based on a measure- ment m that she/he performs. A Bayesian observer ""knows"" that the measurement device is not ideal and therefore, the measurement m is affected by noise. Hence, this observer combines the information gained by the measurement m with a priori knowledge about v. Doing so (and assuming that the prior knowledge is valid), the observer will  on average  perform better in estimating v than just trusting the measurements m. According to Bayes' rule                                                                                     1                                                                              p(v|m) =         p(m|v)p(v)                                                                        (1)                                                                                           the probability of perceiving v given m (posterior) is the product of the likelihood of v for a particular measurements m and the a priori knowledge about the estimated variable v (prior).  is a normalization constant independent of v that ensures that the posterior is a proper probability distribution.
                                                                                    1 Pcum=0.875

                                                            )1^                                                                                              P                                 +                                                            cum=0.5                                                                         > v2^                                                                                  P(v



                                                                                    0                                v2        a                                                   b                                            vmatch vthres

Figure 2: 2AFC speed discrimination experiment. a) Two patches of drifting gratings were displayed simultaneously (motion without movement). The subject was asked to fixate the center cross and decide after the presentation which of the two gratings was moving faster. b) A typical psychometric curve obtained under such paradigm. The dots represent the empirical probability that the subject perceived stimulus2 moving faster than stimulus1. The speed of stimulus1 was fixed while v2 is varied. The point of subjective equality, vmatch, is the value of v2 for which Pcum = 0.5. The threshold velocity vthresh is the velocity for which Pcum = 0.875.
It is important to note that the measurement m is an internal variable of the observer and is not necessarily represented in the same space as v. The likelihood embodies both the mapping from v to m and the noise in this mapping. So far, we assume that there is a monotonic function f (v) : v  vm that maps v into the same space as m (m-space). Doing so allows us to analytically treat m and vm in the same space. We will later propose a suitable form of the mapping function f (v).
An ideal Bayesian observer selects the estimate that minimizes the expected loss, given the posterior and a loss function. We assume a least-squares loss function. Then, the optimal estimate ^                v is the mean of the posterior in Equation (1). It is easy to see why this model of a Bayesian observer is consistent with the fact that perceived speed decreases with con- trast. The width of the likelihood varies inversely with the accuracy of the measurements performed by the observer, which presumably decreases with decreasing contrast due to a decreasing signal-to-noise ratio. As illustrated in Figure 1, the shift in perceived speed towards slow velocities grows with the width of the likelihood, and thus a Bayesian model can qualitatively explain the psychophysical results [1].
1.2         Two Alternative Forced Choice Experiment
We would like to examine perceived speeds under a wide range of conditions in order to constrain a Bayesian model. Unfortunately, perceived speed is an internal variable, and it is not obvious how to design an experiment that would allow subjects to express it directly 1. Perceived speed can only be accessed indirectly by asking the subject to compare the speed of two stimuli. For a given trial, an ideal Bayesian observer in such a two-alternative forced choice (2AFC) experimental paradigm simply decides on the basis of the two trial estimates ^v1 (stimulus1) and ^v2 (stimulus2) which stimulus moves faster. Each estimate ^v is based on a particular measurement m. For a given stimulus with speed v, an ideal Bayesian observer will produce a distribution of estimates p(^                                                           v|v) because m is noisy. Over trials, the observers behavior can be described by classical signal detection theory based on the distributions of the estimates, hence e.g. the probability of perceiving stimulus2 moving
   1Although see [10] for an example of determining and even changing the prior of a Bayesian model for a sensorimotor task, where the estimates are more directly accessible.

faster than stimulus1 is given as the cumulative probability                                                                   ^v2                     Pcum(^                           v2 > ^v1) =              p(^                                                     v2|v2)                p(^                                                                            v1|v1) d^v1 d^v2    (2)                                          0                    0 Pcum describes the full psychometric curve. Figure 2b illustrates the measured psychomet- ric curve and its fit from such an experimental situation.
2    Experimental Methods
We measured matching speeds (Pcum = 0.5) and thresholds (Pcum = 0.875) in a 2AFC speed discrimination task.     Subjects were presented simultaneously with two circular patches of horizontally drifting sine-wave gratings for the duration of one second (Fig- ure 2a). Patches were 3deg in diameter, and were displayed at 6deg eccentricity to either side of a fixation cross. The stimuli had an identical spatial frequency of 1.5 cycle/deg. One stimulus was considered to be the reference stimulus having one of two different contrast values (c1=[0.075 0.5]) and one of five different speed values (u1=[1 2 4 8 12] deg/sec) while the second stimulus (test) had one of five different contrast values (c2=[0.05 0.1 0.2 0.4 0.8]) and a varying speed that was determined by an interleaved staircase procedure. For each condition there were 96 trials. Conditions were randomly interleaved, including a random choice of stimulus identity (test vs. reference) and motion direction (right vs. left). Subjects were asked to fixate during stimulus presentation and select the faster mov- ing stimulus. The threshold experiment differed only in that auditory feedback was given to indicate the correctness of their decision. This did not change the outcome of the ex- periment but increased significantly the quality of the data and thus reduced the number of trials needed."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/85353d3b2f39b9c9b5ee3576578c04b7-Abstract.html,Efficient Kernel Machines Using the Improved Fast Gauss Transform,"Changjiang Yang, Ramani Duraiswami, Larry S. Davis","The computation and memory required for kernel machines with N train-          ing samples is at least O(N 2). Such a complexity is significant even for          moderate size problems and is prohibitive for large datasets. We present          an approximation technique based on the improved fast Gauss transform          to reduce the computation to O(N ). We also give an error bound for the          approximation, and provide experimental results on the UCI datasets."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/868b7df964b1af24c8c0a9e43a330c6a-Abstract.html,Responding to Modalities with Different Latencies,"Fredrik Bissmarck, Hiroyuki Nakahara, Kenji Doya, Okihide Hikosaka","Motor control depends on sensory feedback in multiple modalities with different latencies. In this paper we consider within the framework of re- inforcement learning how different sensory modalities can be combined and selected for real-time, optimal movement control. We propose an actor-critic architecture with multiple modules, whose output are com- bined using a softmax function. We tested our architecture in a simu- lation of a sequential reaching task. Reaching was initially guided by visual feedback with a long latency. Our learning scheme allowed the agent to utilize the somatosensory feedback with shorter latency when the hand is near the experienced trajectory. In simulations with different latencies for visual and somatosensory feedback, we found that the agent depended more on feedback with shorter latency."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/86ecfcbc1e9f1ae5ee2d71910877da36-Abstract.html,Two-Dimensional Linear Discriminant Analysis,"Jieping Ye, Ravi Janardan, Qi Li","Linear Discriminant Analysis (LDA) is a well-known scheme for feature extraction and dimension reduction. It has been used widely in many ap- plications involving high-dimensional data, such as face recognition and image retrieval. An intrinsic limitation of classical LDA is the so-called singularity problem, that is, it fails when all scatter matrices are singu- lar. A well-known approach to deal with the singularity problem is to apply an intermediate dimension reduction stage using Principal Com- ponent Analysis (PCA) before LDA. The algorithm, called PCA+LDA, is used widely in face recognition. However, PCA+LDA has high costs in time and space, due to the need for an eigen-decomposition involving the scatter matrices. In this paper, we propose a novel LDA algorithm, namely 2DLDA, which stands for 2-Dimensional Linear Discriminant Analysis. 2DLDA over- comes the singularity problem implicitly, while achieving efﬁciency. The key difference between 2DLDA and classical LDA lies in the model for data representation. Classical LDA works with vectorized representa- tions of data, while the 2DLDA algorithm works with data in matrix representation. To further reduce the dimension by 2DLDA, the combi- nation of 2DLDA and classical LDA, namely 2DLDA+LDA, is studied, where LDA is preceded by 2DLDA. The proposed algorithms are ap- plied on face recognition and compared with PCA+LDA. Experiments show that 2DLDA and 2DLDA+LDA achieve competitive recognition accuracy, while being much more efﬁcient."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/880610aa9f9de9ea7c545169c716f477-Abstract.html,Assignment of Multiplicative Mixtures in Natural Images,"Odelia Schwartz, Terrence J. Sejnowski, Peter Dayan","In the analysis of natural images, Gaussian scale mixtures (GSM) have been used to account for the statistics of (cid:2)lter responses, and to inspire hi- erarchical cortical representational learning schemes. GSMs pose a crit- ical assignment problem, working out which (cid:2)lter responses were gen- erated by a common multiplicative factor. We present a new approach to solving this assignment problem through a probabilistic extension to the basic GSM, and show how to perform inference in the model using Gibbs sampling. We demonstrate the ef(cid:2)cacy of the approach on both synthetic and image data.
Understanding the statistical structure of natural images is an important goal for visual neuroscience. Neural representations in early cortical areas decompose images (and likely other sensory inputs) in a way that is sensitive to sophisticated aspects of their probabilistic structure. This structure also plays a key role in methods for image processing and coding. A striking aspect of natural images that has re(cid:3)ections in both top-down and bottom-up modeling is coordination across nearby locations, scales, and orientations. From a top- down perspective, this structure has been modeled using what is known as a Gaussian Scale Mixture model (GSM).1(cid:150)3 GSMs involve a multi-dimensional Gaussian (each di- mension of which captures local structure as in a linear (cid:2)lter), multiplied by a spatialized collection of common hidden scale variables or mixer variables(cid:3) (which capture the coordi- nation). GSMs have wide implications in theories of cortical receptive (cid:2)eld development, eg the comprehensive bubbles framework of Hyv¤arinen.4 The mixer variables provide the top-down account of two bottom-up characteristics of natural image statistics, namely the ‘bowtie’ statistical dependency,5,6 and the fact that the marginal distributions of receptive (cid:2)eld-like (cid:2)lters have high kurtosis.7,8 In hindsight, these ideas also bear a close relation- ship with Ruderman and Bialek’s multiplicative bottom-up image analysis framework9 and statistical models for divisive gain control.6 Coordinated structure has also been addressed in other image work,10(cid:150)14 and in other domains such as speech15 and (cid:2)nance.16 Many approaches to the unsupervised speci(cid:2)cation of representations in early cortical areas rely on the coordinated structure.17(cid:150)21 The idea is to learn linear (cid:2)lters (eg modeling simple cells as in22,23), and then, based on the coordination, to (cid:2)nd combinations of these (perhaps non-linearly transformed) as a way of (cid:2)nding higher order (cid:2)lters (eg complex cells). One critical facet whose speci(cid:2)cation from data is not obvious is the neighborhood arrangement, ie which linear (cid:2)lters share which mixer variables.
(cid:3)Mixer variables are also called mutlipliers, but are unrelated to the scales of a wavelet.
Here, we suggest a method for (cid:2)nding the neighborhood based on Bayesian inference of the GSM random variables. In section 1, we consider estimating these components based on information from different-sized neighborhoods and show the modes of failure when inference is too local or too global. Based on these observations, in section 2 we propose an extension to the GSM generative model, in which the mixer variables can overlap prob- abilistically. We solve the neighborhood assignment problem using Gibbs sampling, and demonstrate the technique on synthetic data. In section 3, we apply the technique to image data.
1 GSM inference of Gaussian and mixer variables
In a simple, n-dimensional, version of a GSM, (cid:2)lter responses l are synthesized y by mul- tiplying an n-dimensional Gaussian with values g = fg1 : : : gng, by a common mixer variable v. (1) We assume g are uncorrelated ((cid:27)2 along diagonal of the covariance matrix). For the ana- lytical calculations, we assume that v has a Rayleigh distribution:"
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/88fee0421317424e4469f33a48f50cb0-Abstract.html,Convergence and No-Regret in Multiagent Learning,Michael Bowling,"Learning in a multiagent system is a challenging problem due to two key          factors. First, if other agents are simultaneously learning then the envi-          ronment is no longer stationary, thus undermining convergence guaran-          tees. Second, learning is often susceptible to deception, where the other          agents may be able to exploit a learner's particular dynamics. In the          worst case, this could result in poorer performance than if the agent was          not learning at all. These challenges are identifiable in the two most com-          mon evaluation criteria for multiagent learning algorithms: convergence          and regret. Algorithms focusing on convergence or regret in isolation          are numerous. In this paper, we seek to address both criteria in a single          algorithm by introducing GIGA-WoLF, a learning algorithm for normal-          form games. We prove the algorithm guarantees at most zero average          regret, while demonstrating the algorithm converges in many situations          of self-play. We prove convergence in a limited setting and give empir-          ical results in a wider variety of situations. These results also suggest          a third new learning criterion combining convergence and regret, which          we call negative non-convergence regret (NNR)."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/8bd39eae38511daad6152e84545e504d-Abstract.html,Learning Efficient Auditory Codes Using Spikes Predicts Cochlear Filters,"Evan C. Smith, Michael S. Lewicki","The representation of acoustic signals at the cochlear nerve must serve a           wide range of auditory tasks that require exquisite sensitivity in both time           and frequency. Lewicki (2002) demonstrated that many of the filtering           properties of the cochlea could be explained in terms of efficient coding           of natural sounds. This model, however, did not account for properties           such as phase-locking or how sound could be encoded in terms of action           potentials. Here, we extend this theoretical approach with algorithm for           learning efficient auditory codes using a spiking population code. Here,           we propose an algorithm for learning efficient auditory codes using a           theoretical model for coding sound in terms of spikes. In this model,           each spike encodes the precise time position and magnitude of a local-           ized, time varying kernel function. By adapting the kernel functions to           the statistics natural sounds, we show that, compared to conventional           signal representations, the spike code achieves far greater coding effi-           ciency. Furthermore, the inferred kernels show both striking similarities           to measured cochlear filters and a similar bandwidth versus frequency           dependence."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/8c59fd6fbe0e9793ec2b27971221cace-Abstract.html,Active Learning for Anomaly and Rare-Category Detection,"Dan Pelleg, Andrew W. Moore","We introduce a novel active-learning scenario in which a user wants to          work with a learning algorithm to identify useful anomalies. These are          distinguished from the traditional statistical definition of anomalies as          outliers or merely ill-modeled points. Our distinction is that the useful-          ness of anomalies is categorized subjectively by the user. We make two          additional assumptions. First, there exist extremely few useful anoma-          lies to be hunted down within a massive dataset. Second, both useful          and useless anomalies may sometimes exist within tiny classes of similar          anomalies. The challenge is thus to identify ""rare category"" records in an          unlabeled noisy set with help (in the form of class labels) from a human          expert who has a small budget of datapoints that they are prepared to cat-          egorize. We propose a technique to meet this challenge, which assumes          a mixture model fit to the data, but otherwise makes no assumptions on          the particular form of the mixture components. This property promises          wide applicability in real-life scenarios and for various statistical mod-          els. We give an overview of several alternative methods, highlighting          their strengths and weaknesses, and conclude with a detailed empirical          analysis. We show that our method can quickly zoom in on an anomaly          set containing a few tens of points in a dataset of hundreds of thousands."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/8cff9bf6694dccfc3b6a613d05d51d16-Abstract.html,A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound,"Dori Peleg, Ron Meir","A novel linear feature selection algorithm is presented based on the          global minimization of a data-dependent generalization error bound.          Feature selection and scaling algorithms often lead to non-convex opti-          mization problems, which in many previous approaches were addressed          through gradient descent procedures that can only guarantee convergence          to a local minimum. We propose an alternative approach, whereby the          global solution of the non-convex optimization problem is derived via          an equivalent optimization problem. Moreover, the convex optimization          task is reduced to a conic quadratic programming problem for which effi-          cient solvers are available. Highly competitive numerical results on both          artificial and real-world data sets are reported."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/8dc5983b8c4ef1d8fcd5f325f9a65511-Abstract.html,Semi-supervised Learning with Penalized Probabilistic Clustering,"Zhengdong Lu, Todd K. Leen","While clustering is usually an unsupervised operation, there are circum-          stances in which we believe (with varying degrees of certainty) that items          A and B should be assigned to the same cluster, while items A and C          should not. We would like such pairwise relations to influence cluster          assignments of out-of-sample data in a manner consistent with the prior          knowledge expressed in the training set. Our starting point is proba-          bilistic clustering based on Gaussian mixture models (GMM) of the data          distribution. We express clustering preferences in the prior distribution          over assignments of data points to clusters. This prior penalizes cluster          assignments according to the degree with which they violate the prefer-          ences. We fit the model parameters with EM. Experiments on a variety          of data sets show that PPC can consistently improve clustering results."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/8e065119c74efe3a47aec8796964cf8b-Abstract.html,A Direct Formulation for Sparse PCA Using Semidefinite Programming,"Alexandre D'aspremont, Laurent E. Ghaoui, Michael I. Jordan, Gert R. Lanckriet","We examine the problem of approximating, in the Frobenius-norm sense,          a positive, semidefinite symmetric matrix by a rank-one matrix, with an          upper bound on the cardinality of its eigenvector. The problem arises          in the decomposition of a covariance matrix into sparse factors, and has          wide applications ranging from biology to finance. We use a modifica-          tion of the classical variational representation of the largest eigenvalue          of a symmetric matrix, where cardinality is constrained, and derive a          semidefinite programming based relaxation for our problem."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/8e68c3c7bf14ad0bcaba52babfa470bd-Abstract.html,Stable adaptive control with online learning,"H. J. Kim, Andrew Y. Ng","Learning algorithms have enjoyed numerous successes in robotic control          tasks. In problems with time-varying dynamics, online learning methods          have also proved to be a powerful tool for automatically tracking and/or          adapting to the changing circumstances. However, for safety-critical ap-          plications such as airplane flight, the adoption of these algorithms has          been significantly hampered by their lack of safety, such as ""stability,""          guarantees. Rather than trying to show difficult, a priori, stability guar-          antees for specific learning methods, in this paper we propose a method          for ""monitoring"" the controllers suggested by the learning algorithm on-          line, and rejecting controllers leading to instability. We prove that even if          an arbitrary online learning method is used with our algorithm to control          a linear dynamical system, the resulting system is stable."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/8f125da0b3432ed853c0b6f7ee5aaa6b-Abstract.html,The Rescorla-Wagner Algorithm and Maximum Likelihood Estimation of Causal Parameters,Alan L. Yuille,"This paper analyzes generalization of the classic Rescorla-Wagner (R-          W) learning algorithm and studies their relationship to Maximum Like-          lihood estimation of causal parameters. We prove that the parameters          of two popular causal models, P and P C, can be learnt by the same          generalized linear Rescorla-Wagner (GLRW) algorithm provided gener-          icity conditions apply. We characterize the fixed points of these GLRW          algorithms and calculate the fluctuations about them, assuming that the          input is a set of i.i.d. samples from a fixed (unknown) distribution. We          describe how to determine convergence conditions and calculate conver-          gence rates for the GLRW algorithms under these conditions."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/908a6f6a6c131a850ecb0e3f11b08189-Abstract.html,Contextual Models for Object Detection Using Boosted Random Fields,"Antonio Torralba, Kevin P. Murphy, William T. Freeman","We seek to both detect and segment objects in images. To exploit both lo-          cal image data as well as contextual information, we introduce Boosted          Random Fields (BRFs), which uses Boosting to learn the graph struc-          ture and local evidence of a conditional random field (CRF). The graph          structure is learned by assembling graph fragments in an additive model.          The connections between individual pixels are not very informative, but          by using dense graphs, we can pool information from large regions of          the image; dense models also support efficient inference. We show how          contextual information from other objects can improve detection perfor-          mance, both in terms of accuracy and speed, by using a computational          cascade. We apply our system to detect stuff and things in office and          street scenes. 1    Introduction
Our long-term goal is to build a vision system that can examine an image and describe what objects are in it, and where. In many images, such as Fig. 5(a), objects of interest, such as the keyboard or mouse, are so small that they are impossible to detect just by using local features. Seeing a blob next to a keyboard, humans can infer it is likely to be a mouse; we want to give a computer the same abilities.
There are several pieces of related work. Murphy et al [9] used global scene context to help object recognition, but did not model relationships between objects. Fink and Perona [4] exploited local dependencies in a boosting framework, but did not allow for multiple rounds of communication between correlated objects. He et al [6] do not model connections between objects directly, but rather they induce such correlations indirectly, via a bank of hidden variables, using a ""restricted Boltzmann machine"" architecture.
In this paper, we exploit contextual correlations between the object classes by introducing Boosted Random Fields (BRFs). Boosted random fields build on both boosting [5, 10] and conditional random fields (CRFs) [8, 7, 6]. Boosting is a simple way of sequentially constructing ""strong"" classifiers from ""weak"" components, and has been used for single- class object detection with great success [12]. Dietterich et al [3] combine boosting and 1D CRFs, but they only consider the problem of learning the local evidence potentials; we consider the much harder problem of learning the structure of a 2D CRF.
Standard applications of MRFs/ CRFs to images [7] assume a 4-nearest neighbor grid structure. While successful in low-level vision, this structure will fail in capturing im- portant long distance dependencies between whole regions and across classes. We propose a method for learning densely connected random fields with long range connections. The
topology of these connections is chosen by a weak learner which has access to a library of graph fragments, derived from patches of labeled training images, which reflect typical spatial arrangments of objects (similar to the segmentation fragments in [2]). At each round of the learning algorithm, we add more connections from other locations in the image and from other classes (detectors). The connections are assumed to be spatially invariant, which means this update can be performed using convolution followed by a sigmoid nonlinearity. The resulting architecture is similar to a convolutional neural network, although we used a stagewise training procedure, which is much faster than back propagation.
In addition to recognizing things, such as cars and people, we are also interested in recog- nizing spatially extended ""stuff"" [1], such as roads and buildings. The traditional sliding window approach to object detection does not work well for detecting ""stuff"". Instead, we combine object detection and image segmentation (c.f., [2]) by labeling every pixel in the image. We do not rely on a bottom-up image segmentation algorithm, which can be fragile without top-down guidance.
2    Learning potentials and graph structure
A conditional random field (CRF) is a distribution of the form                                                      1                                P (S|x) =                                                                                                    Z                i(Si)                 i,j (Si, Sj )                                                             i                  jNi
where x is the input (e.g., image), Ni are the neighbors of node i, and Si are labels. We have assumed pairwise potentials for notational simplicity. Our goal is to learn the local evidence potentials, i, the compatibility potentials , and the set of neighbors Ni.
We propose the following simple approximation: use belief propagation (BP) to estimate the marginals, P (Si|x), and then use boosting to maximize the likelihood of each node's training data with respect to i and .
In more detail, the algorithm is as follows. At iteration t, the goal is to minimize the negative log-likelihood of the training data. As in [11], we consider the per-label loss (i.e., we use marginal probabilities), as opposed to requiring that the joint labeling be correct (as in Viterbi decoding). Hence the cost function to be minimized is
  Jt =         Jti = -                     bti,m(Si,m) = -                         bti,m(+1)Si,mbti,m(-1)1-Si,m                  (1)               i                m         i                                m      i

where Si,m  {-1, +1} is the true label for pixel i in training case m, Si,m = (Si,m + 1)/2  {0, 1} is just a relabeling, and bti,m = [P (Si = -1|xm, t), P (Si = 1|xm, t)] is the belief state at node i given input image xm after t iterations of the algorithm.
The belief at node i is given by the following (dropping the dependence on case m) bti(1)  ti(1) Mti(1) where Mti is the product of all the messages coming into i from all its neighbors at time t and where the message that k sends to i is given by                                                                                                                      bt (s      M t+1(1) =               t+1 (1)                          t+1 (1) =                                              k    k)       i                                                                                                                                (2)                                ki                               ki                                  k,i(sk, 1)    t         (sk)                      kN                                                                                             ik                           i                                                     sk{-1,+1}
where k,i is the compatility between nodes k and i. If we assume that the local potentials have the form t                         /2           /2                    i(si) = [eF t                                     i          ; e-F ti ], where F ti is some function of the input data, then:               bti(+1) = (F ti + Gti),                            Gti = log Mti(+1) - log Mti(-1)                                      (3) where (u) = 1/(1 + e-u) is the sigmoid function. Hence each term in Eq. 1 simplifies to a cost function similar to that used in boosting:
                   log Jt                                                          +Gt            )                                                                                                i,m                                 i =                 log 1 + e-Si,m(F ti,m                                  .                           (4)                                                m



    1. Input: a set of labeled pairs {xi,m; Si,m}, bound T            Output: Local evidence functions f ti(x) and message update functions gti(bN ).                                                                                                                                                           i

    2. Initialize: bt=0                           i,m = 0; F t=0                                                  i,m = 0; Gt=0                                                                         i,m = 0

    3. For t=1..T.

            (a) Fit local potential fi(xi,m) by weighted LS to                                                           Y t                                                     +Gt      )                                                                                                                     i,m                                                               i,m = Si,m(1 + e-Si,m(F t                                                                                                              i                  )

            (b) .Fit compatibilities gti(bt-1 ) to Y t                                                          N                        i,m by weighted LS.                                                            i ,m

            (c) Compute local potential F t                                                                 i,m = F t-1 + f t                                                                                   i,m           i (xi,m)                 (d) Compute compatibilities Gti,m =                                     t      gn            )                                                                                         n=1    i (bt-1                                                                                                      Ni,m

            (e) Update the beliefs bti,m = (F ti,m + Gti,m)

            (f) Update weights wt+1 = bt                                                   i,m          i,m(-1) bt                                                                                        i,m(+1)


                                             Figure 1: BRF training algorithm.

We assume that the graph is very densely connected so that the information that one single node sends to another is so small that we can make the approximation  t+1 (+1)/ t+1 (-1)  1. (This is a reasonable approximation in the case of images,  ki               ki where each node represents a single pixel; only when the influence of many pixels is taken into account will the messages become informative.) Hence
                                                                                                                                 bt      (s                                                                                                                                       k,m          k )                            M t+1(+1)                                                                                                                                                    s                       k,i(sk, +1) t                     (s          Gt+1 = log               i                 =            log                   k [-1,+1]                                     i            k )                                                                                                                                       k            i                                                                                                                                                          (5)                            M t+1(-1)                                                                                                 bt      (sk)                                   i                                                                                                   k,m                                                           k                                                                                                                             s                       k,i(sk, -1)                                                                                        k [-1,+1]                                    t      (s                                                                                                                                       i            k )                                                                                                                                       k

                                                                                                 k,i(sk, +1) bt                        (s                                                                                                                                      k,m           k)                                                                 log              sk[-1,+1]                                                                          (6)                                                                                                      k,i(sk, -1) bt                        (sk)                                                           k                       sk[-1,+1]                                         k,m

With this simplification, Gt+1                                                                                                                                  (bt                                             i       is now a non-linear function of the beliefs Gt+1                                                                                                                                                           i      m) at iteration t. Therefore, We can write the beliefs at iteration t as a function of the local evidences and the beliefs at time t - 1: bti(+1) = (F ti(xi,m) + Gti(bt-1                                                                                                                                       m )). The key idea behind BRFs is to use boosting to learn the G functions, which approximately implement message passing in densely connected graphs. We explain this in more detail below.
2.1     Learning local evidence potentials
Defining F ti(xi,m) = F t-1(x                                        i          i,m) + f t                                                                        i (xi,m) as an additive model, where xi,m are the features of training sample m at node i, we can learn this function in a stagewise fashion by optimizing the second order Taylor expansion of Eq. 4 wrt f ti, as in logitBoost [5]:
                 arg min log Jti  arg min                                     wti,m(Y ti,m - fti(xi,m))2                                                         (7)                            f t                                   f t                             i                                     i          m

where Y t                                                +Gt           )                                                                i,m           i,m = Si,m(1+e-Si,m(F t                                                     i                       ). In the case that the weak learner is a ""regression stump"", fi(x) = ah(x)+b, we can find the optimal a, b by solving a weighted least squares problem, with weights wti,m = bti(-1) bti(+1); we can find the best basis function h(x) by searching over all elements of a dictionary.
2.2     Learning compatibility potentials and graph structure
In this section, we discuss how to learn the compatibility functions ij, and hence the structure of the graph. Instead of learning the compatibility functions ij, we propose to
    1. Input: a set of inputs {xi,m} and functions f ti, gti            Output: Set of beliefs bi,m and MAP estimates Si,m.

    2. Initialize: bt=0                        i,m = 0; F t=0                                        i,m = 0; Gt=0                                                                i,m = 0

    3. From t = 1 to T , repeat

        (a) Update local evidences F t                                                         i,m = F t-1 + f t                                                                      i,m           i (xi,m)             (b) Update compatibilities Gti,m =                        t      gn              )                                                                       n=1    i (bt-1                                                                                      Ni,m

        (c) Compute current beliefs bti,m = (F ti,m + Gti,m)

    4. Output classification is Si,m =  bti,m > 0.5

                            Figure 2: BRF run-time inference algorithm.

learn directly the function Gt+1                                        i         . We propose to use an additive model for Gt+1                                                                                                                          i    as we did for learning F : Gt+1 =                      t       gn                                i,m               n=1 i (btm), where btm is a vector with the beliefs of all nodes in the graph at iteration t for the training sample m. The weak learners gn                                                                                                                         i (btm) can be regression stumps with the form gn                                                          i (btm) = a(w               btm > ) + b, where a, b,  are the parameters of the regression stump, and wi is a set of weights selected from a dictionary. In the case of a graph with weak and almost symmetrical connections (which holds if (s1, s2)  1, for all (s1, s2), which implies the messages are not very informative) we can further simplify the function Gt+1                                             i          by approximating it as a linear function of the beliefs:
                                  Gt+1 =                                                            i,m                       k,i btk,m(+1) + k,i                                           (8)                                                         kNi

This step reduces the computational cost. The weak learners gn                                                                                                   i (btm) will also be linear functions. Hence the belief update simplifies to bt+1(+1) = (                                                                             i,m                   i btm + i + F t                                                                                                                         i,m), which is similar to the mean-field update equations. The neighborhood Ni over which we sum incoming messages is determined by the graph structure, which is encoded in the non-zero values of i. Each weak learner gn                                                       i will compute a weighted combination of the beliefs of the some subset of the nodes; this subset may change from iteration to iteration, and can be quite large. At iteration t, we choose the weak learner gti so as to minimize
                                                                                                t-1             log Jt                                                                  +gt(bt-1)+             gn(bt-1))                                                                                        i     m              i    m                   i (bt-1) = -                   log 1 + e-Si,m(F ti,m                              n=1                                        m

which reduces to a weighted least squares problem similar to Eq. 7. See Fig. 1 for the pseudo-code for the complete learning algorithm, and Fig. 2 for the pseudo-code for run- time inference.
3      BRFs for multiclass object detection and segmentation
With the BRF training algorithm in hand, we describe our approach for multiclass object detection and region-labeling using densely connected BRFs.
3.1    Weak learners for detecting stuff and things
The square sliding window approach does not provide a natural way of working with irreg- ular objects. Using region labeling as an image representation allows dealing with irregular and extended objects (buildings, bookshelf, road, ...). Extended stuff [1] may be a very important source of contextual information for other objects.
 (a) Examples from the dictionary of about 2000 patches and masks, Ux,y, Vx,y.



                   (b) Examples from the dictionary of 30 graphs, Wx,y,c.               f t=0              f t=1             f t=2                        F                       S                                  +                +                        ... =                             put                         thu                                                                                                                     utO                         Tr                         (c) Example feedforward segmentation for screens.

Figure 3: Examples of patches from the dictionary and an example of the segmentation obtained using boosting trained with patches from (a).
The weak learners we use for the local evidence potentials are based on the segmentation fragments proposed in [2]. Specifically, we create a dictionary of about 2000 image patches U , chosen at random (but overlapping each object), plus a corresponding set of binary (in- class/ out-of-class) image masks, V : see Fig. 3(a). At each round t, for each class c, and for each dictionary entry, we construct the following weak learner, whose output is a binary matrix of the same size as the image I:                                           v(I) = ((I  U ) > )  V > 0                                                                                (9) where  represents normalized cross-correlation and  represents convolution. The in- tuition behind this is that I  U will produce peaks at image locations that contain this patch/template, and then convolving with V will superimpose the segmentation mask on top of the peaks. As a function of the threshold , the feature will behave more as a template detector (  1) or as a texture descriptor ( << 1).
To be able to detect objects at multiple scales, we first downsample the image to scale , compute v(I  ), and then upsample the result. The final weak learner does this for multiple scales, ORs all the results together, and then takes a linear transformation.                                          f (I) =  ([v(I  )  ]) +                                                                              (10) Fig. 3(c) shows an example of segmentation obtained by using boosting without context. The weak learners we use for the compatibility functions have a similar form:                                                                  C                                          gc(b) =                     bc  Wc           +                                                          (11)                                                             c=1
where bc is the image formed by the beliefs at all pixels for class c. This convolution corresponds to eq. 8 in which the node i is one pixel x, y of class c. The binary kernels (graph fragments) W define, for each node x, y of object class c, all the nodes from which it will receive messages. These kernels are chosen by sampling patches of various sizes from the labeling of images from the training set. This allows generating complicated patterns of connectivity that reflect the statistics of object co-occurrences in the training set. The overall incoming message is given by adding the kernels obtained at each boosting round. (This is the key difference from mutual boosting [4], where the incoming message is just the output of a single weak learner; thus, in mutual boosting, previously learned inter-class connections are only used once.) Although it would seem to take O(t) time to compute Gt, we can precompute a single equivalent kernel W , so at runtime the overall complexity is still linear in the number of boosting rounds, O(T ).                          C                  t                                              C           Gtx,y,c =             bc              nW n                                                        c              +         ndef                                                                                      =            b                              +                                                                                                        c  W                                                                                                                             c                         c=1               n=1                             n              c=1
                                            car    car    building car     road car                              Road                                                                        F                                                                                                                       b=(F+G)


                          Car             car building building building road building



                        Building  x                                                                                                      G                                                car road       building road    road road

                              y                                                             c) A car out of context        a) Incoming messages                                                                   (outside 3rd floor windows)           to a car node.                      b) Compatibilities (W').                                 is less of a car.

                                   t=1             t=2             t=4           t=20      t=40           Final labeling                        b(car)



                    S(all)

d) Evolution of the beliefs for the car nodes (b) and labeling (S) for road, building, car.
       Figure 4: Street scene. The BRF is trained to detect cars, buildings and the road.

In Fig. 4(a-b), we show the structures of the graph and the weights W  defined by GT for a BRF trained to detect cars, buildings and roads in street scenes.
3.2     Learning and inference
For training we used a labeled dataset of office and street scenes with about 100 images in each set. During the training, in the first 5 rounds we only update the local potentials, to allow local evidence to accrue. After the 5th iteration we start updating also the compatibil- ity functions. At each round, we update only the local potential and compatibility function associated with a single object class that reduces the most the multiclass cost. This allows objects that need many features to have more complicated local potentials.
The algorithm learns to first detect easy (and large) objects, since these reduce the error of all classes the fastest. The easy-to-detect objects can then pass information to the harder ones. For instance, in office scenes, the system first detects screens, then keyboards, and finally computer mice. Fig. 5 illustrates this behavior on the test set. A similar behavior is obtained for the car detector (Fig. 4(d)). The detection of building and road provides strong constraints for the locations of the car.
3.3     Cascade of classifiers with BRFs
The BRF can be turned into a cascade [12] by thresholding the beliefs. Computations can then be reduced by doing the convolutions (required for computing f and g) only in pixels that are still candidates for the presence of the target. At each round we update a binary rejection mask for each object class, Rtx,y,c, by thresholding the beliefs at round t: Rtx,y,c = Rt-1               x,y,c (btx,y,c > tc). A pixel in the rejection mask is set to zero when we can decide that the object is not present (when btx,y,c is below the threshold tc  0), and it is set to 1 when more processing is required. The threshold tc is chosen so that the percentage of missed detections is below a predefined level (we use 1%). Similarity we can define a detection mask that will indicate pixels in which we decide the object is present. The mask is then used for computing the features v(I) and messages G by applying the convolutions only on the pixels not yet classified. We can denote those operators as R and R. This
                                    Input image           screen               mouse              Ground truth                  Output labeling





                                                                   keyboard




                               t=5                      t=10                     t=15                    t=25                     t=50                                              b (screen)              b (screen)               b (screen)              b (screen)                 b (screen)                               F

 G

                                       b (keyboard)             b (keyboard)             b (keyboard)            b (keyboard)              b (keyboard)                               F

 G

                                         b (mouse)               b (mouse)                b (mouse)               b (mouse)                  b (mouse)                               F

 G

                               1      ROC                                     Screen

                                                                                                                                          Boosting                                            BRF                                               Mouse             a under                                        Keyboard                         re                                                                                                                    Iteration (t)                               A 0.5 t=0                                            t=20                                                                t=50

Figure 5: Top. In this desk scene, it is easy to identify objects like the screen, keyboard and mouse, even though the local information is sometimes insufficient. Middle: the evolution of the beliefs (b and F and G) during detection for a test image. Bottom. The graph bellow shows the average evolution of the area under the ROC for the three objects on 120 test images.
results in a more efficient classifier with only a slight decrease of performance. In Fig. 6 we compare the reduction of the search space when implementing a cascade using independent boosting (which reduces to Viola and Jones [12]), and when using BRF's. We see that for objects for which context is the main source of information, like the mouse, the reduction in search space is much more dramatic using BRFs than using boosting alone.
4                                  Conclusion
The proposed BRF algorithm combines boosting and CRF's, providing an algorithm that is easy for both training and inference. We have demonstrated object detection in cluttered scenes by exploiting contextual relationships between objects. The BRF algorithm is com- putationally efficient and provides a natural extension of the cascade of classifiers by inte- grating evidence from other objects in order to quickly reject certain image regions. The BRF's densely connected graphs, which efficiently collect information over large image regions, provide an alternative framework to nearest-neighbor grids for vision problems."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/921c2dc40d0b979c2910298d2f880152-Abstract.html,An Auditory Paradigm for Brain-Computer Interfaces,"N. J. Hill, Thomas N. Lal, Karin Bierig, Niels Birbaumer, Bernhard Schölkopf","Motivated by the particular problems involved in communicating         with ""locked-in"" paralysed patients, we aim to develop a brain-         computer interface that uses auditory stimuli.            We describe a         paradigm that allows a user to make a binary decision by focusing         attention on one of two concurrent auditory stimulus sequences.         Using Support Vector Machine classification and Recursive Chan-         nel Elimination on the independent components of averaged event-         related potentials, we show that an untrained user's EEG data can         be classified with an encouragingly high level of accuracy. This         suggests that it is possible for users to modulate EEG signals in a         single trial by the conscious direction of attention, well enough to         be useful in BCI."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/92426b262d11b0ade77387cf8416e153-Abstract.html,Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms,"Nicolò Cesa-bianchi, Claudio Gentile, Luca Zaniboni","We provide a worst-case analysis of selective sampling algorithms for learning linear threshold functions. The algorithms considered in this paper are Perceptron-like algorithms, i.e., algorithms which can be efﬁ- ciently run in any reproducing kernel Hilbert space. Our algorithms ex- ploit a simple margin-based randomized rule to decide whether to query the current label. We obtain selective sampling algorithms achieving on average the same bounds as those proven for their deterministic coun- terparts, but using much fewer labels. We complement our theoretical ﬁndings with an empirical comparison on two text categorization tasks. The outcome of these experiments is largely predicted by our theoreti- cal results: Our selective sampling algorithms tend to perform as good as the algorithms receiving the true label after each classiﬁcation, while observing in practice substantially fewer labels."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/92a0e7a415d64ebafcb16a8ca817cde4-Abstract.html,Markov Networks for Detecting Overalpping Elements in Sequence Data,"Mark Craven, Joseph Bockhorst","Many sequential prediction tasks involve locating instances of pat-           terns in sequences. Generative probabilistic language models, such           as hidden Markov models (HMMs), have been successfully applied           to many of these tasks. A limitation of these models however, is           that they cannot naturally handle cases in which pattern instances           overlap in arbitrary ways. We present an alternative approach,           based on conditional Markov networks, that can naturally repre-           sent arbitrarily overlapping elements. We show how to efficiently           train and perform inference with these models. Experimental re-           sults from a genomics domain show that our models are more accu-           rate at locating instances of overlapping patterns than are baseline           models based on HMMs.
1      Introduction
Hidden Markov models (HMMs) and related probabilistic sequence models have been among the most accurate methods used for sequence-based prediction tasks in genomics, natural language processing and other problem domains. One key limitation of these models, however, is that they cannot represent general overlaps among sequence elements in a concise and natural manner. We present a novel approach to modeling and predicting overlapping sequence elements that is based on undirected Markov networks. Our work is motivated by the task of predicting DNA sequence elements involved in the regulation of gene expression in bacteria. Like HMM-based methods, our approach is able to represent and exploit relationships among different sequence elements of interest. In contrast to HMMs, however, our approach can naturally represent sequence elements that overlap in arbitrary ways.
We describe and evaluate our approach in the context of predicting a bacterial genome's genes and regulatory ""signals"" (together its regulatory elements). Part of the process of understanding a given genome is to assemble a ""parts list"", often using computational methods, of its regulatory elements. Predictions, in this case, entail specifying the start and end coordinates of subsequences of interest. It is common in bacterial genomes for these important sequence elements to overlap.
                                                                         START            END (a)                                                                   (b)        prom                  prom prom                   term                1                2         3                      1                     gene1                      gene 2                                                                              prom     gene           term

Figure 1: (a) Example arrangement of two genes, three promoters and one terminator in a DNA sequence. (b) Topology of an HMM for predicting these elements. Large circles represent element-specific sub-models and small gray circles represent inter-element sub- models, one for each allowed pair of adjacent elements. Due to the overlapping elements, there is no path through the HMM consistent with the configuration in (a).
Our approach to predicting overlapping sequence elements, which is based on dis- criminatively trained undirected graphical models called conditional Markov net- works [5, 10] (also called conditional random fields), uses two key steps to make a set of predictions. In the first step, candidate elements are generated by having a set of models independently make predictions. In the second step, a Markov network is constructed to decide which candidate predictions to accept.
Consider the task of predicting gene, promoter, and terminator elements encoded in bacterial DNA. Figure 1(a) shows an example arrangement of these elements in a DNA sequence. Genes are DNA sequences that encode information for constructing proteins. Promoters and terminators are DNA sequences that regulate transcrip- tion, the first step in the synthesis of a protein from a gene. Transcription begins at a promoter, proceeds downstream (left-to-right in Figure 1(a)), and ends at a terminator. Regulatory elements often overlap each other, for example prom2 and prom3 or gene1 and prom2 in Figure 1.
One technique for predicting these elements is first to train a probabilistic sequence model for each element type (e.g. [2, 9]) and then to ""scan"" an input sequence with each model in turn. Although this approach can predict overlapping elements, it is limited since it ignores inter-element dependencies. Other methods, based on HMMs (e.g. [11, 1]), explicitly consider these dependencies. Figure 1(b) shows an example topology of such an HMM. Given an input sequence, this HMM defines a probability distribution over parses, partitionings of the sequence into subsequences corresponding to elements and the regions between them. These models are not nat- urally suited to representing overlapping elements. For the case shown in Figure 1(a) for example, even if the subsequences for gene1 and prom2 match their respective sub-models very well, since both elements cannot be in the same parse there is a competition between predictions of gene1 and prom2. One could expand the state set to include states for specific overlap situations however, the number of states in- creases exponentially with the number of overlap configurations. Alternatively, one could use the factorized state representation of factorial HMMs [4]. These models, however, assume a fixed number of loosely connected processes evolving in parallel, which is not a good match to our genomics domain.
Like HMMs, our method, called CMN-OP (conditional Markov networks for over- lapping patterns), employs element-specific sub-models and probabilistic constraints on neighboring elements qualitatively expressed in a graph. The key difference be- tween CMN-OP and HMMs is the probability distributions they define for an input sequence. While, as mentioned above, an HMM defines a probability distribution over partitions of the sequence, a CMN-OP defines a probability distribution over all possible joint arrangements of elements in an input sequence. Figure 2 illustrates this distinction.
(a) HMM                                                                                     (b) CMN-OP
 predicted labels                           sample space                                   predicted signals                                                sample space

                                                                                                                                                                       end position                                                                                                                                                                  1    2     3    4    5    6    7    8      1    2    3    4    5     6    7    8          1    2    3    4    5    6    7    8                                                              1

                                                                                                                                                  2

                                                                                           1    2    3    4    5    6    7    8                   3

                                                                                                                                                  4

                                                                                                                                                  5

                                                                                                                                                  6                                                                                                                                             start position 7                                                                                                                                                       8

Figure 2: An illustration of the difference in the sample spaces on which probability distributions over labelings are defined by (a) HMMs and (b) CMN-OP models. The left side of (a) shows a sequence of length eight for which an HMM has predicted that an element of interest occupies two subsequences, [1:3] and [6:7]. The darker subsequences, [4:5] and [8:8], represent sequence regions between predicted elements. The right side of (a) shows the corresponding event in the sample space of the HMM, which associates one label with each position. The left side of (b) shows four predicted elements made by a CMN-OP model. The right side of (b) illustrates the corresponding event in the CMN-OP sample space. Each square corresponds to a subsequence, and an event in this sample space assigns a (possibly empty) label to each sub-sequence."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/92f54963fc39a9d87c2253186808ea61-Abstract.html,Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms,"Omid Madani, David M. Pennock, Gary W. Flake","In the context of binary classification, we define disagreement as a mea-          sure of how often two independently-trained models differ in their clas-          sification of unlabeled data. We explore the use of disagreement for error          estimation and model selection. We call the procedure co-validation,          since the two models effectively (in)validate one another by comparing          results on unlabeled data, which we assume is relatively cheap and plen-          tiful compared to labeled data. We show that per-instance disagreement          is an unbiased estimate of the variance of error for that instance. We also          show that disagreement provides a lower bound on the prediction (gen-          eralization) error, and a tight upper bound on the ""variance of prediction          error"", or the variance of the average error across instances, where vari-          ance is measured across training sets. We present experimental results on          several data sets exploring co-validation for error estimation and model          selection. The procedure is especially effective in active learning set-          tings, where training sets are not drawn at random and cross validation          overestimates error."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/9457fc28ceb408103e13533e4a5b6bd1-Abstract.html,Co-Training and Expansion: Towards Bridging Theory and Practice,"Maria-florina Balcan, Avrim Blum, Ke Yang","Ke Yang
Computer Science Dept. Carnegie Mellon Univ. Pittsburgh, PA 15213"
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/94aef38441efa3380a3bed3faf1f9d5d-Abstract.html,Probabilistic Inference of Alternative Splicing Events in Microarray Data,"Ofer Shai, Brendan J. Frey, Quaid D. Morris, Qun Pan, Christine Misquitta, Benjamin J. Blencowe","Alternative splicing (AS) is an important and frequent step in mammalian          gene expression that allows a single gene to specify multiple products,          and is crucial for the regulation of fundamental biological processes. The          extent of AS regulation, and the mechanisms involved, are not well un-          derstood. We have developed a custom DNA microarray platform for          surveying AS levels on a large scale. We present here a generative model          for the AS Array Platform (GenASAP) and demonstrate its utility for          quantifying AS levels in different mouse tissues. Learning is performed          using a variational expectation maximization algorithm, and the parame-          ters are shown to correctly capture expected AS trends. A comparison of          the results obtained with a well-established but low through-put experi-          mental method demonstrate that AS levels obtained from GenASAP are          highly predictive of AS levels in mammalian tissues.
1    Biological diversity through alternative splicing
Current estimates place the number of genes in the human genome at approximately 30,000, which is a surprisingly small number when one considers that the genome of yeast, a single- celled organism, has 6,000 genes. The number of genes alone cannot account for the com- plexity and cell specialization exhibited by higher eukaryotes (i.e. mammals, plants, etc.). Some of that added complexity can be achieved through the use of alternative splicing, whereby a single gene can be used to code for a multitude of products.
Genes are segments of the double stranded DNA that contain the information required by the cell for protein synthesis. That information is coded using an alphabet of 4 (A, C, G, and T), corresponding to the four nucleotides that make up the DNA. In what is known as the central dogma of molecular biology, DNA is transcribed to RNA, which in turn is translated into proteins. Messenger RNA (mRNA) is synthesized in the nucleus of the cell and carries the genomic information to the ribosome. In eukaryotes, genes are generally comprised of both exons, which contain the information needed by the cell to synthesize proteins, and introns, sometimes referred to as spacer DNA, which are spliced out of the pre-mRNA to create mature mRNA. An estimated 35%-75% of human genes [1] can be
                                                                                                                            C                    A                C                                                                                                                                 1                                           2                     (a)                   C                              A                        C                                                 1                                                      2                                                                                                                                            C                   C                                                                                                                                            1                    2



                                                                                                                       C    A          C                   C A          C                                                                                                                            1         3'         2              1      5'         2                     (b)    C         A                         C                                                 C                                 1              3'                   2         C1                  A5'                 2                                                                                                                                            C                   C                                                                                                                                            1                    2


                                                                                                                            C                    A                C                                                                                                                                 1                         1                 2                     (c)              C                         A              A                             C                                           1                         1              2                             2                                                                                                                                 C                    A                C                                                                                                                                 1                         2                 2


                                                                                                                                       C                   C                                                                                                                                            1                    2                     (d)                              C                                  C                                                           1                                  2                                                                                                                                 C                                     C                                                                                                                                 1                                           2

Figure 1: Four types of AS. Boxes represent exons and lines represent introns, with the possible splicing alternatives indicated by the connectors. (a) Single cassette exon inclusion/exclusion. C1 and C2 are constitutive exons (exons that are included in all isoforms) and flank a single alternative exon (A). The alternative exon is included in one isoform and excluded in the other. (b) Alternative 3' (or donor) and alternative 5' (acceptor) splicing sites. Both exons are constitutive, but may con- tain alternative donor and/or acceptor splicing sites. (c) Mutually exclusive exons. One of the two alternative exons (A1 and A2) may be included in the isoform, but not both. (d) Intron inclusion. An intron may be included in the mature mRNA strand.
spliced to yield different combinations of exons (called isoforms), a phenomenon referred to as alternative splicing (AS). There are four major types of AS as shown in Figure 1. Many multi-exon genes may undergo more than one alternative splicing event, resulting in many possible isoforms from a single gene. [2]
In addition to adding to the genetic repertoire of an organism by enabling a single gene to code for more than one protein, AS has been shown to be critical for gene regulation, con- tributing to tissue specificity, and facilitating evolutionary processes. Despite the evident importance of AS, its regulation and impact on specific genes remains poorly understood. The work presented here is concerned with the inference of single cassette exon AS levels (Figure 1a) based on data obtained from RNA expression arrays, also known as microar- rays.
1.1      An exon microarray data set that probes alternative splicing events
Although it is possible to directly analyze the proteins synthesized by a cell, it is easier, and often more informative, to instead measure the abundance of mRNA present. Traditionally, gene expression (abundance of mRNA) has been studied using low throughput techniques (such as RT-PCR or Northern blots), limited to studying a few sequences at a time and making large scale analysis nearly impossible.
In the early 1990s, microarray technology emerged as a method capable of measuring the expression of thousands of DNA sequences simultaneously. Sequences of interest are de- posited on a substrate the size of a small microscope slide, to form probes. The mRNA is extracted from the cell and reverse-transcribed back into DNA, which is labelled with red and green fluorescent dye molecules (cy3 and cy5 respectively). When the sample of tagged DNA is washed over the slide, complementary strands of DNA from the sample hy- bridize to the probes on the array forming A-T and C-G pairings. The slide is then scanned and the fluorescent intensity is measured at each probe. It is generally assumed that the intensity measure at the probe is linearly related to the abundance of mRNA in the cell over a wide dynamic range.
Despite significant improvements in microarray technologies in recent years, microarray data still presents some difficulties in analysis. Low measurements tend to have extremely low signal to noise ratio (SNR) [7] and probes often bind to sequences that are very similar, but not identical, to the one for which they were designed (a process referred to as cross-
                   C                        A                    C                         1                                            2

                    C                       A                    C         3 Body probes                          1                                                2

                                  C :A           A:C                                        1                   2

                             C              A               C              2 Inclusion junction probes                                  1                              2

                                          C :C                                                 1    2

                                   C              C                        1 Exclusion junction probe                                          1                2

Figure 2: Each alternative splicing event is studied using six probes. Probes were chosen to measure the expression levels of each of the three exons involved in the event. Additionally, 3 probes are used that target the junctions that are formed by each of the two isoforms. The inclusion isoform would express the junctions formed by C1 and A, and A and C2, while the exclusion isoform would express the junction formed by C1 and C2
hybridization). Additionally, probes exhibit somewhat varying hybridization efficiency, and sequences exhibit varying labelling efficiency.
To design our data sets, we mined public sequence databases and identified exons that were strong candidates for exhibiting AS (the details of that analysis are provided elsewhere [4, 3]). Of the candidates, 3,126 potential AS events in 2,647 unique mouse genes were selected for the design of Agilent Custom Oligonucleotide microarray. The arrays were hybridized with unamplified mRNA samples extracted from 10 wild-type mouse tissues (brain, heart, intestine, kidney, liver, lung, salivary gland, skeletal muscle, spleen, and testis). Each AS event has six target probes on the arrays, chosen from regions of the C1 exon, C2 exon, A exon, C1:A splice junction, A:C2 splice junction, and C1:C2 splice junction, as shown in Figure 2.
2      Unsupervised discovery of alternative splicing
With the exception of the probe measuring the alternative exon, A (Figure 2), all probes measure sequences that occur in both isoforms. For example, while the sequence of the probe measuring the junction A:C1 is designed to measure the inclusion isoform, half of it corresponds to a sequence that is found in the exclusion isoform. We can therefore safely assume that the measured intensity at each probe is a result of a certain amount of both isoforms binding to the probe. Due to the generally assumed linear relationship between the abundance of mRNA hybridized at a probe and the fluorescent intensity measured, we model the measured intensity as a weighted sum of the overall abundance of the two isoforms.
A stronger assumption is that of a single, consistent hybridization profile for both isoforms across all probes and all slides. Ideally, one would prefer to estimate an individual hy- bridization profile for each AS event studied across all slides. However, in our current setup, the number of tissues is small (10), resulting in two difficulties. First, the number of parameters is very large when compared to the number of data point using this model, and second, a portion of the events do not exhibit tissue specific alternative splicing within our small set of tissues. While the first hurdle could be accounted for using Baysian parameter estimation, the second cannot.
2.1    GenASAP - a generative model for alternative splicing array platform
Using the setup described above, the expression vector x, containing the six microarray measurements as real numbers, can be decomposed as a linear combination of the abun- dance of the two splice isoforms, represented by the real vector s, with some added noise: x = s + noise, where  is a 6  2 weight matrix containing the hybridization profiles for
                                                       s                                     s                                                            1                                          2





                        x^                  x                                                 ^               x^              x^                         x                                                                                                            ^               x^                                   C                  C                A          C :A                           A:C              C :C                                    1                  2                               1                               2           1    2


                                                                       r



                        x                   x               x               x                          x               x                                  C                   C               A           C :A                           A:C              C :C                                    1                  2                               1                               2           1    2





                        o                   o               o               o                          o               o                               C                      C           A               C :A                           A:C         C :C                                    1                  2                               1                               2           1     2




                                                                   n    2

Figure 3: Graphical model for alternative splicing. Each measurement in the observed expression profile, x, is generated by either using a scale factor, r, on a linear combination of the isoforms, s, or drawing randomly from an outlier model. For a detailed description of the model, see text.
the two isoforms across the six probes. Note that we may not have a negative amount of a given isoform, nor can the presence of an isoform deduct from the measured expression, and so both s and  are constrained to be positive.
Expression levels measured by microarrays have previously been modelled as having expression-dependent noise [7]. To address this, we rewrite the above formulation as
                                                       x = r(s + ),                                                                    (1)

where r is a scale factor and  is a zero-mean normally distributed random variable with a diagonal covariance matrix, , denoted as p() = N (; 0, ). The prior distribution for the abundance of the splice isoforms is given by a truncated normal distribution, denoted as p(s)  N (s, 0, I)[s  0], where [] is an indicator function such that [s  0] = 1 if i, si  0, and [s  0] = 0 otherwise.
Lastly, there is a need to account for aberrant observations (e.g. due to faulty probes, flakes of dust, etc.) with an outlier model. The complete GenASAP model (shown in Figure 3) accounts for the observations as the outcome of either applying equation (1) or an outlier model. To avoid degenerate cases and ensure meaningful and interpretable results, the number of faulty probes considered for each AS event may not exceed two, as indicated by the filled-in square constraint node in Figure 3.
The distribution of x conditional on the latent variables, s, r, and o, is:
            p(x|s, r, o) =                  N (xi; ris, r2i)[oi=0]N (xi; Ei, Vi)[oi=1],                                                (2)                                            i

where oi  {0, 1} is a bernoulli random variable indicating if the measurement at probe xi is the result of the AS model or the outlier model parameterized by p(oi = 1) = i. The parameters of the outlier model, E and V, are not optimized and are set to the mean and variance of the data.
2.2     Variational learning in the GenASAP model
To infer the posterior distribution over the splice isoform abundances while at the same time learning the model parameters we use a variational expectation-maximization algorithm (EM). EM maximizes the log likelihood of the data by iteratively estimating the posterior distribution of the model given the data in the expectation (E) step, and maximizing the log likelihood with respect to the parameters, while keeping the posterior fixed, in the maximization (M) step. Variational EM is used when, as in the case of GenASAP, the exact posterior is intractable. Variational EM minimizes the free energy of the model, defined as the KL-divergence between the joint distribution of the latent and observed variables and the approximation to the posterior under the model parameters [5, 6].
We approximate the true posterior using the Q distribution given by
                                        T          Q({s(t)}, {o(t)}, {r(t)}) =              Q(r(t))Q(o(t)|r(t))          Q(s(t)|o(t), r(t))                                                                                      i     i                                            t=1                            i                                  (3)                                                      T                                         =Z(t)-1            (t)(t)N (s(t); (t)d                                                                                ro , (t)d                                                                                           ro )[s(t)  0],                                                     t=1

where Z is a normalization constant, the superscript d indicates that  is constrained to be diagonal, and there are T iid AS events. For computational efficiency, r is selected from a finite set, r  {r1, r2, . . . , rC } with uniform probability. The variational free energy is given by
                                                                     Q({s(t)}, {o(t)}, {r(t)})   F(Q, P ) =                      Q({s(t)}, {o(t)}, {r(t)}) log                                              .                                                                      P ({s(t)}, {o(t)}, {r(t)}, {x(t)})                   r    o     s                                                                                                              (4) Variational EM minimizes the free energy by iteratively updating the Q distribution's vari-

ational parameters ((t), (t), (t)d                                         ro , and (t)d                                                      ro ) in the E-step, and the model parameters (, , {r1, r2, . . . , rC}, and ) in the M-step. The resulting updates are too long to be shown in the context of this paper and are discussed in detail elsewhere [3]. A few particular points regarding the E-step are worth covering in detail here.
If the prior on s was a full normal distribution, there would be no need for a variational approach, and exact EM is possible. For a truncated normal distribution, however, the mix- ing proportions, Q(r)Q(o|r) cannot be calculated analytically except for the case where s is scalar, necessitating the diagonality constraint. Note that if  was allowed to be a full covariance matrix, equation (3) would be the true posterior, and we could find the sufficient statistics of Q(s(t)|o(t), r(t)):
   (t)         ro = (I + T (I - O(t))T -1(I - O(t)))-1T (I - O(t))T -1x(t)r(t)-1                               (5)

                   (t)-1                             ro      = (I + T (I - O(t))T -1(I - O(t)))                                    (6)

where O is a diagonal matrix with elements Oi,i = oi. Furthermore, it can be easily shown that the optimal settings for d and d approximating a normal distribution with full covariance  and mean  is
                                              doptimal =                                               (7)

                                      d-1                                            optimal = diag(-1)                                               (8)

In the truncated case, equation (8) is still true. Equation (7) does not hold, though, and doptimal cannot be found analytically. In our experiments, we found that using equation (7) still decreases the free energy every E-step, and it is significantly more efficient than using, for example, a gradient decent method to compute the optimal d.
                              Intuitive Weigh Matrix                                              Optimal Weight Matrix

                    50                                                               50


                    40                                                               40


                    30                                                               30


                    20                                                               20


                    10                                                               10


                     0                                                                    0                                 Inclusion Isoform           Exclusion Isoform                      Inclusion Isoform      Exclusion Isoform                                                      (a)                                                                (b)

Figure 4: (a) An intuitive set of weights. Based on the biological background, one would expect to see the inclusion isoform hybridize to the probes measuring C1, C2, A, C1:A, and A:C2, while the exclusion isoform hybridizes to C1, C2, and C1:C2. (b) The learned set of weights closely agrees with the intuition, and captures cross hybridization between the probes
                                                                                                                           RT-PCR     AS model                               Contribution of         Contribution of                                                    measurement      prediction                                                                                  AS model          Original Data                          exclusion isoform           inclusion isoform                                                   (% exclusion) (% exclusion)



             (a)                                                                                                                                14%             27%





             (b)                                                                                                           72%             70%



                                                                                               outliers





             (c)                                                                                                            8%             22%

Figure 5: Three examples of data cases and their predictions. (a) The data does not follow our notion of single cassette exon AS, but the AS level is predicted accurately by the model.(b) The probe C1:A is marked as outlier, allowing the model to predict the other probes accurately. (c) Two probes are marked as outliers, and the model is still successful in predicting the AS levels.
3    Making biological predictions about alternative splicing
The results presented in this paper were obtained using two stages of learning. In the first step, the weight matrix, , is learned on a subset of the data that is selected for quality. Two selection criteria were used: (a) sequencing data was used to select those cases for which, with high confidence, no other AS event is present (Figure 1) and (b) probe sets were selected for high expression, as determined by a set of negative controls. The second selection criterion is motivated by the common assumption that low intensity measurements are of lesser quality (see Section 1.1). In the second step,  is kept fixed, and we introduce the additional constraint that the noise is isotropic ( = I) and learn on the entire data set. The constraint on the noise is introduced to prevent the model from using only a subset of the six probes for making the final set of predictions.
We show a typical learned set of weights in Figure 4. The weights fit well with our intuition of what they should be to capture the presence of the two isoforms. Moreover, the learned weights account for the specific trends in the data. Examples of model prediction based on the microarray data are shown in Figure 5.
Due to the nature of the microarray data, we do not expect all the inferred abundances to be equally good, and we devised a scoring criterion that ranks each AS event based on its fit to the model. Intuitively, given two input vectors that are equivalent up to a scale factor, with inferred MAP estimations that are equal up to the same scale factor, we would like their scores to be identical. The scoring criterion used, therefore is                                                                                           (x                                                                                                                                                       k     k - rks)2/(xk +
                     Rank     Pearson's correlation      False positive                                         coefficient                 rate                          500               0.94                   0.11                         1000               0.95                   0.08                         2000               0.95                   0.05                         5000               0.79                    0.2                         10000              0.79                   0.25                         15000              0.78                   0.29                         20000              0.75                   0.32                         30000              0.65                   0.42

Table 1: Model performance evaluated at various ranks. Using 180 RT-PCR measurements, we are able to predict the model's performance at various ranks. Two evaluation criteria are used: Pearson's correlation coefficient between the model's predictions and the RT-PCR measurements and false positive rate, where a prediction is considered to be false positive if it is more than 15% away from the RT-PCR measurement.
rks)2, where the MAP estimations for r and s are used. This scoring criterion can be viewed as proportional to the sum of noise to signal ratios, as estimated using the two values given by the observation and the model's best prediction of that observation.
Since it is the relative amount of the isoforms that is of most interest, we need to use the inferred distribution of the isoform abundances to obtain an estimate for the relative levels of AS. It is not immediately clear how this should be done. We do, however, have RT- PCR measurements for 180 AS events to guide us (see figure 6 for details). Using the top 50 ranked RT-PCR measurement, we fit three parameters, {a1, a2, a3}, such that the proportion of excluded isoform present, p, is given by p = a                  s2                                                                    1                   + a                                                                         s                     3, where s1 is the                                                                              1+a2s2 MAP estimation of the abundance of the inclusion isoform, s2 is the MAP estimation of the abundance of the exclusion isoform, and the RT-PCR measurement are used for target p. The parameters are fitted using gradient descent on a least squared error (LSE) evaluation criterion.
We used two criteria to evaluate the quality of the AS model predictions. Pearson's cor- relation coefficient (PCC) is used to evaluate the overall ability of the model to correctly estimate trends in the data. PCC is invariant to affine transformation and so is independent of the transformation parameters a1 and a3 discussed above, while the parameter a2 was found to effect PCC very little. The PCC stays above 0.75 for the top two thirds ranked pre- dictions. The second evaluation criterion used is the false positive rate, where a prediction is considered to be false positive if it is more than 15% away from the RT-PCR measure- ment. This allows us to say, for example, that if a prediction is within the top 10000, we are 75% confident that it is within 15% of the actual levels of AS."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/96f2b50b5d3613adf9c27049b2a888c7-Abstract.html,Semi-supervised Learning by Entropy Minimization,"Yves Grandvalet, Yoshua Bengio","We consider the semi-supervised learning problem, where a decision rule            is to be learned from labeled and unlabeled data. In this framework, we            motivate minimum entropy regularization, which enables to incorporate            unlabeled data in the standard supervised learning. Our approach in-            cludes other approaches to the semi-supervised problem as particular or            limiting cases. A series of experiments illustrates that the proposed solu-            tion benefits from unlabeled data. The method challenges mixture mod-            els when the data are sampled from the distribution class spanned by the            generative model. The performances are definitely in favor of minimum            entropy regularization when generative models are misspecified, and the            weighting of unlabeled data provides robustness to the violation of the            ""cluster assumption"". Finally, we also illustrate that the method can also            be far superior to manifold learning in high dimension spaces.
1     Introduction
In the classical supervised learning classification framework, a decision rule is to be learned from a learning set Ln = {xi, yi}ni , where each example is described by a pattern                                      =1                                                   xi  X and by the supervisor's response yi   = {1, . . . , K }. We consider semi-supervised learning, where the supervisor's responses are limited to a subset of Ln.
In the terminology used here, semi-supervised learning refers to learning a decision rule on X from labeled and unlabeled data. However, the related problem of transductive learning, i.e. of predicting labels on a set of predefined patterns, is addressed as a side issue. Semi- supervised problems occur in many applications where labeling is performed by human experts. They have been receiving much attention during the last few years, but some important issues are unresolved [10].
In the probabilistic framework, semi-supervised learning can be modeled as a missing data problem, which can be addressed by generative models such as mixture models thanks to the EM algorithm and extensions thereof [6].Generative models apply to the joint den- sity of patterns and class (X, Y ). They have appealing features, but they also have major drawbacks. Their estimation is much more demanding than discriminative models, since the model of P (X, Y ) is exhaustive, hence necessarily more complex than the model of
 This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence IST-2002-506778. This publication only reflects the authors' views.

P (Y |X). More parameters are to be estimated, resulting in more uncertainty in the es- timation process. The generative model being more precise, it is also more likely to be misspecified. Finally, the fitness measure is not discriminative, so that better models are not necessarily better predictors of class labels. These difficulties have lead to proposals aiming at processing unlabeled data in the framework of supervised classification [1, 5, 11]. Here, we propose an estimation principle applicable to any probabilistic classifier, aiming at making the most of unlabeled data when they are beneficial, while providing a control on their contribution to provide robustness to the learning scheme.
2      Derivation of the Criterion"
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/978fce5bcc4eccc88ad48ce3914124a2-Abstract.html,Detecting Significant Multidimensional Spatial Clusters,"Daniel B. Neill, Andrew W. Moore, Francisco Pereira, Tom M. Mitchell","Assume a uniform, multidimensional grid of bivariate data, where each          cell of the grid has a count ci and a baseline bi. Our goal is to find          spatial regions (d-dimensional rectangles) where the ci are significantly          higher than expected given bi. We focus on two applications: detection of          clusters of disease cases from epidemiological data (emergency depart-          ment visits, over-the-counter drug sales), and discovery of regions of in-          creased brain activity corresponding to given cognitive tasks (from fMRI          data). Each of these problems can be solved using a spatial scan statistic          (Kulldorff, 1997), where we compute the maximum of a likelihood ratio          statistic over all spatial regions, and find the significance of this region          by randomization. However, computing the scan statistic for all spatial          regions is generally computationally infeasible, so we introduce a novel          fast spatial scan algorithm, generalizing the 2D scan algorithm of (Neill          and Moore, 2004) to arbitrary dimensions. Our new multidimensional          multiresolution algorithm allows us to find spatial clusters up to 1400x          faster than the naive spatial scan, without any loss of accuracy.
1     Introduction
One of the core goals of modern statistical inference and data mining is to discover patterns and relationships in data. In many applications, however, it is important not only to discover patterns, but to distinguish those patterns that are significant from those that are likely to have occurred by chance. This is particularly important in epidemiological applications, where a rise in the number of disease cases in a region may or may not be indicative of an emerging epidemic. In order to decide whether further investigation is necessary, epidemiologists must know not only the location of a possible outbreak, but also some measure of the likelihood that an outbreak is occurring in that region. Similarly, when investigating brain imaging data, we want to not only find regions of increased activity, but determine whether these increases are significant or due to chance fluctuations.
More generally, we are interested in spatial data mining problems where the goal is detec- tion of overdensities: spatial regions with high counts relative to some underlying baseline. In the epidemiological datasets, the count is some quantity (e.g. number of disease cases, or units of cough medication sold) in a given area, where the baseline is the expected value of that quantity based on historical data. In the brain imaging datasets, our count is the total fMRI activation in a given set of voxels under the experimental condition, while our baseline is the total activation in that set of voxels under the null or control condition.
We consider the case in which data has been aggregated to a uniform, d-dimensional grid. For the fMRI data, we have three spatial dimensions; for the epidemiological data, we have two spatial dimensions but also use several other quantities (time, patients' age and gender) as ""pseudo-spatial"" dimensions; this is discussed in more detail below.
In the general case, let G be a d-dimensional grid of cells, with size N1  N2  ...  Nd. Each cell si  G (where i is a d-dimensional vector) is associated with a count ci and a baseline bi. Our goal is to search over all d-dimensional rectangular regions S  G, and find regions where the total count C(S) = S ci is higher than expected, given the baseline B(S) = S bi. In addition to discovering these high-density regions, we must also perform statistical testing to determine whether these regions are significant. As is necessary in the scan statistics framework, we focus on finding the single, most significant region; the method can be iterated (removing each significant cluster once it is found) to find multiple significant regions.
1.1      Likelihood ratio statistics Our basic model assumes that counts ci are generated by an inhomogeneous Poisson pro- cess with mean qbi, where q (the underlying ratio of count to baseline) may vary spatially. We wish to detect hyper-rectangular regions S such that q is significantly higher inside S than outside S. To do so, for a given region S, we assume that q = qin uniformly for cells si  S, and q = qout uniformly for cells si  G-S. We then test the null hypothesis H0(S): qin  (1+)qout against the alternative hypothesis H1(S): qin > (1+)qout. If  = 0, this is equivalent to the classical spatial scan statistic [1-2]: we are testing for regions where qin is greater than qout . However, in many real-world applications (including the epidemiological and fMRI datasets discussed later) we expect some fluctuation in the underlying baseline; thus, we do not want to detect all deviations from baseline, but only those where the amount of deviation is greater than some threshold. For example, a 10% increase in disease cases in some region may not be interesting to epidemiologists, even if the underlying population is large enough to conclude that this is a ""real"" (statistically significant) increase in q. By increasing , we can focus the scan statistic on regions with larger ratios of count to base- line. For example, we can use the scan statistic with  = 0.25 to test for regions where qin is more than 25% higher than qout . Following Kulldorff [1], our spatial scan statistic is the maximum, over all regions S, of the ratio of the likelihoods under the alternative and null hypotheses. Taking logs for convenience, we have:
                        sup                                                             q                         s          D                                                        i               (S) = log                in>(1+)qout                   S P(ci  Po(qinbi))siG-S P(ci  Po(qoutbi))                             sup                                                             qin(1+)qout siS P(ci  Po(qinbi))siG-S P(ci  Po(qoutbi))                                          C(S)                                           C                       C  = (                                                                                     tot                      tot          sgn) C(S) log                                  + (C                                    -C(S)                              (                                         tot                               1 + )B(S)                                      -C(S))log Btot -B(S) -CtotlogBtot+B(S) where C(S) and B(S) are the count and baseline of the region S under consideration, Ctot and Btot are the total count and baseline of the entire grid G, and sgn = +1 if C(S) > (1 +                                                                                                                 B(S) )Ctot-C(S) and -1 otherwise. Then the scan statistic D   B                                                                                      ,max is equal to the maximum D(S)        tot -B(S) over all spatial regions (d-dimensional rectangles) under consideration. We note that our statistical and computational methods are not limited to the Poisson model given here; any model of null and alternative hypotheses such that the resulting statistic D(S) satisfies the conditions given in [4] can be used for the fast spatial scan.

1.2      Randomization testing Once we have found the highest scoring region S = arg maxS D(S) of grid G, we must still determine the statistical significance of this region. Since the exact distribution of the test statistic Dmax is only known in special cases, in general we must find the region's p-value by randomization. To do so, we run a large number R of random replications, where a replica
has the same underlying baselines bi as G, but counts are randomly drawn from the null hypothesis H0(S). More precisely, we pick ci  Po(qbi), where q = qin = (1+) Ctot                                                                                                          Btot +B(S) for si  S, and q = qout =                Ctot       for s                                       B                         i                                        tot +B(S)                    G - S. The number of replicas G with Dmax(G )  Dmax(G), divided by the total number of replications R, gives us the p-value for our most significant region S. If this p-value is less than  (where  is the false positive rate, typically chosen to be 0.05 or 0.1), we can conclude that the discovered region is statistically significant at level .
1.3    The naive spatial scan The simplest method of finding Dmax is to compute D(S) for all rectangular regions of sizes k1 k2 ...kd, where 1  kj  Nj. Since there are a total of d (N                                                                                       j=1      j - kj + 1) regions of each size, there are a total of O(d                   N2)                                                    j=1               regions to examine. We can compute D(S)                                                            j for any region S in constant time, by first finding the count C(S) and baseline B(S), then computing D.1 This allows us to compute Dmax of a grid G in O(d                              N2)                                                                                        j=1           time. However,                                                                                                j significance testing by randomization also requires us to find Dmax for each replica G , and compare this to Dmax(G); thus the total complexity is multiplied by the number of replications R. When the size of the grid is large, as is the case for the epidemiological and fMRI datasets we are considering, this naive approach is computationally infeasible.
Instead, we apply our ""overlap-multiresolution partitioning"" algorithm [3-4], generalizing this method from two-dimensional to d-dimensional datasets. This reduces the complexity to O(d         N          j=1         j log N j ) in cases where the most significant region S has a sufficiently high ra- tio of count to baseline, and (as we show in Section 3) typically results in tens to thousands of times speedup over the naive approach. We note that this fast spatial scan algorithm is exact (always finds the correct value of Dmax and the corresponding region S); the speedup results from the observation that we do not need to search a given set of regions if we can prove that none of them have score > Dmax. Thus we use a top-down, branch-and-bound approach: we maintain the current maximum score of the regions we have searched so far, calculate upper bounds on the scores of subregions contained in a given region, and prune regions whose upper bounds are less than the current value of Dmax. When searching a replica grid, we care only whether Dmax of the replica grid is greater than Dmax(G). Thus we can use Dmax of the original grid for pruning on the replicas, and can stop searching a replica if we find a region with score > Dmax(G).
2      Overlap-multiresolution partitioning
As in [4], we use a multiresolution search method which relies on an overlap-kd tree data structure. The overlap-kd tree, like kd-trees [5] and quadtrees [6], is a hierarchical, space- partitioning data structure. The root node of the tree represents the entire space under consideration (i.e. the entire grid G), and each other node represents a subregion of the grid. Each non-leaf node of a d-dimensional overlap-kd tree has 2d children, an ""upper"" and a ""lower"" child in each dimension. For example, in three dimensions, a node has six children: upper and lower children in the x, y, and z dimensions. The overlap-kd tree is different from the standard kd-tree and quadtree in that adjacent regions overlap: rather than splitting the region in half along each dimension, instead each child contains more than half the area of the parent region. For example, a 64  64  64 grid will have six children: two of size 48 6464, two of size 644864, and two of size 646448.      1An old trick makes it possible to compute the count and baseline of any rectangular region in time constant in N: we first form a d-dimensional array of the cumulative counts, then compute each region's count by adding/subtracting at most 2d cumulative counts. Note that because of the exponential dependence on d, these techniques suffer from the ""curse of dimensionality"": neither the naive spatial scan, nor the fast spatial scan discussed below, are appropriate for very high dimensional datasets.
In general, let region S have size k1 k2...kd. Then the two children of S in dimension j (for j = 1 . . . d) have size k1 ...kj-1  fjkj kj+1 ...kd, where 1 < f                                                                                                   2           j < 1. This partitioning (for the two-dimensional case, where f1 = f2 = 3 ) is illustrated in Figure 1.                                                                                    4 Note that there is a region SC common to all of these children; we call this region the center of S. When we partition region S in this manner, it can be proved that any subregion of S either a) is contained entirely in (at least) one of S1 . . . S2d, or b) contains the center region SC. Figure 1 illustrates each of these possibilities, for the simple case of d = 2.
                                  S                              Figure 1: Overlap-multires partitioning                                                                      of region S (for d = 2). Any subregion                                                                      of S either a) is contained in some S       S_1          S_2            S_3                                                                                    i,                                             S_4         S_C          i = 1 . . . 4, or b) contains SC.

Now we can search all subregions of S by recursively searching S1 . . . S2d, then searching all of the regions contained in S which contain the center SC. There may be a large number of such ""outer regions,"" but since we know that each such region contains the center, we can place very tight bounds on the score of these regions, often allowing us to prune most or all of them. Thus the basic outline of our search procedure (ignoring pruning, for the moment) is: overlap-search(S) {      call base-case-search(S)      define child regions S1..S2d, center SC as above      call overlap-search(Si) for i=1..2d      for all S' such that S' is contained in S and contains S_C, call base-case-search(S') }
The fractions fi are selected based on the current sizes ki of the region being searched: if ki = 2m, then fi = 3 , and if k                                           . For simplicity, we assume that                                  4           i = 3  2m, then fi = 23 all Ni are powers of two, and thus all region sizes ki will fall into one of these two cases. Repeating this partitioning recursively, we obtain the overlap-kd tree structure. For d = 2, the first two levels of the overlap-kd tree are shown in Figure 2.
                                                                 Figure 2: The first two levels of the two-                                                                      dimensional overlap-kd tree. Each node                                                                      represents a gridded region (denoted by                                                                      a thick rectangle) of the entire dataset                                                                      (thin square and dots).

The overlap-kd tree has several useful properties, which we present here without proof. First, for every rectangular region S  G, either S is a gridded region (contained in the overlap-kd tree), or there exists a unique gridded region S such that S is an outer region of S (i.e. S is contained in S , and contains the center region of S ). This means that, if overlap-search is called exactly once for each gridded region2, and no pruning is done, then base-case-search will be called exactly once for every rectangular region S  G. In practice, we will prune many regions, so base-case-search will be called at most once for every rect- angular region, and every region will be either searched or pruned. The second nice prop- erty of our overlap-kd tree is that the total number of gridded regions is O(d                               N                                                                                                        j=1         j log N j ). This implies that, if we are able to prune (almost) all outer regions, we can find Dmax of the grid in O(d            N                                            N2)                  j=1         j log N j ) time rather than O(dj=1           . In fact, we may not even need to                                                                        j search all gridded regions, so in many cases the search will be even faster.       2As in [4], we use ""lazy expansion"" to ensure that gridded regions are not multiply searched.
2.1      Score bounds and pruning We now consider which regions can be pruned (discarded without searching) during our multiresolution search procedure. First, given some region S, we must calculate an upper bound on the scores D(S ) for regions S  S. More precisely, we are interested in two upper bounds: a bound on the score of all subregions S  S, and a bound on the score of the outer subregions of S (those regions contained in S and containing its center SC). If the first bound is less than or equal to Dmax, we can prune region S completely; we do not need to search any (gridded or outer) subregion of S. If only the second bound is less than or equal to Dmax, we do not need to search the outer subregions of S, but we must recursively call overlap-search on the gridded children of S. If both bounds are greater than Dmax, we must both recursively call overlap-search and search the outer regions.
Score bounds are calculated based on various pieces of information about the subregions of S, including: upper and lower bounds bmax, bmin on the baseline of subregions S ; an upper bound dmax on the ratio C of S ; an upper bound d                            of S                                 B                           inc on the ratio C                                                                               B            -SC; and a lower bound dmin on the ratio C of S                                      B    -S . We also know the count C and baseline B of region S, and the count ccenter and baseline bcenter of region SC. Let cin and bin be the count and baseline of S . To find an upper bound on D(S ), we must calculate the values of cin and bin which maximize D subject to the given constraints: cin-ccenter                                                                  bin-bcenter  dinc, cin                                                                                            bin  dmax, C-cin B-bin  dmin, and bmin  bin  bmax. The solution to this maximization problem is derived in [4], and (since scores are based only on count and baseline rather than the size and shape of the region) it applies directly to the multidimensional case. The bounds on baselines and ratios C are first calculated using global values (as a fast, ""first-pass"" pruning technique).          B For the remaining, unpruned regions, we calculate tighter bounds using the quartering method of [4], and use these to prune more regions.
2.2      Related work Our work builds most directly on the results of Kulldorff [1], who presents the two- dimensional spatial scan framework and the classical ( = 0) likelihood ratio statistic. It also extends [4], in which we present the two-dimensional fast spatial scan. Our major extensions in the present work are twofold: the d-dimensional fast spatial scan, and the generalized likelihood ratio statistics D. A variety of other cluster detection techniques exist in the literature on epidemiology [1-3, 7-8], brain imaging [9-11], and machine learn- ing [12-15]. The machine learning literature focuses on heuristic or approximate cluster- finding techniques, which typically cannot deal with spatially varying baselines, and more importantly, give no information about the statistical significance of the clusters found. Our technique is exact (in that it calculates the maximum of the likelihood ratio statistic over all hyper-rectangular spatial regions), and uses a powerful statistical test to determine significance. Nevertheless, other methods in the literature have some advantages over the present approach, such as applicability to high-dimensional data and fewer assumptions on the underlying model. The fMRI literature generally tests significance on a per-voxel basis (after applying some method of spatial smoothing); clusters must then be inferred by grouping individually significant voxels, and (with the exception of [10]) no per-cluster false positive rate is guaranteed. The epidemiological literature focuses on detecting signif- icant circular, two-dimensional clusters, and thus cannot deal with multidimensional data or elongated regions. Detection of elongated regions is extremely important in both epi- demiology (because of the need to detect windborne or waterborne pathogens) and brain imaging (because of the ""folded sheet"" structure of the brain); the present work, as well as [4], allow detection of such clusters."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/9813b270ed0288e7c0388f0fd4ec68f5-Abstract.html,Message Errors in Belief Propagation,"Alexander T. Ihler, John W. Fisher, Alan S. Willsky","Belief propagation (BP) is an increasingly popular method of perform-          ing approximate inference on arbitrary graphical models.          At times,          even further approximations are required, whether from quantization or          other simplified message representations or from stochastic approxima-          tion methods. Introducing such errors into the BP message computations          has the potential to adversely affect the solution obtained. We analyze          this effect with respect to a particular measure of message error, and show          bounds on the accumulation of errors in the system. This leads both to          convergence conditions and error bounds in traditional and approximate          BP message passing."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/98b418276d571e623651fc1d471c7811-Abstract.html,Methods Towards Invasive Human Brain Computer Interfaces,"Thomas N. Lal, Thilo Hinterberger, Guido Widman, Michael Schröder, N. J. Hill, Wolfgang Rosenstiel, Christian E. Elger, Niels Birbaumer, Bernhard Schölkopf","During the last ten years there has been growing interest in the develop-          ment of Brain Computer Interfaces (BCIs). The field has mainly been          driven by the needs of completely paralyzed patients to communicate.          With a few exceptions, most human BCIs are based on extracranial elec-          troencephalography (EEG). However, reported bit rates are still low. One          reason for this is the low signal-to-noise ratio of the EEG [16]. We are          currently investigating if BCIs based on electrocorticography (ECoG) are          a viable alternative. In this paper we present the method and examples          of intracranial EEG recordings of three epilepsy patients with electrode          grids placed on the motor cortex. The patients were asked to repeat-          edly imagine movements of two kinds, e.g., tongue or finger movements.          We analyze the classifiability of the data using Support Vector Machines          (SVMs) [18, 21] and Recursive Channel Elimination (RCE) [11]."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/9a5748a2fbaa6564d05d7f2ae29a9355-Abstract.html,A Three Tiered Approach for Articulated Object Action Modeling and Recognition,"Le Lu, Gregory D. Hager, Laurent Younes","Visual action recognition is an important problem in computer vision.          In this paper, we propose a new method to probabilistically model and          recognize actions of articulated objects, such as hand or body gestures,          in image sequences. Our method consists of three levels of representa-          tion. At the low level, we first extract a feature vector invariant to scale          and in-plane rotation by using the Fourier transform of a circular spatial          histogram. Then, spectral partitioning [20] is utilized to obtain an initial          clustering; this clustering is then refined using a temporal smoothness          constraint. Gaussian mixture model (GMM) based clustering and density          estimation in the subspace of linear discriminant analysis (LDA) are then          applied to thousands of image feature vectors to obtain an intermediate          level representation. Finally, at the high level we build a temporal multi-          resolution histogram model for each action by aggregating the clustering          weights of sampled images belonging to that action. We discuss how this          high level representation can be extended to achieve temporal scaling in-          variance and to include Bi-gram or Multi-gram transition information.          Both image clustering and action recognition/segmentation results are          given to show the validity of our three tiered representation."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/9d28de8ff9bb6a3fa41fddfdc28f3bc1-Abstract.html,Temporal-Difference Networks,"Richard S. Sutton, Brian Tanner","We introduce a generalization of temporal-difference (TD) learning to          networks of interrelated predictions. Rather than relating a single pre-          diction to itself at a later time, as in conventional TD methods, a TD          network relates each prediction in a set of predictions to other predic-          tions in the set at a later time. TD networks can represent and apply TD          learning to a much wider class of predictions than has previously been          possible. Using a random-walk example, we show that these networks          can be used to learn to predict by a fixed interval, which is not possi-          ble with conventional TD methods. Secondly, we show that if the inter-          predictive relationships are made conditional on action, then the usual          learning-efficiency advantage of TD methods over Monte Carlo (super-          vised learning) methods becomes particularly pronounced. Thirdly, we          demonstrate that TD networks can learn predictive state representations          that enable exact solution of a non-Markov problem. A very broad range          of inter-predictive temporal relationships can be expressed in these net-          works. Overall we argue that TD networks represent a substantial ex-          tension of the abilities of TD methods and bring us closer to the goal of          representing world knowledge in entirely predictive, grounded terms.
Temporal-difference (TD) learning is widely used in reinforcement learning methods to learn moment-to-moment predictions of total future reward (value functions). In this set- ting, TD learning is often simpler and more data-efficient than other methods. But the idea of TD learning can be used more generally than it is in reinforcement learning. TD learn- ing is a general method for learning predictions whenever multiple predictions are made of the same event over time, value functions being just one example. The most pertinent of the more general uses of TD learning have been in learning models of an environment or task domain (Dayan, 1993; Kaelbling, 1993; Sutton, 1995; Sutton, Precup & Singh, 1999). In these works, TD learning is used to predict future values of many observations or state variables of a dynamical system. The essential idea of TD learning can be described as ""learning a guess from a guess"". In all previous work, the two guesses involved were predictions of the same quantity at two points in time, for example, of the discounted future reward at successive time steps. In this paper we explore a few of the possibilities that open up when the second guess is allowed to be different from the first.
To be more precise, we must make a distinction between the extensive definition of a predic- tion, expressing its desired relationship to measurable data, and its TD definition, express- ing its desired relationship to other predictions. In reinforcement learning, for example, state values are extensively defined as an expectation of the discounted sum of future re- wards, while they are TD defined as the solution to the Bellman equation (a relationship to the expectation of the value of successor states, plus the immediate reward). It's the same prediction, just defined or expressed in different ways. In past work with TD methods, the TD relationship was always between predictions with identical or very similar extensive semantics. In this paper we retain the TD idea of learning predictions based on others, but allow the predictions to have different extensive semantics.
1 The Learning-to-predict Problem
The problem we consider in this paper is a general one of learning to predict aspects of the interaction between a decision making agent and its environment. At each of a series of discrete time steps t, the environment generates an observation ot  O, and the agent takes an action at  A. Whereas A is an arbitrary discrete set, we assume without loss of gener- ality that ot can be represented as a vector of bits. The action and observation events occur in sequence, o1, a1, o2, a2, o3   , with each event of course dependent only on those pre- ceding it. This sequence will be called experience. We are interested in predicting not just each next observation but more general, action-conditional functions of future experience, as discussed in the next section. In this paper we use a random-walk problem with seven states, with left and right actions available in every state:
     1            0              0          0           0            0            1          1            2              3         4            5            6            7

The observation upon arriving in a state consists of a special bit that is 1 only at the two ends of the walk and, in the first two of our three experiments, seven additional bits explicitly indicating the state number (only one of them is 1). This is a continuing task: reaching an end state does not end or interrupt experience. Although the sequence depends determinis- tically on action, we assume that the actions are selected randomly with equal probability so that the overall system can be viewed as a Markov chain. The TD networks introduced in this paper can represent a wide variety of predictions, far more than can be represented by a conventional TD predictor. In this paper we take just a few steps toward more general predictions. In particular, we consider variations of the problem of prediction by a fixed interval. This is one of the simplest cases that cannot otherwise be handled by TD methods. For the seven-state random walk, we will predict the special observation bit some numbers of discrete steps in advance, first unconditionally and then conditioned on action sequences."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/9ff7c9eb9d37f434db778f59178012da-Abstract.html,Methods for Estimating the Computational Power and Generalization Capability of Neural Microcircuits,"Wolfgang Maass, Robert A. Legenstein, Nils Bertschinger","What makes a neural microcircuit computationally powerful? Or more           precisely, which measurable quantities could explain why one microcir-           cuit C is better suited for a particular family of computational tasks than           another microcircuit C ? We propose in this article quantitative measures           for evaluating the computational power and generalization capability of a           neural microcircuit, and apply them to generic neural microcircuit mod-           els drawn from different distributions. We validate the proposed mea-           sures by comparing their prediction with direct evaluations of the com-           putational performance of these microcircuit models. This procedure is           applied first to microcircuit models that differ with regard to the spatial           range of synaptic connections and with regard to the scale of synaptic           efficacies in the circuit, and then to microcircuit models that differ with           regard to the level of background input currents and the level of noise           on the membrane potential of neurons. In this case the proposed method           allows us to quantify differences in the computational power and gen-           eralization capability of circuits in different dynamic regimes (UP- and           DOWN-states) that have been demonstrated through intracellular record-           ings in vivo.
1     Introduction
Rather than constructing particular microcircuit models that carry out particular computa- tions, we pursue in this article a different strategy, which is based on the assumption that the computational function of cortical microcircuits is not fully genetically encoded, but rather emerges through various forms of plasticity (""learning"") in response to the actual distribution of signals that the neural microcircuit receives from its environment. From this perspective the question about the computational function of cortical microcircuits C turns into the questions:
   a) What functions (i.e. maps from circuit inputs to circuit outputs) can the circuit C           learn to compute.



   b) How well can the circuit C generalize a specific learned computational function            to new inputs?

We propose in this article a conceptual framework and quantitative measures for the in- vestigation of these two questions. In order to make this approach feasible, in spite of numerous unknowns regarding synaptic plasticity and the distribution of electrical and bio- chemical signals impinging on a cortical microcircuit, we make in the present first step of this approach the following simplifying assumptions:

Particular neurons (""readout neurons"") learn via synaptic plasticity to extract specific information encoded in the spiking activity of neurons in the circuit.
We assume that the cortical microcircuit itself is highly recurrent, but that the impact of feedback that a readout neuron might send back into this circuit can be neglected.1
We assume that synaptic plasticity of readout neurons enables them to learn arbitrary linear transformations. More precisely, we assume that the input to such readout neuron can be approximated by a term          n-1 w                                        i=1      ixi(t), where n - 1 is the number of presynaptic neurons, xi(t) results from the output spike train of the ith presynaptic neuron by filtering it according to the low-pass filtering property of the membrane of the readout neuron,2 and wi is the efficacy of the synaptic connection. Thus wixi(t) models the time course of the contribution of previous spikes from the ith presynaptic neuron to the membrane potential at the soma of this readout neuron. We will refer to the vector x(t) as the circuit state at time t.

Under these unpleasant but apparently unavoidable simplifying assumptions we propose new quantitative criteria based on rigorous mathematical principles for evaluating a neural microcircuit C with regard to questions a) and b). We will compare in sections 4 and 5 the predictions of these quantitative measures with the actual computational performance achieved by 132 different types of neural microcircuit models, for a fairly large number of different computational tasks. All microcircuit models that we consider are based on bio- logical data for generic cortical microcircuits (as described in section 3), but have different settings of their parameters.
2     Measures for the kernel-quality and generalization capability of       neural microcircuits
One interesting measure for probing the computational power of a neural circuit is the pair- wise separation property considered in [Maass et al., 2002]. This measure tells us to what extent the current circuit state x(t) reflects details of the input stream that occurred some time back in the past (see Fig. 1). Both circuit 2 and circuit 3 could be described as being chaotic since state differences resulting from earlier input differences persist. The ""edge-of- chaos"" [Langton, 1990] lies somewhere between points 1 and 2 according to Fig. 1c). But the best computational performance occurs between points 2 and 3 (see Fig. 2b)). Hence the ""edge-of-chaos"" is not a reliable predictor of computational power for circuits of spik- ing neurons. In addition, most real-world computational tasks require that the circuit gives a desired output not just for 2, but for a fairly large number m of significantly different inputs. One could of course test whether a circuit C can separate each of the m pairs of                                                                                           2
 1This assumption is best justified if such readout neuron is located for example in another brain area that receives massive input from many neurons in this microcircuit and only has diffuse back- wards projection. But it is certainly problematic and should be addressed in future elaborations of the present approach.      2One can be even more realistic and filter it also by a model for the short term dynamics of the synapse into the readout neuron, but this turns out to make no difference for the analysis proposed in this article.



                    8                   a                                                           4                                                                          b              state separation                                                    0.25                                                                                                                                    c                         4                                           7         2                                                                                          circuit 3                                                           0.2                                                                               0                         2                                           6                                                  3                                 0                  1             2    3                                                                     5     0.2                         1                                                                                                                                   0.15                        0.7          scale                         2                            4     0.1                        0.5     W                                                                                    circuit 2                                                           0.1                        0.3                                          3                                   1                                           00                      1             2    3              state separation                                                                     2     0.1                                                                               0.05                        0.1                                          1    0.05                   0.05                                                                   circuit 1                                                            0                          0.5     1  1.4    2      3   4   6   8               0                                                                                1.4    1.6    1.8    2    2.2                                                                                   0                  1             2    3                                                                                                                                                                t [s]

Figure 1: Pointwise separation property for different types of neural microcircuit models as specified in section 3. Each circuit C was tested for two arrays u and v of 4 input spike trains at 20 Hz over 3 s that differed only during the first second. a) Euclidean differences between resulting circuit states xu(t) and xv(t) for t = 3 s, averaged over 20 circuits C and 20 pairs u, v for each indicated value of  and Wscale (see section 3). b) Temporal evolution of                                                               xu(t) - xv(t)                                    for 3 different circuits with values of , Wscale according to the 3 points marked in panel a) ( = 1.4, 2, 3 and Wscale = 0.3, 0.7, 2 for circuit 1, 2, and 3 respectively). c) Pointwise separation along a straight line between point 1 and point 2 of panel a).
such inputs. But even if the circuit can do this, we do not know whether a neural readout from such circuit would be able to produce given target outputs for these m inputs.
Therefore we propose here the linear separation property as a more suitable quantitative measure for evaluating the computational power of a neural microcircuit (or more precisely: the kernel-quality of a circuit; see below). To evaluate the linear separation property of a circuit C for m different inputs u1, . . . , um (which are in this article always functions of time, i.e. input streams such as for example multiple spike trains) we compute the rank of the n  m matrix M whose columns are the circuit states xu (t                                                                                                                               i    0 ) resulting at some fixed time t0 for the preceding input stream ui. If this matrix has rank m, then it is guaranteed that any given assignment of target outputs yi  R at time t0 for the inputs ui can be implemented by this circuit C (in combination with a linear readout). In particular, each of the 2m possible binary classifications of these m inputs can then be carried out by a linear readout from this fixed circuit C. Obviously such insight is much more informative than a demonstration that some particular classification task can be carried out by such circuit C. If the rank of this matrix M has a value r < m, then this value r can still be viewed as a measure for the computational power of this circuit C, since r is the number of ""degrees of freedom"" that a linear readout has in assigning target outputs yi to these inputs ui (in a way which can be made mathematically precise with concepts of linear algebra). Note that this rank-measure for the linear separation property of a circuit C may be viewed as an empirical measure for its kernel-quality, i.e. for the complexity and diversity of nonlinear operations carried out by C on its input stream in order to boost the classification power of a subsequent linear decision-hyperplane (see [Vapnik, 1998]).
Obviously the preceding measure addresses only one component of the computational per- formance of a neural circuit C. Another component is its capability to generalize a learnt computational function to new inputs. Mathematical criteria for generalization capability are derived in [Vapnik, 1998] (see ch. 4 of [Cherkassky and Mulier, 1998] for a compact ac- count of results relevant for our arguments). According to this mathematical theory one can quantify the generalization capability of any learning device in terms of the VC-dimension of the class H of hypotheses that are potentially used by that learning device.3 More pre-
3The VC-dimension (of a class H of maps H from some universe Suniv of inputs into {0, 1}) is defined as the size of the largest subset S  Suniv which can be shattered by H. One says that S  Suniv is shattered by H if for every map f : S  {0, 1} there exists a map H in H such that H(u) = f (u) for all u  S (this means that every possible binary classification of the inputs u  S
cisely: if VC-dimension (H) is substantially smaller than the size of the training set Strain, one can prove that this learning device generalizes well, in the sense that the hypothesis (or input-output map) produced by this learning device is likely to have for new examples an error rate which is not much higher than its error rate on Strain, provided that the new examples are drawn from the same distribution as the training examples (see equ. 4.22 in [Cherkassky and Mulier, 1998]).
We apply this mathematical framework to the class HC of all maps from a set Suniv of inputs u into {0, 1} which can be implemented by a circuit C. More precisely: HC consists of all maps from Suniv into {0, 1} that a linear readout from circuit C with fixed internal parameters (weights etc.) but arbitrary weights w  Rn of the readout (that classifies the circuit input u as belonging to class 1 if w  xu(t0)  0, and to class 0 if w  xu(t0) < 0) could possibly implement.
Whereas it is very difficult to achieve tight theoretical bounds for the VC-dimension of even much simpler neural circuits, see [Bartlett and Maass, 2003], one can efficiently estimate the VC-dimension of the class HC that arises in our context for some finite ensemble Suniv of inputs (that contains all examples used for training or testing) by using the following mathematical result (which can be proved with the help of Radon's Theorem):
Theorem 2.1 Let r be the rank of the n  s matrix consisting of the s vectors xu(t0) for all inputs u in Suniv (we assume that Suniv is finite and contains s inputs). Then r  VC-dimension(HC)  r + 1.
We propose to use the rank r defined in Theorem 2.1 as an estimate of VC-dimension(HC ), and hence as a measure that informs us about the generalization capability of a neural microcircuit C. It is assumed here that the set Suniv contains many noisy variations of the same input signal, since otherwise learning with a randomly drawn training set Strain  Suniv has no chance to generalize to new noisy variations. Note that each family of computational tasks induces a particular notion of what aspects of the input are viewed as noise, and what input features are viewed as signals that carry information which is rel- evant for the target output for at least one of these computational tasks. For example for computations on spike patterns some small jitter in the spike timing is viewed as noise. For computations on firing rates even the sequence of interspike intervals and temporal rela- tions between spikes that arrive from different input sources are viewed as noise, as long as these input spike trains represent the same firing rates. Examples for both families of computational tasks will be discussed in this article.
3     Models for generic cortical microcircuits
We test the validity of the proposed measures by comparing their predictions with direct evaluations of the computational performance for a large variety of models for generic cor- tical microcircuits consisting of 540 neurons. We used leaky-integrate-and-fire neurons4 and biologically quite realistic models for dynamic synapses.5 Neurons (20 % of which were randomly chosen to be inhibitory) were located on the grid points of a 3D grid of dimensions 6  6  15 with edges of unit length. The probability of a synaptic connection
can be carried out by some hypothesis H in H).      4Membrane voltage V                     dVm                           m modeled by m           = -(V                                               dt             m -Vresting )+Rm (Isyn(t)+Ibackground + Inoise), where m = 30 ms is the membrane time constant, Isyn models synaptic inputs from other neurons in the circuits, Ibackground models a constant unspecific background input and Inoise models noise in the input.      5Short term synaptic dynamics was modeled according to [Markram et al., 1998], with distribu- tions of synaptic parameters U (initial release probability), D (time constant for depression), F (time constant for facilitation) chosen to reflect empirical data (see [Maass et al., 2002] for details).
from neuron a to neuron b was proportional to exp(-D2(a, b)/2), where D(a, b) is the Euclidean distance between a and b, and  regulates the spatial scaling of synaptic connec- tivity. Synaptic efficacies w were chosen randomly from distributions that reflect biological data (as in [Maass et al., 2002]), with a common scaling factor Wscale.
                                                                                                       8                                          0.7                                                                                                 b 4

            a                                                                                          2                        3                                                                                                                                                       0.65                                                                                                            1                                                                                                           0.7                                                                                                 scale                     2                                                                                                           0.5                                                                                            W                                                                                                           0.3        1                                0.6

                 0    50    100       150    200    0    50    100       150    200                   0.1                                 t [ms]                             t [ms]                                0.05                                                                                                             0.5     1  1.4    2     3   4   6   8

Figure 2: Performance of different types of neural microcircuit models for classification of spike patterns. a) In the top row are two examples of the 80 spike patterns that were used (each consisting of 4 Poisson spike trains at 20 Hz over 200 ms), and in the bottom row are examples of noisy variations (Gaussian jitter with SD 10 ms) of these spike patterns which were used as circuit inputs. b) Fraction of examples (for 200 test examples) that were correctly classified by a linear readout (trained by linear regression with 500 training examples). Results are shown for 90 different types of neural microcircuits C with  varying on the x-axis and Wscale on the y-axis (20 randomly drawn circuits and 20 target classification functions randomly drawn from the set of 280 possible classification functions were tested for each of the 90 different circuit types, and resulting correctness-rates were averaged. The mean SD of the results is 0.028.). Points 1, 2, 3 defined as in Fig. 1.
Linear readouts from circuits with n - 1 neurons were assumed to compute a weighted sum      n-1 w          i=1         ixi(t) + w0 (see section 1). In order to simplify notation we assume that the vector x(t) contains an additional constant component x0(t) = 1, so that one can write w  x(t) instead of                n-1 w                                    i=1           ixi(t) + w0. In the case of classification tasks we assume that the readout outputs 1 if w  x(t)  0, and 0 otherwise.
4      Evaluating the influence of synaptic connectivity on computational        performance
Neural microcircuits were drawn from the distribution described in section 3 for 10 differ- ent values of  (which scales the number and average distance of synaptically connected neurons) and 9 different values of Wscale (which scales the efficacy of all synaptic connec- tions). 20 microcircuit models C were drawn for each of these 90 different assignments of values to  and Wscale. For each circuit a linear readout was trained to perform one (randomly chosen) out of 280 possible classification tasks on noisy variations u of 80 fixed spike patterns as circuit inputs u. The target performance of any such circuit input was to output at time t = 100 ms the class (0 or 1) of the spike pattern from which the preceding circuit input had been generated (for some arbitrary partition of the 80 fixed spike patterns into two classes. Each spike pattern u consisted of 4 Poisson spike trains over 200 ms. Per- formance results are shown in Fig. 2b for 90 different types of neural microcircuit models.
We now test the predictive quality of the two proposed measures for the computational power of a microcircuit on spike patterns. One should keep in mind that the proposed measures do not attempt to test the computational capability of a circuit for one particu- lar computational task, but for any distribution on Suniv and for a very large (in general infinitely large) family of computational tasks that only have in common a particular bias regarding which aspects of the incoming spike trains may carry information that is relevant for the target output of computations, and which aspects should be viewed as noise. Fig. 3a
explains why the lower left part of the parameter map in Fig. 2b is less suitable for any
                     8                                                       8                                                       8                    a                                                       b                                                       c                                                20                          4                                          450          4                                          450          4

                     2                                          400          2                                          400          2                        3                 15                          1                                          350          1                                          350          1                         0.7                                                     0.7                                                     0.7           scale                                                                                                                                         2                         0.5      W                                                                          0.5                                                     0.5                                         10                                                                     300                                                     300                         0.3                                                     0.3                                                     0.3        1                                                                     250                                                     250                                                                                                                                                                                     5                         0.1                                                     0.1                                         200         0.1                                                                     200                    0.05                                                    0.05                                                    0.05                                             0                           0.5     1  1.4    2     3   4   6   8                   0.5     1  1.4    2     3   4   6   8                   0.5     1  1.4    2     3   4   6   8

Figure 3: Values of the proposed measures for computations on spike patterns. a) Kernel-quality for spike patterns of 90 different circuit types (average over 20 circuits, mean SD = 13; For each circuit, the average over 5 different sets of spike patterns was used).6 b) Generalization capability for spike patterns: estimated VC-dimension of HC (for a set Suniv of inputs u consisting of 500 jittered versions of 4 spike patterns), for 90 different circuit types (average over 20 circuits, mean SD = 14; For each circuit, the average over 5 different sets of spike patterns was used). c) Difference of both measures (mean SD = 5.3). This should be compared with actual computational performance plotted in Fig. 2b. Points 1, 2, 3 defined as in Fig. 1.
such computation, since there the kernel-quality of the circuits is too low. Fig. 3b explains why the upper right part of the parameter map in Fig. 2b is less suitable, since a higher VC-dimension (for a training set of fixed size) entails poorer generalization capability. We are not aware of a theoretically founded way of combining both measures into a single value that predicts overall computational performance. But if one just takes the difference of both measures then the resulting number (see Fig. 3c) predicts quite well which types of neural microcircuit models perform well for the particular computational tasks considered in Fig. 2b.
5                  Evaluating the computational power of neural microcircuit models                    in UP- and DOWN-states
Data from numerous intracellular recordings suggest that neural circuits in vivo switch be- tween two different dynamic regimes that are commonly referred to as UP- and DOWN states. UP-states are characterized by a bombardment with synaptic inputs from recurrent activity in the circuit, resulting in a membrane potential whose average value is signifi- cantly closer to the firing threshold, but also has larger variance. We have simulated these different dynamic regimes by varying the background current Ibackground and the noise current Inoise. Fig. 4a shows that one can simulate in this way different dynamic regimes of the same circuit where the time course of the membrane potential qualitatively matches data from intracellular recordings in UP- and DOWN-states (see e.g. [Shu et al., 2003]). We have tested the computational performance of circuits in 42 different dynamic regimes (for 7 values of Ibackground and 6 values of Inoise) with 3 complex nonlinear computations on firing rates of circuit inputs.7 Inputs u consisted of 4 Poisson spike trains with time- varying rates (drawn independently every 30 ms from the interval of 0 to 80 Hz for the first two and the second two of 4 input spike trains, see middle row of Fig. 4a for a sample). Let f1(t) (f2(t)) be the actual sum of rates normalized to the interval [0, 1] for the first
 6The rank of the matrix consisting of 500 circuit states xu(t) for t = 200 ms was computed for 500 spike patterns over 200 ms as described in section 2, see Fig. 2a.      7Computations on firing rates were chosen as benchmark tasks both because UP states were con- jectured to enhance the performance for such tasks, and because we want to show that the proposed measures are applicable to other types of computational tasks than those considered in section 4.



                                                                        16              a                                                                                                                                                        100                      UP-state          [mV]                                 14                                                                   m                                                                                          50                                                V                                                                             12                                                                                0



                                                                        16                                                                                                                                                        100               DOWN-state                             [mV]                   14                                                                        m                                                                                     50                                                              V                                                                             12                                                                                0                                                                               300                350    400                450            500                              350                400              450                500                                                                                                         t [ms]                                                                                t [ms]         b                                                                                                      c                                                                  d                    10                                                                                               10                                                                 10                                                                                                                                                                    120                     6                                                                            70                  6                                                                  6                                          0.2                    4.5                                                            UP                                4.5                                            100                 4.5                                                                                                  60                                                                                                                                0.15                    3.2                                                                                              3.2                                                                3.2                                                                                                                                                                    80         I noise                                                                                  50                                                                                                                                0.1                    1.9      DOWN                                                                                    1.9                                            60                  1.9                                                                                                  40                                                                                                                                0.05                                                                                                                                                                    40                                                                                                  30                                                                                                                                                                                                                                    0                    0.6                                                                                              0.6                                            20                  0.6                     11.5  12 12.5                                           13.5                                                                                           14.3                            11.5  12 12.5      13.5                                                                                                                                                       14.3                                   11.5  12 12.5         13.5                                                                                                                                                                                                                            14.3             e                                                                                                      f                                                                  g                    10                                                                                               10                                                                 10                                                                                                                                                                    0.25                     6                                                                                                6                                                                  6                                          0.3                    4.5                                                                           0.7                4.5                                                                4.5                                                                                                                                                                    0.2                    3.2                                                                                              3.2                                                                3.2

    I noise                                                                                  0.6                    1.9                                                                                              1.9                                            0.15                1.9                                         0.25


                                                                                             0.5                                                               0.1                                                                                                                                                                                                                                    0.2                    0.6                                                                                              0.6                                                                0.6                     11.5  12 12.5                                           13.5                                                                                           14.3                            11.5  12 12.5      13.5                                                                                                                                                       14.3                                   11.5  12 12.5         13.5                                                                                                                                                                                                                            14.3                                      I                                                                                                I                                                                     I                                  background                                                                                       background                                                            background

Figure 4: Analysis of the computational power of simulated neural microcircuits in different dy- namic regimes. a) Membrane potential (for a firing threshold of 15 mV) of two randomly selected neurons from circuits in the two parameter regimes marked in panel b), as well as spike rasters for the same two parameter regimes (with the actual circuit inputs shown between the two rows). b) Estimates of the kernel-quality for input streams u with 34 different combinations of firing rates from 0, 20, 40 Hz in the 4 input spike trains (mean SD = 12). c) Estimate of the VC-dimension for a set Suniv of inputs consisting of 200 different spike trains u that represent 2 different combinations of firing rates (mean SD = 4.6). d) Difference of measures from panels b and c (after scaling each lin- early into a common range [0,1]). e), f), g): Evaluation of the computational performance (correlation coefficient; all for test data; mean SD is 0.06, 0.04, and 0.03 for panels e), f), and g) respectively.) of the same circuits in different dynamic regimes for computations involving multiplication and abso- lute value of differences of firing rates (see text). The theoretically predicted parameter regime with good computational performance for any computations on firing rates (see panel d) agrees quite well with the intersection of areas with good computational performance in panels e, f, g.
two (second two) input spike trains computed from the time interval [t - 30ms, t]. The computational tasks considered in Fig. 4 were to compute online (and in real-time) every 30 ms the functions f1(t)  f2(t) (see panel e), to decide whether the value of the product f1(t)  f2(t) lies in the interval [0.1, 0.3] or lies outside of this interval (see panel f), and to decide whether the absolute value of the difference f1(t) - f2(t) is greater than 0.25 (see panel g).
We wanted to test whether the proposed measures for computational power and general- ization capability were able to make reasonable predictions for this completely different parameter map, and for computations on firing rates instead of spike patterns. It turns out that also in this case the kernel-quality (Fig. 4b) explains why circuits in the dynamic regime corresponding to the left-hand side of the parameter map have inferior computa- tional power for all three computations on firing rates (see Fig. 4 e,f,g). The VC-dimension (Fig. 4c) explains the decline of computational performance in the right part of the pa- rameter map. The difference of both measures (Fig. 4d) predicts quite well the dynamic regime where high performance is achieved for all three computational tasks considered in Fig. 4 e,f,g. Note that Fig. 4e has high performance in the upper right corner, in spite of a very high VC-dimension. This could be explained by the inherent bias of linear readouts
to compute smooth functions on firing rates, which fits particularly well to this particular target output.
If one estimates kernel-quality and VC-dimension for the same circuits, but for computa- tions on sparse spike patterns (for an input ensemble Suniv similarly as in section 4), one finds that circuits at the lower left corner of this parameter map (corresponding to DOWN- states) are predicted to have better computational performance for these computations on sparse input. This agrees quite well with direct evaluations of computational performance (not shown). Hence the proposed quantitative measures may provide a theoretical founda- tion for understanding the computational function of different states of neural activity."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/a3d06db1f8c85b2837b4603a51834425-Abstract.html,The Variational Ising Classifier (VIC) Algorithm for Coherently Contaminated Data,"Oliver Williams, Andrew Blake, Roberto Cipolla","There has been substantial progress in the past decade in the development           of object classifiers for images, for example of faces, humans and vehi-           cles. Here we address the problem of contaminations (e.g. occlusion,           shadows) in test images which have not explicitly been encountered in           training data. The Variational Ising Classifier (VIC) algorithm models           contamination as a mask (a field of binary variables) with a strong spa-           tial coherence prior. Variational inference is used to marginalize over           contamination and obtain robust classification. In this way the VIC ap-           proach can turn a kernel classifier for clean data into one that can tolerate           contamination, without any specific training on contaminated positives.
1     Introduction
Recent progress in discriminative object detection, especially for faces, has yielded good performance and efficiency [1, 2, 3, 4]. Such systems are capable of classifying those positives that can be generalized from positive training data. This is restrictive in practice in that test data may contain distortions that take it outside the strict ambit of the training positives. One example would be lighting changes (to a face) but this can be addressed reasonably effectively by a normalizing transformation applied to training and test images; doing so is common practice in face classification. Other sorts of disruption are not so easily factored out. A prime example is partial occlusion.
The aim of this paper is to extend a classifier trained on clean positives to accept also partially occluded positives, without further training. The approach is to capture some of the regularity inherent in a typical pattern of contamination, namely its spatial coherence. This can be thought of as extending the generalizing capability of a classifier to tolerate the sorts of image distortion that occur as a result of contamination.
As done previously in one-dimension, for image contours [5], the Variational Ising Classi- fier (VIC) models contamination explicitly as switches with a strong coherence prior in the form of an Ising model, but here over the full two-dimensional image array. In addition, the Ising model is loaded with a bias towards non-contamination. The aim is to incorporate these hidden contamination variables into a kernel classifier such as [1, 3]. In fact the Rel- evance Vector Machine (RVM) is particularly suitable [6] as it is explicitly probabilistic, so that contamination variables can be incorporated as a hidden layer of random variables.
                                                                  edge

                neighbours of i                        i

Figure 1: The 2D Ising model is applied over a graph with edges e   between neigh- bouring pixels (connected 4-wise).
Classification is done by marginalization over all possible configurations of the hidden vari- able array, and this is made tractable by variational (mean field) inference. The inference scheme makes use of ""hallucination"" to fill in parts of the object that are unobserved due to occlusion.
Results of VIC are given for face detection. First we show that the classifier performance is not significantly damaged by the inclusion of contamination variables. Then a contam- inated test set is generated using real test images and computer generated contaminations. Over this test data the VIC algorithm does indeed perform significantly better than a con- ventional classifier (similar to [4]). The hidden variable layer is shown to operate effec- tively, successfully inferring areas of contamination. Finally, inference of contamination is shown working on real images with real contaminations.
2      Bayesian modelling of contamination
Classification requires P (F |I), the posterior for the proposition F that an object is present given the image data intensity array I. This can be computed in terms of likelihoods
             P (F | I) = P (I | F )P (F )/ P (I | F )P (F ) + P (I | F )P (F )          (1)

so then the test P (F | I) > 1 becomes                                 2
                             log P (I | F ) - log P (I | F ) > t                        (2)

where t is a prior-dependent threshold that controls the tradeoff between positive and neg- ative classification errors. Suppose we are given a likelihood P (I|, F ) for the presence of a face given contamination , an array of binary ""observation"" variables corresponding to each pixel Ij of I, such that j = 0 indicates contamination at that pixel, whereas j = 1 indicates a successfully observed pixel. Then, in principle,
                                 P (I|F ) =         P (I|, F )P (),                   (3)

(making the reasonable assumption P (|F ) = P (), that the pattern of contamination is object independent) and similarly for log P (I | F ). The marginalization itself is intractable, requiring a summation over all 2N possible configurations of , for images with N pixels. Approximating that marginalization is dealt with in the next section. In the meantime, there are two other problems to deal with: specifying the prior P (); and specifying the likeli- hood under contamination P (I|, F ) given only training data for the unoccluded object.
2.1    Prior over contaminations
The prior contains two terms: the first expresses the belief that contamination will occur in coherent regions of a subimage. This takes the form of an Ising model [7] with energy
UI() that penalizes adjacent pixels which differ in their labelling (see Figure 1); the second term UC biases generally against contamination a priori and its balance with the first term is mediated by the constant . The total prior energy is then
          U () = UI() + UC() =              [1 - (e -  )] +                     (                                                                  1         e2                      j ),     (4)                                              e                                      j

where (x) = 1 if x = 0 and 0 otherwise, and e1, e2 are the indices of the pixels at either end of edge e   (figure 1). The prior energy determines a probability via a temperature constant 1/T0 [7]:
                   P ()  e-U()/T0 = e-UI()/T0e-UC()/T0                                            (5)

2.2    Relevance vector machine
An unoccluded classifier P (F |I,  = 0) can be learned from training data using a Rele- vance Vector Machine (RVM) [6], trained on a database of frontal face and non-face im- ages [8] (see Section 4 for details). The probabilistic properties of the RVM make it a good choice when (later) it comes to marginalising over . For now we consider how to construct the likelihood itself. First the conventional, unoccluded case is considered for which the posterior P (F |I) is learned from positive and negative examples. Kernel functions [9] are computed between a candidate image I and a subset of relevance vectors {xk}, retained from the training set. Gaussian kernels are used here to compute
                       y(I) =         wk exp -             (Ij - xkj)2 .                               (6)                                      k                     j

where wk are learned weights, and xkj is the jth pixel of the kth relevance vector. Then the posterior is computed via the logistic sigmoid function as
                                                                  1                            P (F |I,  = 1) = (y(I)) =                           .                          (7)                                                                  1 + e-y(I)

and finally the unoccluded data-likelihood would be
                          P (I|F,  = 1)  (y(I))/P (F ).                                              (8)

2.3    Hallucinating appearance
The aim now is to derive the occluded likelihood from the unoccluded case, where the con- tamination mask is known, without any further training. To do this, (8) must be extended to give P (I|F, ) for arbitrary masks , despite the fact the pixels Ij from the object are not observed wherever j = 0. In principle one should take into account all possible (or at least probable) values for the occluded pixels. Here, for simplicity, a single fixed hallu- cination is substituted for occluded pixels, then we proceed as if those values had actually been observed. This gives
                            P (I|F, )  (~                                                     y(I, ))/P (F )                                         (9)"
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/a431d70133ef6cf688bc4f6093922b48-Abstract.html,Generalization Error and Algorithmic Convergence of Median Boosting,Balázs Kégl,"We have recently proposed an extension of ADABOOST to regression           that uses the median of the base regressors as the final regressor. In this           paper we extend theoretical results obtained for ADABOOST to median           boosting and to its localized variant. First, we extend recent results on ef-           ficient margin maximizing to show that the algorithm can converge to the           maximum achievable margin within a preset precision in a finite number           of steps. Then we provide confidence-interval-type bounds on the gener-           alization error."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/a4613e8d72a61b3b69b32d040f89ad81-Abstract.html,Supervised Graph Inference,"Jean-philippe Vert, Yoshihiro Yamanishi","We formulate the problem of graph inference where part of the graph is           known as a supervised learning problem, and propose an algorithm to           solve it. The method involves the learning of a mapping of the vertices           to a Euclidean space where the graph is easy to infer, and can be formu-           lated as an optimization problem in a reproducing kernel Hilbert space.           We report encouraging results on the problem of metabolic network re-           construction from genomic data."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/a7789ef88d599b8df86bbee632b2994d-Abstract.html,Confidence Intervals for the Area Under the ROC Curve,"Corinna Cortes, Mehryar Mohri","In many applications, good ranking is a highly desirable performance for          a classifier. The criterion commonly used to measure the ranking quality          of a classification algorithm is the area under the ROC curve (AUC). To          report it properly, it is crucial to determine an interval of confidence for          its value. This paper provides confidence intervals for the AUC based          on a statistical and combinatorial analysis using only simple parameters          such as the error rate and the number of positive and negative examples.          The analysis is distribution-independent, it makes no assumption about          the distribution of the scores of negative or positive examples. The results          are of practical use and can be viewed as the equivalent for AUC of the          standard confidence intervals given in the case of the error rate. They          are compared with previous approaches in several standard classification          tasks demonstrating the benefits of our analysis."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/a8345c3bb9e3896ea538ce77ffaf2c20-Abstract.html,Maximising Sensitivity in a Spiking Network,"Anthony J. Bell, Lucas C. Parra","We use unsupervised probabilistic machine learning ideas to try to ex- plain the kinds of learning observed in real neurons, the goal being to connect abstract principles of self-organisation to known biophysi- cal processes. For example, we would like to explain Spike Timing- Dependent Plasticity (see [5,6] and Figure 3A), in terms of information theory. Starting out, we explore the optimisation of a network sensitiv- ity measure related to maximising the mutual information between input spike timings and output spike timings. Our derivations are analogous to those in ICA, except that the sensitivity of output timings to input tim- ings is maximised, rather than the sensitivity of output ‘ﬁring rates’ to inputs. ICA and related approaches have been successful in explaining the learning of many properties of early visual receptive ﬁelds in rate cod- ing models, and we are hoping for similar gains in understanding of spike coding in networks, and how this is supported, in principled probabilistic ways, by cellular biophysical processes. For now, in our initial simula- tions, we show that our derived rule can learn synaptic weights which can unmix, or demultiplex, mixed spike trains. That is, it can recover inde- pendent point processes embedded in distributed correlated input spike trains, using an adaptive single-layer feedforward spiking network.
1 Maximising Sensitivity.
In this section, we will follow the structure of the ICA derivation [4] in developing the spiking theory. We cannot claim, as before, that this gives us an information maximisation algorithm, for reasons that we will delay addressing until Section 3. But for now, to ﬁrst develop our approach, we will explore an interim objective function called sensitivity which we deﬁne as the log Jacobian of how input spike timings affect output spike timings.
1.1 How to maximise the effect of one spike timing on another.
Consider a spike in neuron j at time tl that has an effect on the timing of another spike in neuron i at time tk. The neurons are connected by a weight wij. We use i and j to index neurons, and k and l to index spikes, but sometimes for convenience we will use spike indices in place of neuron indices. For example, wkl, the weight between an input spike l and an output spike k, is naturally understood to be just the corresponding wij."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/a8aa681aaa4588a8dbd3b42b26d59a1a-Abstract.html,Probabilistic Computation in Spiking Populations,"Richard S. Zemel, Rama Natarajan, Peter Dayan, Quentin J. Huys","As animals interact with their environments, they must constantly update            estimates about their states. Bayesian models combine prior probabil-            ities, a dynamical model and sensory evidence to update estimates op-            timally. These models are consistent with the results of many diverse            psychophysical studies. However, little is known about the neural rep-            resentation and manipulation of such Bayesian information, particularly            in populations of spiking neurons. We consider this issue, suggesting a            model based on standard neural architecture and activations. We illus-            trate the approach on a simple random walk example, and apply it to            a sensorimotor integration task that provides a particularly compelling            example of dynamic probabilistic computation.
Bayesian models have been used to explain a gamut of experimental results in tasks which require estimates to be derived from multiple sensory cues. These include a wide range of psychophysical studies of perception;13 motor action;7 and decision-making.3, 5 Central to Bayesian inference is that computations are sensitive to uncertainties about afferent and efferent quantities, arising from ignorance, noise, or inherent ambiguity (e.g., the aperture problem), and that these uncertainties change over time as information accumulates and dissipates. Understanding how neurons represent and manipulate uncertain quantities is therefore key to understanding the neural instantiation of these Bayesian inferences.
Most previous work on representing probabilistic inference in neural populations has fo- cused on the representation of static information.1, 12, 15 These encompass various strategies for encoding and decoding uncertain quantities, but do not readily generalize to real-world dynamic information processing tasks, particularly the most interesting cases with stim- uli changing over the same timescale as spiking itself.11 Notable exceptions are the re- cent, seminal, but, as we argue, representationally restricted, models proposed by Gold and Shadlen,5 Rao,10 and Deneve.4
In this paper, we first show how probabilistic information varying over time can be repre- sented in a spiking population code. Second, we present a method for producing spiking codes that facilitate further processing of the probabilistic information. Finally, we show the utility of this method by applying it to a temporal sensorimotor integration task.
1      TRAJECTORY ENCODING AND DECODING
We assume that population spikes R(t) arise stochastically in relation to the trajectory X(t) of an underlying (but hidden) variable. We use RT and XT for the whole trajectory and
spike trains respectively from time 0 to T . The spikes RT constitute the observations and are assumed to be probabilistically related to the signal by a tuning function f (X, i):
                           P (R(i, T )|X(T ))  f (X, i)                                                        (1)

for the spike train of the ith neuron, with parameters i. Therefore, via standard Bayesian inference, RT determines a distribution over the hidden variable at time T , P (X(T )|RT ).
We first consider a version of the dynamics and input coding that permits an analytical examination of the impact of spikes. Let X(t) follow a stationary Gaussian process such that the joint distribution P (X(t1), X(t2), . . . , X(tm)) is Gaussian for any finite collection of times, with a covariance matrix which depends on time differences: Ctt = c(|t - t |). Function c(|t|) controls the smoothness of the resulting random walks. Then,
     P (X(T )|RT )  p(X(T ))                      dX(T )P (R                                               X(T )                    T |X(T ))P (X(T )|X (T ))                     (2)

where P (X(T )|X(T )) is the distribution over the whole trajectory X(T ) conditional on the value of X(T ) at its end point. If RT are a set of conditionally independent inhomoge- neous Poisson processes, we have
       P (RT |X(T ))                 f (X(t                                        d f (X( ),                                      i              i ), i) exp -      i                               i)    ,    (3)

where ti  are the spike times  of neuron i in RT . Let  = [X(ti )] be the vector of stimulus positions at the times at which we observed a spike and  = [(ti )] be the vector of spike positions. If the tuning functions are Gaussian f (X, i)  exp(-(X - i)2/22) and sufficiently dense that                d f (X,                                 i                       i) is independent of X (a standard assumption in population coding), then P (RT |X(T ))  exp(-  -  2/22) and in Equation 2, we can marginalize out X(T ) except at the spike times ti :
P (X(T )|RT )  p(X(T ))               d exp -[, X(T )]T C-1 [, X(T )] - - 2                                 (4)                                                                              2                            22
and C is the block covariance matrix between X(ti ), x(T ) at the spike times [tt ] and the final time T . This Gaussian integral has P (X(T )|RT )  N ((T ), (T )), with
           (T ) = CT t(Ctt + I2)-1 = k                         (T ) = CT T - kCtT                           (5)

CT T is the T, T th element of the covariance matrix and CT t is similarly a row vector. The dependence in  on past spike times is specified chiefly by the inverse covariance matrix, and acts as an effective kernel (k). This kernel is not stationary, since it depends on factors such as the local density of spiking in the spike train RT .
For example, consider where X(t) evolves according to a diffusion process with drift:
                                 dX = -Xdt +  dN (t)                                                           (6)

where  prevents it from wandering too far, N (t) is white Gaussian noise with mean zero and 2 variance. Figure 1A shows sample kernels for this process.
Inspection of Figure 1A reveals some important traits. First, the monotonically decreasing kernel magnitude as the time span between the spike and the current time T grows matches the intuition that recent spikes play a more significant role in determining the posterior over X(T ). Second, the kernel is nearly exponential, with a time constant that depends on the time constant of the covariance function and the density of the spikes; two settings of these parameters produced the two groupings of kernels in the figure. Finally, the fully adaptive kernel k can be locally well approximated by a metronomic kernel k (shown in red in Figure 1A) that assumes regular spiking. This takes advantage of the general fact, indicated by the grouping of kernels, that the kernel depends weakly on the actual spike pattern, but strongly on the average rate. The merits of the metronomic kernel are that it is stationary and only depends on a single mean rate rather than the full spike train RT . It also justifies
                                              Kernels  k and ks                                      Variance ratio                                                               Full kernel                                              A                                                      B                                                            D                                                                                                                                          -0.5

                                   -2                                                     10                              10                                                                                2  /                                                                        0                                        -4                                      2              5                                           Space                              10                                                          

 Kernel size (weight)                                                                                               0                                                    0.5                                         0          0.03    0.06     0.09                            0.04         0.06     0.08    0.1                                                       t-tspike                                                      Time                                              C                    True stimulus and means                                                                                     Regular, stationary kernel                                                                                                                                                                  E                                       0.5                                                                                                -0.5





                                   0                                                                                                                    0                              Space                                                                                                                 Space



                         -0.5                                                                                                                  0.5                                        0.03        0.04     0.05       0.06                   0.07          0.08         0.09     0.1                       0.03      0.04    0.05    0.06    0.07    0.08    0.09    0.1                                                                             Time                                                                                                         Time

Figure 1: Exact and approximate spike decoding with the Gaussian process prior. Spikes are shown in yellow, the true stimulus in green, and P (X(T )|RT ) in gray. Blue: exact inference with nonstationary and red: approximate inference with regular spiking. A Ker- nel samples for a diffusion process as defined by equations 5, 6. B, C: Mean and variance of the inference. D: Exact inference with full kernel k and E: approximation based on metronomic kernel k. (Equation 7).
the form of decoder used for the network model in the next section.6 Figure 1D shows an example of how well Equation 5 specifies a distribution over X(t) through very few spikes.
Finally, 1E shows a factorized approximation with the stationary kernel similar to that used by Hinton and Brown6 and in our recurrent network:
                                                  ^                                                                            t                                                      P (X(t)|R(t))                                              f (X,                            kst                                                                                                                                    j=0                      j ij = exp(-E(X(t), R(t), t)),                              (7)                                                                                                             i               i)

By design, the mean is captured very well, but not the variance, which in this example grows too rapidly for long interspike intervals (Figure 1B, C). Using a slower kernel im- proves performance on the variance, but at the expense of the mean. We thus turn to the net- work model with recurrent connections that are available to reinstate the spike-conditional characteristics of the full kernel.
2                                       NETWORK MODEL FORMULATION
Above we considered how population spikes RT specify a distribution over X(T ). We now extend this to consider how interconnected populations of neurons can specify distributions over time-varying variables. We frame the problem and our approach in terms of a two-level network, connecting one population of neurons to another; this construction is intended to apply to any level of processing. The network maps input population spikes R(t) to output population spikes S(t), where input and output evolve over time. As with the input spikes, ST indicates the output spike trains from time 0 to T , and these output spikes are assumed to determine a distribution over a related hidden variable.
For the recurrent and feedforward computation in the network, we start with the de- ceptively simple goal9 of producing output spikes in such a way that the distribution Q(X(T )|ST ) they imply over the same hidden variable X(T ) as the input, faithfully matches P (X(T )|RT ). This might seem a strange goal, since one could surely just lis- ten to the input spikes. However, in order for the output spikes to track the hidden variable, the dynamics of the interactions between the neurons must explicitly capture the dynamics
of the process X(T ). Once this `identity mapping' problem has been solved, more general, complex computations can be performed with ease. We illustrate this on a multisensory integration task, tracking a hidden variable that depends on multiple sensory cues.
The aim of the recurrent network is to take the spikes R(t) as inputs, and produce output spikes that capture the probabilistic dynamics. We proceed in two steps. We first consider the probabilistic decoding process which turns ST into Q(X(t)|ST ). Then we discuss the recurrent and feedforward processing that produce appropriate ST given this decoder. Note that this decoding process is not required for the network processing; it instead provides a computational objective for the spiking dynamics in the system.
We use a simple log-linear decoder based on a spatiotemporal kernel:6
                           Q(X(T )|ST )  exp(-E(X(T ), ST , T )) , where                                                   (8)                                E(X, S                             T                                               T , T ) =                   S(j, T -  )                                                              j     =0                            j (X,  )                     (9)

is an energy function, and the spatiotemporal kernels are assumed separable: j(X,  ) = gj(X)( ). The spatial kernel gj(X) is related to the receptive field f (X, j) of neuron j and the temporal kernel j(X,  ) to k"
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/ab452534c5ce28c4fbb0e102d4a4fb2e-Abstract.html,Theory of localized synfire chain: characteristic propagation speed of stable spike pattern,"Kosuke Hamaguchi, Masato Okada, Kazuyuki Aihara","Repeated spike patterns have often been taken as evidence for the synfire          chain, a phenomenon that a stable spike synchrony propagates through          a feedforward network. Inter-spike intervals which represent a repeated          spike pattern are influenced by the propagation speed of a spike packet.          However, the relation between the propagation speed and network struc-          ture is not well understood. While it is apparent that the propagation          speed depends on the excitatory synapse strength, it might also be related          to spike patterns. We analyze a feedforward network with Mexican-Hat-          type connectivity (FMH) using the Fokker-Planck equation. We show          that both a uniform and a localized spike packet are stable in the FMH          in a certain parameter region. We also demonstrate that the propagation          speed depends on the distinct firing patterns in the same network."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/ad47a008a2f806aa6eb1b53852cd8b37-Abstract.html,Semi-supervised Learning on Directed Graphs,"Dengyong Zhou, Thomas Hofmann, Bernhard Schölkopf","Given a directed graph in which some of the nodes are labeled, we inves- tigate the question of how to exploit the link structure of the graph to infer the labels of the remaining unlabeled nodes. To that extent we propose a regularization framework for functions de(cid:2)ned over nodes of a directed graph that forces the classi(cid:2)cation function to change slowly on densely linked subgraphs. A powerful, yet computationally simple classi(cid:2)cation algorithm is derived within the proposed framework. The experimental evaluation on real-world Web classi(cid:2)cation problems demonstrates en- couraging results that validate our approach."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/b096577e264d1ebd6b41041f392eec23-Abstract.html,Class-size Independent Generalization Analsysis of Some Discriminative Multi-Category Classification,Tong Zhang,"We consider the problem of deriving class-size independent generaliza-          tion bounds for some regularized discriminative multi-category classi-          fication methods. In particular, we obtain an expected generalization          bound for a standard formulation of multi-category support vector ma-          chines.    Based on the theoretical result, we argue that the formula-          tion over-penalizes misclassification error, which in theory may lead to          poor generalization performance. A remedy, based on a generalization          of multi-category logistic regression (conditional maximum entropy), is          then proposed, and its theoretical properties are examined."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/b1c00bcd4b5183705c134b3365f8c45e-Abstract.html,ℓ₀-norm Minimization for Basis Selection,"David P. Wipf, Bhaskar D. Rao","Finding the sparsest, or minimum ℓ0-norm, representation of a signal given an overcomplete dictionary of basis vectors is an important prob- lem in many application domains. Unfortunately, the required optimiza- tion problem is often intractable because there is a combinatorial increase in the number of local minima as the number of candidate basis vectors increases. This deﬁciency has prompted most researchers to instead min- imize surrogate measures, such as the ℓ1-norm, that lead to more tractable computational methods. The downside of this procedure is that we have now introduced a mismatch between our ultimate goal and our objective function. In this paper, we demonstrate a sparse Bayesian learning-based method of minimizing the ℓ0-norm while reducing the number of trou- blesome local minima. Moreover, we derive necessary conditions for local minima to occur via this approach and empirically demonstrate that there are typically many fewer for general problems of interest."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/b21f9f98829dea9a48fd8aaddc1f159d-Abstract.html,The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees,"Ofer Dekel, Shai Shalev-shwartz, Yoram Singer","Prediction suffix trees (PST) provide a popular and effective tool for tasks          such as compression, classification, and language modeling. In this pa-          per we take a decision theoretic view of PSTs for the task of sequence          prediction. Generalizing the notion of margin to PSTs, we present an on-          line PST learning algorithm and derive a loss bound for it. The depth of          the PST generated by this algorithm scales linearly with the length of the          input. We then describe a self-bounded enhancement of our learning al-          gorithm which automatically grows a bounded-depth PST. We also prove          an analogous mistake-bound for the self-bounded algorithm. The result          is an efficient algorithm that neither relies on a-priori assumptions on the          shape or maximal depth of the target PST nor does it require any param-          eters. To our knowledge, this is the first provably-correct PST learning          algorithm which generates a bounded-depth PST while being competi-          tive with any fixed PST determined in hindsight."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/b38e5ff5f816ac6e4169bce9314b2996-Abstract.html,Modelling Uncertainty in the Game of Go,"David H. Stern, Thore Graepel, David MacKay","Go is an ancient oriental game whose complexity has defeated at-           tempts to automate it. We suggest using probability in a Bayesian           sense to model the uncertainty arising from the vast complexity           of the game tree. We present a simple conditional Markov ran-           dom field model for predicting the pointwise territory outcome of           a game. The topology of the model reflects the spatial structure of           the Go board. We describe a version of the Swendsen-Wang pro-           cess for sampling from the model during learning and apply loopy           belief propagation for rapid inference and prediction. The model           is trained on several hundred records of professional games. Our           experimental results indicate that the model successfully learns to           predict territory despite its simplicity.
1      Introduction
The game of Go originated in China over 4000 years ago. Its rules are simple (See www.gobase.org for an introduction). Two players, Black and White, take turns to place stones on the intersections of an N  N grid (usually N = 19 but smaller boards are in use as well). All the stones of each player are identical. Players place their stones in order to create territory by occupying or surrounding areas of the board. The player with the most territory at the end of the game is the winner. A stone is captured if it has been completely surrounded (in the horizontal and vertical directions) by stones of the opponent's colour. Stones in a contiguous `chain' have the common fate property: they are captured all together or not at all [1].
The game that emerges from these simple rules has a complexity that defeats at- tempts to apply minimax search. The best Go programs play only at the level of weak amateur Go players and Go is therefore considered to be a serious AI challenge not unlike Chess in the 1960s. There are two main reasons for this state of affairs: firstly, the high branching factor of Go (typically 200 to 300 potential moves per position) prevents the expansion of a game tree to any useful depth. Secondly, it is difficult to produce an evaluation function for Go positions. A Go stone has no intrinsic value; its value is determined by its relationships with other stones. Go players evaluate positions using visual pattern recognition and qualitative intuitions which are difficult to formalise.
Most Go programs rely on a large amount of hand-tailored rules and expert knowl-
edge [2]. Some machine learning techniques have been applied to Go with limited success. Schraudolph, Dayan and Sejnowski [3] trained a multi-layer perceptron to evaluate board positions by temporal difference learning. Enzenberger [4] improved on this by structuring the topologies of his neural networks according to the rela- tionships between stones on the board. Graepel et al. [1] made use of the common fate property of chains to construct an efficient graph-based representation of the board. They trained a Support Vector Machine to use this representation to solve Go problems.
Our starting point is the uncertainty about the future course of the game that arises from the vast complexity of the game tree. We propose to explicitly model this uncertainty using probability in a Bayesian sense. The Japanese have a word, aji, much used by Go players. Taken literally it means `taste'. Taste lingers, and likewise the influence of a Go stone lingers (even if it appears weak or dead) because of the uncertainty of the effect it may have in the future. We use a probabilistic model that takes the current board position and predicts for every intersection of the board if it will be Black or White territory. Given such a model the score of the game can be predicted and hence an evaluation function produced. The model is a conditional Markov random field [5] which incorporates the spatial structure of the Go board.
2    Models for Predicting Territory
Consider the Go board as an undirected Graph G = (N , E) with N = Nx Ny nodes n  N representing vertices on the board and edges e  E connecting vertically and horizontally neighbouring points. We can denote a position as the vector c  {Black, White, Empty}N for cn = c(n) and similarly the final territory outcome of the game as s  {+1, -1}N for sn = s(n). For convenience we score from the point of view of Black so elements of s representing Black territory are valued +1 and elements representing white territory are valued -1. Go players will note that we are adopting the Chinese method of scoring empty as well as occupied intersections. The distribution we wish to model is P (s|c), that is, the distribution over final territory outcomes given the current position. Such a model would be useful for several reasons.
   Most importantly, the detailed outcomes provide us with a simple evalua-         tion function for Go positions by the expected score, u(c) :=           s                                                                            i         i P (s|c).         An alternative (and probably better) evaluation function is given by the         probability of winning which takes the form P (Black wins) = P (                    s                                                                                        i         i >         komi), where komi refers to the winning threshold for Black.        Connectivity of stones is vital because stones can draw strength from other         stones. Connectivity could be measured by the correlation between nodes         under the distribution P (s|c). This would allow us to segment the board         into `groups' of stones to reduce complexity.        It would also be useful to observe cases where we have an anti-correlation         between nodes in the territory prediction. Japanese refer to such cases as         miai in which only one of two desired results can be achieved at the expense         of the other - a consequence of moving in turns.        The fate of a group of Go stones could be estimated from the distribution         P (s|c) by marginalising out the nodes not involved.

The way stones exert long range influence can be considered recursive. A stone influences its neighbours, who influence their neighbours and so on. A simple model
which exploits this idea is to consider the Go board itself as an undirected graphical model in the form of a Conditional Random Field (CRF) [5]. We factorize the distribution as                       1                                                     1                                                              P (s|c) =                                                                        exp                 log(                 Z(                         f (sf , cf , f ) =                                                    f (sf , cf , f ))                    c, )                                              Z(c, )                                                          .                             f F                                                                 f F                                                                                                                                              (1) The simplest form of this model has one factor for each pair of neighbouring nodes i, j so f (sf , cf , f ) = f (si, sj, ci, cj, f ).
Boltzmann5             For our first model we decompose the factors into coupling' terms andexternal field' terms as follows:                                  1                                                                                                     P (s|c) =                        exp                     {w(c                            Z(                                              i, cj )sisj + h(ci)si + h(cj )sj }                              c, )                                                                                                         (2)                                                        (i,j)F
This gives a Boltzmann machine whose connections have the grid topology of the board. The couplings between territory-outcome nodes depend on the current board position local to those nodes and the external field at each node is determined by the state of the board at that location. We assume that Go positions with their associated territory positions are symmetric with respect to colour reversal so f (si, sj, ci, cj, f ) = f (-si, -sj, -ci, -cj, f ). Pairwise connections are also in- variant to direction reversal so f (si, sj, ci, cj, f ) = f (sj, si, cj, ci, f ). It follows that the model described in 2 can be specified by just five parameters:
    wchains = w(Black, Black) = w(White, White),         winter-chain = w(Black, White) = w(White, Black),         wchain-empty = w(Empty, White) = w(Empty, Black),         wempty = w(Empty, Empty),         hstones = h(Black) = -h(White),

and h(empty) is set to zero by symmetry. We will refer to this model as Boltzmann5. This simple model is interesting because all these parameters are readily interpreted. For example we would expect wchains to take on a large positive value since chains have common fate.
BoltzmannLiberties                    A feature that has particular utility for evaluating Go po- sitions is the number of liberties associated with a chain of stones. A liberty of a chain is an empty vertex adjacent to it. The number of liberties indicates a chain's safety because the opponent would have to occupy all the liberties to capture the chain. Our second model takes this information into account:                                              1                                                                                          P (s|c) =                        exp                    w(c                                       Z(                                             i, cj , si, sj , li, lj )                                              c, )                                                                ,                        (3)                                                                    (i,j)F
where li is element i of a vector l  {+1, +2, +3, 4 or more}N the liberty count of each vertex on the Go board. A group with four or more liberties is considered relatively safe. Again we can apply symmetry arguments and end up with 78 parameters. We will refer to this model as BoltzmannLiberties.
We trained the two models using board positions from a database of 22,000 games between expert Go players1. The territory outcomes of a subset of these games
1The GoGoD database, April 2003. URL:http://www.gogod.demon.co.uk
 (a) Gibbs Sampling                       (b) Swendsen Wang

Figure 1: Comparing ordinary Gibbs with Swendsen Wang sampling for Boltz- mann5. Shown are the differences between the running averages and the exact marginals for each of the 361 nodes plotted as a function of the number of whole- board samples.
were determined using the Go program GnuGo2 to analyse their final positions. Each training example comprised a board position c, with its associated territory outcome s. Training was performed by maximising the likelihood ln P (s |c) using gradient descent. In order to calculate the likelihood it is necessary to perform inference to obtain the marginal expectations of the potentials.
3     Inference Methods
It is possible to perform exact inference on the model by variable elimination [6]. Eliminating nodes one diagonal at a time gave an efficient computation. The cost of exact inference was still too high for general use but it was used to compare other inference methods.
Sampling      The standard method for sampling from a Boltzmann machine is to use Gibbs sampling where each node is updated one at a time, conditional on the others. However, Gibbs sampling mixes slowly for spin systems with strong correlations. A generalisation of the Swendsen-Wang process [7] alleviates this problem. The original Swendsen-Wang algorithm samples from a ferromagnetic Ising model with no external field by adding an additional set of bond' nodes d, one attached to each factor (edge) in the original graph. Each of these nodes can either be in the statebond' or no bond'. The new factor potentials f (sf , cf , df , f ) are chosen such that if a bond exists between a pair of spins then they are forced to be in the same state. Conditional on the bonds, each cluster has an equal probability of having all its spins in theup' state or all in the down' state. The algorithm samples from P (s|d, c, ) and P (d|s, c, ) in turn (flipping clusters and forming bonds respectively). It can be generalised to models with arbitrary couplings and biases [7, 8]. The new factor potentials f (sf , cf , df , f ) have the following effect: if the coupling is positive then when the d node is in thebond' state it forces the two spins to be in the same state; if the coupling is negative the `bond' state forces the two spins to be opposite. The probability of each cluster being in each state depends on the sum of the biases involved. Figure 1 shows that the mixing rate of the sampling process is improved by using Swendsen-Wang allowing us to find accurate marginals for a single position in a couple of seconds.
 2URL:http://www.gnu.org/software/gnugo/gnugo.html

Loopy Belief Propagation                             In order to perform very rapid (approximate) infer- ence we used the loopy belief propagation (BP) algorithm [9] and the results are examined in Section 4. This algorithm is similar to an influence function [10], as often used by Go programmers to segment the board into Black and White territory and for this reason is laid out below.
For each board vertex j  N , create a data structure called a node containing:
   1. A(j), the set of nodes corresponding to the neighbours of vertex j,        2. a set of new messages mnew(s                                                        ij     j )  Mnew , one for each i  A(j),

   3. a set of old messages mold(s                                                       ij     j )  Mold, one for each i  A(j),

   4. a belief bj(sj).

repeat      for all j  N do        for all i  A(j) do          for all sj  {Black, White} do                let variable SUM := 0,                for all si  {Black, White} do                  SUM := SUM + (i,j)(si, sj)                                        mold(s                                                                                      qi       i),                                                                 qA(i)\j                end for                mnew(s                 ij       j ) := SUM,          end for        end for      end for      for all messages, mnew(s                                  xy           y )  Mnew do        mnew(s                         (s                              (s          xy      y ) := mold                                 xy          y ) + (1 - )mnew                                                                 xy          y ),      end for until completed I iterations (typically I=50)
Belief Update: for all j  N do      for all sj  {Black, White} do        bj(sj) :=                mnew(s                                  qj           j )                       qA(j)      end for end for
Here,  (typically 0.5), damps any oscillations. (i,j)(si, sj) is the factor poten- tial (see (1)) and in the case of Boltzmann5 takes on the form (i,j)(si, sj) = exp (w(ci, cj)sisj + h(ci)si + h(cj)sj). Now the probability of each vertex being Black or White territory is found by normalising the beliefs at each node. For example P (sj = Black) = bj(Black)/Z where Z = bj(Black) + bj(White). The accuracy of the loopy BP approximation appears to be improved by using it during the parameter learning stage in cases where it is to be used in evaluation.
4      Results for Territory Prediction
Some Learnt Parameters                               Here are some parameters learnt for the Boltzmann5 model (2). This model was trained on 290 positions from expert Go games at move 80. Training was performed by maximum likelihood as described in Section 2.
(a) Boltzmann5 (Exact)                         (b) Boltzmann5 (Loopy BP)
Figure 2: Comparing territory predictions for a Go position from a professional game at move 90. The circles represent stones. The small black and white squares at each vertex represent the average territory prediction at that vertex, from -1 (maximum white square) to +1 (maximum black square).
   h                                 The values of these parameters can be in-              stones = 0.265               terpreted. For example w        w                                                             chains corresponds              empty = 0.427                to the correlation between the likely territory        wchain-empty = 0.442              outcome of two adjacent vertices in a chain of        w                                 connected stones. The high value of this pa-              chains = 2.74                rameter derives from the `common fate' prop-        winter-chain = 0.521              erty of chains as described in Section 1.

Interestingly, the value of the parameter wempty (corresponding to the coupling between territory predictions of neighbouring vertices in empty space) is 0.427 which is close to the critical coupling for an Ising model, 0.441.
Territory Predictions          Figure 2 gives examples of territory predictions generated by Boltzmann5. In comparison, Figure 3 shows the prediction of BoltzmannLiberties and a territory prediction from The Many Faces of Go [2]. Go players confirm that the territory predictions produced by the models are reasonable, even around loose groups of Black and White stones. Compare Figures 2 (a) and 3 (a); when liberty counts are included as features, the model can more confidently identify which of the two small chains competing in the bottom right of the board is dead. Comparing Figure 2 (a) and (b) Loopy BP appears to give over-confident predictions in the top right of the board where few stones are present. However, it is a good approximation where many stones are present (bottom left).
Comparing Models and Inference Methods                   Figure 4 shows cross-entropies between model territory predictions and true final territory outcomes for a dataset of expert games. As we progress through a game, predictions become more accurate (not surprising) but the spread of the accuracy increases, possibly due to incorrect assessment of the life-and-death status of groups. Swendsen-Wang performs better than Loopy BP, which may suffer from its over-confidence. BoltzmannLiberties performs better than Boltzmann5 (when using Swendsen-Wang) the difference in
(a) BoltzmannLiberties (Exact)                 (b) Many Faces of Go
Figure 3: Diagram (a) is produced by exact inference (training was also by Loopy BP). Diagram (b) shows the territory predicted by The Many Faces of Go (MFG) [2]. MFG uses of a rule-based expert system and its prediction for each vertex has three possible values: White',Black' or `unknown/neutral'.
performance increasing later in the game when liberty counts become more useful.
5    Modelling Move Selection
In order to produce a Go playing program we are interested in modelling the selec- tion of moves. A measure of performance of such a model is the likelihood it assigns to professional moves as measured by
                                      log P (move|model).                    (4)                            games moves

We can obtain a probability over moves by choosing a Gibbs distribution with the negative energy replaced by the evaluation function,
                                                 eu(c ,w)                            P (move|model, w) =                                   (5)                                                       Z(w)

where u(c , w) is an evaluation function evaluated at the board position c resulting from a given move. The inverse temperature parameter  determines the degree to which the move made depends on its evaluation. The territory predictions from the models Boltzmann5 and BoltzmannLiberties can be combined with the evaluation function of Section 2 to produce position evaluators."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/b5baa9c23ac3e015ad287b17a3d4afa3-Abstract.html,Spike Sorting: Bayesian Clustering of Non-Stationary Data,"Aharon Bar-hillel, Adam Spiro, Eran Stark","Spike sorting involves clustering spike trains recorded by a micro-            electrode according to the source neuron. It is a complicated problem,            which requires a lot of human labor, partly due to the non-stationary na-            ture of the data. We propose an automated technique for the clustering            of non-stationary Gaussian sources in a Bayesian framework. At a first            search stage, data is divided into short time frames and candidate descrip-            tions of the data as a mixture of Gaussians are computed for each frame.            At a second stage transition probabilities between candidate mixtures are            computed, and a globally optimal clustering is found as the MAP so-            lution of the resulting probabilistic model. Transition probabilities are            computed using local stationarity assumptions and are based on a Gaus-            sian version of the Jensen-Shannon divergence. The method was applied            to several recordings. The performance appeared almost indistinguish-            able from humans in a wide range of scenarios, including movement,            merges, and splits of clusters.
1      Introduction
Neural spike activity is recorded with a micro-electrode which normally picks up the ac- tivity of multiple neurons. Spike sorting seeks the segmentation of the spike data such that each cluster contains all the spikes generated by a different neuron. Currently, this task is mostly done manually. It is a tedious mission, requiring many hours of human labor for each recording session. Several algorithms were proposed in order to help automating this process (see [7] for a review, [9],[10]) and some tools were implemented to assist in manual sorting [8]. However, the ability of suggested algorithms to replace the human worker has been quite limited.
One of the main obstacles to a successful application is the non-stationary nature of the data [7]. The primary source of this non-stationarity is slight movements of the recording elec-
trode. Slight drifts of the electrode's location, which are almost inevitable, cause changes in the typical shapes of recorded spikes over time. Other sources of non-stationarity include variable background noise and changes in the characteristic spike generated by a certain neuron. The increasing usage of multiple electrode systems turns non-stationarity into an acute problem, as electrodes are placed in a single location for long durations.
Using the first 2 PCA coefficients to represent the data (which preserves up to 93% of the variance in the original recordings [1]), a human can cluster spikes by visual inspection. When dividing the data into small enough time frames, cluster density can be approximated by a multivariate Gaussian with a general covariance matrix without loosing much accu- racy [7]. Problematic scenarios which can appear due to non-stationarity are exemplified in Section 4.2 and include: (1) Movements and considerable shape changes of the clus- ters over time, (2) Two clusters which are reasonably well-separated may move until they converge and become indistinguishable. A split of a cluster is possible in the same manner.
Most spike sorting algorithms do not address the presented difficulties at all, as they assume full stationarity of the data. Some methods [4, 11] try to cope with the lack of stationarity by grouping data into many small clusters and identifying the clusters that can be combined to represent the activity of a single unit. In the second stage, [4] uses ISI information to understand which clusters cannot be combined, while [11] bases this decision on the density of points between clusters. In [3] a semi-automated method is suggested, in which each time frame is clustered manually, and then the correspondence between clusters in consecutive time frames is established automatically. The correspondence is determined by a heuristic score, and the algorithm doesn't handle merge or split scenarios.
In this paper we suggest a new fully automated technique to solve the clustering problem for non-stationary Gaussian sources in a Bayesian framework. We divide the data into short time frames in which stationarity is a reasonable assumption. We then look for good mixture of Gaussians descriptions of the data in each time frame independently. Transi- tion probabilities between local mixture solutions are introduced, and a globally optimal clustering solution is computed by finding the Maximum-A-Posteriori (MAP) solution of the resulting probabilistic model. The global optimization allows the algorithm to success- fully disambiguate problematic time frames and exhibit close to human performance. We present the outline of the algorithm in Section 2. The transition probabilities are computed by optimizing the Jensen-Shannon divergence for Gaussians, as described in Section 3. Empirical results and validation are presented in Section 4.
2    Clustering using a chain of Gaussian mixtures
Denote the observable spike data by D = {d}, where each spike d  Rn is de- scribed by the vector of its PCA coefficients. We break the data into T disjoint groups                  T {Dt = {dt}Nt }         . We assume that in each frame, the data can be well approximated by           i i=1 t=1 a mixture of Gaussians, where each Gaussian corresponds to a single neuron. Each Gaus- sian in the mixture may have a different covariance matrix. The number of components in the mixture is not known a priori, but is assumed to be within a certain range (we used 1-6).
In the search stage, we use a standard EM (Expectation-Maximization) algorithm to find a set of M t candidate mixture descriptions for each time frame t. We build the set of candidates using a three step process. First, we run the EM algorithm with different number of clusters and different initial conditions. In a second step, we import to each time frame t the best mixture solutions found in the neighboring time frames [t - k, .., t + k] (we used k = 2). These solutions are also adapted by using them as the initial conditions for the EM and running a low number of EM rounds. This mixing of solutions between time frames is repeated several times. Finally, the solution list in each time frame is pruned to remove similar solutions. Solutions which don't comply with the assumption of well
shaped Gaussians are also removed.
In order to handle outliers, which are usually background spikes or non-spike events, each mixture candidate contains an additional 'background model' Gaussian. This model's pa- rameters are set to 0, K  t where t is the covariance matrix of the data in frame t and K > 1 is a constant. Only the weight of this model is allowed to change during the EM process.
After the search stage, each time frame t has a list of M t models {t}T,Mt                                                                                                                                    i t=1,i=1. Each mix-
ture model is described by a triplet t = {t , t , t }Ki,t , denoting Gaussian mixture's                                                                     i           i,l     i,l       i,l l=1 weights, means, and covariances respectively. Given these candidate models we define a discrete random vector Z = {zt}T                                          in which each component zt has a value range of                                                               t=1 {1, 2, .., M t}. ""zt = j"" has the semantics of ""at time frame t the data is distributed accord- ing to the candidate mixture t "". In addition we define for each spike dt a hidden discrete                                                     j                                                                                    i 'label' random variable lt. This label indicates which Gaussian in the local mixture hy-                                         i
pothesis is the source of the spike. Denote by Lt = {lt}Nt the vector of labels of time                                                                                                          i i=1 frame t, and by L the vector of all the labels.
O                                                                                                                                                          H   z 1                         O                                 z 2                            O                                                                     z 3                   O                                                                                                 z T                                                     O            O                                                                                                                                                              H                                                                                                                                                             L1                         O                                          L2                                                               O                                                                                                            LT
O   D1                           O                                 D 2                                                       O                                                                                                 D T                                 O                                                                                                                                          L                                O                                                                                                                                                                            D
                                         (A)                                                                                                           (B)

Figure 1: (A) A Bayesian network model of the data generation process. The network has an HMM structure, but unlike HMM it does not have fixed states and transition probabilities over time. The variables and the CPDs are explained in Section 2. (B) A Bayesian network representation of the relations between the data D and the hidden labels H (see Section 3.1). The visible labels L and the sampled data points are independent given the hidden labels.
We describe the probabilistic relations between D, L, and Z using a Bayesian network with the structure described in Figure 1A. Using the network structure and assuming i.i.d samples the joint log probability decomposes into
                            T                                          T     N t

      log P (z1) +                logP (zt|zt-1) +                                  [log P (lt|                                           |                                                                                                          i zt) + log P (dti lt                                                                                                                                                    i , zt)]                     (1)                                t=2                                        t=1 i=1

We wish to maximize this log-likelihood over all possible choices of L, Z. Notice that by maximizing the probability of both data and labels we avoid the tendency to prefer mixtures with many Gaussians, which appears when maximizing the probability for the data alone. The conditional probability distributions (CPDs) of the points' labels and the points themselves, given an assignment to Z, are given by
      log P (lt =                     k         j|zt = i) = log ti,j                                                                                                                             (2)                                                          1                                                                          t                -1    log P (dt | =                                              [                                         | + (         -            ) t                    (         -            )]                    k lt                          i    j, zt = i) = -                  n log 2 + log |t                                dt         t                              dt             t                                                          2                                       i,j             k          i,j               i,j               k          i,j

The transition CPDs P (zt|zt-1) are described in Section 3. For the first frame's prior we use a uniform CPD. The MAP solution for the model is found using the Viterbi algorithm. Labels are then unified using the correspondences established between the chosen mixtures in consecutive time frames. As a final adjustment step, we repeat the mixing process using only the mixtures of the found MAP solution. Using this set of new candidates, we calculate the final MAP solution in the same manner described above.
3      A statistical distance between mixtures
The transition CPDs of the form P (zt|zt-1) are based on the assumption that the Gaus- sian sources' distributions are approximately stationary in pairs of consecutive time frames. Under this assumption, two mixtures candidates estimated at consecutive time frames are viewed as two samples from a single unknown Gaussian mixture. We assume that each Gaussian component from any of the two mixtures arises from a single Gaussian compo- nent in the joint hidden mixture, and so the hidden mixture induces a partition of the set of visible components into clusters. Gaussian components in the same cluster are assumed to arise from the same hidden source. Our estimate of p(zt = j|zt-1 = i) is based on the probability of seeing two large samples with different empirical distributions (t-1 and t                                                                                            i         j respectively) under the assumption of such a single joint mixture. In Section 3.1, the esti- mation of the transition probability is formalized as an optimization of a Jensen-Shannon based score over the possible partitions of the Gaussian components set.
If the family of allowed hidden mixture models is not further constrained, the optimization problem derived in Section 3.1 is trivially solved by choosing the most detailed partition (each visible Gaussian component is a singleton). This happens because a richer partition, which does not merge many Gaussians, gets a higher score. In Section 3.2 we suggest natural constraints on the family of allowed partitions in the two cases of constant and variable number of clusters through time, and present algorithms for both cases.
3.1         A Jensen-Shannon based transition score
Assume that in two consecutive time frames we observed two labeled samples (X1, L1), (X2, L2) of sizes N 1, N 2 with empirical distributions 1, 2 respectively. By 'empirical distribution', or 'type' in the notation of [2], we denote the ML parameters of the sample, for both the multinomial distribution of the mixture weights and the Gaus- sian distributions of the components. As stated above, we assume that the joint sample of size N = N 1 + N 2 is generated by a hidden Gaussian mixture H with KH com- ponents, and its components are determined by a partition of the set of all components in 1, 2. For convenience of notation, let us order this set of K1 + K2 Gaussians and refer to them (and to their parameters) using one index. We can define a function R : {1, .., K1 + K2}  {1, .., KH } which matches each visible Gaussian component in 1 or 2 to its hidden source component in H . Denote the labels of the sample points                                              N j under the hidden mixture H = {hj}                   , j = 1, 2. The values of these variables are given                                        i     i=1 by hj = R(lj), where lj is the label index in the set of all components.        i          i        i
The probabilistic dependence between a data point, its visible label, and its hidden label is explained by the Bayesian network model in Figure 1B. We assume a data point is obtained by choosing a hidden label and then sample the point from the relevant hidden component. The visible label is then sampled based on the hidden label using a multinomial distribution
with parameters  = {q}K1+K2                                 q=1         , where q = P (l = q|h = R(q)), i.e., the probability of the visible label q given the hidden label R(q) (since H is deterministic given L, P (l = q|h) = 0 for h = R(q)). Denote this model, which is fully determined by R, , and H , by M H .
We wish to estimate P ((X1, L1)  1|(X2, L2)  2, M H ). We use ML approxima- tions and arguments based on the method of types [2] to approximate this probability and optimize it with respect to H and . The obtained result is (the derivation is omitted)
            P ((X1, L1)  1|(X2, L2)  2, M H )                                              (3)



                                       KH

       max exp(-N                            H                                                                                              )))                                                    m                            q Dkl(G(x|q , q )|G(x|H                                                                                                                                       m, H                                                                                                                                               m              R                                            m=1           {q:R(q)=m}

where G(x|, ) denotes a Gaussian distribution with the parameters ,  and the opti- mized H ,  appearing here are given as follows. Denote by wq (q  {1, .., K1 + K2}) the weight of model q in a naive joint mixture of 1,2, i.e., wq = Nj                                                                                                                            N     q where j = 1 if component q is part of 1 and the same for j = 2.                                                                                 w             H =                                                                     q                             =                                          m                           wq          ,       q =                        ,          H                                    q q        (4)                                                                            H                               m                             {q:R(q)=m}                                          R(q)                                    {q:R(q)=m}
        H =                                                                    )(                     )t)                  m                                q (q + (q - H                                                                                 m         q - H                                                                                                        m

                        {q:R(q)=m}

Notice that the parameters of a hidden Gaussian, H                                                                                      m and H                                                                                                        m, are just the mean and covari- ance of the mixture                                                                       q:R(q)=m          q G(x|q , q ). The summation over q in expression (3) can be interpreted as the Jensen-Shannon (JS) divergence between the components assigned to the hidden source m, under Gaussian assumptions.
For a given parametric family, the JS divergence is a non-negative measurement which can be used to test whether several samples are derived from a single distribution from the family or from a mixture of different ones [6]. The JS divergence is computed for a mixture of n empirical distributions P1, .., Pn with mixture weights 1, .., n. In the Gaussian case, denote the mean and covariance of the component distributions by {i, i}n . The                                                                                                                                                    i=1 mean and covariance of the mixture distribution ,  are a function of the means and covariances of the components, with the formulae given in (4) for H                                                                                                                          m,H                                                                                                                                 m. The Gaussian JS divergence is given by
                                                    n

     J SG               (                             P1, .., Pn) =                     iDkl(G(x|i, i), G(x|, ))                                                            (5)                  1,..,n                                                    i=1                                                    n                                              1                            n          = H(G(x|, )) -                              iH(G(x|i, i)) =                            (log || -                                                                                                                       2                                        i log |i|)                                                   i=1                                                                          i=1

using this identity in (3), and setting 1 = t, 2 = t-1, we finally get the following                                                                            i                      j expression for the transition probability
                  log P (zt = i|zt-1 = j) =                                                                                                           (6)

                                       KH

                  -N  max                    H                                                    mJ SG                                                                {                                       R                              q :R(q)=m} ({G(x|q , q ) : R(q) = m})                                            m=1

3.2    Constrained optimization and algorithms
Consider first the case in which a one-to-one correspondence is assumed between clusters in two consecutive frames, and hence the number of Gaussian components K is constant over all time frames. In this case, a mapping R is allowed iff it maps to each hidden source i a single Gaussian from mixture 1 and a single Gaussian from 2. Denoting the Gaussians matched to hidden i by R-1                                                                       1 (i), R-1                                                                                      2 (i), the transition score (6) takes the                                  K form of -N  max                      S(R-1                                             1 (i), R-1                                                               2 (i)). Such an optimization of a pairwise matching                        R     i=1 score can be seen as a search for a maximal perfect matching in a weighted bipartite graph. The nodes of the graph are the Gaussian components of 1, 2 and the edges' weights are
given by the scores S(a, b). The global optimum of this problem can be efficiently found using the Hungarian algorithm [5] in O(n3), which is unproblematic in our case.
The one-to-one correspondence assumption is too strong for many data sets in the spike sorting application, as it ignores the phenomena of splits and merges of clusters. We wish to allow such phenomena, but nevertheless enforce strong (though not perfect) demands of correspondence between the Gaussians in two consecutive frames. In order to achieve such balance, we place the following constraints on the allowed partitions R:
    1. Each cluster of R should contain exactly one Gaussian from 1 or exactly one            Gaussian from 2. Hence assignment of different Gaussians from the same mix-            ture to the same hidden source is limited only for cases of a split or a merge.

    2. The label entropy of the partition R should satisfy

                                              N 1                              N 2                     H(H                            1 , .., H                    H(1                             H(2                                          KH )  N           1, .., 1                                                                               K1 ) + N          1, .., 2                                                                                                        K2 )    (7)

Intuitively, the second constraint limits the allowed partitions to ones which are not richer than the visible partition, i.e., do not have much more clusters. Note that the most detailed partition (the partition into singletons) has a label entropy given by the r.h.s of inequality (7) plus H( N1 , N2 ), which is one bit for N 1 = N 2. This extra bit is the price of using the                N     N concatenated 'rich' mixture, so we look for mixtures which do not pay such an extra price.
The optimization for this family of R does not seem to have an efficient global optimiza- tion technique, and thus we resort to a greedy procedure. Specifically, we use a bottom up agglomerative algorithm. We start from the most detailed partition (each Gaussian is a singleton) and merge two clusters of the partition at each round. Only merges that com- ply with the first constraint are considered. At each round we look for a merge which incurs a minimal loss to the accumulated Jensen Shannon score (6) and a maximal loss to the mixture label entropy. For two Gaussian clusters (1, 1, 1), (2, 2, 2) these two quantities are given by
                 log JS = -N (w1 + w2)JSG                                (                                                                              G(x|1, 1), G(x|2, 2))        (8)                                                                       1,2

                H = -N (w1 + w2)H(1, 2)

where 1, 2 are          w1      ,     w2      and w                      w                              i are as in (4). We choose at each round the merge                           1+w2         w1+w2 which minimizes the ratio between these two quantities. The algorithm terminates when the accumulated label entropy reduction is bigger than H( N1 , N2 ) or when no allowed                                                                                      N     N merges exist anymore. In the second case, it may happen that the partition R found by the algorithm violates the constraint (7). We nevertheless compute the score based on the R found, since this partition obeys the first constraint and usually is not far from satisfying the second.
4      Empirical results
4.1    Experimental design and data acquisition
Neural data were acquired from the dorsal and ventral pre-motor (PMd, PMv) cortices of two Macaque monkeys performing a prehension (reaching and grasping) task. At the be- ginning of each trial, an object was presented in one of six locations. Following a delay period, a Go signal prompted the monkey to reach for, grasp, and hold the target object. A recording session typically lasted 2 hours during which monkeys completed 600 tri- als. During each session 16 independently-movable glass-plated tungsten micro-electrodes
    f 1 score        Number of frames (%)                       Number of electrodes (%)               2         0.9-1.0           3386 (75%)                                     13    (30%)         0.8-0.9            860 (19%)                                     10    (23%)         0.7-0.8            243               (5%)                        10    (23%)         0.6-0.7             55               (1%)                        11    (25%)

Table 1: Match scores between manual and automatic clustering. The rows list the appearance frequencies of different f 1 scores.                             2
were inserted through the dura, 8 into each area. Signals from these electrodes were am- plified (10K), bandpass filtered (5-6000Hz), sampled (25 kHz), stored on disk (Alpha-Map 5.4, Alpha-Omega Eng.), and subjected to 3-stage preprocessing. (1) Line influences were cleaned by pulse-triggered averaging: the signal following a pulse was averaged over many pulses and subtracted from the original in an adaptive manner. (2) Spikes were detected by a modified second derivative algorithm (7 samples backwards and 11 forward), accen- tuating spiky features; segments that crossed an adaptive threshold were identified. Within each segment, a potential spike's peak was defined as the time of the maximal derivative. If a sharper spike was not encountered within 1.2ms, 64 samples (10 before peak and 53 after) were registered. (3) Waveforms were re-aligned s.t. each started at the point of max- imal fit with 2 library PCs (accounting, on average, for 82% and 11% of the variance, [1]). Aligned waveforms were projected onto the PCA basis to arrive at two coefficients.
4.2    Results and validation
                                                                1                               1

                                   1             3                          3 4                           3              1          2         4                        2                             4                             2                                                           4                            2                           5



                                                                     1                               1


                                                                            4                             4                   1                    1                                 4                               2          3          2                              1                                   3                              2                             23                              3                        5

    (0.80)                   (0.77)                        (0.98)                (0.95)                        (0.98)

Figure 2: Frames 3,12,24,34, and 47 from a 68-frames data set. Each frame contains 1000 spikes, plotted here (with random number assignments) according to their first two PCs. In this data one cluster moves constantly, another splits into distinguished clusters, and at the end two clusters are merged. The top and bottom rows show manual and automatic clustering solutions respectively. Notice that during the split process of the bottom left area some ambiguous time frames exist in which 1,2, or 3 cluster descriptions are reasonable. This ambiguity can be resolved using global considerations of past and future time frames. By finding the MAP solution over all time frames, the algorithm manages such considerations. The numbers below the images show the f 1 score of the                                                                                                                         2 local match between the manual and the automatic clustering solutions (see text).
We tested the algorithm using recordings of 44 electrodes containing a total of 4544 time frames. Spike trains were manually clustered by a skilled user in the environment of Alpha- Sort 4.0 (Alpha-Omega Eng.). The manual and automatic clustering results were compared using a combined measure of precision P and recall R scores f 1 = 2P R . Figure 2 demon-                                                                                                2    R+P strates the performance of the algorithm using a particularly non-stationary data set.
Statistics on the match between manual and automated clustering are described in Table 1. In order to understand the score's scale we note that random clustering (with the same
label distribution as the manual clustering) gets an f 1 score of 0.5. The trivial clustering                                                                 2 which assigns all the points to the same label gets mean scores of 0.73 and 0.67 for single frame matching and whole electrode matching respectively. The scores of single frames are much higher than the full electrode scores, since the problem is much harder in the latter case. A single wrong correspondence between two consecutive frames may reduce the electrode's score dramatically, while being unnoticed by the single frame score. In most cases the algorithm gives reasonably evolving clustering, even when it disagrees with the manual solution. Examples can be seen at the authors' web site1.
Low matching scores between the manual and the automatic clustering may result from inherent ambiguity in the data. As a preliminary assessment of this hypothesis we obtained a second, independent, manual clustering for the data set for which we got the lowest match scores. The matching scores between manual and automatic clustering are presented in Figure 3A.
         A

0.68            0.62                                 3                                          3                                               2                      2                                                    1                      1  H1      0.68        H2 
        (A)                       (B1)                   (B2)              (B3)            (B4)

Figure 3: (A) Comparison of our automatic clustering with 2 independent manual clustering solu- tions for our worst matched data points. Note that there is also a low match between the humans, forming a nearly equilateral triangle. (B) Functional validation of clustering results: (1) At the begin- ning of a recording session, three clusters were identified. (2) 107 minutes later, some shifted their position. They were tracked continuously. (3) The directional tuning of the top left cluster (number 3) during the delay periods of the first 100 trials (dashed lines are 99% confidence limits). (4) Although the cluster's position changed, its tuning curve's characteristics during the last 100 trials were similar.
In some cases, validity of the automatic clustering can be assessed by checking functional properties associated with the underlying neurons. In Figure 3B we present such a valida- tion for a successfully tracked cluster."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/b628386c9b92481fab68fbf284bd6a64-Abstract.html,Harmonising Chorales by Probabilistic Inference,"Moray Allan, Christopher Williams","We describe how we used a data set of chorale harmonisations composed          by Johann Sebastian Bach to train Hidden Markov Models. Using a prob-          abilistic framework allows us to create a harmonisation system which          learns from examples, and which can compose new harmonisations. We          make a quantitative comparison of our system's harmonisation perfor-          mance against simpler models, and provide example harmonisations."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/b75bd27b5a48a1b48987a18d831f6336-Abstract.html,Nearly Tight Bounds for the Continuum-Armed Bandit Problem,Robert D. Kleinberg,"In the multi-armed bandit problem, an online algorithm must choose           from a set of strategies in a sequence of n trials so as to minimize the           total cost of the chosen strategies. While nearly tight upper and lower           bounds are known in the case when the strategy set is finite, much less is           known when there is an infinite strategy set. Here we consider the case           when the set of strategies is a subset of Rd, and the cost functions are           continuous. In the d = 1 case, we improve on the best-known upper and           lower bounds, closing the gap to a sublogarithmic factor. We also con-           sider the case where d > 1 and the cost functions are convex, adapting a           recent online convex optimization algorithm of Zinkevich to the sparser           feedback model of the multi-armed bandit problem.
1     Introduction
In an online decision problem, an algorithm must choose from among a set of strategies in each of n consecutive trials so as to minimize the total cost of the chosen strategies. The costs of strategies are specified by a real-valued function which is defined on the entire strategy set and which varies over time in a manner initially unknown to the algorithm. The archetypical online decision problems are the best expert problem, in which the entire cost function is revealed to the algorithm as feedback at the end of each trial, and the multi- armed bandit problem, in which the feedback reveals only the cost of the chosen strategy. The names of the two problems are derived from the metaphors of combining expert advice (in the case of the best expert problem) and learning to play the best slot machine in a casino (in the case of the multi-armed bandit problem).
The applications of online decision problems are too numerous to be listed here. In ad- dition to occupying a central position in online learning theory, algorithms for such prob- lems have been applied in numerous other areas of computer science, such as paging and caching [6, 14], data structures [7], routing [4, 5], wireless networks [19], and online auc- tion mechanisms [8, 15]. Algorithms for online decision problems are also applied in a broad range of fields outside computer science, including statistics (sequential design of experiments [18]), economics (pricing [20]), game theory (adaptive game playing [13]), and medical decision making (optimal design of clinical trials [10]).
Multi-armed bandit problems have been studied quite thoroughly in the case of a finite strategy set, and the performance of the optimal algorithm (as a function of n) is known
  M.I.T. CSAIL, Cambridge, MA 02139. Email: rdk@csail.mit.edu. Supported by a Fannie and John Hertz Foundation Fellowship.

up to a constant factor [3, 18]. In contrast, much less is known in the case of an infinite strategy set. In this paper, we consider multi-armed bandit problems with a continuum of strategies, parameterized by one or more real numbers. In other words, we are studying online learning problems in which the learner designates a strategy in each time step by specifying a d-tuple of real numbers (x1, . . . , xd); the cost function is then evaluated at (x1, . . . , xd) and this number is reported to the algorithm as feedback. Recent progress on such problems has been spurred by the discovery of new algorithms (e.g. [4, 9, 16, 21]) as well as compelling applications. Two such applications are online auction mechanism design [8, 15], in which the strategy space is an interval of feasible prices, and online oblivious routing [5], in which the strategy space is a flow polytope.
Algorithms for online decisions problems are often evaluated in terms of their regret, de- fined as the difference in expected cost between the sequence of strategies chosen by the algorithm and the best fixed (i.e. not time-varying) strategy. While tight upper and lower bounds on the regret of algorithms for the K-armed bandit problem have been known for many years [3, 18], our knowledge of such bounds for continuum-armed bandit prob- lems is much less satisfactory. For a one-dimensional strategy space, the first algorithm with sublinear regret appeared in [1], while the first polynomial lower bound on regret ap- peared in [15]. For Lipschitz-continuous cost functions (the case introduced in [1]), the best known upper and lower bounds for this problem are currently O(n3/4) and (n1/2), respectively [1, 15], leaving as an open question the problem of determining tight bounds for the regret as a function of n. Here, we solve this open problem by sharpening the up- per and lower bounds to O(n2/3 log1/3(n)) and (n2/3), respectively, closing the gap to a sublogarithmic factor. Note that this requires improving the best known algorithm as well as the lower bound technique.
Recently, and independently, Eric Cope [11] considered a class of cost functions obeying a more restrictive condition on the shape of the function near its optimum, and for such functions he obtained a sharper bound on regret than the bound proved here for uniformly locally Lipschitz cost functions. Cope requires that each cost function C achieves its op- timum at a unique point , and that there exist constants K0 > 0 and p  1 such that for all x, |C(x) - C()|  K0 x -  p. For this class of cost functions -- which is probably broad enough to capture most cases of practical interest -- he proves that the regret of the optimal continuum-armed bandit algorithm is O(n-1/2), and that this bound is tight.
For a d-dimensional strategy space, any multi-armed bandit algorithm must suffer regret depending exponentially on d unless the cost functions are further constrained. (This is demonstrated by a simple counterexample in which the cost function is identically zero in all but one orthant of Rd, takes a negative value somewhere in that orthant, and does not vary over time.) For the best-expert problem, algorithms whose regret is polynomial in d and sublinear in n are known for the case of cost functions which are constrained to be linear [16] or convex [21]. In the case of linear cost functions, the relevant algorithm has been adapted to the multi-armed bandit setting in [4, 9]. Here we adapt the online convex programming algorithm of [21] to the continuum-armed bandit setting, obtaining the first known algorithm for this problem to achieve regret depending polynomially on d and sublinearly on n. A remarkably similar algorithm was discovered independently and simultaneously by Flaxman, Kalai, and McMahan [12]. Their algorithm and analysis are superior to ours, requiring fewer smoothness assumptions on the cost functions and producing a tighter upper bound on regret.
2    Terminology and Conventions
We will assume that a strategy set S  Rd is given, and that it is a compact subset of Rd. Time steps will be denoted by the numbers {1, 2, . . . , n}. For each t  {1, 2, . . . , n} a cost
function Ct : S  R is given. These cost functions must satisfy a continuity property based on the following definition. A function f is uniformly locally Lipschitz with constant L (0  L < ), exponent  (0 <   1), and restriction  ( > 0) if it is the case that for all u, u  S with u - u  ,                                       |f(u) - f(u )|  L u - u . (Here,     denotes the Euclidean norm on Rd.) The class of all such functions f will be denoted by ulL(, L, ).
We will consider two models which may govern the cost functions. The first of these is identical with the continuum-armed bandit problem considered in [1], except that [1] formulates the problem in terms of maximizing reward rather than minimizing cost. The second model concerns a sequence of cost functions chosen by an oblivious adversary.
Random The functions C1, . . . , Cn are independent, identically distributed random sam-           ples from a probability distribution on functions C : S  R. The expected cost           function                       C : S  R is defined by C(u) = E(C(u)) where C is a random sample           from this distribution. This function                                                                 C is required to belong to ulL(, L, ) for           some specified , L, . In addition, we assume there exist positive constants , s0           such that if C is a random sample from the given distribution on cost functions,           then                                                           1                                      E(esC(u))  e 2s2                                                           2         |s|  s0,u  S.           The ""best strategy"" u is defined to be any element of arg min                                                                                                                 uS C (u). (This           set is non-empty, by the compactness of S.) Adversarial The functions C1, . . . , Cn are a fixed sequence of functions in ulL(, L, ),           taking values in [0, 1]. The ""best strategy"" u is defined to be any element of           arg min             n                      uS             C                               t=1         t(u). (Again, this set is non-empty by compactness.)
A multi-armed bandit algorithm is a rule for deciding which strategy to play at time t, given the outcomes of the first t - 1 trials. More formally, a deterministic multi-armed bandit algorithm U is a sequence of functions U1, U2, . . . such that Ut : (S  R)t-1  S. The interpretation is that Ut(u1, x1, u2, x2, . . . , ut-1, xt-1) defines the strategy to be chosen at time t if the algorithm's first t - 1 choices were u1, . . . , ut-1 respectively, and their costs were x1, . . . , xt-1 respectively. A randomized multi-armed bandit algorithm is a proba- bility distribution over deterministic multi-armed bandit algorithms. (If the cost functions are random, we will assume their randomness is independent of the algorithm's random choices.) For a randomized multi-armed bandit algorithm, the n-step regret Rn is the ex- pected difference in total cost between the algorithm's chosen strategies u1, u2, . . . , un and the best strategy u, i.e.
                                               n                                      Rn = E              Ct(ut) - Ct(u) .                                                   t=1

Here, the expectation is over the algorithm's random choices and (in the random-costs model) the randomness of the cost functions.
3    Algorithms for the one-parameter case (d = 1)
The continuum-bandit algorithm presented in [1] is based on computing an estimate ^                                                                                                      C of the expected cost function                                     C which converges almost surely to                                                                                C as n  . This estimate is obtained by devoting a small fraction of the time steps (tending to zero as n  ) to sampling the random cost functions at an approximately equally-spaced sequence of ""design points"" in the strategy set, and combining these samples using a kernel estimator.
When the algorithm is not sampling a design point, it chooses a strategy which minimizes expected cost according to the current estimate ^                                                              C. The convergence of ^                                                                                     C to                                                                                           C ensures that the average cost in these ""exploitation steps"" converges to the minimum value of                                                                                                 C.
A drawback of this approach is its emphasis on estimating the entire function                                                                                             C. Since the algorithm's goal is to minimize cost, its estimate of                                                                    C need only be accurate for strategies where         C is near its minimum. Elsewhere a crude estimate of                                                                             C would have sufficed, since such strategies may safely be ignored by the algorithm. The algorithm in [1] thus uses its sampling steps inefficiently, focusing too much attention on portions of the strategy interval where an accurate estimate of                                                         C is unnecessary. We adopt a different approach which eliminates this inefficiency and also leads to a much simpler algorithm. First we discretize the strategy space by constraining the algorithm to choose strategies only from a fixed, finite set of K equally spaced design points {1/K, 2/K, . . . , 1}. (For simplicity, we are assuming here and for the rest of this section that S = [0, 1].) This reduces the continuum-armed bandit problem to a finite-armed bandit problem, and we may apply one of the standard algorithms for such problems. Our continuum-armed bandit algorithm is shown in Figure 1. The outer loop uses a standard doubling technique to transform a non-uniform algorithm to a uniform one. The inner loop requires a subroutine MAB which should implement a finite-armed bandit algorithm appropriate for the cost model under consideration. For example, MAB could be the algorithm UCB1 of [2] in the random case, or the algorithm Exp3 of [3] in the adversarial case. The semantics of MAB are as follows: it is initialized with a finite set of strategies; subsequently it recommends strategies in this set, waits to learn the feedback score for its recommendation, and updates its recommendation when the feedback is received.
The analysis of this algorithm will ensure that its choices have low regret relative to the best design point. The Lipschitz regularity of                                                          C guarantees that the best design point performs nearly as well, on average, as the best strategy in S.
             ALGORITHM CAB1                  T  1                  while T  n                    1                                                2+1                        K              T                                       log T

                   Initialize MAB with strategy set {1/K, 2/K, . . . , 1}.                        for t = T, T + 1, . . . , min(2T - 1, n)                           Get strategy ut from MAB.                           Play ut and discover Ct(ut).                           Feed 1 - Ct(ut) back to MAB.                        end                        T  2T                  end


   Figure 1: Algorithm for the one-parameter continuum-armed bandit problem

Theorem 3.1. In both the random and adversarial models, the regret of algorithm CAB1        +1         is O(n 2+1 log 2+1 (n)).
Proof Sketch. Let q =                , so that the regret bound is O(n1-q logq(n)). It suffices to                               2+1 prove that the regret in the inner loop is O(T 1-q logq(T )); if so, then we may sum this bound over all iterations of the inner loop to get a geometric progression with constant ratio, whose largest term is O(n1-q logq(n)). So from now on assume that T is fixed and that K is defined as in Figure 1, and for simplicity renumber the T steps in this iteration of
inner loop so that the first is step 1 and the last is step T . Let u be the best strategy in S, and let u be the element of {1/K, 2/K, . . . , 1} nearest to u. Then                                                                   T                        |u - u| < 1/K, so using the fact that                          C  ulL(,L,) (or that 1                        C                                                         T         t=1         t  ulL(, L, ) in the adversarial case) we obtain
                      T                                  T                    E            Ct(u ) - Ct(u)                   = O T 1-q logq(T ) .                                                              K                          t=1

It remains to show that E           T      C                                     t=1         t(ut) - Ct(u ) = O T 1-q logq(T ) . For the adver- sarial model, this follows directly from Corollary 4.2 in [3], which asserts that the regret of Exp3 is O T K log K . For the random model, a separate argument is required. (The upper bound for the adversarial model doesn't directly imply an upper bound for the random model, since the cost functions are required to take values in [0, 1] in the ad- versarial model but not in the random model.) For u  {1/K, 2/K, . . . , 1} let (u) =
C(u) - C(u ). Let  = K log(T)/T, and partition the set {1/K,2/K,... ,1} into two subsets A, B according to whether (u) <  or (u)  . The time steps in which the algorithm chooses strategies in A contribute at most O(T ) = O(T 1-q logq(T )) to the regret. For each strategy u  B, one may prove that, with high probability, u is played only O(log(T )/(u)2) times. (This parallels the corresponding proof in [2] and is omitted here. Our hypothesis on the moment generating function of the random variable C(u) is strong enough to imply the exponential tail inequality required in that proof.) This im- plies that the time steps in which the algorithm chooses strategies in B contribute at most O(K log(T )/) = O(T 1-q logq(T )) to the regret, which completes the proof.
4    Lower bounds for the one-parameter case
There are many reasons to expect that Algorithm CAB1 is an inefficient algorithm for the continuum-armed bandit problem. Chief among these is that fact that it treats the strategies {1/K,2/K,... ,1} as an unordered set, ignoring the fact that experiments which sample the cost of one strategy j/K are (at least weakly) predictive of the costs of nearby strategies. In this section we prove that, contrary to this intuition, CAB1 is in fact quite close to the optimal algorithm. Specifically, in the regret bound of Theorem 3.1, the exponent of +1                                                                                                            2+1 is the best possible: for any  < +1 , no algorithm can achieve regret O(n). This lower                                          2+1 bound applies to both the randomized and adversarial models.
The lower bound relies on a function f : [0, 1]  [0, 1] defined as the sum of a nested fam- ily of ""bump functions."" Let B be a C bump function defined on the real line, satisfying 0  B(x)  1 for all x, B(x) = 0 if x  0 or x  1, and B(x) = 1 if x  [1/3,2/3]. For an interval [a, b], let B[a,b] denote the bump function B( x-a ), i.e. the function B rescaled                                                                               b-a and shifted so that its support is [a, b] instead of [0, 1]. Define a random nested sequence of intervals [0, 1] = [a0, b0]  [a1, b1]  . . . as follows: for k > 0, the middle third of [ak-1, bk-1] is subdivided into intervals of width wk = 3-k!, and [ak, bk] is one of these subintervals chosen uniformly at random. Now let
                     f (x) = 1/3 + 3-1 - 1/3                         w                                                                               k B[ak,bk](x).                                                                    k=1

Finally, define a probability distribution on functions C : [0, 1]  [0, 1] by the following rule: sample  uniformly at random from the open interval (0, 1) and put C(x) = f(x).
The relevant technical properties of this construction are summarized in the following lemma.
Lemma 4.1. Let {u} =  [a                                      k=1    k, bk]. The function f (x) belongs to ulL(, L, ) for some constants L, , it takes values in [1/3, 2/3], and it is uniquely maximized at u. For each   (0, 1), the function C(x) = f(x) belongs to ulL(, L, ) for some constants L, , and is uniquely minimized at u. The same two properties are satisfied by the function
C(x) = E(0,1) f(x) = (1 + f(x))-1. Theorem 4.2. For any randomized multi-armed bandit algorithm, there exists a probability distribution on cost functions such that for all  < +1 , the algorithm's regret                                                                     2+1                            {Rn}n=1 in the random model satisfies                                                         R                                             lim sup              n = .                                                    n n
The same lower bound applies in the adversarial model.
Proof sketch. The idea is to prove, using the probabilistic method, that there exists a nested sequence of intervals [0, 1] = [a0, b0]  [a1, b1]  . . ., such that if we use these intervals to define a probability distribution on cost functions C(x) as above, then Rn/n diverges as n runs through the sequence n1, n2, n3, . . . defined by nk =                      1 (w                     .                                                                                      k       k-1/wk)w-2                                                                                                         k Assume that intervals [a0, b0]  . . .  [ak-1, bk-1] have already been specified. Subdivide [ak-1, bk-1] into subintervals of width wk, and suppose [ak, bk] is chosen uniformly at random from this set of subintervals. For any u, u  [ak-1, bk-1], the Kullback-Leibler distance KL(C(u) C(u )) between the cost distributions at u and u is O(w2)                                                                                                k    , and it is equal to zero unless at least one of u, u lies in [ak, bk]. This means, roughly speaking, that the algorithm must sample strategies in [ak, bk] at least w-2 times before being able                                                                                 k to identify [ak, bk] with constant probability. But [ak, bk] could be any one of wk-1/wk possible subintervals, and we don't have enough time to play w-2 trials in even a constant                                                                                 k fraction of these subintervals before reaching time nk. Therefore, with constant probability, a constant fraction of the strategies chosen up to time nk are not located in [ak, bk], and each of them contributes (w)                                      k    to the regret. This means the expected regret at time nk is (nkw)          k . From this, we obtain the stated lower bound using the fact that
                                                        +1 -o(1)                                             n               2+1                                                  kw                                                    k = n                   .                                                             k

Although this proof sketch rests on a much more complicated construction than the lower bound proof for the finite-armed bandit problem given by Auer et al in [3], one may follow essentially the same series of steps as in their proof to make the sketch given above into a rigorous proof. The only significant technical difference is that we are working with continuous-valued rather than discrete-valued random variables, which necessitates using the differential Kullback-Leibler distance1 rather than working with the discrete Kullback- Leibler distance as in [3].
5     An online convex optimization algorithm
We turn now to continuum-armed bandit problems with a strategy space of dimension d > 1. As mentioned in the introduction, for any randomized multi-armed bandit al- gorithm there is a cost function C (with any desired degree of smoothness and bound- edness) such that the algorithm's regret is (2d) when faced with the input sequence C1 = C2 = . . . = Cn = C. As a counterpoint to this negative result, we seek interesting classes of cost functions which admit a continuum-armed bandit algorithm whose regret is polynomial in d (and, as always, sublinear in n). A natural candidate is the class of convex, smooth functions on a closed, bounded, convex strategy set S  Rd, since this is the most      1Defined by the formula KL(P Q) = R log (p(x)/q(x)) dp(x), for probability distributions P, Q with density functions p, q.
general class of functions for which the corresponding best-expert problem is known to admit an efficient algorithm, namely Zinkevich's greedy projection algorithm [21]. Greedy projection is initialized with a sequence of learning rates 1 > 2 > . . .. It selects an arbitrary initial strategy u1  S and updates its strategy in each subsequent time step t according to the rule ut+1 = P (ut - t Ct(ut)), where Ct(ut) is the gradient of Ct at ut and P : Rd  S is the projection operator which maps each point of Rd to the nearest point of S. (Here, distance is measured according to the Euclidean norm.) Note that greedy projection is nearly a multi-armed bandit algorithm: if the algorithm's feedback when sampling strategy ut were the vector                 Ct(ut) rather than the number Ct(ut), it would have all the information required to run greedy projection. To adapt this algorithm to the multi-armed bandit setting, we use the following idea: group the timeline into phases of d + 1 consecutive steps, with a cost function C for each phase  defined by averaging the cost functions at each time step of . In each phase use trials at d + 1 affinely independent points of S, located at or near ut, to estimate the gradient C(ut).2 To describe the algorithm, it helps to assume that the convex set S is in isotropic position in Rd. (If not, we may bring it into isotropic position by an affine transformation of the coordi- nate system. This does not increase the regret by a factor of more than d2.) The algorithm, which we will call simulated greedy projection, works as follows. It is initialized with a sequence of ""learning rates"" 1, 2, . . . and ""frame sizes"" 1, 2, . . .. At the beginning of a phase , we assume the algorithm has determined a basepoint strategy u. (An arbitrary u may be used in the first phase.) The algorithm chooses a set of (d + 1) affinely indepen- dent points {x0 = u, x1, x2, . . . , xd} with the property that for any y  S, the difference y - x0 may be expressed as a linear combination of the vectors {xi - x0 : 1  i  d} using coefficients in [-2, 2]. (Such a set is called an approximate barycentric spanner, and may computed efficiently using an algorithm specified in [4].) We then choose a random bijection  mapping the time steps in phase  into the set {0, 1, . . . , d}, and in step t we sample the strategy yt = u + (x(t) -u). At the end of the phase we let B denote the unique affine function whose values at the points yt are equal to the costs observed during the phase at those points. The basepoint for the next phase  is determined according to Zinkevich's update rule u = P (u -  B(u)).3 Theorem 5.1. Assume that S is in isotropic position and that the cost functions satisfy  Ct(x)  1 for all x  S,1tn, and that in addition the Hessian matrix of Ct(x) at each point x  S has Frobenius norm bounded above by a constant. If k = k-3/4 and k = k-1/4, then the regret of the simulated greedy projection algorithm is O(d3n3/4).
Proof sketch. In each phase , let Y = {y0, . . . , yd} be the set of points which were sampled, and define the following four functions: C, the average of the cost functions in phase ; , the linearization of C at u, defined by the formula
                       (x) =      C(u)  (x - u) + C(u); L, the unique affine function which agrees with C at each point of Y; and B, the affine function computed by the algorithm at the end of phase . The algorithm is simply run- ning greedy projection with respect to the simulated cost functions B, and it consequently satisfies a low-regret bound with respect to those functions. The expected value of B(u) is L(u) for every u. (Proof: both are affine functions, and they agree on every point of

2Flaxman, Kalai, and McMahan [12], with characteristic elegance, supply an algorithm which counterintuitively obtains an unbiased estimate of the approximate gradient using only a single sam- ple. Thus they avoid grouping the timeline into phases and improve the algorithm's convergence time by a factor of d.    3Readers familiar with Kiefer-Wolfowitz stochastic approximation [17] will note the similarity with our algorithm. The random bijection  -- which is unnecessary in the Kiefer-Wolfowitz algo- rithm -- is used here to defend against the oblivious adversary.
Y.) Hence we obtain a low-regret bound with respect to L. To transfer this over to a low- regret bound for the original problem, we need to bound several additional terms: the regret experienced because the algorithm was using u + (x(t) - u) instead of u, the dif- ference between L(u) and (u), and the difference between (u) and C(u). In each case, the desired upper bound can be inferred from properties of barycentric spanners, or from the convexity of C and the bounds on its first and second derivatives."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/b7f1f29db7c23648f2bb8d6a8ee0469b-Abstract.html,Mistake Bounds for Maximum Entropy Discrimination,"Philip M. Long, Xinyu Wu","We establish a mistake bound for an ensemble method for classification          based on maximizing the entropy of voting weights subject to margin          constraints. The bound is the same as a general bound proved for the          Weighted Majority Algorithm, and similar to bounds for other variants          of Winnow. We prove a more refined bound that leads to a nearly opti-          mal algorithm for learning disjunctions, again, based on the maximum          entropy principle. We describe a simplification of the on-line maximum          entropy method in which, after each iteration, the margin constraints are          replaced with a single linear inequality. The simplified algorithm, which          takes a similar form to Winnow, achieves the same mistake bounds."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/bbeb0c1b1fd44e392c7ce2fdbd137e87-Abstract.html,A Harmonic Excitation State-Space Approach to Blind Separation of Speech,"Rasmus K. Olsson, Lars K. Hansen","We discuss an identiﬁcation framework for noisy speech mixtures. A
block-based generative model is formulated that explicitly incorporates
the time-varying harmonic plus noise (H+N) model for a number of latent
sources observed through noisy convolutive mixtures. All parameters
including the pitches of the source signals, the amplitudes and phases of
the sources, the mixing ﬁlters and the noise statistics are estimated by
maximum likelihood, using an EM-algorithm. Exact averaging over the
hidden sources is obtained using the Kalman smoother. We show that
pitch estimation and source separation can be performed simultaneously.
The pitch estimates are compared to laryngograph (EGG) measurements.
Artiﬁcial and real room mixtures are used to demonstrate the viability
of the approach. Intelligible speech signals are re-synthesized from the
estimated H+N models."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/bd70364a8fcba02366697df66f50b4d4-Abstract.html,Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection,"Koji Tsuda, Gunnar Rätsch, Manfred K. Warmuth","We address the problem of learning a symmetric positive definite matrix.           The central issue is to design parameter updates that preserve positive           definiteness. Our updates are motivated with the von Neumann diver-           gence. Rather than treating the most general case, we focus on two key           applications that exemplify our methods: On-line learning with a simple           square loss and finding a symmetric positive definite matrix subject to           symmetric linear constraints. The updates generalize the Exponentiated           Gradient (EG) update and AdaBoost, respectively: the parameter is now           a symmetric positive definite matrix of trace one instead of a probability           vector (which in this context is a diagonal positive definite matrix with           trace one). The generalized updates use matrix logarithms and exponen-           tials to preserve positive definiteness. Most importantly, we show how           the analysis of each algorithm generalizes to the non-diagonal case. We           apply both new algorithms, called the Matrix Exponentiated Gradient           (MEG) update and DefiniteBoost, to learn a kernel matrix from distance           measurements.
1      Introduction
Most learning algorithms have been developed to learn a vector of parameters from data. However, an increasing number of papers are now dealing with more structured parame- ters. More specifically, when learning a similarity or a distance function among objects, the parameters are defined as a symmetric positive definite matrix that serves as a kernel (e.g. [14, 11, 13]). Learning is typically formulated as a parameter updating procedure to optimize a loss function. The gradient descent update [6] is one of the most commonly used algorithms, but it is not appropriate when the parameters form a positive definite matrix, because the updated parameter is not necessarily positive definite. Xing et al. [14] solved this problem by always correcting the updated matrix to be positive. However no bound has been proven for this update-and-correction approach. In this paper, we introduce the Matrix Exponentiated Gradient update which works as follows: First, the matrix logarithm of the current parameter matrix is computed. Then a step is taken in the direction of the steepest descent. Finally, the parameter matrix is updated to the exponential of the modified log-matrix. Our update preserves symmetry and positive definiteness because the matrix exponential maps any symmetric matrix to a positive definite matrix.
Bregman divergences play a central role in the motivation and the analysis of on-line learn- ing algorithms [5]. A learning problem is essentially defined by a loss function, and a di- vergence that measures the discrepancy between parameters. More precisely, the updates are motivated by minimizing the sum of the loss function and the Bregman divergence, where the loss function is multiplied by a positive learning rate. Different divergences lead to radically different updates [6]. For example, the gradient descent is derived from the squared Euclidean distance, and the exponentiated gradient from the Kullback-Leibler di- vergence. We use the von Neumann divergence (also called quantum relative entropy) for measuring the discrepancy between two positive definite matrices [8]. We derive a new Matrix Exponentiated Gradient update from this divergence (which is a Bregman diver- gence for positive definite matrices). Finally we prove relative loss bounds using the von Neumann divergence as a measure of progress.
Also the following related key problem has received a lot of attention recently [14, 11, 13]: Find a symmetric positive definite matrix that satisfies a number of symmetric linear inequality constraints. The new DefiniteBoost algorithm greedily chooses the most violated constraint and performs an approximated Bregman projection. In the diagonal case, we recover AdaBoost [9]. We also show how the convergence proof of AdaBoost generalizes to the non-diagonal case.
2     von Neumann Divergence or Quantum Relative Entropy
If F is a real convex differentiable function on the parameter domain (symmetric d  d positive definite matrices) and f (W) :=             F(W), then the Bregman divergence between two parameters W and W is defined as
                   F(W, W) = F(W) - F(W) - tr[(W - W)f(W)]. When choosing F(W) = tr(W log W -W), then f(W) = log W and the corresponding Bregman divergence becomes the von Neumann divergence [8]:

                   F(W, W) = tr(W log W - W log W - W + W).                                        (1) In this paper, we are primarily interested in the normalized case (when tr(W) = 1). In this case, the positive symmetric definite matrices are related to density matrices commonly used in Statistical Physics and the divergence simplifies to F(W, W) = tr(W log W - W log W).

If W =                          i    ivivi is our notation for the eigenvalue decomposition, then we can rewrite the normalized divergence as
                                          ~                   ~                           F(W, W) =            i ln ~                                                     i +           i ln j(~                                                                            vi vj )2.                                           i                 i,j

So this divergence quantifies the difference in the eigenvalues as well as the eigenvectors.
3     On-line Learning
In this section, we present a natural extension of the Exponentiated Gradient (EG) up- date [6] to an update for symmetric positive definite matrices.
At the t-th trial, the algorithm receives a symmetric instance matrix Xt  Rdd. It then produces a prediction ^                              yt = tr(WtXt) based on the algorithm's current symmetric positive definite parameter matrix Wt. Finally it incurs for instance1 a quadratic loss (^                                                                                               yt - yt)2,      1For the sake of simplicity, we use the simple quadratic loss: L                         W                                                                             t (W) = (tr(Xt         ) - yt)2. For the general update, the gradient     Lt(Wt) is exponentiated in the update (4) and this gradient must be symmetric. Following [5], more general loss functions (based on Bregman divergences) are amenable to our techniques.
and updates its parameter matrix Wt. In the update we aim to solve the following problem:
           Wt+1 = argmin                                                   W              F(W, Wt) + (tr(WXt) - yt)2 ,                              (2) where the convex function F defines the Bregman divergence. Setting the derivative with respect to W to zero, we have

              f (Wt+1) - f(Wt) +  [(tr(Wt+1Xt) - yt)2] = 0.                                            (3) The update rule is derived by solving (3) with respect to Wt+1, but it is not solvable in closed form. A common way to avoid this problem is to approximate tr(Wt+1Xt) by tr(WtXt) [5]. Then, we have the following update:

                     Wt+1 = f -1(f (Wt) - 2(^yt - yt)Xt). In our case, F(W) = tr(W log W - W) and thus f(W) = log W and f-1(W) = exp W. We also augment (2) with the constraint tr(W) = 1, leading to the following Matrix Exponential Gradient (MEG) Update:

                              1                        Wt+1 =               exp(log W                                   Z                      t - 2(^yt - yt)Xt),                               (4)                                        t

where the normalization factor Zt is tr[exp(log Wt - 2(^yt - yt)Xt)]. Note that in the above update, the exponent log Wt - 2(^yt - yt)Xt is an arbitrary symmetric matrix and the matrix exponential converts this matrix back into a symmetric positive definite matrix. A numerically stable version of the MEG update is given in Section 3.2.
3.1    Relative Loss Bounds
We now begin with the definitions needed for the relative loss bounds.                                  Let S = (X1, y1), . . . , (XT , yT ) denote a sequence of examples, where the instance matrices Xt  Rdd are symmetric and the labels yt  R. For any symmetric positive semi-definite ma- trix U with tr(U) = 1, define its total loss as LU(S) =                         T      (tr(UX                                                                                t=1         t ) - yt)2. The total loss of the on-line algorithm is LMEG(S) =               T      (tr(W                                                          t=1                   tXt) - yt)2. We prove a bound on the relative loss LMEG(S) -LU(S) that holds for any U. The proof generalizes a sim- ilar bound for the Exponentiated Gradient update (Lemmas 5.8 and 5.9 of [6]). The relative loss bound is derived in two steps: Lemma 3.1 bounds the relative loss for an individual trial and Lemma 3.2 for a whole sequence (Proofs are given in the full paper).
Lemma 3.1 Let Wt be any symmetric positive definite matrix. Let Xt be any symmetric matrix whose smallest and largest eigenvalues satisfy max - min  r. Assume Wt+1 is produced from Wt by the MEG update and let U be any symmetric positive semi-definite matrix. Then for any constants a and b such that 0 < a  2b/(2 + r2b) and any learning rate  = 2b/(2 + r2b), we have
     a(yt - tr(WtXt))2 - b(yt - tr(UXt))2  (U,Wt) - (U,Wt+1)                                         (5)

In the proof, we use the Golden-Thompson inequality [3], i.e., tr[exp(A + B)]  tr[exp(A) exp(B)] for symmetric matrices A and B. We also needed to prove the fol- lowing generalization of Jensen's inequality to matrices: exp(1A + 2(I - A))  exp(1)A + exp(2)(I - A) for finite 1,2  R and any symmetric matrix A with 0 < A  I. These two key inequalities will also be essential for the analysis of Definite- Boost in the next section.
Lemma 3.2 Let W1 and U be arbitrary symmetric positive definite initial and comparison matrices, respectively. Then for any c such that  = 2c/(r2(2 + c)),
                                        c                   1         1                 LMEG(S)  1 +                    LU(S) +             +          r2(U, W                                             2                   2         c                     1).         (6)

Proof For the maximum tightness of (5), a should be chosen as a =  = 2b/(2 + r2b). Let b = c/r2, and thus a = 2c/(r2(2 + c)). Then (5) is rewritten as
     2c (y         2 + c     t - tr(WtXt))2 - c(yt - tr(UXt))2  r2((U, Wt) - (U, Wt+1)) Adding the bounds for t = 1,    , T, we get           2c L         2 + c MEG(S) - cLU(S)  r2((U, W1) - (U, Wt+1))  r2(U, W1),

which is equivalent to (6).
Assuming LU(S)  max and (U, W1)  dmax, the bound (6) is tightest when c = r      2dmax/ max. Then we have LMEG(S) - LU(S)  r2 maxdmax + r2(U,W                                                                                                           2    1).
3.2      Numerically stable MEG update
The MEG update is numerically unstable when the eigenvalues of Wt are around zero. However we can ""unwrap"" Wt+1 as follows:
                              1                                          t                       Wt+1 =                exp(c                                  (^                                                                                    y                                   ~                  tI + log W1                        s                                   Z                                 - 2                     - ys)Xs),          (7)                                        t                                    s=1

where the constant ~                         Zt normalizes the trace of Wt+1 to one. As long as the eigen values of W1 are not too small then the computation of log Wt is stable. Note that the update is inde- pendent of the choice of ct  R. We incrementally maintain an eigenvalue decomposition of the matrix in the exponent (O(n3) per iteration):
                                                                   t

                    VttVTt = ctI + log W1 - 2 (^ys - ys)Xs),                                                                       s=1

where the constant ct is chosen so that the maximum eigenvalue of the above is zero. Now Wt+1 = Vt exp(t)VTt /tr(exp(t)).
4       Bregman Projection and DefiniteBoost
In this section, we address the following Bregman projection problem2
   W = argmin                                    W    F (W, W1), tr(W) = 1, tr(WCj )  0, for j = 1, . . . , n,                          (8) where the symmetric positive definite matrix W1 of trace one is the initial parameter ma- trix, and C1, . . . , Cn are arbitrary symmetric matrices. Prior knowledge about W is en- coded in the constraints, and the matrix closest to W1 is chosen among the matrices satis- fying all constraints. Tsuda and Noble [13] employed this approach for learning a kernel matrix among graph nodes, and this method can be potentially applied to learn a kernel matrix in other settings (e.g. [14, 11]).

The problem (8) is a projection of W1 to the intersection of convex regions defined by the constraints. It is well known that the Bregman projection into the intersection of convex regions can be solved by sequential projections to each region [1]. In the original papers only asymptotic convergence was shown. More recently a connection [4, 7] was made to the AdaBoost algorithm which has an improved convergence analysis [2, 9]. We generalize the latter algorithm and its analysis to symmetric positive definite matrices and call the new algorithm DefiniteBoost. As in the original setting, only approximate projections (Figure 1) are required to show fast convergence.
   2Note that if  is large then the on-line update (2) becomes a Bregman projection subject to a single equality constraint tr(WXt) = yt.



    Approximate                                    Figure 1: In (exact) Bregman projections, the intersection         Projection                                     of convex sets (i.e., two lines here) is found by iterating pro-                                                        jections to each set. We project only approximately, so the                                                        projected point does not satisfy the current constraint. Nev-                                                        ertheless, global convergence to the optimal solution is guar-                                                        anteed via our proofs.                              Exact                              Projection

Before presenting the algorithm, let us derive the dual problem of (8) by means of Lagrange multipliers ,                                                                                          n                                                                 = argmin log                                                                                                               trexp(logW1- jCj), j 0.                                                              (9)                                                                                           j=1
See [13] for a detailed derivation of the dual problem. When (8) is feasible, the opti- mal solution is described as W =                                 1        exp(log W                            C                      ) =                                                             Z(                                  1                      j ), where Z(                                                                     )                              - nj=1 j tr[exp(log W1 - n C                              j=1      j      j )].
4.1      Exact Bregman Projections
First, let us present the exact Bregman projection algorithm to solve (8). We start from the initial parameter W1. At the t-th step, the most unsatisfied constraint is chosen, jt = argmaxj=1, ,n tr(WtCj). Let us use Ct as the short notation for Cj . Then, the                                                                                                                               t following Bregman projection with respect to the chosen constraint is solved.
                Wt+1 = argmin                     (W, W                                                W                            t),    tr(W) = 1, tr(WCt)  0.                               (10) By means of a Lagrange multiplier , the dual problem is described as

                        t = argmin tr[exp(log Wt - Ct)],   0.                                                                   (11) Using the solution of the dual problem, Wt is updated as

                                                        1                                     Wt+1 =                                 exp(log W                                                       Z                                     t - tCt)                                    (12)                                                            t(t)

where the normalization factor is Zt(t) = tr[exp(log Wt -tCt)]. Note that we can use the same numerically stable update as in the previous section.
4.2      Approximate Bregman Projections
The solution of (11) cannot be obtained in closed form. However, one can use the following approximate solution:
                                                   1                                1 + r                                                                                                t/max                                                                                                           t                                       t =                                  log                                  ,                        (13)                                               max                                                 t      - min                                                                   t                     1 + rt/min                                                                                                           t

when the eigenvalues of Ct lie in the interval [min                                                                                    t     , max                                                                                            t         ] and rt = tr(WtCt). Since the most unsatisfied constraint is chosen, rt  0 and thus t  0. Although the projection is done only approximately,3 the convergence of the dual objective (9) can be shown using the following upper bound.
   3The approximate Bregman projection (with t as in (13) can also be motivated as an online algorithm based on an entropic loss and learning rate one (following Section 3 and [4]).

Theorem 4.1 The dual objective (9) is bounded as                                                                   n                     T                           tr explogW1- jCj (rt)                                                                                         (14)                                                               j=1                                       t=1
                                                                     max                                                min                                                                     t                                                    -t                                                rt             max                                                  max                                                                t         -min                                                                                t                         rt          t      -min                                                                                                                                  t              where (rt) =        1 -                                                         1                                       .                                               max                                                 -                                                t                                                        min                                                                                                          t

The dual objective is monotonically decreasing, because (rt)  1. Also, since rt corre- sponds to the maximum value among all constraint violations {rj}nj=1, we have (rt) = 1 only if rt = 0. Thus the dual objective continues to decrease until all constraints are satisfied.
4.3    Relation to Boosting
When all matrices are diagonal, the DefiniteBoost degenerates to AdaBoost [9]: Let {xi,yi}di=1 be the training samples, where xi  Rm and yi  {-1,1}. Let h1(x), . . . , hn(x)  [-1,1] be the weak hypotheses. For the j-th hypothesis hj(x), let us define Cj = diag(y1hj(x1), . . . , ydhj(xd)). Since |yhj(x)|  1, max/min                                                                                                                             t              = 1 for any t. Setting W1 = I/d, the dual objective (14) is rewritten as
                                    d                                 n                                   1                                                                                                                   exp                                                                      d                      -yi jhj(xi),                                        i=1                               j=1

which is equivalent to the exponential loss function used in AdaBoost. Since Cj and W1 are diagonal, the matrix Wt stays diagonal after the update. If wti = [Wt]ii, the updating formula (12) becomes the AdaBoost update: wt+1,i = wti exp(-tyiht(xi))/Zt(t). The approximate solution of t (13) is described as t = 1 log 1+rt , where r                                                                                               2         1-r                      t is the weighted                                                                                                                t training error of the t-th hypothesis, i.e. rt =                               d         w                                                                                i=1         tiyiht(xi).
5      Experiments on Learning Kernels
In this section, our technique is applied to learning a kernel matrix from a set of distance measurements. This application is not on-line per se, but it shows nevertheless that the theoretical bounds can be reasonably tight on natural data.
When K is a d d kernel matrix among d objects, then the Kij characterizes the similarity between objects i and j. In the feature space, Kij corresponds to the inner product between object i and j, and thus the Euclidean distance can be computed from the entries of the kernel matrix [10]. In some cases, the kernel matrix is not given explicitly, but only a set of distance measurements is available. The data are represented either as (i) quantitative distance values (e.g., the distance between i and j is 0.75), or (ii) qualitative evaluations (e.g., the distance between i and j is small) [14, 13]. Our task is to obtain a positive definite kernel matrix which fits well to the given distance data.
On-line kernel learning      In the first experiment, we consider the on-line learning scenario in which only one distance example is shown to the learner at each time step. The distance example at time t is described as {at, bt, yt}, which indicates that the squared Euclidean distance between objects at and bt is yt. Let us define a time-developing sequence of kernel matrices as {Wt}Tt=1, and the corresponding points in the feature space as {xti}di=1 (i.e. [Wt]ab = xtaxtb). Then, the total loss incurred by this sequence is                     T                                                               T                                                     2               2                            xta                                           =                 (tr(W                                t - xtbt                  - yt                                           tXt) - yt)2,                    t=1                                                              t=1
             1.8                                                                                 0.45

             1.6                                                                                  0.4

             1.4                                                                                                      0.35                  1.2                                                                                                       0.3                   1                                                                                                      0.25                  0.8    Total Loss                                                                                         0.2                  0.6                                                         Classification Error                  0.4                                                                                 0.15

             0.2                                                                                  0.1

              0                                                                                  0.05                    0    0.5    1       1.5        2    2.5            3                                 0    0.5    1       1.5        2    2.5            3                                                                  5                                     Iterations                                                                                                        5                                                                                                                          Iterations                                                               x 10                                                                                 x 10

Figure 2: Numerical results of on-line learning. (Left) total loss against the number of iterations. The dashed line shows the loss bound. (Right) classification error of the nearest neighbor classifier using the learned kernel. The dashed line shows the error by the target kernel.
where Xt is a symmetric matrix whose (at, at) and (bt, bt) elements are 0.5, (at, bt) and (bt, at) elements are -0.5, and all the other elements are zero. We consider a controlled experiment in which the distance examples are created from a known target kernel matrix. We used a 52  52 kernel matrix among gyrB proteins of bacteria (d = 52). This data contains three bacteria species (see [12] for details). Each distance example is created by randomly choosing one element of the target kernel. The initial parameter was set as W1 = I/d. When the comparison matrix U is set to the target matrix, LU (S) = 0 and  max = 0, because all the distance examples are derived from the target matrix. Therefore we choose learning rate  = 2, which minimizes the relative loss bound of Lemma 3.2. The total loss of the kernel matrix sequence obtained by the matrix exponential update is shown in Figure 2 (left). In the plot, we have also shown the relative loss bound. The bound seems to give a reasonably tight performance guarantee--it is about twice the actual total loss. To evaluate the learned kernel matrix, the prediction accuracy of bacteria species by the nearest neighbor classifier is calculated (Figure 2, right), where the 52 proteins are randomly divided into 50% training and 50% testing data. The value shown in the plot is the test error averaged over 10 different divisions. It took a large number of iterations ( 2  105) for the error rate to converge to the level of the target kernel. In practice one can often increase the learning rate for faster convergence, but here we chose the small rate suggested by our analysis to check the tightness of the bound.
Kernel learning by Bregman projection                                      Next, let us consider a batch learning sce- nario where we have a set of qualitative distance evaluations (i.e. inequality constraints). Given n pairs of similar objects {aj, bj}nj=1, the inequality constraints are constructed as xaj - xbj  , j = 1, . . ., n, where  is a predetermined constant. If Xj is de- fined as in the previous section and Cj = Xj - I, the inequalities are then rewritten as tr(WCj)  0,j = 1,...,n. The largest and smallest eigenvalues of any Cj are 1 -  and -, respectively. As in the previous section, distance examples are generated from the target kernel matrix between gyrB proteins. Setting  = 0.2/d, we collected all object pairs whose distance in the feature space is less than  to yield 980 inequalities (n = 980). Figure 3 (left) shows the convergence of the dual objective function as proven in Theo- rem 4.1. The convergence was much faster than the previous experiment, because, in the batch setting, one can choose the most unsatisfied constraint, and optimize the step size as well. Figure 3 (right) shows the classification error of the nearest neighbor classifier. As opposed to the previous experiment, the error rate is higher than that of the target kernel matrix, because substantial amount of information is lost by the conversion to inequality constraints.
    55                                                                                  0.8

    50                                                                                  0.7

    45                                                                                  0.6

    40                                                                                  0.5

    35                                                                                  0.4

 Dual Obj 30                                                                            0.3                                                                     Classification Error         25                                                                                  0.2


    20                                                                                  0.1


    15                                                                                   0                0    50    100      150         200    250    300                              0    50    100      150         200    250    300                                  Iterations                                                                     Iterations

Figure 3: Numerical results of Bregman projection. (Left) convergence of the dual objective function. (Right) classification error of the nearest neighbor classifier using the learned kernel.
6          Conclusion
We motivated and analyzed a new update for symmetric positive matrices using the von Neumann divergence. We showed that the standard bounds for on-line learning and Boost- ing generalize to the case when the parameters are a symmetric positive definite matrix (of trace one) instead of a probability vector. As in quantum physics, the eigenvalues act as probabilities.
Acknowledgment                   We would like to thank B. Sch olkopf, M. Kawanabe, J. Liao and W.S. Noble for fruitful discussions. M.W. was supported by NSF grant CCR 9821087 and UC Discovery grant LSIT02-10110. K.T. and G.R. gratefully acknowledge partial support from the PASCAL Network of Excellence (EU #506778). Part of this work was done while all three authors were visiting the National ICT Australia in Canberra."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/c0e90532fb42ac6de18e25e95db73047-Abstract.html,Blind One-microphone Speech Separation: A Spectral Learning Approach,"Francis R. Bach, Michael I. Jordan","We present an algorithm to perform blind, one-microphone speech sep-          aration. Our algorithm separates mixtures of speech without modeling          individual speakers. Instead, we formulate the problem of speech sep-          aration as a problem in segmenting the spectrogram of the signal into          two or more disjoint sets. We build feature sets for our segmenter using          classical cues from speech psychophysics. We then combine these fea-          tures into parameterized affinity matrices. We also take advantage of the          fact that we can generate training examples for segmentation by artifi-          cially superposing separately-recorded signals. Thus the parameters of          the affinity matrices can be tuned using recent work on learning spectral          clustering [1]. This yields an adaptive, speech-specific segmentation al-          gorithm that can successfully separate one-microphone speech mixtures."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/c315f0320b7cd4ec85756fac52d78076-Abstract.html,Resolving Perceptual Aliasing In The Presence Of Noisy Sensors,"Guy Shani, Ronen I. Brafman","Agents learning to act in a partially observable domain may need to          overcome the problem of perceptual aliasing  i.e., different states that          appear similar but require different responses. This problem is exacer-          bated when the agent's sensors are noisy, i.e., sensors may produce dif-          ferent observations in the same state. We show that many well-known          reinforcement learning methods designed to deal with perceptual alias-          ing, such as Utile Suffix Memory, finite size history windows, eligibility          traces, and memory bits, do not handle noisy sensors well. We suggest          a new algorithm, Noisy Utile Suffix Memory (NUSM), based on USM,          that uses a weighted classification of observed trajectories. We compare          NUSM to the above methods and show it to be more robust to noise."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/c4f796afbc6267501964b46427b3f6ba-Abstract.html,Kernels for Multi--task Learning,"Charles A. Micchelli, Massimiliano Pontil","This paper provides a foundation for multitask learning using reproducing ker-          nel Hilbert spaces of vectorvalued functions. In this setting, the kernel is a          matrixvalued function. Some explicit examples will be described which go be-          yond our earlier results in [7]. In particular, we characterize classes of matrix          valued kernels which are linear and are of the dot product or the translation invari-          ant type. We discuss how these kernels can be used to model relations between          the tasks and present linear multitask learning algorithms. Finally, we present a          novel proof of the representer theorem for a minimizer of a regularization func-          tional which is based on the notion of minimal norm interpolation."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/c57168a952f5d46724cf35dfc3d48a7f-Abstract.html,Variational Minimax Estimation of Discrete Distributions under KL Loss,Liam Paninski,"We develop a family of upper and lower bounds on the worst-case ex-          pected KL loss for estimating a discrete distribution on a finite number m          of points, given N i.i.d. samples. Our upper bounds are approximation-          theoretic, similar to recent bounds for estimating discrete entropy; the          lower bounds are Bayesian, based on averages of the KL loss under          Dirichlet distributions. The upper bounds are convex in their parameters          and thus can be minimized by descent methods to provide estimators with          low worst-case error; the lower bounds are indexed by a one-dimensional          parameter and are thus easily maximized. Asymptotic analysis of the          bounds demonstrates the uniform KL-consistency of a wide class of es-          timators as c = N/m   (no matter how slowly), and shows that          no estimator is consistent for c bounded (in contrast to entropy estima-          tion). Moreover, the bounds are asymptotically tight as c  0 or ,          and are shown numerically to be tight within a factor of two for all c.          Finally, in the sparse-data limit c  0, we find that the Dirichlet-Bayes          (add-constant) estimator with parameter scaling like -c log(c) optimizes          both the upper and lower bounds, suggesting an optimal choice of the          ""add-constant"" parameter in this regime."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/c60d870eaad6a3946ab3e8734466e532-Abstract.html,Online Bounds for Bayesian Algorithms,"Sham M. Kakade, Andrew Y. Ng","We present a competitive analysis of Bayesian learning algorithms in the online learning setting and show that many simple Bayesian algorithms (such as Gaussian linear regression and Bayesian logistic regression) per- form favorably when compared, in retrospect, to the single best model in the model class. The analysis does not assume that the Bayesian algo- rithms’ modeling assumptions are “correct,” and our bounds hold even if the data is adversarially chosen. For Gaussian linear regression (us- ing logloss), our error bounds are comparable to the best bounds in the online learning literature, and we also provide a lower bound showing that Gaussian linear regression is optimal in a certain worst case sense. We also give bounds for some widely used maximum a posteriori (MAP) estimation algorithms, including regularized logistic regression."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/c61fbef63df5ff317aecdc3670094472-Abstract.html,Analysis of a greedy active learning strategy,Sanjoy Dasgupta,"We abstract out the core search problem of active learning schemes, to          better understand the extent to which adaptive labeling can improve sam-          ple complexity. We give various upper and lower bounds on the number          of labels which need to be queried, and we prove that a popular greedy          active learning rule is approximately as good as any other strategy for          minimizing this number of labels."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/cc70903297fe1e25537ae50aea186306-Abstract.html,A Cost-Shaping LP for Bellman Error Minimization with Performance Guarantees,"Daniela D. Farias, Benjamin V. Roy","We introduce a new algorithm based on linear programming that            approximates the differential value function of an average-cost            Markov decision process via a linear combination of pre-selected            basis functions. The algorithm carries out a form of cost shaping            and minimizes a version of Bellman error. We establish an error            bound that scales gracefully with the number of states without            imposing the (strong) Lyapunov condition required by its counter-            part in [6]. We propose a path-following method that automates            selection of important algorithm parameters which represent coun-            terparts to the ""state-relevance weights"" studied in [6].
1      Introduction
Over the past few years, there has been a growing interest in linear programming (LP) approaches to approximate dynamic programming (DP). These approaches offer algorithms for computing weights to fit a linear combination of pre-selected basis functions to a dynamic programming value function. A control policy that is ""greedy"" with respect to the resulting approximation is then used to make real-time decisions.
Empirically, LP approaches appear to generate effective control policies for high- dimensional dynamic programs [1, 6, 11, 15, 16]. At the same time, the strength and clarity of theoretical results about such algorithms have overtaken counterparts available for alternatives such as approximate value iteration, approximate policy iteration, and temporal-difference methods. As an example, a result in [6] implies that, for a discrete-time finite-state Markov decision process (MDP), if the span of the basis functions contains the constant function and comes within a distance of of the dynamic programming value function then the approximation generated by a certain LP will come within a distance of O( ). Here, the coefficient of the O( ) term depends on the discount factor and the metric used for measuring distance, but not on the choice of basis functions. On the other hand, the strongest results available for approximate value iteration and approximate policy iteration only promise O( ) error under additional requirements on iterates generated in the course of executing
the algorithms [3, 13]. In fact, it has been shown that, even when = 0, approximate value iteration can generate a diverging sequence of approximations [2, 5, 10, 14].
In this paper, we propose a new LP for approximating optimal policies. We work with a formulation involving average cost optimization of a possibly infinite-state MDP. The fact that we work with this more sophisticated formulation is itself a contribution to the literature on LP approaches to approximate DP, which have been studied for the most part in finite-state discounted-cost settings. But we view as our primary contributions the proposed algorithms and theoretical results, which strengthen in important ways previous results on LP approaches and unify certain ideas in the approximate DP literature. In particular, highlights of our contributions include:
 1. Relaxed Lyapunov Function dependence. Results in [6] suggest that              in order for the LP approach presented there to scale gracefully to large             problems  a certain linear combination of the basis functions must be             a ""Lyapunov function,"" satisfying a certain strong Lyapunov condition.             The method and results in our current paper eliminate this requirement.             Further, the error bound is strengthened because it alleviates an undesirable             dependence on the Lyapunov function that appears in [6] even when the             Lyapunov condition is satisfied.

 2. Restart Distribution Selection. Applying the LP studied in [6] requires             manual selection of a set of parameters called state-relevance weights. That             paper illustrated the importance of a good choice and provided intuition             on how one might go about making the choice. The LP in the current             paper does not explicitly make use of state-relevance weights, but rather,             an analog which we call a restart distribution, and we propose an automated             method for finding a desirable restart distribution.

 3. Relation to Bellman-Error Minimization. An alternative approach             for approximate DP aims at minimizing ""Bellman error"" (this idea was             first suggested in [16]). Methods proposed for this (e.g., [4, 12]) involve             stochastic steepest descent of a complex nonlinear function. There are no             results indicating whether a global minimum will be reached or guaranteeing             that a local minimum attained will exhibit desirable behavior. In this             paper, we explain how the LP we propose can be thought of as a method             for minimizing a version of Bellman error. The important differences here             are that our method involves solving a linear  rather than a nonlinear (and             nonconvex)  program and that there are performance guarantees that can             be made for the outcome.

The next section introduces the problem formulation we will be working with. Sec- tion 3 presents the LP approximation algorithm and an error bound. In Section 4, we propose a method for computing a desirable reset distribution. The LP approx- imation algorithm works with a perturbed version of the MDP. Errors introduced by this perturbation are studied in Section 5. A closing section discusses relations to our prior work on LP approaches to approximate DP [6, 8].
2    Problem Formulation and Perturbation Via Restart
Consider an MDP with a countable state space S and a finite set of actions A available at each state. Under a control policy u : S  A, the system dynamics are defined by a transition probability matrix Pu             |S||S|, where for policies u and u and states x and y, (Pu)xy = (Pu)xy if u(x) = u(x). We will assume
that, under each policy u, the system has a unique invariant distribution, given by u(x) = limt(P tu)yx, for all x, y  S. A cost g(x, a) is associated with each state-action pair (x, a). For shorthand, given any policy u, we let gu(x) = g(x, u(x)). We consider the problem of computing a policy that minimizes the average cost u = Tu gu. Let  = minu u and define the differential value function h(x) = minu limT  Eux[ T (g                                                                                  t=0       u(xt) - )]. Here, the superscript u of the expectation operator denotes the control policy and the subscript x denotes conditioning on x0 = x. It is easy to show that there exists a policy u that simultaneously minimizes the expectation for every x. Further, a policy u is optimal if and only if u(x)  arg minu(g(x, a) +                                  (P                                                                                            y          u)xy h(y)) for all x  S.
While in principle h can be computed exactly by dynamic programming algorithms, this is often infeasible due to the curse of dimensionality. We consider approximating h using a linear combination         K    r                                       k=1 kk of fixed basis functions 1, . . . , K : S       . In this paper, we propose and analyze an algorithm for computing weights r       K to approximate: h             K                                                  k=1         k(x)rk.    It is useful to define a matrix        |S|K so that our approximation to h can be written as r.
The algorithm we will propose operates on a perturbed version of the MDP. The nature of the perturbation is influenced by two parameters: a restart probability (1 - )  [0, 1] and a restart distribution c over the state space. We refer to the new system as an (, c)-perturbed MDP. It evolves similarly with the original MDP, except that at each time, the state process restarts with probability 1 - ; in this event, the next state is sampled randomly according to c. Hence, the perturbed MDP has the same state space, action space, and cost function as the original one, but the transition matrix under each policy u are given by P,u = Pu + (1 - )ecT .
We define some notation that will streamline our discussion and analysis of per- turbed MDPs. Let ,u(x) = limt(P t,u)yx, ,u = T,ugu,  = minu ,u, and let h be the differential value function for the (, c)-perturbed MDP, and let u be a policy satisfying u(x)  arg minu(g(x, a) +                      (P                                                                    y          ,u)xy h                                                                                         (y)) for all x  S . Finally, we will make use of dynamic programming operators T,uh = gu + P,uh and Th = minu T,uh."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/cdcb2f5c7b071143529ef7f2705dfbc4-Abstract.html,Validity Estimates for Loopy Belief Propagation on Binary Real-world Networks,"Joris M. Mooij, Hilbert J. Kappen","We introduce a computationally efficient method to estimate the valid-          ity of the BP method as a function of graph topology, the connectiv-          ity strength, frustration and network size. We present numerical results          that demonstrate the correctness of our estimates for the uniform random          model and for a real-world network (""C. Elegans""). Although the method          is restricted to pair-wise interactions, no local evidence (zero ""biases"")          and binary variables, we believe that its predictions correctly capture the          limitations of BP for inference and MAP estimation on arbitrary graphi-          cal models. Using this approach, we find that BP always performs better          than MF. Especially for large networks with broad degree distributions          (such as scale-free networks) BP turns out to significantly outperform          MF.
1     Introduction
Loopy Belief Propagation (BP) [1] and its generalizations (such as the Cluster Variation Method [2]) are powerful methods for inference and optimization. As is well-known, BP is exact on trees, but also yields surprisingly good results for many other graphs that arise in real-world applications [3, 4]. On the other hand, for densely connected graphs with high interaction strengths the results can be quite bad or BP can simply fail to converge. Despite the fact that BP is often used in applications nowadays, a good theoretical understanding of its convergence properties and the quality of the approximation is still lacking (except for the very special case of graphs with a single loop [5]).
In this article we attempt to answer the question in what way the quality of the BP re- sults depends on the topology of the underlying graph (looking at structural properties such as short cycles and large ""hubs"") and on the interaction potentials (i.e. strength and frus- tration). We do this for the special but interesting case of binary networks with symmetric pairwise potentials (i.e. Boltzmann machines) without local evidence. This has the practical
advantage that analytical calculations are feasible and furthermore we believe that adding local evidence will only serve to extend the domain of convergence, implying this to be the worst-case scenario. We compare the results with those of the variational mean-field (MF) method.
Real-world graphs are often far from uniformly random and possess structure such as clus- tering and power-law degree distributions [6]. Since we expect these structural features to arise in many applications of BP, we focus in this article on graphs modeling this kind of features. In particular, we consider Erdos-Renyi uniform random graphs [7], Barabasi- Albert ""scale-free"" graphs [8], and the neural network of a widely studied worm, the Caenorhabditis elegans.
This paper is organized as follows. In the next section we describe the class of graphical models under investigation and explain our method to efficiently estimate the validity of BP and MF. In section 3 we give a qualitative discussion of how the connectivity strength and frustration generally govern the model behavior and discuss the relevant regimes of the model parameters. We show for uniform random graphs that our validity estimates are in very good agreement with the real behavior of the BP algorithm. In section 4 we study the influence of graph topology. Thanks to the numerical efficiency of our estimation method we are able to study very large (N  10000) networks, for which it would not be feasible to simply run BP and look what happens. We also try our method on the neural network of the worm C. Elegans and find almost perfect agreement of our predictions with observed BP behavior. We conclude that BP is always better than MF and that the difference is particularly striking for the case of large networks with broad degree distributions such as scale-free graphs.
2    Model, paramagnetic solution and stability analysis
Let G = (V, B) be an undirected labelled graph without self-connections, defined by a set of nodes V = {1, . . . , N } and a set of links B  {(i, j) | 1  i < j  N }. The adjacency matrix corresponding to G is denoted M and defined as follows: Mij := 1 if (ij)  B or (ji)  B and 0 otherwise. We denote the set of neighbors of node i  V by Ni := {j  V | (ij)  B} and its degree by di := #(Ni). We define the average degree d := 1            d      N     iV         i and the maximum degree  := maxiV di.
To each node i we associate a binary random variable xi taking values in {-1, +1}. Let W be a symmetric N  N -matrix defining the strength of the links between the nodes. The probability distribution over configurations x = (x1, . . . , xN ) is given by
                                1                            1              1 M                          P(x) :=                   eWijxixj =                  e 2 ijWijxixj    (1)                                     Z                            Z                                          (ij)B                       i,jV

with Z a normalization constant. We will take the weight matrix W to be random, with i.i.d. entries {Wij}1i
For this model, instead of using the single-node and pair-wise beliefs bi(xi) resp. bij(xi, xj), it turns out to be more convenient to use the (equivalent) quantities m := {mi}iV and  := {ij}(ij)B, defined by:
        mi := bi(+1) - bi(-1);             ij := bij(+1, +1) - bij(+1, -1) - bij(-1, +1) + bij(-1, -1).

We will use these throughout this paper. We call the mi magnetizations; note that the expectation values E xi vanish because of the symmetry in the probability distribution (1).
As is well-known [2, 9], fixed points of BP correspond to stationary points of the Bethe free energy, which is in this case given by
                                                   N                                    1 + mixi      FBe(m, ) := -                   Wijij +               (1 - di)                         2                           (ij)B                      i=1                xi=1

                                                                                   1 + mixi + mjxj + xixjij                                                 +                                                      4                                                       (ij)B xi,xj =1

with (x) := x log x. Note that with this parameterization all normalization and overlap constraints (i.e.            b                      x            ij (xi, xj ) = bi(xi)) are satisfied by construction [10]. We can mini-                         j mize the Bethe free energy analytically by setting its derivatives to zero; one then immedi- ately sees that a possible solution of the resulting equations is the paramagnetic1 solution: mi = 0 and ij = tanh Wij (for (ij)  B). For this solution to be a minimum (instead of a saddle point or maximum), the Hessian of FBe at that point should be positive-definite. This condition turns out to be equivalent to the following Bethe stability matrix
                                           2                         ij       (A                                        ik         Be)ij := ij         1 +                              - Mij                          (with ij = tanh Wij)    (2)                                               1 - 2                   1 - 2                                      kN              ik                         ij                                          i

being positive-definite. Whether this is the case obviously depends on the values of the weights Wij and the adjacency matrix M . Since for zero weights (W = 0), the stability matrix is just the identity matrix, the paramagnetic solution is a minimum of the Bethe free energy for small values of the weights Wij. The question of what ""small"" exactly means in terms of J and J0 and how this relates to the graph topology will be taken on in the next two sections.
First we discuss the situation for the mean-field variational method. The mean-field free energy FMF (m) only depends on m; we can set its derivatives to zero, which again yields the paramagnetic solution m = 0. The corresponding stability matrix (equal to the Hes- sian) is given by                                               (AMF )ij := ij - WijMij and should be positive-definite for the paramagnetic solution to be stable. One can prove [11] that ABe is positive-definite whenever AMF is positive-definite. Since the exact mag- netizations are zero, we conclude that the Bethe approximation is better than the mean-field approximation for all possible choices of the weights W . As we will see later on, this dif- ference can become quite large for large networks.
3     Weight dependence
The behavior of the graphical model depends critically on the parameters J0 and J. Taking the graph topology to be uniformly random (see also subsection 4.1) we recover the model known in the statistical physics community as the Viana-Bray model [12], which has been thoroughly studied and is quite well-understood. In the limit N  , there are different relevant regimes (""phases"") for the parameters J and J0 to be distinguished (cf. Fig. 1):
     The paramagnetic phase, where the magnetizations all vanish (m = 0), valid for             J and J0 both small.          The ferromagnetic phase, where two configurations (characterized by all magne-             tizations being either positive or negative) each get half of the probability mass.             This is the phase occurring for large J0.

 1Throughout this article, we will use terminology from statistical physics if there is no good corresponding terminology in the field of machine learning available.



                      BP convergence behavior                                                     Stability m=0 minimum Bethe free energy         0.4                                                                               0.4                                                                                                         m=0 stable                                                                                                                  (spin-glass phase)                        no convergence                                                                                                             ?          0.3                                                                               0.3

                                                                                             marginal instability

J 0.2                                                                            J 0.2
                                                   convergence                                     m=0 stable                          m=0 instable                        convergence                       to ferromagnetic                                (paramagnetic         0.1                                                                               0.1                                              (ferromagnetic                      to m=0                            solutions                                       phase)                               phase)       


     0                                                                                 0                0        0.02        0.04              0.06     0.08        0.1                   0         0.02        0.04              0.06    0.08        0.1                                             J                                                                                  J (a)                                              0                                (b)                                               0

Figure 1: Empirical regime boundaries for the ER graph model with N = 100 and d = 20, averaged over three instances; expectation values are shown as thick black lines, standard- deviations are indicated by the gray areas. See the main text for additional explanation. The exact location of the boundary between the spin-glass and ferromagnetic phase in the right-hand plot (indicated by the dashed line) was not calculated. The red dash-dotted line shows the stability boundary for MF.
       The spin-glass phase where the probability mass is distributed over exponentially                     (in N ) many different configurations. This phase occurs for frustrated weights,                     i.e. for large J .

Consider now the right-hand plot in Fig. 1. Here we have plotted the different regimes con- cerning the stability of the paramagnetic solution of the Bethe approximation.2 We find that the m = 0 solution is indeed stable for J and J0 small and becomes unstable at some point when J0 increases. This signals the paramagnetic-ferromagnetic phase transition. The lo- cation is in good agreement with the known phase boundary found for the N   limit by advanced statistical physics methods as we show in more detail in [11]. For comparison we have also plotted the stability boundary for MF (the red dash-dotted line). Clearly, the mean-field approximation breaks down much earlier than the Bethe approximation and is unable to capture the phase transitions occurring for large connectivity strengths.
The boundary between the spin-glass phase and the paramagnetic phase is more subtle. What happens is that the Bethe stability matrix becomes marginally stable at some point when we increase J , i.e. the minimum eigenvalue of ABe approaches zero (in the limit N  ). This means that the Bethe free energy becomes very flat at that point. If we go on increasing J , the m = 0 solution becomes stable again (in other words, the minimum eigenvalue of the stability matrix ABe becomes positive again). We interpret the marginal instability as signalling the onset of the spin-glass phase. Indeed it coincides with the known phase boundary for the Viana-Bray model [11, 12]. We observe a similar marginal instability for other graph topologies.
Now consider the left-hand plot, Fig. 1(a). It shows the convergence behavior of the BP al- gorithm, which was determined by running BP with a fixed number of maximum iterations and slight damping. The messages were initialized randomly. We find different regimes that are separated by the boundaries shown in the plot. For small J and J0, BP converges to m = 0. For J0 large enough, BP converges to one of the two ferromagnetic solutions
2Although in Fig. 1 we show only one particular graph topology, the general appearance of these plots does not differ much for other graph topologies, especially for large N . The scale of the plots mostly depends on the network size N and the average degree d as we will show in the next section.
                                             Mean Field                                                       Bethe

                         2                                                             2

                       1.5                                                        1.5      1/2                     1              d                                                                             1                   J c                            0.5                                                        0.5

                         0                                                             0                               10                100          1000          10000            10             100              1000     10000

                                                   N                                                           N

Figure 2: Critical values for Bethe and MF for different graph topologies ( : ER,                                                   : BA) in the dense limit with d = 0.1N as a function of network size. Note that the y-axis is                                      rescaled by                              d.
(which one is determined by the random initial conditions). For large J , BP does not con- verge within 1000 iterations, indicating a complex probability distribution. The boundaries coincide within statistical precision with those in the right-hand plot which were obtained by the stability analysis.
The computation time necessary for producing a plot such as Fig. 1(a), showing the conver- gence behavior of BP, quickly increases with increasing N . The computation time needed for the stability analysis (Fig. 1(b)), which amounts to calculating the minimal eigenvalue of the N  N stability matrix, is much less, allowing us to investigate the behavior of BP for large networks.
4                        Graph topology
In this section we will concentrate on the frustrated case, more precisely on the case J0 = 0 (i.e. the y-axis in the regime diagrams) and study the location of the Bethe marginal instability and of the MF instability for various graph topologies as a function of network size N and average degree d. We will denote by J Be                                                                                            c      the critical value of J at which the Bethe paramagnetic solution becomes marginally unstable and we will refer to this as the Bethe critical value. The critical value of J where the MF solution becomes unstable will be denoted as J MF                                           c     and referred to as the MF critical value.
In studying the influence of graph topology for large networks, we have to distinguish two cases, which we call the dense and sparse limits. In the dense limit, we let N   and scale the average degree as d = cN for some fixed constant c. In this limit, we find that the influence of the graph topology is almost negligible. For all graph topologies that we have considered, we find the following asymptotic behavior for the critical values:
                                                                 1                              1                                                             J Be   ,              J MF                                                                         c                       c                                                                      d                            2 d

The constant of proportionality is approximately 1. These results are illustrated in Fig. 2 for two different graph topologies that will be discussed in more detail below.
In the sparse limit, we let N   but keep d fixed. In that case the resulting critical values show significant dependence on the graph topology as we will see.
4.1                      Uniform random graphs (ER)
The first and most elementary random graph model we will consider was introduced and studied by Erdos and Renyi [7]. The ensemble, which we denote as ER(N, p), consists of
      0.5                                                                                             Bethe J           0.4                                                                                             c                                                                                                    1/2                                                                                             1/d           0.3   J c                                                                                       MF Jc           0.2                                                                               1/(21/2)           0.1

        0              10                100                      1000                 10000

                                       N

Figure 3: Critical values for Bethe and MF for Erdos-Renyi uniform random graphs with average degree d = 10.
the graphs with N nodes; links are added between each pair of nodes independently with probability p. The resulting graphs have a degree distribution that is approximately Poisson for large N and the expected average degree is E d = p(N - 1). As was mentioned before, the resulting graphical model is known in the statistical physics literature as the Viana-Bray model (with zero ""external field"").
Fig. 3 shows the results for the sparse limit, where p is chosen such that the expected aver- age degree is fixed to d = 10. The Bethe critical value J Be                                                                 c    appears to be independent of                                             network size and is slightly larger than 1/ d. The MF critical value J MF                                                                              c        does depend on                                                                       network size (it looks to be proportional to 1/  instead of 1/ d); in fact it can be proven that it converges very slowly to 0 as N   [11], implying that the MF approximation breaks down for very large ER networks in the sparse limit. Although this is an interesting result, one could say that for all practical purposes the MF critical value J MF                                                                                        c     is nearly independent of network size N for uniform random graphs.
4.2      Scale-free graphs (BA)
A phenomenon often observed in real-world networks is that the degree distribution be- haves like a power-law, i.e. the number of nodes with degree  is proportional to - for some  > 0. These graphs are also known as ""scale-free"" graphs. The first random graph model exhibiting this behavior is from Barabasi and Albert [8].
We will consider a slightly different model, which we will denote by BA(N, m). It is defined as a stochastic process, yielding graphs with more and more nodes as time goes on. At t = 0 one starts with the graph consisting of m nodes and no links. At each time step, one node is added; it is connected with m different already existing nodes, attaching preferably to nodes with higher degree (""rich get richer""). More specifically, we take the probability to connect to a node of degree  to be proportional to  + 1. The degree dis- tribution turns out to have a power-law dependence for N   with exponent  = 3. In Fig. 4 we illustrate some BA graphs. The difference between the maximum degree  and the average degree d is rather large: whereas the average degree d converges to 2m, the                                             maximum degree  is known to scale as           N .
Fig. 5 shows the results of the stability analysis for BA graphs with average degree d =                                             10. Note that the y-axis is rescaled by          to show that the MF critical value J MF                                                                                                    c           is                      proportional to 1/ . The Bethe critical values are seen to have a scaling behavior that                                          lies somewhere between 1/ d and 1/ . Compared to the situation for uniform ER graphs, BP now even more significantly outperforms MF. The relatively low sensitivity to the maximum degree  that BP exhibits here can be understood intuitively since BA graphs resemble forests of sparsely interconnected stars of high degree, on which BP is exact."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/cdd96eedd7f695f4d61802f8105ba2b0-Abstract.html,Bayesian inference in spiking neurons,Sophie Deneve,"We propose a new interpretation of spiking neurons as Bayesian integra-          tors accumulating evidence over time about events in the external world          or the body, and communicating to other neurons their certainties about          these events. In this model, spikes signal the occurrence of new infor-          mation, i.e. what cannot be predicted from the past activity. As a result,          firing statistics are close to Poisson, albeit providing a deterministic rep-          resentation of probabilities. We proceed to develop a theory of Bayesian          inference in spiking neural networks, recurrent interactions implement-          ing a variant of belief propagation.
Many perceptual and motor tasks performed by the central nervous system are probabilis- tic, and can be described in a Bayesian framework [4, 3]. A few important but hidden properties, such as direction of motion, or appropriate motor commands, are inferred from many noisy, local and ambiguous sensory cues. These evidences are combined with priors about the sensory world and body. Importantly, because most of these inferences should lead to quick and irreversible decisions in a perpetually changing world, noisy cues have to be integrated on-line, but in a way that takes into account unpredictable events, such as a sudden change in motion direction or the appearance of a new stimulus.
This raises the question of how this temporal integration can be performed at the neural level. It has been proposed that single neurons in sensory cortices represent and compute the log probability that a sensory variable takes on a certain value (eg Is visual motion in the neuron's preferred direction?) [9, 7]. Alternatively, to avoid normalization issues and provide an appropriate signal for decision making, neurons could represent the log proba- bility ratio of a particular hypothesis (eg is motion more likely to be towards the right than towards the left) [7, 6]. Log probabilities are convenient here, since under some assump- tions, independent noisy cues simply combine linearly. Moreover, there are physiological evidence for the neural representation of log probabilities and log probability ratios [9, 6, 7].
However, these models assume that neurons represent probabilities in their firing rates. We argue that it is important to study how probabilistic information are encoded in spikes. Indeed, it seems spurious to marry the idea of an exquisite on-line integration of noisy cues with an underlying rate code that requires averaging on large populations of noisy neurons and long periods of time. In particular, most natural tasks require this integration to take place on the time scale of inter-spike intervals. Spikes are more efficiently signaling events
Institute of Cognitive Science, 69645 Bron, France
than analog quantities. In addition, a neural theory of inference with spikes will bring us closer to the physiological level and generate more easily testable predictions.
Thus, we propose a new theory of neural processing in which spike trains provide a de- terministic, online representation of a log-probability ratio. Spikes signals events, eg that the log-probability ratio has exceeded what could be predicted from previous spikes. This form of coding was loosely inspired by the idea of ""energy landscape"" coding proposed by Hinton and Brown [2]. However, contrary to [2] and other theories using rate-based representation of probabilities, this model is self-consistent and does not require different models for encoding and decoding: As output spikes provide new, unpredictable, tempo- rally independent evidence, they can be used directly as an input to other Bayesian neurons.
Finally, we show that these neurons can be used as building blocks in a theory of approx- imate Bayesian inference in recurrent spiking networks. Connections between neurons implement an underlying Bayesian network, consisting of coupled hidden Markov models. Propagation of spikes is a form of belief propagation in this underlying graphical model.
Our theory provides computational explanations of some general physiological properties of cortical neurons, such as spike frequency adaptation, Poisson statistics of spike trains, the existence of strong local inhibition in cortical columns, and the maintenance of a tight balance between excitation and inhibition. Finally, we discuss the implications of this model for the debate about temporal versus rate-based neural coding.
1      Spikes and log posterior odds
1.1       Synaptic integration seen as inference in a hidden Markov chain
We propose that each neuron codes for an underlying ""hidden"" binary variable, xt, whose state evolves over time. We assume that xt depends only on the state at the previous time step, xt-dt, and is conditionally independent of other past states. The state xt can switch from 0 to 1 with a constant rate ron = 1 lim                                             dt     dt0 P (xt = 1|xt-dt = 0), and from 1 to 0 with a constant rate roff . For example, these transition rates could represent how often motion in a preferred direction appears the receptive field and how long it is likely to stay there.
The neuron infers the state of its hidden variable from N noisy synaptic inputs, considered to be observations of the hidden state. In this initial version of the model, we assume that these inputs are conditionally independent homogeneous Poisson processes, synapse i emitting a spike between time t and t + dt (sit = 1) with constant probability qiondt if xt = 1, and another constant probability qi dt                                              off    if xt = 0. The synaptic spikes are assumed to be otherwise independent of previous synaptic spikes, previous states and spikes at other synapses. The resulting generative model is a hidden Markov chain (figure 1-A).
However, rather than estimating the state of its hidden variable and communicating this estimate to other neurons (for example by emitting a spike when sensory evidence for xt = 1 goes above a threshold) the neuron reports and communicates its certainty that the current state is 1. This certainty takes the form of the log of the ratio of the probability that the hidden state is 1, and the probability that the state is 0, given all the synaptic inputs received so far: Lt = log P (xt=1|s0t)                              P (xt=0|s0t) . We use s0t as a short hand notation for the N synaptic inputs received at present and in the past. We will refer to it as the log odds ratio.
Thanks to the conditional independencies assumed in the generative model, we can com- pute this Log odds ratio iteratively. Taking the limit as dt goes to zero, we get the following differential equation:
               L = ron 1 + e-L - roff 1 + eL +                w                                                              i    i(sit - 1) -

A.                                                                                                                                   B.                                r .r                              r .r                                on      off                       on     off              x                                  x                                     x                   t dt                                                                                                          i                                                      t                                     t dt                       st                                                           Ot
               q , q                             q , q                                  q , q                          on     off                                                                                        j                                                           on     off                          on      off            st                                  I                                      O         G                                                                                                                                                               t          Lt                     t           t

                                                                                                                                                               t                                 t             s                                   s                                     s                  t dt                                t                                     t dt

C.                                                                                                                                      E.             4       sd 2       do g 0       oL -2             -4                  500           1000              1500            2000                2500     3000 D. 2                                                                                                          20
        0   L                                                                                                                                        Count        t                                                                                                            Ot             -2                                                                                                 0

                            500           1000              1500            2000               2500      3000                                   0                     200            400              600                                                                 Time                                                                                                             ISI

Figure 1: A. Generative model for the synaptic input. B. Schematic representation of log odds ratio encoding and decoding. The dashed circle represents both eventual downstream elements and the self-prediction taking place inside the model neuron. A spike is fired only when Lt exceeds Gt. C. One example trial, where the state switches from 0 to 1 (shaded area) and back to 0. plain: Lt, dotted: Gt. Black stripes at the top: corresponding spikes train. D. Mean Log odds ratio (dark line) and mean output firing rate (clear line). E. Output spike raster plot (1 line per trial) and ISI distribution for the neuron shown is C. and D. Clear line: ISI distribution for a poisson neuron with the same rate.
wi, the synaptic weight, describe how informative synapse i is about the state of the hidden variable, e.g. wi = log qion . Each synaptic spike (si                                                                          qi                                                                                             t = 1) gives an impulse to the log                                                                                off odds ratio, which is positive if this synapse is more active when the hidden state if 1 (i.e it increases the neuron's confidence that the state is 1), and negative if this synapse is more active when xt = 0 (i.e it decreases the neuron's confidence that the state is 1).
The bias, , is determined by how informative it is not to receive any spike, e.g.  =             qi       i on - qioff . By convention, we will consider that the ""bias"" is positive or zero (if not, we need simply to invert the status of the state x).
1.2               Generation of output spikes
The spike train should convey a sparse representation of Lt, so that each spike reports new information about the state xt that is not redundant with that reported by other, preceding, spikes. This proposition is based on three arguments: First, spikes, being metabolically expensive, should be kept to a minimum. Second, spikes conveying redundant information would require a decoding of the entire spike train, whereas independent spike can be taken into account individually. And finally, we seek a self consistent model, with the spiking output having a similar semantics to its spiking input.
To maximize the independence of the spikes (conditioned on xt), we propose that the neu- ron fires only when the difference between its log odds ratio Lt and a prediction Gt of this log odds ratio based on the output spikes emitted so far reaches a certain threshold. Indeed, supposing that downstream elements predicts Lt as best as they can, the neuron only needs to fire when it expects that prediction to be too inaccurate (figure 1-B). In practice, this
will happen when the neuron receives new evidence for xt = 1. Gt should thereby follow the same dynamics as Lt when spikes are not received. The equation for Gt and the output Ot (Ot = 1 when an output spike is fired) are given by:
                                                       G = ron 1 + e-L - roff 1 + eL + go(Ot - 1)                                                (1)                                                                         g                              O                                           o                              t              = 1. when Lt > Gt +               , 0 otherwise,                           (2)                                                                         2

Here go, a positive constant, is the only free parameter, the other parameters being con- strained by the statistics of the synaptic input."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/ce758408f6ef98d7c7a7b786eca7b3a8-Abstract.html,Computing regularization paths for learning multiple kernels,"Francis R. Bach, Romain Thibaux, Michael I. Jordan","The problem of learning a sparse conic combination of kernel functions          or kernel matrices for classification or regression can be achieved via the          regularization by a block 1-norm [1]. In this paper, we present an al-          gorithm that computes the entire regularization path for these problems.          The path is obtained by using numerical continuation techniques, and          involves a running time complexity that is a constant times the complex-          ity of solving the problem for one value of the regularization parameter.          Working in the setting of kernel linear regression and kernel logistic re-          gression, we show empirically that the effect of the block 1-norm reg-          ularization differs notably from the (non-block) 1-norm regularization          commonly used for variable selection, and that the regularization path is          of particular value in the block case."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/cf9b2d0406020c56599f9a93708832b5-Abstract.html,Saliency-Driven Image Acuity Modulation on a Reconfigurable Array of Spiking Silicon Neurons,"R. J. Vogelstein, Udayan Mallik, Eugenio Culurciello, Gert Cauwenberghs, Ralph Etienne-Cummings","We have constructed a system that uses an array of 9,600 spiking sili- con neurons, a fast microcontroller, and digital memory, to implement a reconﬁgurable network of integrate-and-ﬁre neurons. The system is designed for rapid prototyping of spiking neural networks that require high-throughput communication with external address-event hardware. Arbitrary network topologies can be implemented by selectively rout- ing address-events to speciﬁc internal or external targets according to a memory-based projective ﬁeld mapping. The utility and versatility of the system is demonstrated by conﬁguring it as a three-stage network that accepts input from an address-event imager, detects salient regions of the image, and performs spatial acuity modulation around a high-resolution fovea that is centered on the location of highest salience."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/d37124c4c79f357cb02c655671a432fa-Abstract.html,Common-Frame Model for Object Recognition,"Pierre Moreels, Pietro Perona","A generative probabilistic model for objects in images is presented. An          object consists of a constellation of features. Feature appearance and          pose are modeled probabilistically. Scene images are generated by draw-          ing a set of objects from a given database, with random clutter sprinkled          on the remaining image surface. Occlusion is allowed.          We study the case where features from the same object share a common          reference frame. Moreover, parameters for shape and appearance den-          sities are shared across features. This is to be contrasted with previous          work on probabilistic `constellation' models where features depend on          each other, and each feature and model have different pose and appear-          ance statistics [1, 2]. These two differences allow us to build models          containing hundreds of features, as well as to train each model from a          single example. Our model may also be thought of as a probabilistic          revisitation of Lowe's model [3, 4].          We propose an efficient entropy-minimization inference algorithm that          constructs the best interpretation of a scene as a collection of objects and          clutter. We test our ideas with experiments on two image databases. We          compare with Lowe's algorithm and demonstrate better performance, in          particular in presence of large amounts of background clutter.
1     Introduction
There is broad agreement in the machine vision literature that objects and object categories should be represented as collections of features or parts with distinctive appearance and mutual position [1, 2, 4, 5, 6, 7, 8, 9]. A number of ideas for efficient detection algorithms (find instances of a given object category, e.g. faces) have been proposed by virtually all the cited authors, far fewer for recognition (list all objects and their pose in a given image) where matching would ideally take a logarithmic time with respect to the number of avail- able models [3, 4]. Learning of parameters characterizing features shape or appearance is still a difficult area, with most authors opting for heavy human intervention (typically segmentation and alignment of the training examples, although [1, 2, 3] train without su- pervision) and very large training sets for object categories (typically in the order of 10 3 - 104, although [10] recently demonstrated learning categories from 1-10 examples).
This work is based on two complementary efforts: the deterministic recognition system proposed by Lowe [3, 4], and the probabilistic constellation models by Perona and col- laborators [1, 2]. The first line of work has three attractive characteristics: objects are represented with hundreds of features, thus increasing robustness; models are learned from a single training example; last but not least, recognition is efficient with databases of hun- dreds of objects. The drawback of Lowe's approach is that both modeling decisions and algorithms rely on heuristics, whose design and performance may be far from optimal in
Figure 1: Diagram of our recognition model showing database, test image and two competing hy- potheses. To avoid a cluttered diagram, only one partial hypothesis is displayed for each hypothesis. The predicted position of models according to the hypotheses are overlaid on the test image.
some circumstances. Conversely, the second line of work is based on principled proba- bilistic object models which yield principled and, in some respects, optimal algorithms for learning and recognition/detection. Unfortunately, the large number of parameters em- ployed in each model limit in practice the number of features being used and require many training examples. By recasting Lowe's model and algorithms in probabilistic terms, we hope to combine the advantages of both methods. Besides, in this paper we choose to focus on individual objects as in [3, 4] rather than on categories as in [1, 2].
In [11] we presented a model aimed at the same problem of individual object recogni- tion. A major difference with the work described here lies in the probabilistic treatment of hypotheses, which allows us here to use directly hypothesis likelihood as a guide for the search, instead of the arbitrary admissible heuristic required by A*.
2    Probabilistic framework and notations
Each model object is represented as a collection of features. Features are informative parts extracted from images by an interest point operator. Each model is the set of features extracted from one training image of a given object - although this could be generalized to features from many images of the same object. Models are indexed by k and denoted by mk, while indices i and j are used respectively for features extracted from the test image and from model images: fi denotes the i - th test feature, while f k                                                                             j denotes the j - th feature from the k - th model. The features extracted from model images (training set) form the database. A feature detected in a test image can be a consequence of the presence of a model object in the image, in which case it should be associated to a feature from the database. In the alternative, this feature is attributed to a clutter - or background - detection.
The geometric information associated to each feature contains position information (x and y coordinates, denoted by the vector x), orientation (denoted by ) and scale (denoted by ). It is denoted by Xi = (x, i, i) for test feature fi and X k                                                                       j = (xk                                                                                j k                                                                                  j , k                                                                                        j ) for model feature f k           j . This geometric information is measured relatively to the standard reference frame of the image in which the feature has been detected. All features extracted from the same image share the same reference frame.
The appearance information associated to a feature is a descriptor characterizing the local image appearance near this feature. The measured appearance information is denoted by
Ai for test feature fi and Akj for model feature f kj. In our experiments, features are detected at multiple scales at the extrema of difference-of-gaussians filtered versions of the image [4, 12]. The SIFT descriptor [4] is then used to characterize the local texture about keypoints.
A partial hypothesis h explains the observations made in a fraction of the test image. It combines a model image mh and a corresponding set of pose parameters Xh. Xh encodes position, rotation, scale (this can easily be extended to affine transformations). We assume independence between partial hypotheses. This requires in particular independence be- tween models. Although reasonable, this approximation is not always true (e.g. a keyboard is likely to be detected close to a computer screen). This allows us to search in parallel for multiple objects in a test image.
A hypothesis H is the combination of several partial hypotheses, such that it explains com- pletely the observations made in the test image. A special notation H 0 or h0 denotes any (partial) hypothesis that states that no model object is present in a given fraction of the test image, and that features that could have been detected there are due to clutter.
Our objective is to find which model objects are present in the test scene, given the ob- servations made in the test scene and the information that is present in the database. In probabilistic terms, we look for hypotheses H for which the likelihood ration LR(H) = P (H|{fi},{fkj}) P (H0|{fi},{fkj}) > 1. This ratio characterizes how well models and poses specified by H explain the observations, as opposed to them being generated by clutter. Using Bayes rules and after simplifications,                                 P (H|{fi}, {f k                                                  j })         P ({fi}|{f k                                                                          j }, H )  P (H )                LR(H) =                                   =                                                   (1)                                 P (H0|{fi}, {f kj})           P ({fi}|{f k                                                                         j }, H0)  P (H0) where we used P ({f k                         j }|H ) = P ({f k                                               j }) since the database observations do not depend on the current hypothesis.
A key assumption of this work is that once the pose parameters of the objects (and thus their reference frames) are known, the geometric configuration and appearance of the test features are independent from each other. We also assume independence between features associated to models and features associated to clutter detections, as well as independence between separate clutter detections. Therefore, P ({fi}|{f k                                                                          j }, H ) =          i P (fi|{f k                                                                                                        j }, H ). These assumptions of independence are also made in [13], and undelying in [4].
Assignment vectors v represent matches between features from the test scene, and model features or clutter. The dimension of each assignment vector is the number of test features ntest. Its i - th component v(i) = (k, j) denotes that the test feature fi is matched to fv(i) = f kj, j - th feature from model mk. v(i) = (0, 0) denotes the case where fi is attributed to clutter. The set VH of assignment vectors compatible with a hypothesis H are those that assign test features only to models present in H (and to clutter). In particular, the only assignment vector compatible with h0 is v0 such that i, v0(i) = (0, 0). We obtain                                                                                                                                                                                       P (fi|fv(i), mh, Xh)   LR(H) = P (H)                          P (v|{fk                                                                                                         j }, mh, Xh)                                             (2)               P (H0)                                                                P (f                          vV                                                                i|h0)                                 H hH                                i|fih
P (H) is a prior on hypotheses, we assume it is constant. The term P (v|{f k                                                                                                j }, mh, Xh) is discussed in 3.1, we now explore the other terms.
P (fi|fv(i), mh, Xh) : fi and fv(i) are believed to be one and the same feature. Differences measured between them are noise due to the imaging system as well as distortions caused by viewpoint or lighting conditions changes. This noise probability p n encodes differences in appearance of the descriptors, but also in geometry, i.e. position, scale, orientation Assuming independence between appearance information and geometry information,
     pn(fi|f k                     j , mh, Xh) = pn,A(Ai|Av(i), mh, Xh)  pn,X (Xi|Xv(i), mh, Xh)                           (3)

Figure 2: Snapshots from the iterative matching process. Two competing hypotheses are displayed (top and bottom row) a) Each assignment vector contains one assignment, suggesting a transformation (red box) b) End of iterative process. The correct hypothesis is supported by numerous matches and high belief, while the wrong hypothesis has only a weak support from few matches and low belief.
The error in geometry is measured by comparing the values observed in the test image, with the predicted values that would be observed if the model features were to be trans- formed according to the parameters Xh. Let's denote by Xh(xv(i)),Xh(v(i)),Xh(v(i)) those predicted values, the geometry part of the noise probability can be decomposed into
 pn,X (Xi|Xv(i), h) = pn,x(xi, Xh(xv(i)))  pn,(i, Xh(v(i)))  pn,(i, Xh(v(i))) (4)

P (fi|h0) is a density on appearance and position of clutter detections, denoted by p bg(fi). We can decompose this density as well into an appearance term and a geometry term:
 pbg(fi) = pbg,A(Ai)  pbg,X (Xi) = pbg,A(Ai)  pbg,(x)(xi)  pbg,(i)  pbg,(i)              (5)

pbg,A, pbg,(x)(xi) pbg,(i), pbg,(i) are densities that characterize, for clutter detections, appearance, position, scale and rotation respectively.
Out of lack of space, and since it is not the main focus of this paper, we will not go into the details of how the ""foreground density"" p n and the ""background density"" pbg are learned. The main assumption is that those densities are shared across features, instead of having one set of parameters for each feature as in [1, 2]. This results in an important decrease of the number of parameters to be learned, at a slight cost in the model expressiveness.
3      Search for the best interpretation of the test image
The building block of the recognition process is a question, comparing a feature from a database model with a feature of the test image. A question selects a feature from the database, and tries to identify if and where this feature appears in the test image.
3.1     Assignment vectors compatible with hypotheses
For a given hypothesis H, the set of possible assignment vectors V H is too large for explicit exploration. Indeed, each potential match can either be accepted or rejected, which creates a combinatorial explosion. Hence, we approximate the summation in (2) by its largest term. In particular, each assignment vector v and each model referenced in v implies a set of pose parameters Xv (extracted e.g. with least-squares fitting). Therefore, the term P (v|{f k           j }, mh, Xh) from (2) will be significant only when Xv  Xh, i.e. when the pose implied by the assignment vector agrees with the pose specified by the partial hypothesis. We consider only the assignment vectors v for which Xv  Xh. P (vH|{f k                                                                                       j }, h) is assumed to be close to 1. Eq.(2) becomes
                                                       P (fi|fv                       LR(H)  P (H)                               h(i), mh, Xh)                                  P (H0)                          P (f                                            hH                           i|h                                                   i|f                           0)                   (6)                                                     ih

Our recognition system proceeds by asking questions sequentially and adding matches to assignment vectors. It is therefore natural to define, for a given hypothesis H and the corresponding assignment vector vH and t  ntest, the belief in vH by                                              pn(ft|fv(t), mh , Xh )                     B                                        t    t                     0(vH ) = 1, Bt(vH ) =                               Bt-1(vH)              (7)                                                    pbg(ft|h0) The geometric part of the belief (cf.(3)-(5) characterizes how close the pose X v implied by the assignments is to the pose Xh specified by the hypothesis. The geometric component of the belief characterizes the quality of the appearance match for the pairs (f i, fv(i)).
3.2    Entropy-based optimization
Our goal is finding quickly the hypothesis that best explains the observations, i.e. the hy- pothesis (models+poses) that has the highest likelihood ratio. We compute such hypothesis incrementally by asking questions sequentially. Each time a question is asked we update the beliefs. We stop the process and declare a detection (i.e. a given model is present in the image) as soon as the belief of a corresponding hypothesis exceeds a given confidence threshold. The speed with which we reach such a conclusion depends on choosing cleverly the next question. A greedy strategy says that the best next question is the one that takes us closest to a detection decision. We do so by considering the entropy of the vector of beliefs (the vector may be normalized to 1 so that each belief is in fact a probability): the lower the entropy the closer we are to a detection. Therefore we study the following heuristic: The most informative next question is the one that minimizes the expectation of the entropy of our beliefs. We call this strategy `minimum expected entropy' (MEE). This idea is due to Geman et al. [14].
Calculating the MEE question is, unfortunately, a complex and expensive calculation in itself. In Monte-Carlo simulations of a simplified version of our problem we notice that the MEE strategy tends to ask questions that relate to the maximum-belief hypothesis. Therefore we approximate the MEE strategy with a simple heuristic: The next question consists of attempting to match one feature of the highest-belief model; specifically, the feature with best appearance match to a feature in the test image.
3.3    Search for the best hypotheses
In an initialization step, a geometric hash table [3, 6, 7] is created by discretizing the space of possible transformations Note that we add only partial hypotheses in a hypothesis one at a time, which allows us to discretize only the space of partial hypotheses (models + poses), instead of discretizing the space of combinations of partial hypotheses.
Questions to be examined are created by pairing database features to the test features clos- est in terms of appearance. Note that since features encode location, orientation and scale, any single assignment between a test feature and a model feature contains enough infor- mation to characterize a similarity transformation. It is therefore natural to restrict the set of possible transformations to similarities, and to insert each candidate assignment in the corresponding geometric hash table entry. This forms a pool of candidate assignments. The set of hypotheses is initialized to the center of the hash table entries, and their belief is set to 1. The motivation for this initialization step is to examine, for each partial hypothesis, only a small number of candidate matches. A partial hypothesis corresponds to a hash table entry, we consider only the candidate assignments that fall into this same entry.
Each iteration proceeds as follows. The hypothesis H that currently has the highest likeli- hood ratio is selected. If the geometric hash table entry corresponding to the current partial hypothesis h, contains candidate assignments that have not been examined yet, one of them, (      m  fi, f h        j    ) is picked - currently, the best appearance match - and the probabilities p bg(fi)                m and pn(fi|f h                j    , mh, Xh) are computed. As mentioned in 3.1, only the best assignment
Figure 3: Results from our algorithm in various situations (viewpoint change can be seen in Fig.6). Each row shows the best hypothesis in terms of belief. a) Occlusion b) Change of scale.
Figure 4: ROC curves for both experiments. The performance improvement from our probabilistic formulation is particularly significant when a low false alarm rate is desired. The threshold used is the repeatability rate defined in [15]
                             m vector is explored: if pn(fi|f h                                  j       , mh, Xh) > pbg(fi) the match is accepted and inserted in                                                                                    m the hypothesis. In the alternative, fi is considered a clutter detection and f h                                                                                    j       is a missed detection. The belief B(vH ) and the likelihood ratio LR(H) are updated using (7).

After adding an assignment to a hypothesis, frame parameters X h are recomputed using least-squares optimization, based on all assignments currently associated to this hypothe- sis. This parameter estimation step provides a progressive refinement of the model pose parameters as assignments are added. Fig.2 illustrates this process.
The exploration of a partial hypothesis ends when no more candidate match is available in the hash table entry. We proceed with the next best partial hypothesis. The search ends when all test scene features have been matched or assigned to clutter.
4      Experimental results
4.1    Experimental setting
We tested our algorithm on two sets of images, containing respectively 49 and 161 model images, and 101 and 51 test images (sets P M - gadgets - 03 and JP - 3Dobjects - 04 available from http : //www.vision.caltech.edu/html - f iles/arc hive.html). Each model image contained a single object. Test images contained from zero (negative exam- ples) to five objects, for a total of 178 objects in the first set, and 79 objects in the second set. A large fraction of each test image consists of background. The images were taken with no precautions relatively to lighting conditions or viewing angle.
The first set contains common kitchen items and objects of everyday use. The second set (Ponce Lab, UIUC) includes office pictures. The objects were always moved between model images and test images. The images of model objects used in the learning stage were downsampled to fit in a 500  500 pixels box, the test images were downsampled to 800  800 pixels. With these settings, the number of features generated by the features detector was of the order of 1000 per training image and 2000-4000 per test image.
Figure 5: Behavior induced by clutter detections. A ground truth model was created by cutting a rectangle from the test image and adding noise. The recognition process is therefore expected to find a perfect match. The two rows show the best and second best model found by each algorithm (estimated frame position shown by the red box, features that found a match are shown in yellow)."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/d37b3ca37106b2bfdeaa12647e3bb1c9-Abstract.html,Brain Inspired Reinforcement Learning,"Françcois Rivest, Yoshua Bengio, John Kalaska","Successful application of reinforcement learning algorithms often  involves considerable hand-crafting of the necessary non-linear  features to reduce the complexity of the value functions and hence  to promote convergence of the algorithm. In contrast, the human  brain readily and autonomously finds the complex features when  provided with sufficient training. Recent work in machine learning  and neurophysiology has demonstrated the role of the basal ganglia  and the frontal cortex in mammalian reinforcement learning. This  paper develops and explores new  learning  algorithms  that provides  potential new approaches to the feature construction problem. The  algorithms are compared and evaluated on the Acrobot task. 
inspired by neurological evidence"
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/d3fad7d3634dbfb61018813546edbccb-Abstract.html,Semi-supervised Learning via Gaussian Processes,"Neil D. Lawrence, Michael I. Jordan","We present a probabilistic approach to learning a Gaussian Process         classifier in the presence of unlabeled data. Our approach involves         a ""null category noise model"" (NCNM) inspired by ordered cate-         gorical noise models. The noise model reflects an assumption that         the data density is lower between the class-conditional densities.         We illustrate our approach on a toy problem and present compar-         ative results for the semi-supervised classification of handwritten         digits."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/d756d3d2b9dac72449a6a6926534558a-Abstract.html,Parallel Support Vector Machines: The Cascade SVM,"Hans P. Graf, Eric Cosatto, Léon Bottou, Igor Dourdanovic, Vladimir Vapnik","We describe an algorithm for support vector machines (SVM) that  can be parallelized efficiently and scales to very large problems with  hundreds of thousands of training vectors. Instead of analyzing the  whole training set in one optimization step, the data are split into  subsets and optimized separately with multiple SVMs. The partial  results are combined and filtered again in a ‘Cascade’ of SVMs, until  the global optimum is reached. The Cascade SVM can be spread over  multiple processors with minimal communication overhead and  requires far less memory, since the kernel matrices are much smaller  than for a regular SVM. Convergence to the global optimum is  guaranteed with multiple passes through the Cascade, but already a  single pass provides good generalization. A single pass is 5x – 10x  faster than a regular SVM for problems of 100,000 vectors when  implemented on a single processor. Parallel implementations on a  cluster of 16 processors were tested with over 1 million vectors  (2-class problems), converging in a day or two, while a regular SVM  never converged in over a week."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/d7619beb6eb189509885fbc192d2874b-Abstract.html,Sampling Methods for Unsupervised Learning,"Rob Fergus, Andrew Zisserman, Pietro Perona","We present an algorithm to overcome the local maxima problem in es-
           timating the parameters of mixture models. It combines existing ap-
           proaches from both EM and a robust fitting algorithm, RANSAC, to give
           a data-driven stochastic learning scheme. Minimal subsets of data points,
           sufficient to constrain the parameters of the model, are drawn from pro-
           posal densities to discover new regions of high likelihood. The proposal
           densities are learnt using EM and bias the sampling toward promising
           solutions. The algorithm is computationally efficient, as well as effective
           at escaping from local maxima. We compare it with alternative methods,
           including EM and RANSAC, on both challenging synthetic data and the
           computer vision problem of alpha-matting."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/d790c9e6c0b5e02c87b375e782ac01bc-Abstract.html,Limits of Spectral Clustering,"Ulrike V. Luxburg, Olivier Bousquet, Mikhail Belkin","An important aspect of clustering algorithms is whether the partitions          constructed on finite samples converge to a useful clustering of the whole          data space as the sample size increases. This paper investigates this          question for normalized and unnormalized versions of the popular spec-          tral clustering algorithm. Surprisingly, the convergence of unnormalized          spectral clustering is more difficult to handle than the normalized case.          Even though recently some first results on the convergence of normal-          ized spectral clustering have been obtained, for the unnormalized case          we have to develop a completely new approach combining tools from          numerical integration, spectral and perturbation theory, and probability.          It turns out that while in the normalized case, spectral clustering usually          converges to a nice partition of the data space, in the unnormalized case          the same only holds under strong additional assumptions which are not          always satisfied. We conclude that our analysis gives strong evidence for          the superiority of normalized spectral clustering. It also provides a basis          for future exploration of other Laplacian-based methods."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html,Using the Equivalent Kernel to Understand Gaussian Process Regression,"Peter Sollich, Christopher Williams","The equivalent kernel [1] is a way of understanding how Gaussian pro-             cess regression works for large sample sizes based on a continuum limit.             In this paper we show (1) how to approximate the equivalent kernel of the             widely-used squared exponential (or Gaussian) kernel and related ker-             nels, and (2) how analysis using the equivalent kernel helps to understand             the learning curves for Gaussian processes.
Consider the supervised regression problem for a dataset D with entries (xi, yi) for i = 1, . . . , n. Under Gaussian Process (GP) assumptions the predictive mean at a test point x is given by
                                                               f (x) = k (x)(K + 2I)-1y,                                    (1)

where K denotes the n  n matrix of covariances between the training points with entries k(xi, xj), k(x) is the vector of covariances k(xi, x), 2 is the noise variance on the observations and y is a n  1 vector holding the training targets. See e.g. [2] for further details.
We can define a vector of functions h(x) = (K + 2I)-1k(x) . Thus we have                                                                                            f (x) = h (x)y, making it clear that the mean prediction at a point x is a linear combination of the target values y. Gaussian process regression is thus a linear smoother, see [3, section 2.8] for further details. For a fixed test point x, h(x) gives the vector of weights applied to targets y. Silverman [1] called h (x) the weight function.
Understanding the form of the weight function is made complicated by the matrix inversion of K + 2I and the fact that K depends on the specific locations of the n datapoints. Idealizing the situation one can consider the observations to be ""smeared out"" in x-space at some constant density of observations. In this case analytic tools can be brought to bear on the problem, as shown below. By analogy to kernel smoothing Silverman [1] called the idealized weight function the equivalent kernel (EK).
The structure of the remainder of the paper is as follows: In section 1 we describe how to derive the equivalent kernel in Fourier space. Section 2 derives approximations for the EK for the squared exponential and other kernels. In section 3 we show how use the EK approach to estimate learning curves for GP regression, and compare GP regression to kernel regression using the EK.
1       Gaussian Process Regression and the Equivalent Kernel
It is well known (see e.g. [4]) that the posterior mean for GP regression can be obtained as the function which minimizes the functional                                               1                   1         n                                 J[f ] =             f 2                          (y                                               2       H + 22                          i - f (xi))2,                          (2)                                                                        n i=1
where f H is the RKHS norm corresponding to kernel k. (However, note that the GP framework gives much more than just this mean prediction, for example the predictive variance and the marginal likelihood p(y) of the data under the model.)
Let (x) = E[y|x] be the target function for our regression problem and write E[(y - f (x))2] = E[(y - (x))2] + ((x) - f(x))2. Using the fact that the first term on the RHS is independent of f motivates considering a smoothed version of equation 2,
                                                                                            1                              J[f ] =                 ((x) - f (x))2dx +                             f 2                                              22                                                 2         H,

where  has dimensions of the number of observations per unit of x-space (length/area/volume etc. as appropriate).                          If we consider kernels that are stationary, k(x, x ) = k(x - x ), the natural basis in which to analyse equation 1 is the Fourier basis of complex sinusoids so that f (x) is represented as                                       ~                                                                                                 f (s)e2isxds and similarly for (x). Thus we obtain
                                    1                                                | ~                                                                                                f (s)|2                             J[f ] =                       | ~                                                            f (s) - ~                                                                         (s)|2 +                               ds,                                         2            2                                        S(s)

as      f 2     =    | ~                       f (            H                s)|2/S(s)ds where S(s) is the power spectrum of the kernel k, S(s) =          k(x)e-2isxdx. J[f ] can be minimized using calculus of variations to ob- tain ~         f (s) = S(s)(s)/(2/ + S(s)) which is recognized as the convolution f (x) =       h(x - x)(x)dx. Here the Fourier transform of the equivalent kernel h(x) is
                           ~                    S(s)                                  1                                h(s) =                                  =                                  .                   (3)                                              S(s) + 2/                    1 + 2/(S(s))

The term 2/ in the first expression for ~                                                                   h(s) corresponds to the power spectrum of a white noise process, whose delta-function covariance function becomes a constant in the Fourier domain. This analysis is known as Wiener filtering; see, e.g. [5, 14-1]. Notice that as   , h(x) tends to the delta function. If the input density is non-uniform the analysis above should be interpreted as computing the equivalent kernel for np(x) = . This approximation will be valid if the scale of variation of p(x) is larger than the width of the equivalent kernel.
2       The EK for the Squared Exponential and Related Kernels
For certain kernels/covariance functions the EK h(x) can be computed exactly by Fourier inversion. Examples include the Ornstein-Uhlenbeck process in D = 1 with covariance k(x) = e-|x| (see [5, p. 326]), splines in D = 1 corresponding to the regularizer  P f 2 =         (f (m))2dx [1, 6], and the regularizer P f 2 =                                           ( 2f )2dx in two dimen- sions, where the EK is given in terms of the Kelvin function kei [7].
We now consider the commonly used squared exponential (SE) kernel k(r)                                                         = exp(-r2/2 2), where r2 = ||x-x ||2. (This is sometimes called the Gaussian or radial ba- sis function kernel.) Its Fourier transform is given by S(s) = (2 2)D/2 exp(-22 2|s|2), where D denotes the dimensionality of x (and s) space.
From equation 3 we obtain
                                    ~                                  1                                         hSE(s) =                                                    ,                                                         1 + b exp(22 2|s|2)

where b = 2/(2 2)D/2. We are unaware of an exact result in this case, but the following initial approximation is simple but effective. For large , b will be small. Thus for small s = |s| we have that ~                           hSE                 1, but for large s it is approximately 0. The change takes place around the point sc where b exp(22 2s2c) = 1, i.e. s2c = log(1/b)/22 2. As exp(22 2s2) grows quickly with s, the transition of ~                                                                                   hSE between 1 and 0 can be expected to be rapid, and thus be well-approximated by a step function.
Proposition 1 The approximate form of the equivalent kernel for the squared-exponential kernel in D-dimensions is given by
                                                     s          D/2                                    h                          c                                          SE(r) =                           J                                                          r                      D/2(2scr).

Proof: hSE(s) is a function of s = |s| only, and for D > 1 the Fourier integral can be simplified by changing to spherical polar coordinates and integrating out the angular variables to give
                                          s +1             hSE(r) = 2r                                J(2rs)~hSE(s) ds                                                 (4)                                    0           r                                         sc     s +1                                     s          D/2                          2r                            J                                     c                                                               (2rs) ds =                                 JD/2(2scr).                                    0           r                                         r

where  = D/2 - 1, J(z) is a Bessel function of the first kind and we have used the identity z+1J(z) = (d/dz)[z+1J+1(z)].
Note that in D = 1 by computing the Fourier transform of the boxcar function we obtain hSE(x) = 2scsinc(2scx) where sinc(z) = sin(z)/z. This is consistent with Proposition 1 and J1/2(z) = (2/z)1/2 sin(z). The asymptotic form of the EK in D = 2 is shown in Figure 2(left) below.
Notice that sc scales as (log())1/2 so that the width of the EK (which is proportional to 1/sc) will decay very slowly as  increases. In contrast for a spline of order m (with power spectrum  |s|-2m) the width of the EK scales as -1/2m [1].
If instead of RD we consider the input set to be the unit circle, a stationary kernel can be periodized by the construction kp(x, x ) =                                          k(x - x + 2n). This kernel will                                                                                 nZ be represented as a Fourier series (rather than with a Fourier transform) because of the periodicity. In this case the step function in Fourier space approximation would give rise to a Dirichlet kernel as the EK (see [8, section 4.4.3] for further details on the Dirichlet kernel).
We now show that the result of Proposition 1 is asymptotically exact for   , and calcu- late the leading corrections for finite . The scaling of the width of the EK as 1/sc suggests writing hSE(r) = (2sc)Dg(2scr). Then from equation 4 and using the definition of sc
                          z                       2s           +1                  J             g(z) =                                            cs                                    (zs/sc)        ds                       sc(2sc)D 0                        z                      1 + exp[22 2(s2 - s2c)]                                        u       +1                   J                 = z                                                         (zu)                          du              (5)                          0         2z                 1 + exp[22 2s2c(u2 - 1)] where we have rescaled s = scu in the second step. The value of sc, and hence , now enters only in the exponential via a = 22 2s2c. For a  , the exponential tends to zero

for u < 1 and to infinity for u > 1. The factor 1/[1 + exp(. . .)] is therefore a step function (1 - u) in the limit and Proposition 1 becomes exact, with g(z)  lima g(z) = (2z)-D/2JD/2(z). To calculate corrections to this, one uses that for large but finite a the difference (u) = {1 + exp[a(u2 - 1)]}-1 - (1 - u) is non-negligible only in a range of order 1/a around u = 1. The other factors in the integrand of equation 5 can thus be Taylor-expanded around that point to give
                                      I dk             u       +1                                                     g(z) = g                                     k              (z) + z                                                      J                    ,         I                     (u)(u - 1)k du                                             k! duk         2z                   (zu)                         k =                                     k=0                                                      u=1                      0

The problem is thus reduced to calculating the integrals Ik. Setting u = 1 + v/a one has
                           0                     1                                                                    vk        ak+1Ik =                                                      - 1 vk dv +                                                                 dv                           -a             1 + exp(v2/a + 2v)                                     0         1 + exp(v2/a + 2v)                                a            (-1)k+1vk                                                    vk                 =                                                     dv +                                                        dv                           0         1 + exp(-v2/a + 2v)                            0         1 + exp(v2/a + 2v)

In the first integral, extending the upper limit to  gives an error that is exponentially small in a. Expanding the remaining 1/a-dependence of the integrand one then gets, to leading order in 1/a, I0 = c0/a2, I1 = c1/a2 while all Ik with k  2 are smaller by at least 1/a2. The numerical constants are -c0 = c1 = 2/24. This gives, using that (d/dz)[z+1J(z)] = zJ(z) + z+1J-1(z) = (2 + 1)zJ(z) - z+1J+1(z):
Proposition 2 The equivalent kernel for the squared-exponential kernel is given for large  by hSE(r) = (2sc)Dg(2scr) with
           1                                     z                                                                                                 1 g(z) =                              J                      (c                                                                                                )                      D                   D/2(z) +                0 + c1(D - 1))JD/2-1(z) - c1zJD/2(z)                                      +O(             (2z) 2                                  a2                                                                                                a4

For e.g. D = 1 this becomes g(z) = -1{sin(z)/z - 2/(24a2)[cos(z) + z sin(z)]}. Here and in general, by comparing the second part of the 1/a2 correction with the leading order term, one estimates that the correction is of relative size z2/a2. It will therefore provide a useful improvement as long as z = 2scr < a; for larger z the expansion in powers of 1/a becomes a poor approximation because the correction terms (of all orders in 1/a) are comparable to the leading order.
2.1      Accuracy of the approximation
To evaluate the accuracy of the approximation we can compute the EK numerically as follows: Consider a dense grid of points in RD with a sampling density grid. For making predictions at the grid points we obtain the smoother matrix K(K + 2                                                                   I)-1                                                                                                                                 grid            , where1 2       = 2  grid             grid/, as per equation 1. Each row of this matrix is an approximation to the EK at the appropriate location, as this is the response to a y vector which is zero at all points except one. Note that in theory one should use a grid over the whole of RD but in practice one can obtain an excellent approximation to the EK by only considering a grid around the point of interest as the EK typically decays with distance. Also, by only considering a finite grid one can understand how the EK is affected by edge effects.
   1To understand this scaling of 2grid consider the case where grid >  which means that the effective variance at each of the grid points per unit x-space is larger, but as there are correspondingly more points this effect cancels out. This can be understood by imagining the situation where there are grid/ independent Gaussian observations with variance 2grid at a single x-point; this would be equivalent to one Gaussian observation with variance 2. In effect the  observations per unit x-space have been smoothed out uniformly.



   0.16       0.35                                                    0.35                                             Numerical                                                    Numerical                    0.3                                                     0.3                                             Proposition 1                                                Proposition 1                   0.25                      Proposition 2                 0.25                           Proposition 2        0.14                    0.2                                                     0.2


              0.15                                                    0.15

   0.12        0.1                                                     0.1


              0.05                                                    0.05

    0.1         0                                                          0

             -0.05                                                   -0.05


   0.08       -0.1                                                    -0.1                      0             5        10               15                0           5             10               15




   0.06


   0.04


   0.02


     0


 -0.02                                                                                             Numerical                                                                                                        Proposition 1      -0.04                                                                                             Sample


    -0.5              -0.4    -0.3    -0.2      -0.1           0    0.1         0.2         0.3            0.4         0.5

Figure 1: Main figure: plot of the weight function corresponding to  = 100 training points/unit length, plus the numerically computed equivalent kernel at x = 0.0 and the sinc approximation from Proposition 1. Insets: numerically evaluated g(z) together with sinc and Proposition 2 approximations for  = 100 (left) and  = 104 (right).
Figure 1 shows plots of the weight function for  = 100, the EK computed on the grid as described above and the analytical sinc approximation. These are computed for parameter values of 2 = 0.004 and 2 = 0.1, with grid/ = 5/3. To reduce edge effects, the interval [-3/2, 3/2] was used for computations, although only the centre of this is shown in the figure. There is quite good agreement between the numerical computation and the analytical approximation, although the sidelobes decay more rapidly for the numerically computed EK. This is not surprising because the absence of a truly hard cutoff in Fourier space means one should expect less ""ringing"" than the analytical approximation predicts. The figure also shows good agreement between the weight function (based on the finite sample) and the numerically computed EK. The insets show the approximation of Proposi- tion 2 to g(z) for  = 100 (a = 5.67, left) and  = 104 (a = 9.67, right). As expected, the addition of the 1/a2-correction gives better agreement with the numerical result for z < a. Numerical experiments also show that the mean squared error between the numerically computed EK and the sinc approximation decreases like 1/ log(). The is larger than the naive estimate (1/a2)2  1/(log())4 based on the first correction term from Proposition 2, because the dominant part of the error comes from the region z > a where the 1/a expansion breaks down.
2.2      Other kernels
Our analysis is not in fact restricted to the SE kernel. Consider an isotropic kernel, for which the power spectrum S(s) depends on s = |s| only. Then we can again define from equation 3 an effective cutoff sc on the range of s in the EK via 2/ = S(sc), so that ~ h(s) = [1 + S(sc)/S(s)]-1. The EK will then have the limiting form given in Proposi- tion 1 if ~                 h(s) approaches a step function (sc - s), i.e. if it becomes infinitely ""steep"" around the point s = sc for sc  . A quantitative criterion for this is that the slope
|~ h (sc)| should become much larger than 1/sc, the inverse of the range of the step func- tion. Since ~                     h (s) = S (s)S(sc)S-2(s)[1 + S(sc)/S(s)]-2, this is equivalent to requiring that -scS (sc)/4S(sc)  -d log S(sc)/d log sc must diverge for sc  . The result of Proposition 1 therefore applies to any kernel whose power spectrum S(s) decays more rapidly than any positive power of 1/s.
A trivial example of a kernel obeying this condition would be a superposition of finitely many SE kernels with different lengthscales 2; the asymptotic behaviour of sc is then governed by the smallest .             A less obvious case is the ""rational quadratic"" k(r) = [1 + (r/l)2]-(D+1)/2 which has an exponentially decaying power spectrum S(s)  exp(-2 s). (This relationship is often used in the reverse direction, to obtain the power spectrum of the Ornstein-Uhlenbeck (OU) kernel exp(-r/ ).) Proposition 1 then applies, with the width of the EK now scaling as 1/sc  1/ log().
The previous example is a special case of kernels which can be written as superpositions of SE kernels with a distribution p( ) of lengthscales , k(r) =                          exp(-r2/2 2)p( ) d . This is in fact the most general representation for an isotropic kernel which defines a valid covariance function in any dimension D, see [9, 2.10]. Such a kernel has power spectrum                                                                                     S(s) = (2)D/2              D exp(-22 2s2)p( ) d                                (6)                                                 0
and one easily verifies that the rational quadratic kernel, which has S(s)  exp(-2 0s), is obtained for p( )  -D-2 exp(- 20/2 2). More generally, because the exponential factor in equation 6 acts like a cutoff for               > 1/s, one estimates S(s)             1/s Dp( ) d                                                                                                  0 for large s. This will decay more strongly than any power of 1/s for s   if p( ) itself decreases more strongly than any power of                   for           0. Any such choice of p( ) will therefore yield a kernel to which Proposition 1 applies.
3          Understanding GP Learning Using the Equivalent Kernel
We now turn to using EK analysis to get a handle on average case learning curves for Gaus- sian processes. Here the setup is that a function  is drawn from a Gaussian process, and we obtain  noisy observations of  per unit x-space at random x locations. We are concerned with the mean squared error (MSE) between the GP prediction f and . Averaging over the noise process, the x-locations of the training data and the prior over  we obtain the average MSE as a function of . See e.g. [10] and [11] for an overview of earlier work on GP learning curves.
To understand the asymptotic behaviour of                   for large , we now approximate the true GP predictions with the EK predictions from noisy data, given by fEK(x) =                                  h(x - x )y(x )dx in the continuum limit of ""smoothed out"" input locations. We assume as before that y = target + noise, i.e. y(x) = (x) + (x) where E[(x)(x )] = (2/)(x - x ). Here 2 denotes the true noise variance, as opposed to the noise variance assumed in the EK; the scaling of 2 with  is explained in footnote 1. For a fixed target , the MSE is         = ( dx)-1 [(x) - fEK(x)]2dx. Averaging over the noise process  and target function  gives in Fourier space
                                                               2       (2/)S       =        S                                                                        (s)/S2(s) + 2                                                                                                        /2                      (s)[1 - ~                              h(s)]2 + (2/)~h2(s) ds =                                                       ds                                                                                 [1 + 2/(S(s))]2                                                                                                                (7) where S(s) is the power spectrum of the prior over target functions. In the case S(s) = S(s) and 2 = 2 where the kernel is exactly matched to the structure of the target, equation 7 gives the Bayes error B and simplifies to B = (2/) [1 + 2/(S(s))]-1ds (see also [5, eq. 14-16]). Interestingly, this is just the analogue (for a continuous power spectrum of the kernel rather than a discrete set of eigenvalues) of the lower bound of [10]



                                                                                        0.5                                                                                             0.5                        =2


     0.03


    0.025                                                                                         0.02


    0.015


     0.01                                                                                             0.1                        =4         0.005


       0


   -0.005            1                                                                               0.05                  0.5                                                             1

                                                                      0.5                         0                                                                   0                               -0.5                                                                 25            50             100                                                     -0.5                                                                                250         500                                        -1    -1

Figure 2: Left: plot of the asymptotic form of the EK (sc/r)J1(2scr) for D = 2 and  = 1225. Right: log-log plot of against log() for the OU and Matern-class processes ( = 2, 4 respectively). The dashed lines have gradients of -1/2 and -3/2 which are the predicted rates.
on the MSE of standard GP prediction from finite datasets. In experiments this bound provides a good approximation to the actual average MSE for large dataset size n [11]. This supports our approach of using the EK to understand the learning behaviour of GP regression.
Treating the denominator in the expression for B again as a hard cutoff at s = sc, which is justified for large , one obtains for an SE target and learner  2sc/  (log())D/2/. To get analogous predictions for the mismatched case, one can write equation 7 as
                    2            [1 + 2/(S(s))] - 2/(S(s))                                                      S                   =                                                                                d                          (s)                                                                                                          s +                             ds.                                                   [1 + 2/(S(s))]2                                             [S(s)/2 + 1]2

The first integral is smaller than (2/2) B and can be neglected as long as                                                               B. In the second integral we can again make the cutoff approximation--though now with s having to be above sc  to get the scaling                                              sD-1S                                                                                  s                        (s) ds. For target functions with a                                                                                       c
power-law decay S(s)  s- of the power spectrum at large s this predicts  sD-                                                                                                                                                 c       (log())(D-)/2. So we generically get slow logarithmic learning, consistent with the observations in [12]. For D = 1 and an OU target ( = 2) we obtain  (log())-1/2, and for the Matern-class covariance function k(r) = (1 + r/ ) exp(-r/ ) (which has power spectrum  (3/ 2 + 42s2)-2, so  = 4) we get                                                             (log())-3/2. These predictions were tested experimentally using a GP learner with SE covariance function ( = 0.1 and assumed noise level 2 = 0.1) against targets from the OU and Matern-class priors (with   = 0.05) and with noise level 2 = 0.01, averaging over 100 replications for each value of . To demonstrate the predicted power-law dependence of on log(), in Figure 2(right) we make a log-log plot of                            against log(). The dashed lines show the gradients of -1/2 and -3/2 and we observe good agreement between experimental and theoretical results for large .
3.1       Using the Equivalent Kernel in Kernel Regression
Above we have used the EK to understand how standard GP regression works. One could alternatively envisage using the EK to perform kernel regression, on given finite data sets, producing a prediction -1                                       h(x                                                             i            - xi)yi at x. Intuitively this seems appealing as a cheap alternative to full GP regression, particularly for kernels such as the SE where the EK can be calculated analytically, at least to a good approximation. We now analyze briefly how such an EK predictor would perform compared to standard GP prediction.
Letting  denote averaging over noise, training input points and the test point and setting f(x) =         h(x, x)(x)dx, the average MSE of the EK predictor is
pred = [(x) - (1/)                 h(x, x                                  i              i)yi]2
  = [(x) - f(x)]2 + 2               h2(x, x )dx + 1                    h2(x, x )2(x )dx - 1 f 2                                                                                                           (x)

       2       (2/)S                                              2              ds       =                         (s)/S2(s) + 2                                                       /2 ds +                         [1 + 2/(S(s))]2                                       [1 + 2/(S(s))]2

Here we have set 2 = ( dx)-1                     2(x) dx =        S(s) ds for the spatial average of the squared target amplitude. Taking the matched case, (S(s) = S(s) and 2 = 2) as an example, the first term (which is the one we get for the prediction from ""smoothed out"" training inputs, see eq. 7) is of order 2sD                                                           c /, while the second one is       2 sD                                                                                                        c /. Thus both terms scale in the same way, but the ratio of the second term to the first is the signal- to-noise ratio 2 /2, which in practice is often large. The EK predictor will then perform significantly worse than standard GP prediction, by a roughly constant factor, and we have confirmed this prediction numerically. This result is somewhat surprising given the good agreement between the weight function h(x) and the EK that we saw in figure 1, leading to the conclusion that the detailed structure of the weight function is important for optimal prediction from finite data sets.
In summary, we have derived accurate approximations for the equivalent kernel (EK) of GP regression with the widely used squared exponential kernel, and have shown that the same analysis in fact extends to a whole class of kernels. We have also demonstrated that EKs provide a simple means of understanding the learning behaviour of GP regression, even in cases where the learner's covariance function is not well matched to the structure of the target function. In future work, it will be interesting to explore in more detail the use of the EK in kernel smoothing. This is suboptimal compared to standard GP regression as we saw. However, it does remain feasible even for very large datasets, and may then be competitive with sparse methods for approximating GP regression. From the theoretical point of view, the average error of the EK predictor which we calculated may also provide the basis for useful upper bounds on GP learning curves.
Acknowledgments: This work was supported in part by the IST Programme of the Eu- ropean Community, under the PASCAL Network of Excellence, IST-2002-506778. This publication only reflects the authors' views."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html,Newscast EM,"Wojtek Kowalczyk, Nikos Vlassis","We propose a gossip-based distributed algorithm for Gaussian mixture learning, Newscast EM. The algorithm operates on network topologies where each node observes a local quantity and can communicate with other nodes in an arbitrary point-to-point fashion. The main difference between Newscast EM and the standard EM algorithm is that the M-step in our case is implemented in a decentralized manner: (random) pairs of nodes repeatedly exchange their local parameter estimates and com- bine them by (weighted) averaging. We provide theoretical evidence and demonstrate experimentally that, under this protocol, nodes converge ex- ponentially fast to the correct estimates in each M-step of the EM algo- rithm."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/daaaf13651380465fc284db6940d8478-Abstract.html,Dynamic Bayesian Networks for Brain-Computer Interfaces,"Pradeep Shenoy, Rajesh P. Rao","We describe an approach to building brain-computer interfaces (BCI)          based on graphical models for probabilistic inference and learning. We          show how a dynamic Bayesian network (DBN) can be used to infer          probability distributions over brain- and body-states during planning and          execution of actions. The DBN is learned directly from observed data          and allows measured signals such as EEG and EMG to be interpreted in          terms of internal states such as intent to move, preparatory activity, and          movement execution. Unlike traditional classification-based approaches          to BCI, the proposed approach (1) allows continuous tracking and predic-          tion of internal states over time, and (2) generates control signals based          on an entire probability distribution over states rather than binary yes/no          decisions. We present preliminary results of brain- and body-state es-          timation using simultaneous EEG and EMG signals recorded during a          self-paced left/right hand movement task."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/dbab2adc8f9d078009ee3fa810bea142-Abstract.html,Edge of Chaos Computation in Mixed-Mode VLSI - A Hard Liquid,"Felix Schürmann, Karlheinz Meier, Johannes Schemmel",Computation without stable states is a computing paradigm dif-         ferent from Turing's and has been demonstrated for various types         of simulated neural networks. This publication transfers this to a         hardware implemented neural network. Results of a software im-         plementation are reproduced showing that the performance peaks         when the network exhibits dynamics at the edge of chaos. The         liquid computing approach seems well suited for operating analog         computing devices such as the used VLSI neural network.
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/dbbf603ff0e99629dda5d75b6f75f966-Abstract.html,Breaking SVM Complexity with Cross-Training,"Léon Bottou, Jason Weston, Gökhan H. Bakir","We propose to selectively remove examples from the training set using probabilistic estimates related to editing algorithms (Devijver and Kittler, 1982). This heuristic procedure aims at creating a separable distribution of training examples with minimal impact on the position of the decision boundary. It breaks the linear dependency between the number of SVs and the number of training examples, and sharply reduces the complexity of SVMs during both the training and prediction stages."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/dbd22ba3bd0df8f385bdac3e9f8be207-Abstract.html,Linear Multilayer Independent Component Analysis for Large Natural Scenes,"Yoshitatsu Matsuda, Kazunori Yamaguchi","In this paper, linear multilayer ICA (LMICA) is proposed for extracting            independent components from quite high-dimensional observed signals            such as large-size natural scenes. There are two phases in each layer of            LMICA. One is the mapping phase, where a one-dimensional mapping            is formed by a stochastic gradient algorithm which makes more highly-            correlated (non-independent) signals be nearer incrementally. Another            is the local-ICA phase, where each neighbor (namely, highly-correlated)            pair of signals in the mapping is separated by the MaxKurt algorithm.            Because LMICA separates only the highly-correlated pairs instead of all            ones, it can extract independent components quite efficiently from ap-            propriate observed signals. In addition, it is proved that LMICA always            converges. Some numerical experiments verify that LMICA is quite ef-            ficient and effective in large-size natural image processing.
1     Introduction
Independent component analysis (ICA) is a recently-developed method in the fields of signal processing and artificial neural networks, and has been shown to be quite useful for the blind separation problem [1][2][3] [4]. The linear ICA is formalized as follows. Let s and A are N -dimensional source signals and N  N mixing matrix. Then, the observed signals x are defined as                                             x = As.                                      (1)
The purpose is to find out A (or the inverse W ) when the observed (mixed) signals only are given. In other words, ICA blindly extracts the source signals from M samples of the observed signals as follows:                                             ^                                            S = W X,                                      (2)
 http://www.graco.c.u-tokyo.ac.jp/~matsuda

where X is an N  M matrix of the observed signals and ^                                                                          S is the estimate of the source signals. This is a typical ill-conditioned problem, but ICA can solve it by assuming that the source signals are generated according to independent and non-gaussian probability dis- tributions. In general, the ICA algorithms find out W by maximizing a criterion (called the contrast function) such as the higher-order statistics (e.g. the kurtosis) of every com- ponent of ^              S. That is, the ICA algorithms can be regarded as an optimization method of such criteria. Some efficient algorithms for this optimization problem have been proposed, for example, the fast ICA algorithm [5][6], the relative gradient algorithm [4], and JADE [7][8].
Now, suppose that quite high-dimensional observed signals (namely, N is quite large) are given such as large-size natural scenes. In this case, even the efficient algorithms are not much useful because they have to find out all the N 2 components of W . Recently, we pro- posed a new algorithm for this problem, which can find out global independent components by integrating the local ICA modules. Developing this approach in this paper, we propose a new efficient ICA algorithm named "" the linear multilayer ICA algorithm (LMICA)."" It will be shown in this paper that LMICA is quite efficient than other standard ICA algo- rithms in the processing of natural scenes. This paper is an extension of our previous works [9][10].
This paper is organized as follows. In Section 2, the algorithm is described. In Section 3, numerical experiments will verify that LMICA is quite efficient in image processing and can extract some interesting edge detectors from large natural scenes. Lastly, this paper is concluded in Section 4."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/dc2b690516158a874dd8aabe1365c6a0-Abstract.html,Identifying Protein-Protein Interaction Sites on a Genome-Wide Scale,"Haidong Wang, Eran Segal, Asa Ben-Hur, Daphne Koller, Douglas L. Brutlag","Protein interactions typically arise from a physical interaction of one or            more small sites on the surface of the two proteins. Identifying these sites            is very important for drug and protein design. In this paper, we propose            a computational method based on probabilistic relational model that at-            tempts to address this task using high-throughput protein interaction data            and a set of short sequence motifs. We learn the model using the EM            algorithm, with a branch-and-bound algorithm as an approximate infer-            ence for the E-step. Our method searches for motifs whose presence in a            pair of interacting proteins can explain their observed interaction. It also            tries to determine which motif pairs have high affinity, and can therefore            lead to an interaction. We show that our method is more accurate than            others at predicting new protein-protein interactions. More importantly,            by examining solved structures of protein complexes, we find that 2/3 of            the predicted active motifs correspond to actual interaction sites.
1     Introduction
Many cellular functions are carried out through physical interactions between proteins. Discovering the protein interaction map can therefore help to better understand the work- ings of the cell. Indeed, there has been much work recently on developing high-throughput methods to produce a more complete map of protein-protein interactions [1, 2, 3].       Interactions between two proteins arise from physical interactions between small re- gions on the surface of the proteins [4] (see Fig. 2(b)). Finding interaction sites is an important task, which is of particular relevance to drug design. There is currently no high- throughput experimental method to achieve this goal, so computational methods are re- quired. Existing methods either require solving a protein's 3D structure (e.g., [5]), and therefore are computationally very costly and not applicable on a genome-wide scale, or use known interaction sites as training data (e.g., [6]), which are relatively scarce and hence have poor coverage. Other work focuses on refining the highly noisy high-throughput inter- action maps [7, 8, 9], or on assessing the confidence levels of the observed interactions [10].       In this paper, we propose a computational method for predicting protein interactions
                                                                                     P1                          P2                          P5                                                     d                      a          d                                             A                A               A           A                     A                                                          P                          a          d                b          d                          d                                                               5                           P1


                                                                          A                     A     A                A               A                                                                                    ab               db         ad               dd          bd



                                                                          B          S B              B          S Bab                 B               S                                                                                                                ab                               ab                                                b                                   ab               db                                                                          c       d                                                  b                    T                           T                           T            P                                                       P4              12                          15                          25                 2                    P                                                   O                           O                           O                                           3       b

                                 (a)                                                                            (b)

Figure 1: (a) Simple illustration of our assumptions for protein-protein interactions. The small elements denote motif occurrences on proteins, with red denoting active and gray denoting inactive motifs. (b) A fragment of our probabilistic model, for the proteins P1, P2, P5. We use yellow to denote an assignment of the value true, and black to denote the value false; full circles denote an assignment observed in the data, and patterned circles an assignment hypothesized by our algorithm. The dependencies involving inactive motif pairs were removed from the graph because they do not affect the rest of the model.
and the sites at which the interactions take place, which uses as input only high-throughput protein-protein interaction data and the protein sequences. In particular, our method as- sumes no knowledge of the 3D protein structure, or of the sites at which binding occurs.     Our approach is based on the assumption that interaction sites can be described using a limited repertoire of conserved sequence motifs [11]. This is a reasonable assumption since interaction sites are significantly more conserved than the rest of the protein surface [12]. Given a protein interaction map, our method tries to explain the observed interactions by identifying a set of sites of motif occurrence on every pair of interacting proteins through which the interaction is mediated. To understand the intuition behind our approach, con- sider the example of Fig. 1(a). Here, the interaction pattern of the protein P1 can best be explained using the motif pair a, b, where a appears in P1 and b in the proteins P2, P3, P4 but not in P5. By contrast, the motif pair d, b is not as good an explanation, because d also appears in P5, which has a different interaction pattern. In general, our method aims to identify motif pairs that have high affinity, potentially leading to interaction between protein pairs that contain them.     However, a sequence motif might be used for a different purpose, and not give rise to an active binding site; it might also be buried inside the protein, and thus be inaccessible for interaction. Thus, the appearance of an appropriate motif does not always imply interaction. A key feature of our approach is that we allow each motif occurrence in a protein to be either active or inactive. Interactions are then induced only by the interactions of high- affinity active motifs in the two proteins. Thus, in our example, the motif d in p2 is inactive, and hence does not lead to an interaction between p2 and p4, despite the affinity between the motif pair c, d. We note that Deng et al. [8] proposed a somewhat related method for genome-wide analysis of protein interaction data, based on protein domains. However, their method is focused on predicting protein-protein interactions and not on revealing the site of interaction, and they do not allow for the possibility that some domains are inactive.     Our goal is thus to identify two components: the affinities between pairs of motifs, and the activity of the occurrences of motifs in different proteins. Our algorithm addresses this problem by using the framework of Bayesian networks [13] and probabilistic relational models [14], which allows us to handle the inherent noise in the protein interaction data and the uncertain relationship between interactions and motif pairs. We construct a model encoding our assumption that protein interactions are induced by the interactions of active motif pairs. We then use the EM algorithm [15], to fill in the details of the model, learning both the motif affinities and activities from the observed data of protein-protein interactions and protein motif occurrences. We address the computational complexity of the E-step in
these large, densely connected models by using an approximate inference procedure based on branch-and-bound.      We evaluated our model on protein-protein interactions in yeast and Prosite motifs [11]. As a basic performance measure, we evaluated the ability of our method to predict new protein-protein interactions, showing that it achieves better performance than several other models. In particular, our results validate our assumption that we can explain interactions via the interactions of active sequence motifs. More importantly, we analyze the ability of our method to discover the mechanism by which the interaction occurs. Finally, we examined co-crystallized protein pairs where the 3D structure of the interaction is known, so that we can determine the sites at which the interaction took place. We show that our active motifs are more likely to participate in interactions.
2    The Probabilistic Model
The basic entities in our probabilistic model are the proteins and the set of sequence motifs that can mediate protein interactions. Our model therefore contains a set of protein entities P = {P1, . . . , Pn}, with the motifs that occur in them. Each protein P is associated with the set of motifs that occur in it, denoted by P.M . As we discussed, a key premise of our approach is that a specific occurrence of a sequence motif may or may not be active. Thus, each motif occurrence a  P.M is associated with a binary-value variable P.Aa, which takes the value true if Aa is active in protein P and false otherwise. We structure the prior probability P (P.Aa = true) = min{0.8, 3+0.1|P.M| }, to capture our intuition that                                                       |P.M | the number of active motifs in a protein is roughly a constant fraction of the total number of motifs in the protein, but that even proteins with few motifs tend to have at least some number of active motifs.      A pair of active motifs in two proteins can potentially bind and induce an interaction between the corresponding proteins. Thus, in our model, a pair of proteins interact if each contains an active motif, and this pair of motifs bind to each other. The probability with which two motifs bind to each other is called their affinity. We encode this assumption by including in our model entities Tij corresponding to a pair of proteins Pi, Pj. For each pair of motifs a  Pi.M and b  Pj.M , we introduce a variable Tij.Aab, which is a deterministic AND of the activity of these two motifs. Intuitively, this variable represents whether the pair of motifs can potentially interact. The probability with which two active motif occurrences bind is their affinity. We model the binding event between two motif occurrences using a variable Tij.Bab, and define: P (Tij.Bab = true | Tij.Aab = true) = ab and P (Tij.Bab = true | Tij.Aab = false) = 0, where ab is the affinity between motifs a and b. This model reflects our assumption that two motif occurrences can bind only if they are both active, but their actual binding probability depends on their affinity. Note that this affinity is a feature of the motif pair and does not depend on the proteins in which they appear.      We must also account for interactions that are not explained by our set of motifs, whether because of false positives in the data, or because of inadequacies of our model or of our motif set. Thus, we add a spurious binding variable Tij.S, for cases where an interaction between Pi and Pj exists, but cannot be explained well by our set of active motifs. The probability that a spurious binding occurs is given by P (Tij.S = true) = S.      Finally, we observe an interaction between two proteins if and only if some form of binding occurs, whether by a motif pair or a spurious binding. Thus, we define a variable Tij.O, which represents whether protein i was observed to interact with protein j, to be a deterministic OR of all the binding variables Tij.S and Tij.Bab. Overall, Tij.O is a noisy-OR [13] of all motif pair variables Tij.Aab.      Note that our model accounts for both types of errors in the protein interaction data. False negatives (missing interactions) in the data are addressed through the fact that the presence of an active motif pair only implies that binding takes place with some probability. False positives (wrong interactions) in the data are addressed through the introduction of
the spurious interaction variables.      The full model defines a joint probability distribution over the entire set of attributes:
  P (P.A, T.A, T.B, T.S, T.O) =                                         P (P                                                         i    aP                    i.Aa)                                                                     i .M                                            P (T                     aP                            ij .Aab | Pi.Aa, Pj .Ab)P (Tij .Bab | Tij .Aab)                            i .M,bPj .M             ij    P (Tij.S)P (Tij.O | Tij.B, Tij.S)

where each of these conditional probability distributions is as specified above. We use  to denote the entire set of model parameters {a,b}a,b  {S}. An instantiation of our probabilistic model is illustrated in Fig. 1(b).
3    Learning the Model
We now turn to the task of learning the model from the data. In a typical setting, we are given as input a protein interaction data set, specifying a set of proteins P and a set of observed interacting pairs T.O. We are also given a set of potentially relevant motifs, and the occurrences of these motifs in the different proteins in P. Thus, all the variables except for the O variables are hidden. Our learning task is thus twofold: we need to infer the values of the hidden variables, both the activity variables P.A, T.A, and the binding variables T.B, T.S; we also need to find a setting of the model parameters , which specify the motif affinities. We use a variant of the EM algorithm [15] to find both an assignment to the parameters , and an assignment to the motif variables P.A, which is a local maximum of the likelihood function P (T.O, P.A | ). Note that, to maximize this objective, we search for a MAP assignment to the motif activity variables, but sum out over the other hidden variables. This design decision is reasonable in our setting, where determining motif activities is an important goal; it is a key assumption for our computational procedure.      As in most applications of EM, our main difficulty arises in the E-step, where we need to compute the distribution over the hidden variables given the settings of the observed variables and the current parameter settings. In our model, any two motif variables (both within the same protein and across different proteins) are correlated, as there exists a path of influence between them in the underlying Bayesian network (see Fig. 1(c)). These cor- relations make the task of computing the posterior distribution over the hidden variables intractable, and we must resort to an approximate computation. While we could apply a general purpose approximate inference algorithm such as loopy belief propagation [16], such methods may not converge in densely connected model such as this one, and there are few guarantees on the quality of the results even if they do converge. Fortunately, our model turns out to have additional structure that we can exploit. We now describe an ap- proximate inference algorithm that is tailored to our model, and is guaranteed to converge to a (strong) local maximum.      Our first observation is that the only variables that correlate the different protein pairs Tij are the motif variables P.A. Given an assignment to these activity variables, the net- work decomposes into a set of independent subnetworks, one for each protein pair. Based on this observation, we divide our computation of the E-step into two parts. In the first, we find an assignment to the motif variables in each protein, P.A; in the second, we com- pute the posterior probability over the binding motif pair variables T.B, T.S, given the assignment to the motif variables.      We begin by describing the second phase. We observe that, as all the motif pair vari- ables, T.A, are fully determined by the motif variables, the only variables left to reason about are the binding variables T.B and T.S. The variables for any pair Tij are inde- pendent of the rest of the model given the instantiation to T.A and the interaction evi- dence. That fact, combined with the noisy-OR form of the interaction, allows us to com- pute the posterior probability required in the E-step exactly and efficiently. Specifically, the computation for the variables associated with a particular protein pair Tij is as fol- lows, where we omit the common prefix Tij to simplify notation. If Aab = false, then
P (Bab = true | Aab = false, O, ) = 0. Otherwise, if Aab = true, then
                                                               P (B           P (B                                                             ab | A, )P (O | Bab = true, A, )                   ab = true | A, O, )                    =                                                                          .                                                                                        P (O | A, )

The first term in the numerator is simply the motif affinity ab; the second term is 1 if O = true and 0 otherwise. The numerator can easily be computed as P (O | A, ) = 1 - (1 - S)                               (1 -                           A                           ab). The computation for P (S) is very similar.                               a,b =true
We now turn to the first phase, of finding a setting to all of the motif variables. Un- fortunately, as we discussed, the model is highly interconnected, and a finding an optimal joint setting to all of these variables P.A is intractable. We thus approximate finding this joint assignment using a method that exploits our specific structure. Our method iterates over proteins, finding in each iteration the optimal assignment to the motif variables of each protein given the current assignment to the motif activities in the remaining proteins. The process repeats, iterating over proteins, until convergence.    As we discussed, the likelihood of each assignment to Pi.A can be easily computed using the method described above. However, the computation for each protein is still ex- ponential in the number of motifs it contains, which can be large (e.g., 15). However, in our specific model, we can apply the following branch-and-bound algorithm (similar to an approach proposed by Henrion [17] for BN2O networks) to find the globally optimal as- signment to the motif variables of each protein. The idea is that we search over the space of possible assignments Pi.A for one that maximizes the objective we wish to maximize. We can show that if making a motif active relative to one assignment does not improve the objective, it will also not improve the objective relative to a large set of other assignments.    More precisely, let f (Pi.A) = P (Pi.A, P-i.A|O, ) denote the objective we wish to maximize, where P-i.A is the fixed assignment to motif variables in all proteins except Pi. Let Pi.A-a denote the assignment to all the motif variables in Pi except for Aa. We compute the ratio of f after we switch Pi.Aa from false to true. Let ha(Pj) =                                (1 -                      P                               ab) denote the probability that motif a does not bind with                          j .Ab =true any active motif in Pj. We can now compute:
                                f (P                                               g                                            i.Aa = true, Pi.A-a)           a(Pi.A-a) =                                                          =                                     f (Pi.Aa = false, Pi.A-a)                        1 - g                                                                    1 - (1 - S)ha(Pj)                                        hb(Pj)                                   h                                                             a=b,Pi.Ab=true                                         a(Pj )                                                                                           (1)                                                                       1 - (1 - S)                                    h                                                                                               a=b,P                        b(Pj )                    1jn                              1jn                                            i .Ab =true                  Tij .O=false                       Tij .O=true

where g is the prior probability for a motif in protein Pi to be active.    Now, consider a different point in the search, where our current motif activity assign- ment is Pi.A-a, which has all the active motifs in Pi.A-a and some additional ones. The first two terms in the product of Eq. (1) are the same for a(Pi.A-a) and a(Pi.A-a). For the final term (the large fraction), one can show using some algebraic manipulation that this term in a(Pi.A-a) is lower than that for a(Pi.A-a). We conclude that a(Pi.A-a)  a(Pi.A-a), and hence that:
        f (Pi.Aa = true, Pi.A-a)                                         f (P                                                                 1                  i.Aa = true, Pi.A-a)  1.             f (Pi.Aa = false, Pi.A-a)                                        f (Pi.Aa = false, Pi.A- )                                                                                                                 a

It follows that, if switching motif a from inactive to active relative to Pi.A decreases f , it will also decrease f if we have some additional active motifs.    We can exploit this property in a branch-and-bound algorithm in order to find the glob- ally optimal assignment Pi.A. Our algorithm keeps a set V of viable candidates for motif assignments. For presentation, we encode assignments via the set of active motifs they contain. Initially, V contains only the empty assignment {}. We start out by considering
motif assignments with a single active motif. We put such an assignment {a} in V if its f -score is higher than f ({}). Now, we consider assignments {a, b} that have two active motifs. We consider {a, b} only if both {a} and {b} are in V . If so, we evaluate its f -score, and add it to V if this score is greater than that of {a} and {b}. Otherwise, we throw it away. We continue this process for all assignments of size k: For each assignment with active motif set S, we test whether S - {a}  V for all a  S; if we compare f (S) to each f (S - {a}), and add it if it dominates all of them. The algorithm terminates when, from some k, no assignment of size k is saved.      To understand the intuition behind this pruning procedure, consider a candidate assign- ment {a, b, c, d}, and assume that {a, b, c}  V , but {b, c, d}  V . In this case, we must have that {b, c}  V , but adding d to that assignment reduces the f -score. In this case, as shown by our analysis, adding d to the superset {a, b, c} would also reduce the f -score.      This algorithm is still exponential in worst case. However, in our setting, a protein with many motifs has a low prior probability that each of them is active. Hence, adding new motifs is less likely to increase the f -score, and the algorithm tends to terminate quickly. As we show in Section 4, this algorithm significantly reduces the cost of our procedure.      Our E-step finds an assignment to P.A which is a strong local optimum of the ob- jective function max P (P.A | T.O, ): The assignment has higher probability than any assignment that changes any of the motif variables for any single protein. For that assign- ment, our algorithm also computes the distribution over all of the binding variables, as described above. Using this completion, we can now easily compute the (expected) suffi- cient statistics for the different parameters in the model. As each of these parameters is a simple binomial distribution, the maximum likelihood estimation in the M-step is entirely standard; we omit details."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/dcda54e29207294d8e7e1b537338b1c0-Abstract.html,Proximity Graphs for Clustering and Manifold Learning,"Richard S. Zemel, Miguel Á. Carreira-Perpiñán","Many machine learning algorithms for clustering or dimensionality re- duction take as input a cloud of points in Euclidean space, and construct a graph with the input data points as vertices. This graph is then parti- tioned (clustering) or used to redeﬁne metric information (dimensional- ity reduction). There has been much recent work on new methods for graph-based clustering and dimensionality reduction, but not much on constructing the graph itself. Graphs typically used include the fully- connected graph, a local ﬁxed-grid graph (for image segmentation) or a nearest-neighbor graph. We suggest that the graph should adapt locally to the structure of the data. This can be achieved by a graph ensemble that combines multiple minimum spanning trees, each ﬁt to a perturbed version of the data set. We show that such a graph ensemble usually pro- duces a better representation of the data manifold than standard methods; and that it provides robustness to a subsequent clustering or dimension- ality reduction algorithm based on the graph."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/dd17e652cd2a08fdb8bf7f68e2ad3814-Abstract.html,Fast Rates to Bayes for Kernel Machines,"Ingo Steinwart, Clint Scovel","We establish learning rates to the Bayes risk for support vector machines          (SVMs) with hinge loss. In particular, for SVMs with Gaussian RBF          kernels we propose a geometric condition for distributions which can be          used to determine approximation properties of these kernels. Finally, we          compare our methods with a recent paper of G. Blanchard et al.."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/dda04f9d634145a9c68d5dfe53b21272-Abstract.html,Discriminant Saliency for Visual Recognition from Cluttered Scenes,"Dashan Gao, Nuno Vasconcelos","Saliency mechanisms play an important role when visual recognition          must be performed in cluttered scenes. We propose a computational defi-          nition of saliency that deviates from existing models by equating saliency          to discrimination. In particular, the salient attributes of a given visual          class are defined as the features that enable best discrimination between          that class and all other classes of recognition interest. It is shown that          this definition leads to saliency algorithms of low complexity, that are          scalable to large recognition problems, and is compatible with existing          models of early biological vision. Experimental results demonstrating          success in the context of challenging recognition problems are also pre-          sented."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/de594ef5c314372edec29b93cab9d72e-Abstract.html,Bayesian Regularization and Nonnegative Deconvolution for Time Delay Estimation,"Yuanqing Lin, Daniel D. Lee",Bayesian Regularization and Nonnegative Deconvolution (BRAND) is          proposed for estimating time delays of acoustic signals in reverberant          environments. Sparsity of the nonnegative filter coefficients is enforced          using an L1-norm regularization. A probabilistic generative model is          used to simultaneously estimate the regularization parameters and filter          coefficients from the signal data. Iterative update rules are derived under          a Bayesian framework using the Expectation-Maximization procedure.          The resulting time delay estimation algorithm is demonstrated on noisy          acoustic data.
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/df0e09d6f25a15a815563df9827f48fa-Abstract.html,Algebraic Set Kernels with Application to Inference Over Local Image Representations,"Amnon Shashua, Tamir Hazan","This paper presents a general family of algebraic positive definite simi-           larity functions over spaces of matrices with varying column rank. The           columns can represent local regions in an image (whereby images have           varying number of local parts), images of an image sequence, motion tra-           jectories in a multibody motion, and so forth. The family of set kernels           we derive is based on a group invariant tensor product lifting with param-           eters that can be naturally tuned to provide a cook-book of sorts covering           the possible ""wish lists"" from similarity measures over sets of varying           cardinality. We highlight the strengths of our approach by demonstrat-           ing the set kernels for visual recognition of pedestrians using local parts           representations.
1     Introduction
In the area of learning from observations there are two main paths that are often mutually exclusive: (i) the design of learning algorithms, and (ii) the design of data representations. The algorithm designers take pride in the fact that their algorithm can generalize well given straightforward data representations (most notable example is SVM [11]), whereas those who work on data representations demonstrate often remarkable results with sophisticated data representations using only straightforward learning algorithms (e.g. [5, 10, 6]). This dichotomy is probably most emphasized in the area of computer vision, where image under- standing from observations involve data instances of images or image sequences containing huge amounts of data. A straightforward representation treating all the measurements as a single vector, such as the raw pixel data, or a transformed raw-pixel data, places un- reasonable demands on the learning algorithm. The ""holistic"" representations suffer also from sensitivity to occlusions, invariance to local and global transformations, non-rigidity of local parts of the object, and so forth.
Practitioners in the area of data representations have long noticed that a collection of local representations (part-based representations) can be most effective to ameliorate changes of appearance [5, 10, 6]. The local data representations vary in their sophistication, but share the same principle where an image corresponds to a collection of points each in a relatively small dimensional space -- instead of a single point in high-dimensional space induced by holistic representations. In general, the number of points (local parts) per image may vary and the dimension of each point may vary as well. The local representations tend
 School of Engineering and Computer Science, Hebrew University of Jerusalem, Jerusalem 91904, Israel

to be robust against occlusions, local and global transformations and preserve the original resolution of the image (the higher the resolution the more parts are generated per image).
The key for unifying local and holistic representations for inference engines is to design positive definite similarity functions (a.k.a. kernels) over sets (of vectors) of varying cardi- nalities. A Support Vector Machine (SVM) [11] can then handle sets of vectors as a single instance via application of those ""set kernels"". A set kernel would be useful also to other types of inference engines such as kernel versions of PCA, LDA, CCA, ridge regression and any algorithm which can be mapped onto inner-products between pairs of data instances (see [8] for details on kernel methods).
Formally, we consider an instance being represented by a collection of vectors, which for the sake of convenience, form the columns of a matrix. We would like to find an algebraic family of similarity functions sim(A, B) over matrices A, B which satisfy the following requirements: (i) sim(A, B) is an inner product, i.e., sim(A, B) = (A) (B) for some mapping () from matrices to vectors, (ii) sim(A, B) is built over local kernel functions k(ai, bj) over columns ai and bj of A, B respectively, (iii) The column cardinality (rank of column space) of A and B need not be the same (number of local parts may differ from image to image), and (iv) the parameters of sim(A, B) should induce the properties of in- variance to order (alignement) of parts, part occlusions, and degree of interactions between local parts. In a nutshell, our work provides a cook-book of sorts which fundamentally covers the possible algebraic kernels over collections of local representations built on top of local kernels by combining (linearly and non-linearly) local kernels to form a family of global kernels over local representations.
The design of a kernel over sets of vectors has been recently attracting much attention in the computer vision and machine learning literature. A possible approach is to fit a distribution to the set of vectors and define the kernel as a distribution matching measure [9, 12, 4]. This has the advantage that the number of local parts can vary but at the expense of fitting a distribution to the variation over parts. The variation could be quite complex at times, unlikely to fit into a known family of distributions in many situations of interest, and in practice the sample size (number of columns of A) is not sufficiently large to reliably fit a distribution. The alternative, which is the approach taken in this paper, is to create a kernel over sets of vectors in a direct manner. When the column cardinality is equal it is possible to model the similarity measure as a function over the principal angles between the two column spaces ([14] and references therein) while for varying column cardinality only heuristic similarity measures (which are not positive definite) have so far been introduced [13].
It is important to note that although we chose SVM over local representations as the appli- cation to demonstrate the use of set kernels, the need for adequately working with instances made out of sets of various cardinalities spans many other application domains. For exam- ple, an image sequence may be represented by a set (ordered or unordered) of vectors, where each vector stands for an image, the pixels in an image can be represented as a tuple consisting of position, intensity and other attributes, motion trajectories of multiply mov- ing bodies can be represented as a collection of vectors, and so on. Therefore, the problem addressed in this paper is fundamental both theoretically and from a practical perspective as well.
2    The General Family of Inner-Products over Matrices
We wish to derive the general family of positive definite similarity measures sim(A, B) over matrices A, B which have the same number of rows but possibly different column rank (in particular, different number of columns). Let A be of dimensions n  k and B of dimension n  q where n is fixed and k, q can vary at will over the application of sim(, ) on pairs of matrices. Let m = max{n, k, q} be the upper bound over all values
of k, q encountered by the data. Let ai, bj be the column vectors of matrices A, B and let k(ai, bj) be the local kernel function. For example, in the context where the column vectors represent local parts of an image, then the matching function k(, ) between pairs of local parts provides the building blocks of the overall similarity function. The local kernel is some positive definite function k(x, y) = (x) (y) which is the inner-product between the ""feature""-mapped vectors x, y for some feature map (). For example, if () is the polynomial map of degree up to d, then k(x, y) = (1 + x y)d.
The local kernels can be combined in a linear or non-linear manner. When the combination is linear the similarity becomes the analogue of the inner-product between vectors extended to matrices. We will refer to the linear family as sim(A, B) =< A, B > and that will be the focus of this section. In the next section we will derive the general (algebraic) non- linear family which is based on ""lifting"" the input matrices A, B onto higher dimensional spaces and feeding the result onto the < ,  > machinery developed in this section, i.e., sim(A, B) =< (A), (B) >.
We will start by embedding A, B onto m  m matrices by zero padding as follows. Let ei denote the i'th standard basis vector (0, .., 0, 1, 0, .., 0) of Rm. The the embedding is represented by linear combinations of tensor products:
                       n     k                                            n         q

                A                 aijei  ej,                 B                         bltel  et.                            i=1 j=1                                            l=1 t=1

Note that A, B are the upper-left blocks of the zero-padded matrices. Let S be a positive semi definite m2  m2 matrix represented by S =                           p         G                                                                          r=1            r  Fr where Gr , Fr are m  m matrices1. Let ^                  Fr be the q  k upper-left sub-matrix of Fr , and let ^                                                                                                               Gr be the n  n upper-left sub-matrix of Gr. We will be using the following three identities:                                 Gx1  F x2 = (G  F )(x1  x2),                                 (G  F )(G  F ) = GG  F F ,
                            < x1  x2, y                      >= (                 )(              ).                                                        1     y2           x1 y1 x2 y2 The inner-product < A, B > over all p.s.d. matrices S has the form:

        < A, B >       =    <              aijei  ej, (            Gr  Fr)                        bltel  et >                                        i,j                         r                             l,t

                       =                      aijblt < ei  ej, Grel  Fret >                                   r    i,j,l,t

                       =                      aijblt(e G                       F                                                              i      r el)(ej            r et)                                   r    i,j,l,t

                       =                      aijblt(Gr)il(Fr)jt                                   r    i,j,l,t

                       =                    (A     ^                                                        GrB)jt(Fr)jt                                   r      lt


                       =    trace                  (A    ^                                                              GrB) ^                                                                         Fr                                                   r

We have represented the inner product < A, B > using the choice of m  m matrices Gr, Fr instead of the choice of a single m2  m2 p.s.d. matrix S. The matrices Gr, Fr
1Any S can be represented as a sum over tensor products: given column-wise ordering, the matrix G  F is composed of n  n blocks of the form fij G. Therefore, take Gr to be the n  n blocks of S and Fr to be the elemental matrices which have ""1"" in coordinate r = (i, j) and zero everywhere else.
must be selected such that      p       G                                 r=1          r  Fr is positive semi definite. The problem of decid- ing on the the necessary conditions on Fr and Gr such that the sum over tensor products is p.s.d is difficult. Even deciding whether a given S has a separable decomposition is known to be NP-hard [3]. The sufficient conditions are easy -- choosing Gr, Fr to be positive semi definite would make          p      G                                  r=1         r  Fr positive semi definite as well. In this context (of separable S) we need one more constraint in order to work with non-linear local ker- nels k(x, y) = (x) (y): the matrices ^                                                   G                     ~                                                        r =    ~                                                               M M                                                                    r         r must ""distribute with the kernel"", namely there exist Mr such that          k(M                                                                      ~                 r x, Mr y) = (Mr x)    (Mry) = (x)                   ~                                                                         M M                                                                              r         r (y) = (x)         ^                                                                                                              Gr(y).
To summarize the results so far, the most general, but seperable, analogue of the inner- product over vectors to the inner-product of matrices of varying column cardinality has the form:                                  < A, B >=                  trace(H ^                                                                              r Fr )                                          (1)                                                        r
Where the entries of Hr consists of k(Mrai, Mrbj) over the columns of A, B after possibly undergoing global coordinate changes by Mr (the role of ^                                                                                   Gr), and ^                                                                                                  Fr are the q  k upper- left sub-matrix of positive definite m  m matrices Fr .
The role of the matrices ^                             Gr is to perform global coordinate changes of Rn before applica- tion of the kernel k() on the columns of A, B. These global transformations include pro- jections (say onto prototypical ""parts"") that may be given or ""learned"" from a training set. The matrices ^                  Fr determine the range of interaction between columns of A and columns of B. For example, when ^                            Gr = I then < A, B >= trace(A B ^                                                                                          F ) where ^                                                                                                           F is the upper-left submatrix with the appropriate dimension of some fixed m  m p.s.d matrix F =                                                 F                                                                                                                         r         r . Note that entries of A B are k(ai, bj). In other words, when Gr = I, < A, B > boils down to a simple linear super-position of the local kernels,                                   k(a                                                                                          ij           i, bj )fij where the en- tries fij are part of the upper-left block of a fixed positive definite matrix F where the block dimensions are commensurate with the number of columns of A and those of B. The various choices of F determine the type of invariances one could obtain from the simi- larity measure. For example, when F = I the similarity is simply the sum (average) of the local kernels k(ai, bi) thereby assuming we have a strict alignment between the local parts represented by A and the local parts represented by B. On the other end of the in- variance spectrum, when F = 11                (all entries are ""1"") the similarity measure averages over all interactions of local parts k(ai, bj) thereby achieving an invariance to the order of the parts. A decaying weighted interaction such as fij = -|i-j| would provide a middle ground between the assumption of strict alignment and the assumption of complete lack of alignment. In the section below we will derive the non-linear version of sim(A, B) based on the basic machinery of < A, B > of eqn. (1) and lifting operations on A, B.
3    Lifting Matrices onto Higher Dimensions
The family of sim(A, B) =< A, B > forms a weighted linear superposition of the local kernel k(ai, bj).      Non-linear combinations of local kernels emerge using map- pings (A) from the input matrices onto other higher-dimensional matrices, thus forming sim(A, B) =< (A), (B) >. Additional invariance properties and parameters control- ling the perfromance of sim(A, B) emerge with the introduction of non-linear combina- tions of local kernels, and those will be discussed later on in this section.
Consider the general d-fold lifting (A) = Ad which can be viewed as a nd  kd matrix. Let Fr be a p.s.d. matrix of dimension md  md and ^                                                                         Fr be the upper-left qd  kd block of Fr. Let Gr = ( ^                     Gr)d be a p.s.d matrix of dimension nd  nd where ^                                                                                                          Gr is p.s.d. n  n matrix. Using the identity (Ad) Bd = (A B)d we obtain the inner-product in the
lifted space:                               < Ad, Bd >=                     trace (A             ^                                                                                     GrB)d ^                                                                                                  Fr .                                                            r
By taking linear combinations of < Al, Bl >, l = 1, ..., d, we get the general non- homogenous d-fold inner-product simd(A, B). A this point the formulation is general but somewhat unwieldy computational-wise. The key for computational simplification lay in the fact that choices of Fr determine not only local interactions (as in the linear case) but also group invariances. The group invariances are a result of applying symmetric operators on the tensor product space -- we will consider two of those operators here, known as the the d-fold alternating tensor Ad = A  ....  A and the d-fold symmetric tensor Ad = A  ...  A. These lifting operations introduce the determinant and permanent operations on submatrices of A         ^                     GrB, as described below.
The alternating tensor is a multilinear map of Rn, (A  ....  A)(x1  ...  xd) = Ax1  ...  Axd, where                                                1                          x1  ...  xd =                        sign()x                                                d!                                 (1)  ....  x(d),                                                      Sd
where Sd is the symmetric group over d letters and   Sd are the permutations of the group. If x1, ..., xn form a basis of Rn, then the n elements x                                         ...  x , where 1                                                                               d                   i1            id i1 < ... < id  n form a basis of the alternating d - f old tensor product of Rn, denoted as dRn. If A  Rnk is a linear map on Rn sending points to Rk, then Ad is a linear map on dRn sending x1  ...  xd to Ax1  ...  Axd, i.e., sending points in dRn to points in dRk. The matrix representation of Ad is called the ""d'th compound matrix"" Cd(A) whose (i1, ..., id|j1, ..., jd) entry has the value det(A[i1, ..., id : j1, ..., jd]) where the determinant is of the d  d block constructed by choosing the rows i1, ..., id and the columns j1, ..., jd of A. In other words, Cd(A) has n rows and k columns                                                                                                  d                   d (instead of nd  kd necessary for Ad) whose entries are equal to the d  d minors of A. When k = d, Ck(A) is a vector known as the Grasmanian of A, and when n = k = d then Cd(A) = det(A). Finally, the identity (Ad) Bd = (A B)d specializes to (Ad) Bd = (A B)d which translates to the identity Cd(A) Cd(B) = Cd(A B) known as the Binet-Cauchy theorem [1]. Taken together, the ""d-fold alternating kernel"" d(A, B) is defined by:
d(A, B) =< Ad, Bd >=< Cd(A), Cd(B) >=                                                trace Cd(A ^                                                                                                               GrB) ^                                                                                                                           Fr , (2)                                                                                      r
where ^        Fr is the q  k upper-left submatrix of the p.s.d m  m matrix F                     d           d                                                           d            d                 r . Note that the local kernel plugs in as the entries of (A                     ^                                                                         GrB)ij = k(Mrai, Mrbj) where ^                                                                                                                              Gr = M M   r    r .
Another symmetric operator on the tensor product space is via the d-fold symmetric tensor space SymdRn whose points are:                                                      1                                 x1    xd =                      x                                                      d!            (1)  ....  x(d).                                                            Sd
The analogue of Cd(A) is the ""d'th power matrix"" Rd(A) whose (i1, ..., id|j1, ..., jd) entry has the value perm(A[i1, ..., id : j1, ..., jd]) and which stands for the map Ad
                                 (A    A)(x1    xd) = Ax1    Axd.

In other words, Rd(A) has n+d-1 rows and k+d-1 columns whose entries are equal to                                           d                             d the dd permanents of A. The analogue of the Binet-Cauchy theorem is Rd(A) Rd(B) =
Rd(A B). The ensuing kernel similarity function, referred to as the ""d-fold symmetric kernel"" is:
Symd(A, B) =< Ad, Bd >=< Rd(A), Rd(B) >=                                     trace Rd(A ^                                                                                               GrB) ^                                                                                                     Fr     (3)                                                                             r
where ^             Fr is the q+d-1  k+d-1 upper-left submatrix of the positive definite m+d-1                                 d            d                                                         d  n+d-1            matrix F        d                      r . Due to lack of space we will stop here and spend the remainder of this section in describing in laymen terms what are the properties of these similarity measures, how they can be constructed in practice and in a computationally efficient manner (despite the combinatorial element in their definition).
3.1         Practical Considerations
To recap, the family of similarity functions sim(A, B) comprise of the linear version < A, B > (eqn. 1) and non-linear versions l(A, B), Syml(A, B) (eqns. 2,3) which are group projections of the general kernel < Ad, Bd >. These different similarity func- tions are controlled by the choice of three items: Gr, Fr and the parameter d representing the degree of the tensor product operator. Specifically, we will focus on the case Gr = I and on d(A, B) as a representative of the non-linear family. The role of ^                                                                                               Gr is fairly in- teresting as it can be viewed as a projection operator from ""parts"" to prototypical parts that can be learned from a training set but we leave this to the full length article that will appear later.
Practically, to compute d(A, B) one needs to run over all d  d blocks of the k  q ma- trix A B (whose entries are k(ai, bj)) and for each block compute the determinant. The similarity function is a weighted sum of all those determinants weighted by fij. By appro- priate selection of F one can control both the complexity (avoid running over all possible d  d blocks) of the computation and the degree of interaction between the determinants. These determinants have an interesting geometric interpretation if those are computed over unitary matrices -- as described next.
Let A = QARA and B = QBRB be the QR factorization of the matrices, i.e., QA has orthonormal columns which span the column space of A, then it has been recently shown [14] that R-1 can be computed from A using only operations over k(a                    A                                                                       i, aj ). Therefore, the product Q Q                                               A BR-1, can be computed using only local                         A    B , which is equal to R-T                                                          A             B kernel applications. In other words, for each A compute R-1 (can be done using only                                                                                  A inner-products over columns of A), then when it comes to compute A B compute in- stead R-T A BR-1 which is equivalent to computing Q Q              A                B                                             A    B . Thus effectively we have replaced every A with QA (unitary matrix).
Now, d(QA, QB) for unitary matrices is the sum over the product of the cosine principal angles between d-dim subspaces spanned by columns of A and B. The value of each determinant of the d  d blocks of Q Q                                                     A    B is equal to the product of the cosine principal angles between the respective d-dim subspaces determined by corresponding selection of d columns from A and d columns from B. For example, the case k = q = d produces d(QA, QB) = det(Q Q                                                                                       Q                                     A    B ) which is the product of the eigenvalues of the matrix QA      B . Those eigenvalues are the cosine of the principal angles between the column space of A and the column space of B [2]. Therefore, det(Q Q                                                                   A    B ) measures the ""angle"" between the two subspaces spanned by the respective columns of the input matrices -- in particular is invariant to the order of the columns. For smaller values of d we obtain the sum over such products between subspaces spanned by subsets of d columns between A and B.
The advantage of smaller values of d is two fold: first it enables to compute the similarity when k = q and second breaks down the similarity between subspaces into smaller pieces. The entries of the matrix F determine which subspaces are being considered and the inter- action between subspaces in A and B. A diagonal F compares corresponding subspaces
     (a)                                                    (b)

Figure 1: (a) The configuration of the nine sub-regions is displayed over the gradient image. (b) some of the positive examples -- note the large variation in appearance, pose and articulation.
between A and B whereas off-diagonal entries would enable comparisons between differ- ent choices of subspaces in A and in B. For example, we may want to consider choices of d columns arranged in a ""sliding"" fashion, i.e., column sets {1, .., d}, {2, ..., d + 1}, ... and so forth, instead of the combinatorial number of all possible choices. This selection is associated with a sparse diagonal F where the non-vanishing entries along the diagonal have the value of ""1"" and correspond to the sliding window selections.
To conclude, in the linear version < A, B > the role of F is to determine the range of interaction between columns of A and columns of B, whereas with the non-linear version it is the interaction between d-dim subspaces rather than individual columns. We could select all possible interactions (exponential number) or any reduced interaction set such as the sliding window rule (linear number of choices) as described above."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/e02e27e04fdff967ba7d76fb24b8069d-Abstract.html,The Correlated Correspondence Algorithm for Unsupervised Registration of Nonrigid Surfaces,"Dragomir Anguelov, Praveen Srinivasan, Hoi-cheung Pang, Daphne Koller, Sebastian Thrun, James Davis","We present an unsupervised algorithm for registering 3D surface scans of            an object undergoing significant deformations. Our algorithm does not            need markers, nor does it assume prior knowledge about object shape, the            dynamics of its deformation, or scan alignment. The algorithm registers            two meshes by optimizing a joint probabilistic model over all point-to-            point correspondences between them. This model enforces preservation            of local mesh geometry, as well as more global constraints that capture            the preservation of geodesic distance between corresponding point pairs.            The algorithm applies even when one of the meshes is an incomplete            range scan; thus, it can be used to automatically fill in the remaining sur-            faces for this partial scan, even if those surfaces were previously only            seen in a different configuration. We evaluate the algorithm on several            real-world datasets, where we demonstrate good results in the presence            of significant movement of articulated parts and non-rigid surface defor-            mation. Finally, we show that the output of the algorithm can be used for            compelling computer graphics tasks such as interpolation between two            scans of a non-rigid object and automatic recovery of articulated object            models.
1     Introduction
The construction of 3D object models is a key task for many graphics applications. It is becoming increasingly common to acquire these models from a range scan of a physical object. This paper deals with an important subproblem of this acquisition task -- the problem of registering two deforming surfaces corresponding to different configurations of the same non-rigid object.      The main difficulty in the 3D registration problem is determining the correspondences of points on one surface to points on the other. Local regions on the surface are rarely distinc- tive enough to determine the correct correspondence, whether because of noise in the scans, or because of symmetries in the object shape. Thus, the set of candidate correspondences to a given point is usually large. Determining the correspondence for all object points results in a combinatorially large search problem. The existing algorithms for deformable surface
  A results video is available at http://robotics.stanford.edu/drago/cc/video.mp4

Figure 1: A) Registration results for two meshes. Nonrigid ICP and its variant augmented with spin images get stuck in local maxima. Our CC algorithm produces a largely correct registration, although with an artifact in the right shoulder (inset). B) Illustration of the link deformation process C) The CC algorithm which uses only deformation potentials can violate mesh geometry. Near regions can map to far ones (segment AB) and far regions can map to near ones (points C,D).
registration make the problem tractable by assuming significant prior knowledge about the objects being registered. Some rely on the presence of markers on the object [1, 20], while others assume prior knowledge about the object dynamics [16], or about the space of non- rigid deformations [15, 5]. Algorithms that make neither restriction [18, 12] simplify the problem by decorrelating the choice of correspondences for the different points in the scan. However, this approximation is only good in the case when the object deformation is small; otherwise, it results in poor local maxima as nearby points in one scan are allowed to map to far-away points in the other.   Our algorithm defines a joint probabilistic model over all correspondences, which ex- plicitly model the correlations between them -- specifically, that nearby points in one mesh should map to nearby points in the other. Importantly, the notion of ""nearby"" used in our model is defined in terms of geodesic distance over the mesh. We define a probabilistic model over the set of correspondences, that encodes these geodesic distance constraints as well as penalties for link twisting and stretching, and high-level local surface features [14]. We then apply loopy belief propagation [21] to this model, in order to solve for the entire set of correspondences simultaneously. The result is a registration that respects the surface geometry. To the best of our knowledge, the algorithm we present in this paper is the first algorithm which allows the registration of 3D surfaces of an object where the object config- urations can vary significantly, there is no prior knowledge about object shape or dynamics of deformation, and nothing whatsoever is known about the object alignment. Moreover, unlike many methods, our algorithm can be used to register a partial scan to a complete model, greatly increasing its applicability.   We apply our approach to three datasets containing 3D scans of a wooden puppet, a human arm and entire human bodies in different configurations. We demonstrate good registration results for scan pairs exhibiting articulated motion, non-rigid deformations, or both. We also describe three applications of our method. In our first application, we show how a partial scan of an object can be registered onto a fully specified model in a dif- ferent configuration. The resulting registration allows us to use the model to ""complete"" the partial scan in a way that preserves the local surface geometry. In the second, we use the correspondences found by our algorithm to smoothly interpolate between two different poses of an object. In our final application, we use a set of registered scans of the same object in different positions to recover a decomposition of the object into approximately rigid parts, and recover an articulated skeleton linking the parts. All of these applications are done in an unsupervised way, using only the output of our Correlated Correspondence algorithm applied to pairs of poses with widely varying deformations, and unknown initial alignments. These results demonstrate the value of a high-quality solution to the registra- tion problem to a range of graphics tasks.
2      Previous Work
Surface registration is a fundamental building block in computer graphics. The classical so- lution for registering rigid surfaces is the Iterative Closest Point algorithm (ICP) [4, 6, 17]. Recently, there has been work extending ICP to non-rigid surfaces [18, 8, 12, 1]. These algorithms treat one of the scans (usually a complete model of the surface) as a deformable template. The links between adjacent points on the surface can be thought of as springs, which are allowed to deform at a cost. Similarly to ICP, these algorithms iterate between two subproblems -- estimating the non-rigid transformation  and estimating the set of correspondences C between the scans. The step estimating the correspondences assumes that a good estimate of the nonrigid transformation  is available. Under this assumption, the assignments to the correspondence variables become decorrelated: each point in the second scan is associated with the nearest point (in the Euclidean distance sense) in the deformed template scan. However, the decomposition also induces the algorithm's main limitation. By assigning points in the second scan to points on the deformed model inde- pendently, nearby points in the scan can get associated to remote points in the model if the estimate of  is poor (Fig. 1A). While several approaches have been proposed to address this problem of incorrect correspondences, their applicability is largely limited to problems where the deformation is local, and the initial alignment is approximately correct.      Another line of related work is the work on deformable template matching in the com- puter vision community. In the 3D case, this framework is used for detection of articulated object models in images [13, 22, 19]. The algorithms assume the decomposition of the object into a relatively small number of parts is known, and that a detector for each object part is available. Template matching approaches have also been applied to deformable 2D objects, where very efficient solutions exist [9, 11]. However, these methods do not extend easily to the case of 3D surfaces.
3      The Correlated Correspondence Algorithm
The input to the algorithm is a set of two meshes (surfaces tessellated into polygons). The model mesh X = (V X , EX ) is a complete model of the object, in a particular pose. V X = (x1, . . . , xN ) denotes the mesh points, while EX is the set of links between adjacent points on the mesh surface. The data mesh Z = (V Z , EZ ) is either a complete model or a partial view of the object in a different configuration. Each data mesh point zk is associated with a correspondence variable ck, specifying the corresponding model mesh point. The task of registration is one of estimating the set of all correspondences C and a non-rigid transformation  which aligns the corresponding points.
3.1    Probabilistic Model
We formulate the registration problem as one of finding an embedding of the data mesh Z into the model mesh X, which is encoded as an assignment to all correspondence vari- ables C = (c1, . . . , cK ). The main idea behind our approach is to preserve the consis- tency of the embedding by explicitly correlating the assignments to the correspondence variables. We define a joint distribution over the correspondence variables c1, . . . , cK , rep- resented as a Markov network. For each pair of adjacent data mesh points zk, zl, we want to define a probabilistic potential (ck, cl) that constrains this pair of correspondences to reasonable and consistent. This gives rise to a joint probability distribution of the form p(C) = 1             (c                  (c            Z    k           k )    k,l           k , cl) which contains only single and pairwise potentials. Performing probabilistic inference to find the most likely joint assignment to the entire set of correspondence variables C should yield a good and consistent registration.
Deformation Potentials.                   We want our model to encode a preference for embeddings of mesh Z into mesh X, which minimize the amount of deformation  induced by the embedding. In order to quantify the amount of deformation , applied to the model, we
will follow the ideas of Hahnel et al. [12] and treat the links in the set EX as springs, which resist stretching and twisting at their endpoints. Stretching is easily quantified by looking at changes in the link length induced by the transformation . Link twisting, however, is ill- specified by looking only at the Cartesian coordinates of the points alone. Following [12], we attach an imaginary local coordinate system to each point on the model. This local coordinate system allows us to quantify the ""twist"" of a point xj relative to a neighbor xi. A non-rigid transformation  defines, for each point xi, a translation of its coordinates and a rotation of its local coordinate system.   To evaluate the deformation penalty, we parameterize each link in the model in terms of its length and its direction relative to its endpoints (see Fig. 1B). Specifically, we define li,j to be the distance between xi and xj; dij is a unit vector denoting the direction of the point xj in the coordinate system of xi (and vice versa). We use ei,j to denote the set of edge parameters (li,j, dij, dji). It is now straightforward to specify the penalty for model deformations. Let  be a transformation, and let ~                                                                       ei,j denote the triple of parameters associated with the link between xi and xj after applying . Our model penalizes twisting and stretching, using a separate zero-mean Gaussian noise model for each:
                P (~                       ei,j | ei,j) = P (~li,j | li,j) P ( ~                                                         dij | dij) P ( ~                                                                              dji | dji)                (1)

In the absence of prior information, we assume that all links are equally likely to deform.   In order to quantify the deformation induced by an embedding C, we need to include a potential d(ck, cl) for each link eZ  EZ . Every probability                                                 k,l                                    d(ck = i, cl = j) corresponds to the deformation penalty incurred by deforming model link ei,j to generate link eZ and is defined in (1). We do not restrict ourselves to the set of links in EX , since       k,l the original mesh tessellation is sparse and local. Any two points in X are allowed to implicitly define a link.   Unfortunately, we cannot directly estimate the quantity P (eZ | e                                                                              k,l    i,j ), since the link pa- rameters eZ depend on knowing the nonrigid transformation, which is not given as part              k,l of the input. The key issue is estimating the (unknown) relative rotation of the link end- points. In effect, this rotation is an additional latent variable, which must also be part of the probabilistic model. To remain within the realm of discrete Markov networks, allowing the application of standard probabilistic inference algorithms, we discretize the space of the possible rotations, and fold it into the domains of the correspondence variables. For each possible value of the correspondence variable ck = i we select a small set of candidate rotations, consistent with local geometry. We do this by aligning local patches around the points xi and zk using rigid ICP. We extend the domain of each correspondence variables ck, where each value encodes a matching point and a particular rotation from the precom- puted set for that point. Now the edge parameters eZ are fully determined and so is the                                                                k,l probabilistic potential.
Geodesic Distances.           Our proposed approach raises the question as to what constitutes the best constraint between neighboring correspondence variables. The literature on scan registration -- for rigid and non-rigid models alike -- relies on the preserving Euclidean distance. While Euclidean distance is meaningful for rigid objects, it is very sensitive to de- formations, especially those induced by moving parts. For example, in Fig. 1C, we see that the two legs in one configuration of our puppet are fairly close together, allowing the algo- rithm to map two adjacent points in the data mesh to the two separate legs, with minimal deformation penalty. In the complementary situation, especially when object symmetries are present, two distant yet similar points in one scan might get mapped to the same region in the other. For example, in the same figure, we see that points in both an arm and a leg in the data mesh get mapped to a single leg in the model mesh.   We therefore want to enforce constraints preserving distance along the mesh surface (geodesic distance). Our probabilistic framework easily incorporate such constraints as correlations between pairs of correspondence variables. We encode a nearness preservation
Figure 2: A) Automatic interpolation between two scans of an arm and a wooden puppet. B) Regis- tration results on two scans of the same man sitting and standing up (select points were displayed) C) Registration results on scans of a larger man and a smaller woman. The algorithm is robust to small changes in object scale.
constraint which prevents adjacent points in mesh Z to be mapped to distant points in X in the geodesic distance sense. For adjacent points zk, zl in the data mesh, we define the following potential:
                                            0    dist                                                             Geodesic (xi, xj ) >                         n(ck = i, cl = j) =                                                        (2)                                                 1    otherwise

where  is the data mesh resolution and  is some constant, chosen to be 3.5.   The farness preservation potentials encode the complementary constraint. For every pair of points zk, zl whose geodesic distance is more than 5 on the data mesh, we have a potential:                                                 0    dist                                                             Geodesic(xi, xj ) <                          f (ck = i, cl = j) =                                                      (3)                                                 1    otherwise
where  is also a constant, chosen to be 2 in our implementation. The intuition behind this constraint is fairly clear: if zk, zl are far apart on the data mesh, then their corresponding points must be far apart on the model mesh.
Local Surface Signatures.            Finally, we encode a set of potentials that correspond to the preservation of local surface properties between the model mesh and data mesh. The use of local surface signatures is important, because it helps to guide the optimization in the exponential space of assignments. We use spin images [14] compressed with prin- cipal component analysis to produce a low-dimensional signature sx of the local surface geometry around a point x. When data and model points correspond, we expect their lo- cal signatures to be similar. We introduce a potential whose values s(ck) = i enforce a zero-mean Gaussian penalty for discrepancies between sx and s .                                                                      i      zk
3.2    Optimization
In the previous section, we defined a Markov network, which encodes a joint probability distribution over the correspondence variables as a product of single and pairwise poten- tials. Our goal is to find a joint assignment to these variables that maximizes this proba- bility. This problem is one of standard probabilistic inference over the Markov network. However, the Markov network is quite large, and contains a large number of loops, so that exact inference is computationally infeasible. We therefore apply an approximate inference method known as loopy belief propagation (LBP)[21], which has been shown to work in a wide variety of applications. Running LBP until convergence results in a set of probabilis- tic assignments to the different correspondence variables, which are locally consistent. We then simply extract the most likely assignment for each variable to obtain a correspondence.   One remaining complication arises from the form of our farness preservation constraints. In general, most pairs of points in the mesh are not close, so that the total number of such potentials grows as O(M 2), where M is the number of points in the data mesh. However, rather than introducing all these potentials into the Markov net from the start, we
introduce them as needed. First, we run LBP without any farness preservation potentials. If the solution violates a set of farness preservation constraints, we add it and rerun BP. In practice, this approach adds a very small number of such constraints.
4     Experimental Results
Basic Registration.        We applied our registration algorithm to three different datasets, containing meshes of a human arm, wooden puppet and the CAESAR dataset of whole human bodies [1], all acquired by a 3D range scanner. The meshes were not complete surfaces, but several techniques exist for filling the holes (e.g., [10]).      We ran the Correlated Correspondence algorithm using the same probabilistic model and the same parameters on all data sets. We use a coarse-to-fine strategy, using the result of a coarse sub-sampling of the mesh surface to constrain the correspondences at a finer-grained level. The resulting set of correspondences were used as markers to initialize the non-rigid ICP algorithm of Hahnel et al. [12].      The Correlated Correspondence algorithm successfully aligned all mesh pairs in our hu- man arm data set containing 7 arms. In the puppet data set we registered one of the meshes to the remaining 6 puppets. The algorithm correctly registered 4 out of 6 data meshes to the model mesh. In the two remaining cases, the algorithm produced a registration where the torso was flipped, so that the front was mapped to the back. This problem arises from am- biguities induced by the puppet symmetry, whose front and back are almost identical. Im- portantly, our probabilistic model assigns a higher likelihood score to the correct solution, so that the incorrect registration is a consequence of local maxima in the LBP algorithm.      This fact allows us to address the issue in an unsupervised way simply by running loopy BP several times, with different initialization. For details on the unsupervised initialization scheme we used, please refer to our technical report [2]. We ran the modified algorithm to register one puppet mesh to the remaining 6 meshes in the dataset, obtaining the correct registration in all cases. In particular, as shown in Fig. 1A, we successfully deal with the case on which the straightforward nonrigid ICP algorithm failed. The modified algorithm was applied to the CAESAR dataset and produced very good registration for challenging cases exhibiting both articulated motion and deformation (Fig. 2B), or exhibiting deforma- tion and a (small) change in object scale (Fig. 2C).      Overall, the algorithm performed robustly, producing a close-to-optimal registrations even for pairs of meshes that involve large deformations, articulated motion or both. The registration is accomplished in an unsupervised way, without any prior knowledge about object shape, dynamics, or alignment.
Partial view completion. The Correlated Correspondence algorithm allows us to register a data mesh containing only a partial scan of an object to a known complete surface model of the object, which serves as a template. We can then transform the template mesh to the partial scan, a process which leaves undisturbed the links that are not involved in the partial mesh. The result is a mesh that matches the data on the observed points, while completing the unknown portion of the surface using the template.      We take a partial mesh, which is missing the entire back part of the puppet in a particular pose. The resulting partial model is displayed in Fig. 3B-1; for comparison, the correct complete model in this configuration (which was not available to the algorithm), is shown in Fig. 3B-2. We register the partial mesh to models of the object in a different pose (Fig. 3B- 3), and compare the completions we obtain (Fig. 3B-4), to the ground truth represented in Fig. 3B-2. The result demonstrates a largely correct reconstruction of the complete surface geometry from the partial scan and the deformed template. We report additional shape completion results in [2].
Interpolation.      Current research [20] shows that if a nonrigid transformation  between the poses is available, believable animation can be produced by linear interpolation be-
Figure 3: A) The results produced by the CC algorithm were used for unsupervised recovery of articulated models. 15 puppet parts and 4 arm parts, as well as the articulated object skeletons, were recovered. B) Partial view completion results. The missing parts of the surface were estimated by registering the partial view to a complete model of the object in a different configuration.
tween the model mesh and the transformed model mesh. The interpolation is performed in the space of local link parameters (li,j, dij, dji), We demonstrate that transforma- tion estimates produced by our algorithm can be used to automatically generate believable animation sequences between fairly different poses, as shown in Fig. 2A.
Recovering Articulated Models.             Articulated object models have a number of appli- cations in animation and motion capture, and there has been work on recovering them automatically from 3D data [7, 3]. We show that our unsupervised registration capability can greatly assist articulated model recovery. In particular, the algorithm in [3] requires an estimate of the correspondences between a template mesh and the remaining meshes in the dataset. We supplied it with registration computed with the Correlated Correspondence algorithm. As a result we managed to recover in a completely unsupervised way all 15 rigid parts of the puppet, as well as the joints between them (Fig. 3A). We demonstrate successful articulation recovery even for objects which are not purely rigid, as is the case with the human arm (see Fig. 3A)."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/e0688d13958a19e087e123148555e4b4-Abstract.html,Maximum-Margin Matrix Factorization,"Nathan Srebro, Jason Rennie, Tommi S. Jaakkola","We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-deﬁnite program, and discuss generalization error bounds for them."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/e113bb92c69391dd39e2488f9f588382-Abstract.html,Real-Time Pitch Determination of One or More Voices by Nonnegative Matrix Factorization,"Fei Sha, Lawrence K. Saul","An auditory ""scene"", composed of overlapping acoustic sources, can be          viewed as a complex object whose constituent parts are the individual          sources. Pitch is known to be an important cue for auditory scene analy-          sis. In this paper, with the goal of building agents that operate in human          environments, we describe a real-time system to identify the presence of          one or more voices and compute their pitch. The signal processing in the          front end is based on instantaneous frequency estimation, a method for          tracking the partials of voiced speech, while the pattern-matching in the          back end is based on nonnegative matrix factorization, an unsupervised          algorithm for learning the parts of complex objects. While supporting a          framework to analyze complicated auditory scenes, our system maintains          real-time operability and state-of-the-art performance in clean speech."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/e354fd90b2d5c777bfec87a352a18976-Abstract.html,Efficient Out-of-Sample Extension of Dominant-Set Clusters,"Massimiliano Pavan, Marcello Pelillo","Dominant sets are a new graph-theoretic concept that has proven to          be relevant in pairwise data clustering problems, such as image seg-          mentation. They generalize the notion of a maximal clique to edge-          weighted graphs and have intriguing, non-trivial connections to continu-          ous quadratic optimization and spectral-based grouping. We address the          problem of grouping out-of-sample examples after the clustering process          has taken place. This may serve either to drastically reduce the compu-          tational burden associated to the processing of very large data sets, or          to efficiently deal with dynamic situations whereby data sets need to be          updated continually. We show that the very notion of a dominant set of-          fers a simple and efficient way of doing this. Numerical experiments on          various grouping problems show the effectiveness of the approach."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/e6acf4b0f69f6f6e60e9a815938aa1ff-Abstract.html,Instance-Specific Bayesian Model Averaging for Classification,"Shyam Visweswaran, Gregory F. Cooper","Classification algorithms typically induce population-wide models  that are trained to perform well on average on expected future  instances. We introduce a Bayesian framework for learning  instance-specific models from data that are optimized to predict  well for a particular instance. Based on this framework, we present  a  that performs  selective model averaging over a restricted class of Bayesian  networks. On experimental evaluation, this algorithm shows  superior performance over model selection. We intend to apply  such instance-specific algorithms to improve the performance of  patient-specific predictive models induced from medical data. 
instance-specific algorithm called ISA"
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/e727fa59ddefcefb5d39501167623132-Abstract.html,Hierarchical Eigensolver for Transition Matrices in Spectral Methods,"Chakra Chennubhotla, Allan D. Jepson","We show how to build hierarchical, reduced-rank representation for large stochastic matrices and use this representation to design an efﬁcient al- gorithm for computing the largest eigenvalues, and the corresponding eigenvectors. In particular, the eigen problem is ﬁrst solved at the coars- est level of the representation. The approximate eigen solution is then interpolated over successive levels of the hierarchy. A small number of power iterations are employed at each stage to correct the eigen solution. The typical speedups obtained by a Matlab implementation of our fast eigensolver over a standard sparse matrix eigensolver [13] are at least a factor of ten for large image sizes. The hierarchical representation has proven to be effective in a min-cut based segmentation algorithm that we proposed recently [8].
1 Spectral Methods Graph-theoretic spectral methods have gained popularity in a variety of application do- mains: segmenting images [22]; embedding in low-dimensional spaces [4, 5, 8]; and clus- tering parallel scientiﬁc computation tasks [19]. Spectral methods enable the study of prop- erties global to a dataset, using only local (pairwise) similarity or afﬁnity measurements be- tween the data points. The global properties that emerge are best understood in terms of a random walk formulation on the graph. For example, the graph can be partitioned into clus- ters by analyzing the perturbations to the stationary distribution of a Markovian relaxation process deﬁned in terms of the afﬁnity weights [17, 18, 24, 7]. The Markovian relaxation process need never be explicitly carried out; instead, it can be analytically expressed using the leading order eigenvectors, and eigenvalues, of the Markov transition matrix. In this paper we consider the practical application of spectral methods to large datasets. In particular, the eigen decomposition can be very expensive, on the order of O(n3), where n is the number of nodes in the graph. While it is possible to compute analytically the ﬁrst eigenvector (see x3 below), the remaining subspace of vectors (necessary for say clustering) has to be explicitly computed. A typical approach to dealing with this difﬁculty is to ﬁrst sparsify the links in the graph [22] and then apply an efﬁcient eigensolver [13, 23, 3]. In comparison, we propose in this paper a specialized eigensolver suitable for large stochas- tic matrices with known stationary distributions. In particular, we exploit the spectral prop- erties of the Markov transition matrix to generate hierarchical, successively lower-ranked approximations to the full transition matrix. The eigen problem is solved directly at the coarsest level of representation. The approximate eigen solution is then interpolated over successive levels of the hierarchy, using a small number of power iterations to correct the solution at each stage.
2 Previous Work One approach to speeding up the eigen decomposition is to use the fact that the columns of the afﬁnity matrix are typically correlated. The idea then is to pick a small number of representative columns to perform eigen decomposition via SVD. For example, in the Nystrom approximation procedure, originally proposed for integral eigenvalue problems, the idea is to randomly pick a small set of m columns; generate the corresponding afﬁnity matrix; solve the eigenproblem and ﬁnally extend the solution to the complete graph [9, 10]. The Nystrom method has also been recently applied in the kernel learning methods for fast Gaussian process classiﬁcation and regression [25]. Other sampling-based approaches include the work reported in [1, 2, 11]. Our starting point is the transition matrix generated from afﬁnity weights and we show how building a representational hierarchy follows naturally from considering the stochas- tic matrix. A closely related work is the paper by Lin on reduced rank approximations of transition matrices [14]. We differ in how we approximate the transition matrices, in par- ticular our objective function is computationally less expensive to solve. In particular, one of our goals in reducing transition matrices is to develop a fast, specialized eigen solver for spectral clustering. Fast eigensolving is also the goal in ACE [12], where successive levels in the hierarchy can potentially have negative afﬁnities. A graph coarsening process for clustering was also pursued in [21, 3]. 3 Markov Chain Terminology We ﬁrst provide a brief overview of the Markov chain terminology here (for more details see [17, 15, 6]). We consider an undirected graph G = (V; E) with vertices vi, for i = f1; : : : ; ng, and edges ei;j with non-negative weights ai;j. Here the weight ai;j represents the afﬁnity between vertices vi and vj. The afﬁnities are represented by a non-negative, symmetric n (cid:2) n matrix A having weights ai;j as elements. The degree of a node j is j=1 aj;i, where we deﬁne D = diag(d1; : : : ; dn). A Markov chain is deﬁned using these afﬁnities by setting a transition probability matrix M = AD(cid:0)1, where the columns of M each sum to 1. The transition probability matrix deﬁnes the random walk of a particle on the graph G. The random walk need never be explicitly carried out; instead, it can be analytically ex- pressed using the leading order eigenvectors, and eigenvalues, of the Markov transition matrix. Because the stochastic matrices need not be symmetric in general, a direct eigen decomposition step is not preferred for reasons of instability. This problem is easily circum- vented by considering a normalized afﬁnity matrix: L = D(cid:0)1=2AD(cid:0)1=2, which is related to the stochastic matrix by a similarity transformation: L = D(cid:0)1=2M D1=2. Because L is symmetric, it can be diagonalized: L = U (cid:3)U T , where U = [~u1; ~u2; (cid:1) (cid:1) (cid:1) ; ~un] is an orthogonal set of eigenvectors and (cid:3) is a diagonal matrix of eigenvalues [(cid:21)1; (cid:21)2; (cid:1) (cid:1) (cid:1) ; (cid:21)n] sorted in decreasing order. The eigenvectors have unit length k~ukk = 1 and from the form of A and D it can be shown that the eigenvalues (cid:21)i 2 ((cid:0)1; 1], with at least one eigenvalue equal to one. Without loss of generality, we take (cid:21)1 = 1. Because L and M are similar we can perform an eigen decomposition of the Markov transition matrix as: M = D1=2LD(cid:0)1=2 = D1=2U (cid:3) U T D(cid:0)1=2. Thus an eigenvector ~u of L corresponds to an eigenvector D1=2~u of M with the same eigenvalue (cid:21). The Markovian relaxation process after (cid:12) iterations, namely M (cid:12), can be represented as: M (cid:12) = D1=2U (cid:3)(cid:12)U T D(cid:0)1=2. Therefore, a particle undertaking a random walk with an initial distribution ~p 0 acquires after (cid:12) steps a distribution ~p (cid:12) given by: ~p (cid:12) = M (cid:12)~p 0. Assuming the graph is connected, as (cid:12) ! 1, the Markov chain approaches a unique i=1 di, and thus, M 1 = ~(cid:25)1T , where 1 is a n-dim column vector of all ones. Observe that ~(cid:25) is an eigenvector of M as it is easy to show that M~(cid:25) = ~(cid:25) and the corresponding eigenvalue is 1. Next, we show how to generate hierarchical, successively low-ranked approximations for the transition matrix M.
stationary distribution given by ~(cid:25) = diag(D)=Pn
deﬁned to be: dj = Pn"
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/e97c864e8ac67f7aed5ce53ec28638f5-Abstract.html,Exponentiated Gradient Algorithms for Large-margin Structured Classification,"Peter L. Bartlett, Michael Collins, Ben Taskar, David A. McAllester","We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal struc- ture. Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expecta- tions under Gibbs distributions can be calculated efﬁciently. The method for structured labels relies on a more general result, speciﬁcally the ap- plication of exponentiated gradient updates [7, 8] to quadratic programs."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/eaa52f3366768bca401dca9ea5b181dd-Abstract.html,A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning,"Saharon Rosset, Ji Zhu, Hui Zou, Trevor J. Hastie","We consider the situation in semi-supervised learning, where the ""label            sampling"" mechanism stochastically depends on the true response (as            well as potentially on the features). We suggest a method of moments            for estimating this stochastic dependence using the unlabeled data. This            is potentially useful for two distinct purposes: a. As an input to a super-            vised learning procedure which can be used to ""de-bias"" its results using            labeled data only and b. As a potentially interesting learning task in it-            self. We present several examples to illustrate the practical usefulness of            our method.
1     Introduction
In semi-supervised learning, we assume we have a sample (xi, yi, si)n                                                                                 i=1, of i.i.d. draws from a joint distribution on (X, Y, S), where:1
     xi  Rp are p-vectors of features.          yi is a label, or response (yi  R for regression, yi  {0, 1} for 2-class classifica-            tion).

     si  {0, 1} is a ""labeling indicator"", that is yi is observed if and only if si = 1,            while xi is observed for all i.

In this paper we consider the interesting case of semi-supervised learning, where the prob- ability of observing the response depends on the data through the true response, as well as
 1Our notation here differs somewhat from many semi-supervised learning papers, where the un- labeled part of the sample is separated from the labeled part and sometimes called ""test set"".

potentially through the features. Our goal is to model this unknown dependence:
                                l(x, y) = P r(S = 1|x, y)                                      (1)

Note that the dependence on y (which is unobserved when S = 0) prevents us from using standard supervised modeling approaches to learn l. We show here that we can use the whole data-set (labeled+unlabeled data) to obtain estimates of this probability distribution within a parametric family of distributions, without needing to ""impute"" the unobserved responses.2
We believe this setup is of significant practical interest. Here are a couple of examples of realistic situations: 1. The problem of learning from positive examples and unlabeled data is of significant interest in document topic learning [4, 6, 8]. Consider a generalization of that problem, where we observe a sample of positive and negative examples and unlabeled data, but we believe that the positive and negative labels are supplied with different probabilities (in the document learning example, positive examples are typically more likely to be labeled than negative ones, which are much more abundant). These probabilities may also not be uniform within each class, and depend on the features as well. Our methods allow us to infer these labeling probabilities by utilizing the unlabeled data. 2. Consider a satisfaction survey, where clients of a company are requested to report their level of satisfaction, but they can choose whether or not they do so. It is reasonable to assume that their willingness to report their satisfaction depends on their actual satisfaction level. Using our methods, we can infer the dependence of the reporting probability on the actual satisfaction by utilizing the unlabeled data, i.e., the customers who declined to respond.
Being able to infer the labeling mechanism is important for two distinct reasons. First, it may be useful for ""de-biasing"" the results of supervised learning, which uses only the labeled examples. The generic approach for achieving this is to use ""inverse sampling"" weights (i.e. weigh labeled examples by 1/l(x, y)). The us of this for maximum likeli- hood estimation is well established in the literature as a method for correcting sampling bias (of which semi-supervised learning is an example) [10]. We can also use the learned mechanism to post-adjust the probabilities from a probability estimation methods such as logistic regression to attain ""unbiasedness"" and consistency [11]. Second, understanding the labeling mechanism may be an interesting and useful learning task in itself. Consider, for example, the ""satisfaction survey"" scenario described above. Understanding the way in which satisfaction affects the customers' willingness to respond to the survey can be used to get a better picture of overall satisfaction and to design better future surveys, regardless of any supervised learning task which models the actual satisfaction.
Our approach is described in section 2, and is based on a method of moments. Observe that for every function of the features g(x), we can get an unbiased estimate of its mean         n as 1           g(x    n    i=1      i). We show that if we know the underlying label sampling mechanism l(x, y) we can get a different unbiased estimate of Eg(x), which uses only the labeled examples, weighted by 1/l(x, y). We suggest inferring the unknown function l(x, y) by requiring that we get identical estimates of Eg(x) using both approaches. We illustrate our method's implementation on the California Housing data-set in section 3. In section 4 we review related work in the machine learning and statistics literature, and we conclude with a discussion in section 5.
2The importance of this is that we are required to hypothesize and fit a conditional probability model for l(x, y) only, as opposed to the full probability model for (S, X, Y ) required for, say, EM."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/eb06b9db06012a7a4179b8f3cb5384d3-Abstract.html,Semi-Markov Conditional Random Fields for Information Extraction,"Sunita Sarawagi, William W. Cohen","We describe semi-Markov conditional random ﬁelds (semi-CRFs), a con- ditionally trained version of semi-Markov chains. Intuitively, a semi- CRF on an input sequence x outputs a “segmentation” of x, in which labels are assigned to segments (i.e., subsequences) of x rather than to individual elements xi of x. Importantly, features for semi-CRFs can measure properties of segments, and transitions within a segment can be non-Markovian. In spite of this additional power, exact learning and inference algorithms for semi-CRFs are polynomial-time—often only a small constant factor slower than conventional CRFs. In experiments on ﬁve named entity recognition problems, semi-CRFs generally outper- form conventional CRFs."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/eb0ecdb070a1a0ac46de0cd733d39cf3-Abstract.html,Adaptive Manifold Learning,"Jing Wang, Zhenyue Zhang, Hongyuan Zha","Recently, there have been several advances in the machine learning and           pattern recognition communities for developing manifold learning algo-           rithms to construct nonlinear low-dimensional manifolds from sample           data points embedded in high-dimensional spaces. In this paper, we de-           velop algorithms that address two key issues in manifold learning: 1)           the adaptive selection of the neighborhood sizes; and 2) better fitting the           local geometric structure to account for the variations in the curvature           of the manifold and its interplay with the sampling density of the data           set. We also illustrate the effectiveness of our methods on some synthetic           data sets."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/ec1f850d934f440cfa8e4a18d2cf5463-Abstract.html,Euclidean Embedding of Co-Occurrence Data,"Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby","Embedding algorithms search for low dimensional structure in complex          data, but most algorithms only handle objects of a single type for which          pairwise distances are specified. This paper describes a method for em-          bedding objects of different types, such as images and text, into a single          common Euclidean space based on their co-occurrence statistics. The          joint distributions are modeled as exponentials of Euclidean distances in          the low-dimensional embedding space, which links the problem to con-          vex optimization over positive semidefinite matrices. The local struc-          ture of our embedding corresponds to the statistical correlations via ran-          dom walks in the Euclidean space. We quantify the performance of our          method on two text datasets, and show that it consistently and signifi-          cantly outperforms standard methods of statistical correspondence mod-          eling, such as multidimensional scaling and correspondence analysis."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/ed277964a8959e72a0d987e598dfbe72-Abstract.html,Heuristics for Ordering Cue Search in Decision Making,"Peter M. Todd, Anja Dieckmann","Simple lexicographic decision heuristics that consider cues one at a 
          time  in  a  particular  order  and  stop  searching  for  cues  as  soon  as  a 
          decision  can  be  made  have  been  shown  to  be  both  accurate  and 
          frugal  in  their  use  of  information.    But  much  of  the  simplicity  and 
          success  of  these  heuristics  comes  from  using  an  appropriate  cue 
          order.  For instance, the Take The Best heuristic uses validity order 
          for  cues,  which  requires  considerable  computation,  potentially 
          undermining  the  computational  advantages  of  the  simple  decision 
          mechanism.    But  many  cue  orders  can  achieve  good  decision 
          performance, and studies of sequential search for data records have 
          proposed  a  number  of  simple  ordering  rules  that  may  be  of  use  in 
          constructing  appropriate  decision  cue  orders  as  well.    Here  we 
          consider  a  range  of  simple  cue  ordering  mechanisms,  including 
          tallying, swapping, and move-to-front rules, and show that they can 
          find  cue  orders  that  lead  to  reasonable  accuracy  and  considerable 
          frugality when used with lexicographic decision heuristics."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/ed57844fa5e051809ead5aa7e3e1d555-Abstract.html,Coarticulation in Markov Decision Processes,"Khashayar Rohanimanesh, Robert Platt, Sridhar Mahadevan, Roderic Grupen","We investigate an approach for simultaneously committing to mul-         tiple activities, each modeled as a temporally extended action in         a semi-Markov decision process (SMDP). For each activity we de-         fine a set of admissible solutions consisting of the redundant set of         optimal policies, and those policies that ascend the optimal state-         value function associated with them. A plan is then generated by         merging them in such a way that the solutions to the subordinate         activities are realized in the set of admissible solutions satisfying         the superior activities. We present our theoretical results and em-         pirically evaluate our approach in a simulated domain."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/ef0917ea498b1665ad6c701057155abe-Abstract.html,Integrating Topics and Syntax,"Thomas L. Griffiths, Mark Steyvers, David M. Blei, Joshua B. Tenenbaum","Statistical approaches to language learning typically focus on either          short-range syntactic dependencies or long-range semantic dependencies          between words. We present a generative model that uses both kinds of          dependencies, and can be used to simultaneously find syntactic classes          and semantic topics despite having no representation of syntax or seman-          tics beyond statistical dependency. This model is competitive on tasks          like part-of-speech tagging and document classification with models that          exclusively use short- and long-range dependencies respectively."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/ef1e491a766ce3127556063d49bc2f98-Abstract.html,Object Classification from a Single Example Utilizing Class Relevance Metrics,Michael Fink,"We describe a framework for learning an object classifier from a single          example. This goal is achieved by emphasizing the relevant dimensions          for classification using available examples of related classes. Learning          to accurately classify objects from a single training example is often un-          feasible due to overfitting effects. However, if the instance representa-          tion provides that the distance between each two instances of the same          class is smaller than the distance between any two instances from dif-          ferent classes, then a nearest neighbor classifier could achieve perfect          performance with a single training example. We therefore suggest a two          stage strategy. First, learn a metric over the instances that achieves the          distance criterion mentioned above, from available examples of other          related classes. Then, using the single examples, define a nearest neigh-          bor classifier where distance is evaluated by the learned class relevance          metric. Finding a metric that emphasizes the relevant dimensions for          classification might not be possible when restricted to linear projections.          We therefore make use of a kernel based metric learning algorithm. Our          setting encodes object instances as sets of locality based descriptors and          adopts an appropriate image kernel for the class relevance metric learn-          ing. The proposed framework for learning from a single example is          demonstrated in a synthetic setting and on a character classification task."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/f0f6ba4b5e0000340312d33c212c3ae8-Abstract.html,A Large Deviation Bound for the Area Under the ROC Curve,"Shivani Agarwal, Thore Graepel, Ralf Herbrich, Dan Roth","The area under the ROC curve (AUC) has been advocated as an evalu- ation criterion for the bipartite ranking problem. We study large devi- ation properties of the AUC; in particular, we derive a distribution-free large deviation bound for the AUC which serves to bound the expected accuracy of a ranking function in terms of its empirical AUC on an inde- pendent test sequence. A comparison of our result with a corresponding large deviation result for the classiﬁcation error rate suggests that the test sample size required to obtain an -accurate estimate of the expected ac- curacy of a ranking function with δ-conﬁdence is larger than that required to obtain an -accurate estimate of the expected error rate of a classiﬁ- cation function with the same conﬁdence. A simple application of the union bound allows the large deviation bound to be extended to learned ranking functions chosen from ﬁnite function classes."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/f12f2b34a0c3174269c19e21c07dee68-Abstract.html,Adaptive Discriminative Generative Model and Its Applications,"Ruei-sung Lin, David A. Ross, Jongwoo Lim, Ming-Hsuan Yang","This paper presents an adaptive discriminative generative model that gen-          eralizes the conventional Fisher Linear Discriminant algorithm and ren-          ders a proper probabilistic interpretation. Within the context of object          tracking, we aim to find a discriminative generative model that best sep-          arates the target from the background. We present a computationally          efficient algorithm to constantly update this discriminative model as time          progresses. While most tracking algorithms operate on the premise that          the object appearance or ambient lighting condition does not significantly          change as time progresses, our method adapts a discriminative genera-          tive model to reflect appearance variation of the target and background,          thereby facilitating the tracking task in ever-changing environments. Nu-          merous experiments show that our method is able to learn a discrimina-          tive generative model for tracking target objects undergoing large pose          and lighting changes."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/f15eda31a2da646eea513b0f81a5414d-Abstract.html,Optimal sub-graphical models,"Mukund Narasimhan, Jeff A. Bilmes","We investigate the problem of reducing the complexity of a graphical            model (G, PG) by finding a subgraph H of G, chosen from a class of            subgraphs H, such that H is optimal with respect to KL-divergence. We            do this by first defining a decomposition tree representation for G, which            is closely related to the junction-tree representation for G. We then give            an algorithm which uses this representation to compute the optimal H             H. Gavril [2] and Tarjan [3] have used graph separation properties to            solve several combinatorial optimization problems when the size of the            minimal separators in the graph is bounded. We present an extension of            this technique which applies to some important choices of H even when            the size of the minimal separators of G are arbitrarily large. In particular,            this applies to problems such as finding an optimal subgraphical model            over a (k - 1)-tree of a graphical model over a k-tree (for arbitrary k)            and selecting an optimal subgraphical model with (a constant) d fewer            edges with respect to KL-divergence can be solved in time polynomial in            |V (G)| using this formulation. 1      Introduction and Preliminaries
The complexity of inference in graphical models is typically exponential in some parame- ter of the graph, such as the size of the largest clique. Therefore, it is often required to find a subgraphical model that has lower complexity (smaller clique size) without introducing a large error in inference results. The KL-divergence between the original probability dis- tribution and the probability distribution on the simplified graphical model is often used to measure the impact on inference. Existing techniques for reducing the complexity of graph- ical models including annihilation and edge-removal [4] are greedy in nature and cannot make any guarantees regarding the optimality of the solution. This problem is NP-complete [9] and so, in general, one cannot expect a polynomial time algorithm to find the optimal solution. However, we show that when we restrict the problem to some sets of subgraphs, the optimal solution can be found quite quickly using a dynamic programming algorithm in time polynomial in the tree-width of the graph.
1.1    Notation and Terminology
A graph G = (V, E) is said to be triangulated if every cycle of length greater than 3 has a chord. A clique of G is a non-empty set S  V such that {a, b}  E for all
  This work was supported by NSF grant IIS-0093430 and an Intel Corporation Grant.



                                         {b, c, d}                         {c, f, g}      d                                                       {b, c}                       {f, c}                                                                   {c, e}                                                      {b, e, c}                 {e, c, f }      b             c              g                                                       {b, e}


                                         {a, b, e}      a             e              f

            Figure 1: A triangulated graph G and a junction-tree for G

a, b  S. A clique S is maximal if S is not properly contained in another clique. If  and  are non-adjacent vertices of G then a set of vertices S  V \ {, } is called an (, )-separator if  and  are in distinct components of G[V \ S]. S is a minimal (, )-separator if no proper subset of S is an (, )-separator. S is said to be a minimal separator if S is a minimal (, )-separator for some non adjacent a, b  V . If T = (K, S) is a junction-tree for G (see [7]), then the nodes K of T correspond to the maximal- cliques of G, while the links S correspond to minimal separators of G (We reserve the terms vertices/edges for elements of G, and nodes/links for the elements of T ). If G is triangulated, then the number of maximal cliques is at most |V |. For example, in the graph G shown in Figure 1, K = {{b, c, d} , {a, b, e} , {b, e, c} , {e, c, f } , {c, f, g}}. The links S of T correspond to minimal-separators of G in the following way. If ViVj  S (where Vi, Vj  K and hence are cliques of G), then Vi  Vj = . We label each edge ViVj  S with the set Vij = Vi  Vj, which is a non-empty complete separator in G. The removal of any link ViVj  S disconnects T into two subtrees which we denote T (i) and T (j) (chosen so that T (i) contains Vi). We will let K(i) be the nodes of T (i), and V (i) = V K(i)V be the set of vertices corresponding to the subtree T (i). The junction tree property ensures that V (i)  V (j) = Vi  Vj = Vij. We will let G(i) be the subgraph induced by V (i).
A graphical model is a pair (G, P ) where P is the joint probability distribution for random variables X1, X2, . . . , Xn, and G is a graph with vertex set V (G) = {X1, X2, . . . , Xn} such that the separators in G imply conditional independencies in P (so P factors according to G). If G is triangulated, then the junction-tree algorithm can be used for exact inference in the probability distribution P . The complexity of this algorithm grows with the treewidth of G (which is one less than the size of the largest clique in G when G is triangulated). The growth is exponential when P is a discrete probability distribution, thus rendering exact inference for graphs with large treewidth impractical. Therefore, we seek another graphical model (H, PH ) which allows tractable inference (so H should have lower treewidth than G has). The general problem of finding a graphical model of tree-width at most k so as to minimize the KL-divergence from a specified probability distribution is NP complete for general k ([9]) However, it is known that this problem is solvable in polynomial time (in |V (G)|) for some special cases cases (such as when G has bounded treewidth or when k = 1 [1]). If (G, PG) and (H, PH ) are graphical models, then we say that (H, PH ) is a subgraphical model of (G, PG) if H is a spanning subgraph of G. Note in particular that separators in G are separators in H, and hence (G, PH ) is also a graphical model.
2    Graph Decompositions and Divide-and-Conquer Algorithms
For the remainder of the paper, we will be assuming that G = (V, E) is some triangulated graph, with junction tree T = (K, S). As observed above, if ViVj  S, then the removal
                                            {b, c, d}                                    {c, f, g} d                                                                {b, c}                            {f, c}

                                                           {b, e, c}                     {e, c, f } b                 c       c             g                                                                {b, e}


                                            {a, b, e} a                 e       e             f

Figure 2: The graphs G(i), G(j) and junction-trees T (i) and T (j) resulting from the removal of the link Vij = {c, e}
of Vij = Vi  Vj disconnects G into two (vertex-induced) subgraphs G(i) and G(j) which are both triangulated, with junction-trees T (i) and T (j) respectively. We can recursively decompose each of G(i) and G(j) into smaller and smaller subgraphs till the resulting sub- graphs are cliques. When the size of all the minimal separators are bounded, we may use these decompositions to easily solve problems that are hard in general. For example, in [5] it is shown that NP-complete problems like vertex coloring, and finding maximum inde- pendent sets can be solved in polynomial time on graphs with bounded tree-width (which are equivalent to spanning graphs with bounded size separators). We will be interested in finding (triangulated) subgraphs of G that satisfy some conditions, such as a bound on the number of edges, or a bound on the tree-width and which optimize separable objective functions (described in Section 2)
One reason why problems such as this can often be solved easily when the tree-width of G is bounded by some constant is this : If Vij is a separator decomposing G into G(i) and G(j), then a divide-and-conquer approach would suggest that we try and find optimal subgraphs of G(i) and G(j) and then splice the two together to get an optimal subgraph of G. There are two issues with this approach. First, the optimal subgraphs of G(i) and G(j) need not necessarily match up on Vij, the set of common vertices. Second, even if the two subgraphs agree on the set of common vertices, the graph resulting from splicing the two subgraphs together need not be triangulated (which could happen even if the two subgraphs individually are triangulated). To rectify the situation, we can do the following. We parti- tion the set of subgraphs of G(i) and G(j) into classes, so that any subgraph of G(i) and any subgraph G(j) corresponding to the same class are compatible in the sense that they match up on their intersection namely Vij, and so that by splicing the two subgraphs together, we get a subgraph of G which is acceptable (and in particular is triangulated). Then given op- timal subgraphs of both G(i) and G(j) corresponding to each class, we can enumerate over all the classes and pick the best one. Of course, to ensure that we do not repeatedly solve the same problem, we need to work bottom-up (a.k.a dynamic programming) or memoize our solutions. This procedure can be carried out in polynomial (in |V |) time as long as we have only a polynomial number of classes. Now, if we have a polynomial number of classes, these classes need not actually be a partition of all the acceptable subgraphs, though the union of the classes must cover all acceptable subgraphs (so the same subgraph can be contained in more than one class). For our application, every class can be thought of to be the set of subgraphs that satisfy some constraint, and we need to pick a polynomial number of constraints that cover all possibilities. The bound on the tree-width helps us here. If |V                                                                                   )       ij | = k, then in any subgraph H of G, H [Vij ] must be one of the 2(k                                                                                 2         possible subgraphs of G[V                                                   )           ij ]. So, if k is sufficiently small (so 2(k                                                     2         is bounded by some polynomial in |V |),
then this procedure results in a polynomial time algorithm. In this paper, we show that in some cases we can characterize the space H so that we still have a polynomial number of constraints even when the tree-width of G is not bounded by a small constant.
2.1    Separable objective functions
For cases where exact inference in the graphical model (G, PG) is intractable, it is natural to try to find a subgraphical model (H, PH ) such that D(PG PH ) is minimized, and inference using H is tractable. We will denote by H the set of subgraphs of G that are tractable for inference. For example, this set could be the set of subgraphs of G with treewidth one less than the treewidth of G, or perhaps the set of subgraphs of G with at d fewer edges. For a specified subgraph H of G, there is a unique probability distribution PH factoring over H that minimizes D(PG PH ). Hence, finding a optimal subgraphical model is equivalent to finding a subgraph H for which D(PG PH ) is minimized. If Vij is a separator of G, we will attempt to find optimal subgraphs of G by finding optimal subgraphs of G(i) and G(j) and splicing them together. However, to do this, we need to ensure that the objective criteria also decomposes along the separator Vij. Suppose that H is any triangulated subgraph of G. Let PG(i) and PG(j) be the (marginalized) distributions of PG on V (i) and V (j) respectively, and PH(i) and PH(j) be the (marginalized) distributions of the distribution PH on V (i) and V (j) where H(i) = H[V (i)] and H(j) = H[V (j)], The following result assures us that the KL-divergence also factors according to the separator Vij. Lemma 1. Suppose that (G, PG) is a graphical model, H is a triangulated subgraph of G, and PH factors over H. Then D(PG PH ) = D(PG(i) PH(i)) + D(PG(j) PH(j)) - D(PG[Vij] PH[Vij]).
Proof. Since H is a subgraph of G, and Vij is a separator of G, Vij must also be a sepa-                                                      P rator of H. Therefore, P                                  H(i) ({Xv }vV (i) )PH(j) ({Xv }vV (j) )                             H    {Xv}           =                                                       . The result                                          vV                       PH[V                        )                                                                            ij ] ({Xv }vVij follows immediately.
Therefore, there is hope that we can reduce our our original problem of finding an optimal subgraph H  H as one of finding subgraphs of H (i)  G(i) and H(j)  G(j) that are compatible, in the sense that they match up on the overlap Vij, and for which D(PG PH ) is minimized. Throughout this paper, for the sake of concreteness, we will assume that the objective criterion is to minimize the KL-divergence. However, all the results can be extended to other objective functions, as long as they ""separate"" in the sense that for any separator, the objective function is the sum of the objective functions of the two parts, possibly modulo some correction factor which is purely a function of the separator. Another example might be the complexity r(H) of representing the graphical model H. A very natural representation satisfies r(G) = r(G(i)) + r(G(j)) if G has a separator G(i)  G(j). Therefore, the representation cost reduction would satisfy r(G) - r(H) = (r(G(i)) - r(H(i))) + (r(G(j)) - r(H(j))), and so also factors according to the separators. Finally note that any linear combinations of such separable functions is also separable, and so this technique could also be used to determine tradeoffs (representation cost vs. KL-divergence loss for example). In Section 4 we discuss some issues regarding computing this function.
2.2    Decompositions and decomposition trees
For the algorithms considered in this paper, we will be mostly interested in the decompo- sitions that are specified by the junction tree, and we will represent these decompositions by a rooted tree called a decomposition tree. This representation was introduced in [2, 3], and is similar in spirit to Darwiche's dtrees [6] which specify decompositions of directed acyclic graphs. In this section and the next, we show how a decomposition tree for a graph may be constructed, and show how it is used to solve a number of optimization problems.
                                               abd; ce; gf

                             a; be; cd

                                                                e; cf ; g                                               d; bc; e


                      abe        dbc             ebc          cef            cf g


                Figure 3: The separator tree corresponding to Figure 1

A decomposition tree for G is a rooted tree whose vertices correspond to separators and cliques of G. We describe the construction of the decomposition tree in terms of a junction- tree T = (K, S) for G. The interior nodes of the decomposition tree R(T ) correspond to S (the links of T and hence the minimal separators of G). The leaf or terminal nodes represent the elements of K (the nodes of T and hence the maximal cliques of G). R(T ) can be recursively constructed from T as follows : If T consists of just one node K, (and hence no edges), then R consists of just one node, which is given the label K as well. If however, T has more than one node, then T must contain at least one link. To begin, let ViVj  S be any link in T . Then removal of the link ViVj results in two disjoint junction- trees T (i) and T (j). We label the root of R by the decomposition (V (i); Vij; V (j)). The rest of R is recursively built by successively picking links of T (i) and T (j) (decompositions of G(i) and G(j)) to form the interior nodes of R. The effect of this procedure on the junction tree of Figure 1 is shown in Figure 3, where the decomposition associated with the interior nodes is shown inside the nodes. Let M be the set of all nodes of R(T ). For any interior node M induced by the the link ViVj  S of T , then we will let M (i) and M (j) represent the left and right children of M , and R(i) and R(j) be the left and right trees below M .
3      Finding optimal subgraphical models
3.1    Optimal sub (k - 1)-trees of k-trees
Suppose that G is a k-tree. A sub (k - 1)-tree of G is a subgraph H of G that is (k - 1)- tree. Now, if Vij is any minimal separator of G, then both G(i) and G(j) are k-trees on vertex sets V (i) and V (j) respectively. It is clear that the induced subgraphs H[V (i)] and H[V (j)] are subgraphs of G(i) and G(j) and are partial (k - 1)-trees. We will be interested in finding sub (k - 1)-trees of k trees and this problem is trivial by the result of [1] when k = 2. Therefore, we assume that k  3. The following result characterizes the various possibilities for H[Vij] in this case.
Lemma 2. Suppose that G is a k-tree, and S = Vij is a minimal separator of G corre- sponding to the link ij of the junction-tree T . In any (k - 1)-tree H  G either
    1. There is a u  S such that u is not connected to vertices in both V (i) \ S and            V (j) \ S. Then S \ {u} is a minimal separator in H and hence is complete.

    2. Every vertex in S is connected to vertices in both V (i) \S and V (j) \S. Then there            are vertices {x, y}  S such that the edge H[S] is missing only the edge {x, y}.            Further either H[V (i)] or H[V (j)] does not contain a unchorded x-y path.

Proof. We consider two possibilities. In the first, there is some vertex u  S such that u is not connected to vertices in both V (i) \ S and V (j). Since the removal of S disconnects G, the removal of S must also disconnect H. Therefore, S must contain a minimal separator of H. Since H is a (k - 1)-tree, all minimal separators of H must contain k - 1 vertices which must therefore be S {u}. This corresponds to case (1) above. Clearly this possiblity can occur.
If there is no such u  S, then every vertex in S is connected to vertices in both V (i) \ S and V (j) \ S. If x  S is connected to some yi  V (i) \ S and yj  V (j) \ S, then x is contained in every minimal yi/yj separator (see [5]). Therefore, every vertex in S is part of a minimal separator. Since each minimal separator contains k - 1 vertices, there must be at least two distinct minimum separators contained in S. Let Sx = S \ {x} and Sy = S \ {y} be two distinct minimal separators. We claim that H[S] contains all edges except the edge {x, y}. To see this, note that if z, w  S, with z = w and {z, w} = {x, y} (as sets), then either {z, w}  Sy or {z, w}  Sx. Since both Sx and Sy are complete in H, this edge must be present in H. The edge {x, y} is not present in H[S] because all minimal separators in H must be of size k - 1. Further, if both V (i) and V (j) contain an unchorded path between x and y, then by joining the two paths at x and y, we get a unchorded cycle in H which contradicts the fact that H is triangulated.
Therefore, we may associate k  2 + 2  k constraints with each separator V                                 2                                                 ij of G as follows. There are k possible constraints corresponding to case (1) above (one for each choice of x), and k  2 choices corresponding to case (2) above. This is because for each                     2 pair {x, y} corresponding to the missing edge, we have either V (i) contains no unchorded xy paths or V (j) contains no unchorded xy paths. More explicitly, we can encode the set of constraints CM associated with each separator S corresponding to an interior node M of the decomposition tree as follows: CM = { (x, y, s) : x  S, y  S, s  {i, j}}. If y = x, then this corresponds to case (1) of the above lemma. If s = i, then x is connected only to H(i) and if s = j, then x is connected only to H(j). If y = x, then this corresponds to case (2) in the above lemma. If s = i, then H (i) does not contain any unchorded path between x and y, and there is no constraint on H(j). Similarly if s = j, then H(j) does not contain any unchorded path between x and y, and there is no constraint on H (i).
Now suppose that H(i) and H(j) are triangulated subgraphs of G(i) and G(j) respectively, then it is clear that if H(i) and H(j) both satisfy the same constraint they must match up on the common vertices Vij. Therefore to splice together two solutions corresponding to the same constraint, we only need to check that the graph obtained by splicing the graphs is triangulated.
Lemma 3. Suppose that H(i) and H(j) are triangulated subgraphs of G(i) and G(j) re- spectively such that both of them satisfy the same constraint as described above. Then the graph H obtained by splicing H(i) and H(j) together is triangulated.
Proof. Suppose that both H(i) and H(j) are both triangulated and both satisfy the same constraint. If both H(i) and H(j) satisfy the same constraint corresponding to case (1) in Lemma 2 and H has an unchorded cycle, then this cycle must involve elements of both H(i) and H(j). Therefore, there must be two vertices of S {u} on the cycle, and hence this cycle has a chord as S \ {u} is complete. This contradiction shows that H is triangulated. So assume that both of them satisfy the constraint corresponding to case (2) of Lemma 2. Then if H is not triangulated, there must be a t-cycle (for t  4) with no chord. Now, since {x, y} is the only missing edge of S in H, and because H (i) and H(j) are individually triangulated, the cycle must contain x, y and vertices of both V (i) \ S and V (j) \ S. We
may split this unchorded cycle into two unchorded paths, one contained in V (i) and one in V (j) thus violating our assumption that both H(i) and H(j) satisfy the same constraint.
If |S| = k, then there are 2k + 2  k  O(k2)  O(n2). We can use a divide and conquer                                         2 strategy to find the optimal sub (k - 1) tree once we have taken care of the base case, where G is just a single clique (of k + 1) elements. However, for this case, it is easily checked that any subgraph of G obtained by deleting exactly one edge results in a (k - 1) tree, and every sub (k-1)-tree results from this operation. Therefore, the optimal (k-1)-tree can be found using Algorithm 1, and in this case, the complexity of Algorithm 1 is O(n(k + 1)2). This procedure can be generalized to find the optimal sub (k - d)- tree for any fixed d. However, the number of constraints grows exponentially with d (though it is still polynomial in n). Therefore for small, fixed values of d, we can compute the optimal sub (k - d)-tree of G. While we can compute (k - d)-trees of G by first going from a k tree to a (k - 1) tree, then from a (k - 1)-tree to a (k - 2)-tree, and so on in a greedy fashion, this will not be optimal in general. However, this might be a good algorithm to try when d is large.
3.2     Optimal triangulated subgraphs with |E(G)| - d edges
Suppose that we are interested in a (triangulated) subgraph of G that contains d fewer edges that G does. That is, we want to find an optimal subgraph H  G such that |E(H)| = |E(G)| - d. Note that by the result of [4] there is always a triangulated subgraph with d fewer edges (if d < |E(G)|). Two possibilities for finding such an optimal subgraph are
     1. Use the procedure described in [4]. This is a greedy procedure which works in             d steps by deleting an edge at each step. At each state, the edge is picked from             the set of edges whose deletion leaves a triangulated graph. Then the edge which             causes the least increase in KL-divergence is picked at each stage.

     2. For each possible subset A of E(G) of size d, whose deletion leaves a triangulated             graph, compute the KL divergence using the formula above, and then pick the             optimal one. Since there are |E(G)| such sets, this can be done in polynomial                                                 d             time (in |V (G)|) when d is a constant.

The first greedy algorithm is not guaranteed to yield the optimal solution. The second takes time that is O(n2d). Now, let us solve this problem using the framework we've described.
Let H be the set of subgraphs of G which may be obtained by deletion of d edges.      For each M = ij  M corresponding to the separator Vij, let CM =
 (l, r, c, s, A) : l + r - c = d, s a d bit string, A  E(G[Vij])    .    The constraint repre-                                                                c

sented by (l, r, c, A) is this : A is a set of d edges of G[Vij] that are missing in H, l edges are missing from the left subgraph, and r edges are missing from the right subgraph. c rep- resents the double count, and so is subtracted from the total. If k is the size of the largest                                                                                       ) clique, then the total number of such constraints is bounded by 2d  2d  (k2               O(k2d)                                                                                      d which could be better than O(n2d) and is polynomial in |V | when d is constant. See [10] for additional details.
4      Conclusions
Algorithm 1 will compute the optimal H  H for the two examples discussed above and is polynomial (for fixed constant d) even if k is O(n). In [10] a generalization is presented which will allow finding the optimal solution for other classes of subgraphical models. Now, we assume an oracle model for computing KL-divergences of probability distribu- tions on vertex sets of cliques. It is clear that these KL-divergences can be computed
R  separator-tree for G; for each vertex M of R in order of increasing height (bottom up) do    for each constraint cM of M do          if M is an interior vertex of R corresponding to edge ij of the junction tree then              Let Ml and Mr be the left and right children of M ;              Pick constraint cl  CM compatible with c                                     l                     M to minimize table[Ml, cl];              Pick constraint cr  CM compatible with c                                      r                    M to minimize table[Mr , cr ];              loss  D(PG[M ] PH [M ]);              table[M, cM ]  table[Ml, cl] + table[Mr, cr] - loss;
     else              table[M, cM ]  D(PG[M ] PH [M ]);

     end    end end

                   Algorithm 1: Finding optimal set of constraints

efficiently for distributions like Gaussians, but for discrete distributions this may not be possible when k is large. However even in this case this algorithm will result in only polynomial calls to the oracle. The standard algorithm [3] which is exponential in the treewidth will make O(2k) calls to this oracle. Therefore, when the cost of computing the KL-divergence is large, this algorithm becomes even more attractive as it results in expo- nential speedup over the standard algorithm. Alternatively, if we can compute approximate KL-divergences, or approximately optimal solutions, then we can compute an approximate solution by using the same algorithm."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/f1748d6b0fd9d439f71450117eba2725-Abstract.html,Modeling Conversational Dynamics as a Mixed-Memory Markov Process,"Tanzeem Choudhury, Sumit Basu","influences 
In  this  work,  we  quantitatively  investigate  the  ways  in  which  a  given  person  the  joint  turn-taking  behavior  in  a  conversation.  After  collecting  an  auditory  database  of  social  interactions  among  a  group  of twenty-three  people  via  wearable  sensors  (66  hours  of data  each  over  two  weeks),  we  apply  speech  and conversation detection methods to  the auditory streams.  These  methods  automatically  locate  the  conversations,  determine  their  participants,  and  mark  which  participant  was  speaking  when.  We  then  model  the  joint  turn-taking  behavior  as  a  Mixed-Memory  Markov  Model  [1]  that  combines  the  statistics  of the  individual  subjects'  self-transitions  and  the  partners '  cross-transitions.  The  mixture parameters in  this  model describe how much each person's  individual  behavior contributes  to  the joint turn-taking behavior of  the  pair.  By  estimating  these  parameters,  we  thus  estimate  how  much  influence  each participant  has  in  determining  the  joint turn(cid:173) this  measure  correlates  taking  behavior.  We  significantly  with  betweenness  centrality  [2],  an  independent  measure  of an  individual's  importance  in  a  social  network.  This  result  suggests  that  our  estimate  of  conversational  influence  is  predictive of social influence."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/f1b0775946bc0329b35b823b86eeb5f5-Abstract.html,Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices,"Nathan Srebro, Noga Alon, Tommi S. Jaakkola","We prove generalization error bounds for predicting entries in a partially            observed matrix by fitting the observed entries with a low-rank matrix. In            justifying the analysis approach we take to obtain the bounds, we present            an example of a class of functions of finite pseudodimension such that            the sums of functions from this class have unbounded pseudodimension.
1     Introduction
""Collaborative filtering"" refers to the general task of providing users with information on what items they might like, or dislike, based on their preferences so far and how they relate to the preferences of other users. This approach contrasts with a more traditional feature- based approach where predictions are made based on features of the items.
For feature-based approaches, we are accustomed to studying prediction methods in terms of probabilistic post-hoc generalization error bounds. Such results provide us a (proba- bilistic) bound on the performance of our predictor on future examples, in terms of its performance on the training data. These bounds hold without any assumptions on the true ""model"", that is the true dependence of the labels on the features, other than the central assumptions that the training examples are drawn i.i.d. from the distribution of interest.
In this paper we suggest studying the generalization ability of collaborative prediction methods. By ""collaborative prediction"" we indicate that the objective is to be able to pre- dict user preferences for items, that is, entries in some unknown target matrix Y of user- item ""ratings"", based on observing a subset YS of the entries in this matrix1. We present
 1In other collaborative filtering tasks, the objective is to be able to provide each user with a few items that overlap his top-rated items, while it is not important to be able to correctly predict the users ratings for other items. Note that it is possible to derive generalization error bounds for this objective based on bounds for the ""prediction"" objective.



           arbitrary source distribution                target matrix Y                    random training set              random set S of observed entries                           hypothesis                      predicted matrix X                          training error             observed discrepancy DS(X; Y )                    generalization error                true discrepancy D(X; Y )

Figure 1: Correspondence with post-hoc bounds on the generalization error for standard feature-based prediction tasks
bounds on the true average overall error D(X; Y ) =            1     n      m      loss(X                                                               nm     i=1    a=1              ia; Yia) of the predictions X in terms of the average error over the observed entries DS(X; Y ) =  1             loss(X |S|    iaS              ia; Yia), without making any assumptions on the true nature of the pref- erences Y . What we do assume is that the subset S of entries that we observe is chosen uniformly at random. This strong assumption parallels the i.i.d. source assumption for feature-based prediction.
In particular, we present generalization error bounds on prediction using low-rank models.
Collaborative prediction using low-rank models is fairly straight forward. A low-rank ma- trix X is sought that minimizes the average observed error DS(X; Y ). Unobserved entries in Y are then predicted according to X. The premise behind such a model is that there are only a small number of factors influencing the preferences, and that a user's preference vector is determined by how each factor applies to that user. Different methods differ in how they relate real-valued entries in X to preferences in Y , and in the associated measure of discrepancy. For example, entries in X can be seen as parameters for a probabilistic models of the entries in Y , either mean parameters [1] or natural parameters [2], and a maximum likelihood criterion used. Or, other loss functions, such as squared error [3, 2], or zero-one loss versus the signs of entries in X, can be minimized.
Prior Work         Previous results bounding the error of collaborative prediction using a low- rank matrix all assume the true target matrix Y is well-approximated by a low-rank matrix. This corresponds to a large eigengap between the top few singular values of Y and the remaining singular values. Azar et al [3] give asymptotic results on the convergence of the predictions to the true preferences, assuming they have an eigengap. Drineas et al [4] analyze the sample complexity needed to be able to predict a matrix with an eigengap, and suggests strategies for actively querying entries in the target matrix. To our knowledge, this is the first analysis of the generalization error of low-rank methods that do not make any assumptions on the true target matrix.
Generalization error bounds (and related online learning bounds) were previously discussed for collaborative prediction applications, but only when prediction was done for each user separately, using a feature-based method, with the other user's preferences as features [5, 6]. Although these address a collaborative prediction application, the learning setting is a standard feature-based setting. These methods are also limited, in that learning must be performed separately for each user.
Shaw-Taylor et al [7] discuss assumption-free post-hoc bounds on the residual errors of low-rank approximation. These results apply to a different setting, where a subset of the rows are fully observed, and bound a different quantity--the distance between rows and the learned subspace, rather then the distance to predicted entries.
Organization        In Section 2 we present a generalization error bound for zero-one loss, based on a combinatorial result which we prove in Section 3. In Section 4 we generalize the bound to arbitrary loss functions. Finally, in Section 5 we justify the combinatorial
approach taken, by considering an alternate approach (viewing rank-k matrices as combi- nation of k rank-1 matrices) and showing why it does not work.
2     Generalization Error Bound for Zero-One Error
We begin by considering binary labels Yia   and a zero-one sign agreement loss:
                                    loss(Xia; Yia) = 1YiaXia0                                                    (1)

Theorem 1. For any matrix Y  {1}nm, n, m > 2,  > 0 and integer k, with proba- bility at least 1 -  over choosing a subset S of entries in Y uniformly among all subsets of |S| entries, the discrepancy with respect to the zero-one sign agreement loss satisfies2:
                                                                   k(n + m) log 16em - log                                                                                                  k               X,rank X<k D(X ; Y ) < D (X ; Y ) +                                                    S                                   2|S|

To prove the theorem we employ standard arguments about the generalization error for finite hypothesis classes with bounded cardinality.
First fix Y as well as X               nm                                   R           . When an index pair (i, a) is chosen uniformly at random, loss(Xia; Yia) is a Bernoulli random variable with probability D(X; Y ) of being one. If the entries of S are chosen independently and uniformly, |S|D(X; Y ) is Binomially                                                                                             S distributed with mean |S|D(X; Y ) and using Chernoff's inequality:
                        Pr D(X; Y )  D(X; Y ) +                     e-2|S| 2                                   (2)                                                              S                              S

The distribution of S in Theorem 1 is slightly different, as S is chosen without repetitions. The mean of D(X; Y ) is the same, but it is more concentrated, and (2) still holds.                       S
Now consider all rank-k matrices. Noting that loss(Xia; Yia) depends only on the sign of Xia, it is enough to consider the equivalence classes of matrices with the same sign patterns. Let f (n, m, k) be the number of such equivalence classes, i.e. the number of possible sign configurations of n  m matrices of rank at most k:
               F (n, m, k) = {sign X  {-, 0, +}nm|X                      nm                                                                            R           , rank X  k}                                          f (n, m, k) = F (n, m, k)

                                                                                             1     If Xia > 0 where sign X denotes the element-wise sign matrix (sign X)ia =                                   0     If Xia = 0 .                                                                                                  -1    If Xia < 1

For all matrices in an equivalence class, the random variable D(X; Y ) is the same, and                                                                                        S taking a union bound of the events D(X; Y )  D(X; Y )+ for each of these f (n, m, k)                                                                   S random variables we have:
                                                                    log f (n, m, k) - log          Pr         X,rank XkD(X; Y )  D(X; Y ) +                                                                (3)                                                         S          S                                                                        2|S|

by using (2) and setting =              log f (n,m,k)-log  . The proof of Theorem 1 rests on bounding                                                   2|S| f (n, m, k), which we will do in the next section.
Note that since the equivalence classes we defined do not depend on the sample set, no symmetrization argument is necessary.
 2All logarithms are base two

3      Sign Configurations of a Low-Rank Matrix
In this section, we bound the number f (n, m, k) of sign configurations of n  m rank- k matrices over the reals. Such a bound was previously considered in the context of unbounded error communication complexity. Alon, Frankl and Rodl [8] showed that f (n, m, k)  minh (8 nm/h )(n+m)k+h+m, and used counting arguments to establish that some (in fact, most) binary matrices can only be realized by high-rank matrices, and therefore correspond to functions with high unbounded error communication complexity.
Here, we follow a general course outlined by Alon [9] to obtain a simpler, and slightly tighter, bound based on the following result due to Warren:
Let P1, . . . , Pr be real polynomials in q variables, and let C be the complement of the variety defined by iPi, i.e. the set of points in which all the m polynomials are non-zero:                                           C = {x                     q                                                                  R |iPi(x) = 0} Theorem 2 (Warren [10]). If all r polynomials are of degree at most d, then the number of connected components of C is at most:                                                                  q                                            q                                                                                   r               4edr                                c(C)  2(2d)q                               2i                                                                                              i                q                                                                 i=0
where the second inequality holds when r > q > 2.
The signs of the polynomials P1, . . . , Pr are fixed inside each connected component of C. And so, c(C) bounds the number of sign configurations of P1, . . . , Pr that do not contain zeros. To bound the overall number of sign configurations the polynomials are modified slightly (see Appendix), yielding: Corollary 3 ([9, Proposition 5.5]). The number of -/0/+ sign configurations of r polyno- mials, each of degree at most d, over q variables, is at most (8edr/q)q (for r > q > 2).
In order to apply these bounds to low-rank matrices, recall that any matrix X of rank at most k can be written as a product X = U V where U                                                    nk                 km                                                                                                   R           and V  R           . Consider the k(n+m) entries of U, V as variables, and the nm entries of X as polynomials of degree two over these variables:                                                                        k
                                             Xia =                           UiVa                                                                      =1 Applying Corollary 3 we obtain:                                                       k(n+m) Lemma 4. f (n, m, k)                8e2nm                                (16em/k)k(n+m)                                      k(n+m)

Substituting this bound in (3) establishes Theorem 1. The upper bound on f (n, m, k) is tight up to a multiplicative factor in the exponent:                                                                 1 Lemma 5. For m > k2, f (n, m, k)  m (k-1)n                                                                 2
Proof. Fix any matrix V                  mk                                      R           with rows in general position, and consider the number f (n, V, k) of sign configurations of matrices U V , where U varies over all n  k matrices. Focusing only on +/- sign configurations (no zeros in U V ), each row of sign U V is a homogeneous linear classification of the rows of V , i.e. of m vectors in general position in     k                                              m       R . There are exactly     2         k-1                   possible homogeneous linear classifications of m                                           i=0         i vectors in general position in            k                                       R , and so these many options for each row of sign U V . We can therefore bound:                                                                  n                                                k-1                                                                                         n                     n(k-1)        1 f (n, m, k)  f (n, V, k)                2           m                          m                    m               = m (k-1)n                                                                                                                             2                                                            i                     k-1               k-1                                                i=0
4    Generalization Error Bounds for Other Loss Functions
In Section 2 we considered generalization error bounds for a zero-one loss function. More commonly, though, other loss functions are used, and it is desirable to obtain generalization error bounds for general loss functions.
When dealing with other loss functions, the magnitude of the entries in the matrix are important, and not only their signs. It is therefore no longer enough to bound the number of sign configurations. Instead, we will bound not only the number of ways low rank matrices behave with regards to a threshold of zero, but the number of possible ways low- rank matrices can behave relative to any set of thresholds. That is, for any threshold matrix T          nm        R           , we will show that the number of possible sign configurations of (X - T ), where X is low-rank, is small. Intuitively, this captures the complexity of the class of low-rank matrices not only around zero, but throughout all possible values.
We then use standard results from statistical machine learning to obtain generalization error bounds from the bound on the number of relative sign configurations. The number of rela- tive sign configurations serves as a bound on the pseudodimension--the maximum number of entries for which there exists a set of thresholds such that all relative sign configurations (limited to these entries) is possible. The pseudodimension can in turn be used to show the existence of a small -net, which is used to obtain generalization error bounds.
Recall the definition of the pseudodimension of a class of real-valued functions:
Definition 1. A class F of real-valued functions pseudo-shatters the points x1, . . . , xn with thresholds t1, . . . , tn if for every binary labeling of the points (s1, . . . , sn)  {+, -}n there exists f  F s.t. f (xi)  ti iff si = -. The pseudodimension of a class F is the supremum over n for which there exist n points and thresholds that can be shattered.
In order to apply known results linking the pseudodimension to covering numbers, we consider matrices X               nm                               R            as real-valued functions X : [n]  [m]  R over index pairs to entries in the matrix. The class Xk of rank-k matrices can now be seen as a class of real-valued functions over the domain [n]  [m]. We bound the pseudodimension of this class by bounding, for any threshold matrix T                          nm                                                                         R           the number of relative sign matrices:
    F                                                                           nm              T (n, m, k) = {sign (X - T )  {-, 0, +}nm|X  R                             , rank X  k}                                          fT (n, m, k) = FT (n, m, k)

                                                                                 k(n+m) Lemma 6. For any T                 nm                                R           , we have fT (n, m, k)  16em                       .                                                                                k

Proof. We take a similar approach to that of Lemma 4, writing rank-k matrices as a product X = U V where U                  nk                 km                              R           and V  R              . Consider the k(n + m) entries of U, V as variables, and the nm entries of X - T as polynomials of degree two over these variables:
                                                       k

                                 (X - T )ia =               UiVa - Tia                                                       =1

Applying Corollary 10 yields the desired bound.
Corollary 7. The pseudodimension of the class Xk of n  m matrices over the reals of rank at most k, is at most k(n + m) log 16em .                                                       k
We can now invoke standard generalization error bounds in terms of the pseudodimension (Theorem 11 in the Appendix) to obtain:
Theorem 8. For any monotone loss function with |loss|  M , any matrix Y  {1}nm, n, m > 2,  > 0 and integer k, with probability at least 1 -  over choosing a subset S of entries in Y uniformly among all subsets of |S| entries:
                                                   k(n + m) log 16em log M|S| - log                                                                           k           k(n+m)        X,rank X<k D(X ; Y ) < DS (X ; Y ) + 6                                  |S|

5     Low-Rank Matrices as Combined Classifiers
Rank-k matrices are those matrices which are a sum of k rank-1 matrices. If we view matrices as functions from pairs of indices to the reals, we can think of rank-k matrices as ""combined"" classifiers, and attempt to bound their complexity as such, based on the low complexity of the ""basis"" functions, i.e. rank-1 matrices.
A similar approach is taken in related work on learning with low-norm (maximum margin) matrix factorization [11, 12], where the hypothesis class can be viewed as a convex combi- nation of rank-1 unit-norm matrices. Scale-sensitive (i.e. dependent on the margin, or the slope of the loss function) generalization error bounds for this class are developed based on the graceful behavior of scale-sensitive complexity measures (e.g. log covering numbers and the Rademacher complexity) with respect to convex combinations. Taking a similar view, it is possible to obtain scale-sensitive generalization error bounds for low-rank ma- trices. In this Section we question whether it is possible to obtain scale-insensitive bounds, similar to Theorems 1 and 8, by viewing low-rank matrices as combined classifiers.
It cannot be expected that scale-insensitive complexity would be preserved when taking convex combinations of an unbounded number of base functions. However, the VC- dimension, a scale-insensitive measure of complexity, does scale gracefully when taking linear combinations of a bounded number of functions from a low VC-dimension class of indicator function. Using this, we can obtain generalization error bounds for linear com- binations of signs of rank-one matrices, but not signs of linear combinations of rank-one matrices. An alternate candidate scale-insensitive complexity measure is the pseudodi- mension of a class of real-valued functions. If we could bound the pseudodimension of the class of sums of k functions from a bounded-pseudodimension base class of real valued functions, we could avoid the sign-configuration counting and obtain generalization error bounds for rank-k matrices. Unfortunately, the following counterexample shows that this is not possible.
Theorem 9. There exists a family F closed under scalar multiplication whose pseudodi- mension is at most five, and such that {f1 + f2|f1, f2  F } does not have a finite pseu- dodimension.
Proof. We describe a class F of real-valued functions over the positive integers N. To do so, consider a one-to-one mapping of finite sets of positive integers to the positive integers. For each A  N define two functions3, fA(x) = 2xA + 1xA and gA(x) = 2xA. Let F be the set of all scalar multiplications of these functions.
For every A  N , fA - gA is the indicator function of A, implying that every finite subset can be shattered, and the pseudodimension of {f1 + f2 : f1, f2  F } is unbounded.
It remains to show that the pseudodimension of F is less than six. To do so, we note that there are no positive integers A < B and x < y and positive reals ,  > 0 such that (2xB + 1) > 2xA and 2yB < (2yA + 1). It follows that for any A < B and any ,  > 0, on an initial segment (possibly empty) of N we have gB  fB  gA  fA while on the rest of N we have gA  fA < gB  fB. In particular, any pair of
 3We use A to refer both to a positive integer and the finite set it maps to.

functions (fA, fB) or (fA, gB) or (gA, gB) in F that are not associated with the same subset (i.e. A = B), cross each other at most once. This holds also when  or  are negative, as the functions never change signs.
For any six naturals x1 < x2 <    < x6 and six thresholds, consider the three labellings (+, -, +, -, +, -), (-, +, -, +, -, +), (+, +, -, -, +, +). The three functions realizing these labellings must cross each other at least twice, but by the above arguments, there are no three functions in F such that every pair crosses each other at least twice.4
6          Discussion
Alon, Frankl and Rodl [8] use a result of Milnor similar to Warren's Theorem 2. Milnor's and Warren's theorems were previously used for bounding the VC-dimension of certain geometric classes [13], and of general concept classes parametrized by real numbers, in terms of the complexity of the boolean formulas over polynomials used to represent them [14]. This last general result can be used to bound the VC-dimension of signs of nm rank- k matrices by 2k(n + m) log(48enm), yielding a bound similar to Theorem 1 with an extra log |S| term. In this paper, we take a simpler path, applying Warren's theorem directly, and thus avoiding the log |S| term and reducing the other logarithmic term. Applying Warren's theorem directly also enables us to bound the pseudodimension and obtain the bound of Theorem 8 for general loss functions.
Another notable application of Milnor's result, which likely inspired these later uses, is for bounding the number of configurations of n points in                      d                                                                        R with different possible linear clas- sifications [15, 16]. Viewing signs of rank-k n  m matrices as n linear classification of m points in         k                  R , this bound can be used to bound f (n, m, k) < 2km log 2n+k(k+1)n log n with- out using Warren's Theorem directly [8, 12]. The bound of Lemma 4 avoids the quadratic dependence on k in the exponent.
Acknowledgments              We would like to thank Peter Bartlett for pointing out [13, 14]. N.S. and T.J. would like to thank Erik Demaine for introducing them to oriented matroids.
A           Proof of Corollary 3
Consider a set R            q                            R containing one variable configuration for each possible sign pattern. Set      .      = 1 min                                                                                                    (x) = P            2     1iq,xRPi(x)=0 |Pi(x)| > 0. Now consider the 2q polynomials P +                                                                                                            i               i(x) + and P -(x) = P                                        q |         (x) = 0, P -(x) = 0 . Different points in R             i          i(x) -     and C =    x  R       iP +                                                               i                     i (representing all sign configurations) lie in different connected components of C . Invoking Theorem 2 on C establishes Corollary 3.
The count in Corollary 3 differentiates between positive, negative and zero signs. However, we are only concerned with the positivity of YiaXia (in the proof of Theorem 1) or of Xia - Tia (in the proof of Theorem 8), and do not need to differentiate between zero and negative values. Invoking Theorem 2 on C+ = x               q                                   R |iP +(x) = 0 , yields:                                         i
Corollary 10. The number of -/+ sign configurations (where zero is considered negative) of r poly- nomials, each of degree at most d, over q variables, is at most (4edr/q)q (for r > q > 2).
Applying Corollary 10 on the nm degree-two polynomials Y                                 k                                                                               ia                U                                                                                          =1         iVa establishes that for any Y , the number of configurations of sign agreements of rank-k matrices with Y is bounded by (8em/k)k(n+m) and yields a constant of 8 instead of 16 inside the logarithm in Theorem 1. Applying Corollary 10 instead of Corollary 3 allows us to similarly tighten in the bounds in Corollary 7 and in Theorem 8.
      4A more careful analysis shows that F has pseudodimension three.

B     Generalization Error Bound in terms of the Pseudodimension
Theorem 11. Let F be a class of real-valued functions f : X  R with pseudodimension d, and loss : R  Y  R be a bounded monotone loss function (i.e.               for all y, loss(x, y) is mono- tone in x), with loss < M . For any joint distribution over (X, Y ), consider an i.i.d. sample S = (X1, Y1), . . . , (Xn, Yn). Then for any      > 0:
                                        n                                                 d                                        1                                              32eM            2 n Pr    fF EX,Y [loss(f (X), Y )] >              loss(f (Xi), Yi) +    < 4e(d + 1)                 e- 32  S                                     n i=1

The bound is a composition of a generalization error bound in terms of the L1 covering number [17, Theorem 17.1], a bound on the L1 covering number in terms of the pseudodimension [18] and the observation that composition with a monotone function does not increase the pseudodimension [17, Theorem 12.3]."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/f21e255f89e0f258accbe4e984eef486-Abstract.html,Incremental Learning for Visual Tracking,"Jongwoo Lim, David A. Ross, Ruei-sung Lin, Ming-Hsuan Yang","Most existing tracking algorithms construct a representation of a target           object prior to the tracking task starts, and utilize invariant features to           handle appearance variation of the target caused by lighting, pose, and           view angle change. In this paper, we present an efficient and effec-           tive online algorithm that incrementally learns and adapts a low dimen-           sional eigenspace representation to reflect appearance changes of the tar-           get, thereby facilitating the tracking task. Furthermore, our incremental           method correctly updates the sample mean and the eigenbasis, whereas           existing incremental subspace update methods ignore the fact the sample           mean varies over time. The tracking problem is formulated as a state           inference problem within a Markov Chain Monte Carlo framework and           a particle filter is incorporated for propagating sample distributions over           time. Numerous experiments demonstrate the effectiveness of the pro-           posed tracking algorithm in indoor and outdoor environments where the           target objects undergo large pose and lighting changes.
1     Introduction
The main challenges of visual tracking can be attributed to the difficulty in handling appear- ance variability of a target object. Intrinsic appearance variabilities include pose variation and shape deformation of a target object, whereas extrinsic illumination change, camera motion, camera viewpoint, and occlusions inevitably cause large appearance variation. Due to the nature of the tracking problem, it is imperative for a tracking algorithm to model such appearance variation.
Here we developed a method that, during visual tracking, constantly and efficiently up- dates a low dimensional eigenspace representation of the appearance of the target object. The advantages of this adaptive subspace representation are several folds. The eigenspace representation provides a compact notion of the ""thing"" being tracked rather than treating the target as a set of independent pixels, i.e., ""stuff"" [1]. The use of an incremental method continually updates the eigenspace to reflect the appearance change caused by intrinsic and extrinsic factors, thereby facilitating the tracking process. To estimate the locations of the target objects in consecutive frames, we used a sampling algorithm with likelihood estimates, which is in direct contrast to other tracking methods that usually solve complex optimization problems using gradient-descent approach.
The proposed method differs from our prior work [14] in several aspects. First, the pro- posed algorithm does not require any training images of the target object before the tracking task starts. That is, our tracker learns a low dimensional eigenspace representation on-line and incrementally updates it as time progresses (We assume, like most tracking algorithms, that the target region has been initialized in the first frame). Second, we extend our sam- pling method to incorporate a particle filter so that the sample distributions are propagated over time. Based on the eigenspace model with updates, an effective likelihood estimation function is developed. Third, we extend the R-SVD algorithm [6] so that both the sample mean and eigenbasis are correctly updated as new data arrive. Though there are numerous subspace update algorithms in the literature, only the method by Hall et al. [8] is also able
to update the sample mean. However, their method is based on the addition of a single col- umn (single observation) rather than blocks (a number of observations in our case) and thus is less efficient than ours. While our formulation provides an exact solution, their algorithm gives only approximate updates and thus it may suffer from numerical instability. Finally, the proposed tracker is extended to use a robust error norm for likelihood estimation in the presence of noisy data or partial occlusions, thereby rendering more accurate and robust tracking results.
2    Previous Work and Motivation Black et al. [4] proposed a tracking algorithm using a pre-trained view-based eigenbasis representation and a robust error norm. Instead of relying on the popular brightness con- stancy working principal, they advocated the use of subspace constancy assumption for visual tracking. Although their algorithm demonstrated excellent empirical results, it re- quires to build a set of view-based eigenbases before the tracking task starts. Furthermore, their method assumes that certain factors, such as illumination conditions, do not change significantly as the eigenbasis, once constructed, is not updated.
Hager and Belhumeur [7] presented a tracking algorithm to handle the geometry and illu- mination variations of target objects. Their method extends a gradient-based optical flow algorithm to incorporate research findings in [2] for object tracking under varying illumi- nation conditions. Prior to the tracking task starts, a set of illumination basis needs to be constructed at a fixed pose in order to account for appearance variation of the target due to lighting changes. Consequently, it is not clear whether this method is effective if a target object undergoes changes in illumination with arbitrary pose.
In [9] Isard and Blake developed the Condensation algorithm for contour tracking in which multiple plausible interpretations are propagated over time. Though their probabilistic ap- proach has demonstrated success in tracking contours in clutter, the representation scheme is rather primitive, i.e., curves or splines, and is not updated as the appearance of a target varies due to pose or illumination change.
Mixture models have been used to describe appearance change for motion estimation [3] [10]. In Black et al. [3] four possible causes are identified in a mixture model for estimating appearance change in consecutive frames, and thereby more reliable image motion can be obtained. A more elaborate mixture model with an online EM algorithm was recently proposed by Jepson et al. [10] in which they use three components and wavelet filters to account for appearance changes during tracking. Their method is able to handle variations in pose, illumination and expression. However, their WSL appearance model treats pixels within the target region independently, and therefore does not have notion of the ""thing"" being tracked. This may result in modeling background rather than the foreground, and fail to track the target.
In contrast to the eigentracking algorithm [4], our algorithm does not require a training phase but learns the eigenbases on-line during the object tracking process, and constantly updates this representation as the appearance changes due to pose, view angle, and illumi- nation variation. Further, our method uses a particle filter for motion parameter estimation rather than the Gauss-Newton method which often gets stuck in local minima or is dis- tracted by outliers [4]. Our appearance-based model provides a richer description than simple curves or splines as used in [9], and has notion of the ""thing"" being tracked. In addition, the learned representation can be utilized for other tasks such as object recog- nition. In this work, an eigenspace representation is learned directly from pixel values within a target object in the image space. Experiments show that good tracking results can be obtained with this representation without resorting to wavelets as used in [10], and better performance can potentially be achieved using wavelet filters. Note also that the view-based eigenspace representation has demonstrated its ability to model appearance of objects at different pose [13], and under different lighting conditions [2].
3         Incremental Learning for Tracking We present the details of the proposed incremental learning algorithm for object tracking in this section.
3.1       Incremental Update of Eigenbasis and Mean
The appearance of a target object may change drastically due to intrinsic and extrinsic factors as discussed earlier. Therefore it is important to develop an efficient algorithm to update the eigenspace as the tracking task progresses. Numerous algorithms have been developed to update eigenbasis from a time-varying covariance matrix as more data arrive [6] [8] [11] [5]. However, most methods assume zero mean in updating the eigenbasis except the method by Hall et al. [8] in which they consider the change of the mean when updating eigenbasis as each new datum arrives. Their update algorithm only handles one datum per update and gives approximate results, while our formulation handles multiple data at the same time and renders exact solutions.
We extend the work of the classic R-SVD method [6] in which we update the eigenbasis while taking the shift of the sample mean into account. To the best of our knowledge, this formulation with mean update is new in the literature.
Given a d  n data matrix A = {I1, . . . , In} where each column Ii is an observation (a d- dimensional image vector in this paper), we can compute the singular value decomposition (SVD) of A, i.e., A = U V . When a dm matrix E of new observations is available, the R-SVD algorithm efficiently computes the SVD of the matrix A = (A|E) = U  V based on the SVD of A as follows:
       1. Apply QR decomposition to and get orthonormal basis ~                                                                                                               E of E, and U = (U | ~                                                                                                                                                      E).

       2. Let V = V 0                               0 I          where Im is an m  m identity matrix. It follows then,                                      m


                   = U A V =                        U       (A|E) V 0                     =     U AV         U E               =        U E    .                                                          ~                                                          E                        0 Im               ~                                                                                                      E AV         ~                                                                                                                   E E                    0    ~                                                                                                                                               E E

       3. Compute the SVD of  = ~                                                                 U ~                                                                    ~                                                                        V     and the SVD of A is

                                       A = U ( ~                                                                 U ~                                                                    ~                                                                        V )V          = (U ~                                                                                                     U ) ~                                                                                                           ( ~                                                                                                              V V              ).

Exploiting the properties of orthonormal bases and block structures, the R-SVD algorithm computes the new eigenbasis efficiently. The computational complexity analysis and more details are described in [6].
One problem with the R-SVD algorithm is that the eigenbasis U is computed from AA with the zero mean assumption. We modify the R-SVD algorithm and compute the eigen- basis with mean update. The following derivation is based on scatter matrix, which is same as covariance matrix except a scalar factor.
Proposition 1 Let Ip = {I1, I2, . . . , In}, Iq= {In+1, In+2, . . . , In+m}, and Ir = (Ip|Iq). Denote the means and scatter matrices of Ip, Iq, Ir as Ip, Iq, Ir, and Sp, Sq, Sr respec- tively, then Sr = Sp + Sq + nm (I                                                n+m             q -                                                                     Ip)(Iq - Ip) .
Proof: By definition,                                     I                                                                                 r =         n    I                   I                                         (I                                                n+m p +                  m                                                                    n+m q ,           Ip - Ir = m                                                                                                              n+m         p -                                                                                                                                      Iq); Iq - Ir =      n    (I n+m         q -                   Ip) and,           Sr     =       n    (                                                           (                          i=1 Ii -                                            Ir)(Ii - Ir) +                         n+m                                                                                   i=n+1 Ii -                                                                                                       Ir)(Ii - Ir)                  =       n    (                          i=1 Ii -                                            Ip + Ip - Ir)(Ii - Ip + Ip - Ir) +                          n+m       (                          i=m+1 Ii -                                                 Iq + Iq - Ir)(Ii - Iq + Iq - Ir)                  =     Sp + n(Ip - Ir)(Ip - Ir) + Sq + m(Iq - Ir)(Iq - Ir)                  =     Sp + nm2 (                                                                                (                               (                I                                                                  I                               n+m)2                 p -                                                                Iq)(Ip - Iq) + Sq + n2m                                                                                                     (n+m)2             p -                                                                                                                                 Iq)(Ip - Iq)                  =     Sp + Sq + nm (I                                           n+m            p -                                                                  Iq)(Ip - Iq)
Let ^        Ip = {I1 - Ip, . . . , In - Ip}, ^                                                      Iq = {In+1 - Iq, . . . , In+m - Iq}, and ^                                                                                                            Ir = {I1 - Ir, . . . , In+m - Ir}, and the SVD of ^Ir = UrrVr . Let ~                                                                                       E =    ^                                                                                              Iq|     nm (I                                                                                                      n+m           p -                                                                                                                        Iq) ,
and use Proposition 1, Sr = (^                                              Ip| ~                                                 E)(^                                                        Ip| ~                                                                  E) . Therefore, we compute SVD on ( ^                                                                                                                    Ip| ~                                                                                                                       E) to get Ur. This can be done efficiently by the R-SVD algorithm as described above.
In summary, given the mean                                           Ip and the SVD of existing data Ip, i.e., UppVp and new data Iq, we can compute the the mean Ir and the SVD of Ir, i.e., UrrVr easily:
     1. Compute                        I                                                                r =     n    I                   I                                               (I                                   n+m p +             m                                                  n+m q , and ~                                                                       E = Iq - Ir 1(1m) |          nm                                                                                                      n+m      p -                                                                                                                        Iq) .

     2. Compute R-SVD with (UppVp ) and ~                                                                            E to obtain (UrrVr ).

In numerous vision problems, we can further exploit the low dimensional approximation of image data and put larger weights on the recent observations, or equivalently downweight the contributions of previous observations. For example as the appearance of a target object gradually changes, we may want to put more weights on recent observations in updating the eigenbasis since they are more likely to be similar to the current appearance of the target. The forgetting factor f can be used under this premise as suggested in [11] , i.e., A = (f A |E) = (U (f )V |E) where A and A are original and weighted data matrices, respectively.
3.2      Sequential Inference Model
The visual tracking problem is cast as an inference problem with a Markov model and hidden state variable, where a state variable Xt describes the affine motion parameters (and thereby the location) of the target at time t. Given a set of observed images It = {I1, . . . , It}. we aim to estimate the value of the hidden state variable Xt. Using Bayes' theorem, we have
              p(Xt| It)  p(It|Xt)                       p(Xt|Xt-1) p(Xt-1| It-1) dXt-1

The tracking process is governed by the observation model p(It|Xt) where we estimate the likelihood of Xt observing It, and the dynamical model between two states p(Xt|Xt-1). The Condensation algorithm [9], based on factored sampling, approximates an arbitrary distribution of observations with a stochastically generated set of weighted samples. We use a variant of the Condensation algorithm to model the distribution over the object's location, as it evolves over time.
3.3      Dynamical and Observation Models
The motion of a target object between two consecutive frames can be approximated by an affine image warping. In this work, we use the six parameters of affine transform to model the state transition from Xt-1 to Xt of a target object being tracked. Let Xt = (xt, yt, t, st, t, t) where xt, yt, t, st, t, t, denote x, y translation, rotation angle, scale, aspect ratio, and skew direction at time t. Each parameter in Xt is modeled independently by a Gaussian distribution around its counterpart in Xt-1. That is,                                         p(Xt|Xt-1) = N (Xt; Xt-1, ) where  is a diagonal covariance matrix whose elements are the corresponding variances of affine parameters, i.e., 2x, 2y, 2, 2                                .                                                           s , 2                                                                   , 2                                                                      
Since our goal is to use a representation to model the ""thing"" that we are tracking, we model the image observations using a probabilistic interpretation of principal component analysis [16]. Given an image patch predicated by Xt, we assume the observed image It was generated from a subspace spanned by U centered at . The probability that a sample being generated from the subspace is inversely proportional to the distance d from the sample to the reference point (i.e., center) of the subspace, which can be decomposed into the distance-to-subspace, dt, and the distance-within-subspace from the projected sample
to the subspace center, dw. This distance formulation, based on a orthonormal subspace and its complement space, is similar to [12] in spirit.
The probability of a sample generated from a subspace, pd (I                                                                         t     t|Xt), is governed by a Gaus- sian distribution:                                 pd (I                                    t     t | Xt) = N (It ; , U U + I ) where I is an identity matrix,  is the mean, and I term corresponds to the additive Gaus- sian noise in the observation process. It can be shown [15] that the negative exponential distance from It to the subspace spanned by U , i.e., exp(-||(It - ) - U U (It - )||2), is proportional to N (It; , U U + I) as   0.
Within a subspace, the likelihood of the projected sample can be modeled by the Maha- lanobis distance from the mean as follows:                                 pd (I                                    w      t | Xt) = N (It ; , U -2U ) where  is the center of the subspace and  is the matrix of singular values corresponding to the columns of U . Put together, the likelihood of a sample being generated from the subspace is governed by      p(It|Xt) = pd (I                    (I                       t     t|Xt) pdw          t|Xt) = N (It; , U U         + I) N (It; , U-2U ) (1)
Given a drawn sample Xt and the corresponding image region It, we aim to compute p(It|Xt) using (1). To minimize the effects of noisy pixels, we utilize a robust error norm [4], (x, ) =        x2     instead of the Euclidean norm d(x) = ||x||2, to ignore the ""outlier""                   2+x2 pixels (i.e., the pixels that are not likely to appear inside the target region given the current eigenspace). We use a method similar to that used in [4] in order to compute dt and dw. This robust error norm is helpful especially when we use a rectangular region to enclose the target (which inevitably contains some noisy background pixels).
4      Experiments To test the performance of our proposed tracker, we collected a number of videos recorded in indoor and outdoor environments where the targets change pose in different lighting con- ditions. Each video consists of 320  240 gray scale images and are recorded at 15 frames per second unless specified otherwise. For the eigenspace representation, each target image region is resized to 32  32 patch, and the number of eigenvectors used in all experiments is set to 16 though fewer eigenvectors may also work well.                      Implemented in MATLAB with MEX, our algorithm runs at 4 frames per second on a standard computer with 200 particles. We present some tracking results in this section and more tracking results as well as videos can be found at http://vision.ucsd.edu/~jwlim/ilt/.
4.1     Experimental Results
Figure 1 shows the tracking results using a challenging sequence recorded with a mov- ing digital camera in which a person moves from a dark room toward a bright area while changing his pose, moving underneath spot lights, changing facial expressions and taking off glasses. All the eigenbases are constructed automatically from scratch and constantly updated to model the appearance of the target object while undergoing appearance changes. Even with the significant camera motion and low frame rate (which makes the motions be- tween frames more significant, or equivalently to tracking fast moving objects), our tracker stays stably on the target throughout the sequence.
The second sequence contains an animal doll moving in different pose, scale, and lighting conditions as shown in Figure 2. Experimental results demonstrate that our tracker is able to follow the target as it undergoes large pose change, cluttered background, and lighting variation. Notice that the non-convex target object is localized with an enclosing rectan- gular window, and thus it inevitably contains some background pixels in its appearance representation. The robust error norm enables the tracker to ignore background pixels and estimate the target location correctly. The results also show that our algorithm faithfully
Figure 1: A person moves from dark toward bright area with large lighting and pose changes. The images in the second row shows the current sample mean, tracked region, reconstructed image, and the reconstruction error respectively. The third and forth rows shows 10 largest eigenbases.
Figure 2: An animal doll moving with large pose, lighting variation in a cluttered background.
models the appearance of the target, as shown in eigenbases and reconstructed images, in the presence of noisy background pixels.
We recorded a sequence to demonstrate that our tracker performs well in outdoor environ- ment where lighting conditions change drastically. The video was acquired when a person walking underneath a trellis covered by vines.          As shown in Figure 3, the cast shadow changes the appearance of the target face drastically. Furthermore, the combined pose and lighting variation with low frame rate makes the tracking task extremely difficult. Nev- ertheless, the results show that our tracker successfully follows the target accurately and robustly. Due to heavy shadows and drastic lighting change, other tracking methods based on gradient, contour, or color information are unlikely to perform well in this case."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/f2e43fa3400d826df4195a9ac70dca62-Abstract.html,Face Detection --- Efficient and Rank Deficient,"Wolf Kienzle, Matthias O. Franz, Bernhard Schölkopf, Gökhan H. Bakir","This paper proposes a method for computing fast approximations to sup-          port vector decision functions in the field of object detection. In the          present approach we are building on an existing algorithm where the set          of support vectors is replaced by a smaller, so-called reduced set of syn-          thesized input space points. In contrast to the existing method that finds          the reduced set via unconstrained optimization, we impose a structural          constraint on the synthetic points such that the resulting approximations          can be evaluated via separable filters. For applications that require scan-          ning large images, this decreases the computational complexity by a sig-          nificant amount. Experimental results show that in face detection, rank          deficient approximations are 4 to 6 times faster than unconstrained re-          duced set systems.
1     Introduction
It has been shown that support vector machines (SVMs) provide state-of-the-art accuracies in object detection. In time-critical applications, however, they are of limited use due to their computationally expensive decision functions. In particular, the time complexity of an SVM classification operation is characterized by two parameters. First, it is linear in the number of support vectors (SVs). Second, it scales with the number of operations needed for computing the similarity between an SV and the input, i.e. the complexity of the kernel function. When classifying image patches of size h  w using plain gray value features, the decision function requires an h  w dimensional dot product for each SV. As the patch size increases, these computations become extremely expensive. As an example, the evaluation of a single 20  20 patch on a 320  240 image at 25 frames per second already requires 660 million operations per second.
In the past, research towards speeding up kernel expansions has focused exclusively on the first issue, i.e. on how to reduce the number of expansion points (SVs) [1, 2]. In [2], Burges introduced a method that, for a given SVM, creates a set of so-called reduced set vectors (RSVs) that approximate the decision function. This approach has been successfully ap- plied in the image classification domain -- speedups on the order of 10 to 30 have been re- ported [2, 3, 4] while the full accuracy was retained. Additionally, for strongly unbalanced classification problems such as face detection, the average number of RSV evaluations can be further reduced using cascaded classifiers [5, 6, 7]. Unfortunately, the above example illustrates that even with as few as three RSVs on average (as in [5]), such systems are not competitive for time-critical applications.
The present work focuses on the second issue, i.e. the high computational cost of the kernel evaluations. While this could be remedied by switching to a sparser image representation (e.g. a wavelet basis), one could argue that in connection with SVMs, not only are plain gray values straightforward to use, but they have shown to outperform Haar wavelets and gradients in the face detection domain [8]. Alternatively, in [9], the authors suggest to com- pute the costly correlations in the frequency domain. In this paper, we develop a method that combines the simplicity of gray value correlations with the speed advantage of more sophisticated image representations. To this end, we borrow an idea from image processing: by constraining the RSVs to have a special structure, they can be evaluated via separable convolutions. This works for most standard kernels (e.g. linear, polynomial, Gaussian and sigmoid) and decreases the average computational complexity of the RSV evaluations from O(h  w) to O(r  (h + w)), where r is a small number that allows the user to balance be- tween speed and accuracy. To evaluate our approach, we examine the performance of these approximations on the MIT+CMU face detection database (used in [10, 8, 5, 6]).
2    Burges' method for reduced set approximations
The present section briefly describes Burges' reduced set method [2] on which our work is based. For reasons that will become clear below, h  w image patches are written as h  w matrices (denoted by bold capital letters) whose entries are the respective pixel intensities. In this paper, we refer to this as the image-matrix notation.
Assume that an SVM has been successfully trained on the problem at hand.                  Let {X1, . . . Xm} denote the set of SVs, {1, . . . m} the corresponding coefficients, k(, ) the kernel function and b the bias of the SVM solution. The decision rule for a test pattern X reads                                            m
                      f (X) = sgn             yiik(Xi, X) + b .                       (1)                                            i=1

In SVMs, the decision surface induced by f corresponds to a hyperplane in the reproducing kernel Hilbert space (RKHS) associated with k. The corresponding normal
                                       m

                                =            yiik(Xi, )                             (2)                                           i=1

can be approximated using a smaller, so-called reduced set (RS) {Z1, . . . Zm } of size m < m, i.e. an approximation to  of the form
                                        m

                                 =              ik(Zi, ).                           (3)                                            i=1

This speeds up the decision process by a factor of m/m . To find such  , we fix a desired set size m and solve                                     min  -  2RKHS                                        (4)
for i and Zi. Here,  RKHS denotes the Euclidian norm in the RKHS. The resulting RS decision function f is then given by
                                            m

                      f (X) = sgn                                             ik(Zi,X)+b                                             i=1                     .                     (5) In practice, i, Zi are found using a gradient based optimization technique. Details can be found in [2].

3       From separable filters to rank deficient reduced sets
We now describe the concept of separable filters in image processing and show how this idea extends to a broader class of linear filters and to a special class of nonlinear filters, namely those used by SVM decision functions. Using the image-matrix notation, it will become clear that the separability property boils down to a matrix rank constraint.
3.1      Linear separable filters
Applying a linear filter to an image amounts to a two-dimensional convolution of the image with the impulse response of the filter. In particular, if I is the input image, H the impulse response, i.e. the filter mask, and J the output image, then
                                           J = I  H.                                                 (6)

If H has size h  w, the convolution requires O(h  w) operations for each output pixel. However, in special cases where H can be decomposed into two column vectors a and b, such that                                                     H = ab                                                (7)
holds, we can rewrite (6) as                                            J = [I  a]  b ,                                              (8)
since the convolution is associative and in this case, ab                  = a  b . This splits the original problem (6) into two convolution operations with masks of size h1 and 1w, respectively. As a result, if a linear filter is separable in the sense of equation (7), the computational complexity of the filtering operation can be reduced from O(h  w) to O(h + w) per pixel by computing (8) instead of (6).
3.2      Linear rank deficient filters
In view of (7) being equivalent to rank(H)  1, we now generalize the above concept to linear filters with low rank impulse responses. Consider the singular value decomposition (SVD) of the h  w matrix H,                                               H = USV ,                                                   (9)
and recall that U and V are orthogonal matrices of size h  h and w  w, respectively, whereas S is diagonal (the diagonal entries are the singular values) and has size h  w. Now let r = rank(H). Due to rank(S) = rank(H), we may write H as a sum of r rank one matrices                                                       r
                                      H =               s u v                                                             i    i    i                                 (10)                                                      i=1

where si denotes the ith singular value of H and ui, vi are the iths columns of U and V (i.e. the ith singular vectors of the matrix H), respectively. As a result, the correspond- ing linear filter can be evaluated (analogously to (8)) as the weighted sum of r separable convolutions                                                r
                                   J =           si [I  ui]  vi                                   (11)                                               i=1

and the computational complexity drops from O(h  w) to O(r  (h + w)) per output pixel. Not surprisingly, the speed benefit depends on r, which can be seen to measure the structural complexity1 of H. For square matrices (w = h) for instance, (11) does not give any speedup compared to (6) if r > w/2.
   1In other words, the flatter the spectrum of HH , the less benefit can be expected from (11).

3.3      Nonlinear rank deficient filters and reduced sets
Due to the fact that in 2D, correlation is identical with convolution if the filter mask is rotated by 180 degrees (and vice versa), we can apply the above idea to any image filter f (X) = g(c(H, X)) where g is an arbitrary nonlinear function and c(H, X) denotes the correlation between images patches X and H (both of size h  w). In SVMs this amounts to using a kernel of the form
                                       k(H, X) = g(c(H, X)).                                                           (12)

If H has rank r, we may split the kernel evaluation into r separable correlations plus a scalar nonlinearity. As a result, if the RSVs in a kernel expansion such as (5) satisfy this constraint, the average computational complexity decreases from O(m  h  w) to O(m  r  (h + w)) per output pixel. This concept works for many off-the-shelf kernels used in SVMs. While linear, polynomial and sigmoid kernels are defined as functions of input space dot products and therefore immediately satisfy equation (12), the idea applies to kernels based on the Euclidian distance as well. For instance, the Gaussian kernel reads
                  k(H, X) = exp((c(X, X) - 2c(H, X) + c(H, H))).                                                      (13)

Here, the middle term is the correlation which we are going to evaluate via separable filters. The first term is independent of the SVs -- it can be efficiently pre-computed and stored in a separate image. The last term is merely a constant scalar independent of the image data. Finally, note that these kernels are usually defined on vectors. Nevertheless, we can use our image-matrix notation due to the fact that the squared Euclidian distance between two vectors of gray values x and z may be written as
                                            x - z 2 = X - Z 2 ,                                                        (14)                                                                                           F

whereas the dot product amounts to                                            1                              x z =               X 2 + Z 2 - X - Z 2 ,                                                     (15)                                            2           F                 F                               F where X and Z are the corresponding image patches and                                              F is the Frobenius norm for matrices.
4       Finding rank deficient reduced sets
In our approach we consider a special class of the approximations given by (3), namely those where the RSVs can be evaluated efficiently via separable correlations. In order to obtain such approximations, we use a constrained version of Burges' method. In particular, we restrict the RSV search space to the manifold spanned by all image patches that -- viewed as matrices -- have a fixed, small rank r (which is to be chosen a priori by the user). To this end, the Zi in equation (3) are replaced by their singular value decompositions
                                             Z                 S V                                                       i  Ui        i          i     .                                     (16)

The rank constraint can then be imposed by allowing only the first r diagonal elements of Si to be non-zero. Note that this boils down to using an approximation of the form
                                             m

                                =                                  S           V                                       r                     ik(Ui,r           i,r         i,r    , )                      (17)                                                 i=1

with Si,r being r  r (diagonal) and Ui,r, Vi,r being h  r, w  r (orthogonal2) matrices, respectively. Analogously to (4) we fix m and r and find Si,r, Ui,r, Vi,r and i that mini- mize the approximation error  =  -  2                                  .                                                              r                 The minimization problem is solved via                                                                   RKHS
   2In this paper we call a non-square matrix orthogonal if its columns are pairwise orthogonal and have unit length.

gradient decent. Note that when computing gradients, the image-matrix notation (together with (14) or (15), and the equality X 2 = tr(XX )) allows a straightforward computa-                                                 F tion of the kernel derivatives w.r.t. the components of the decomposed RSV image patches, i.e. the row, column and scale information in Vi,r, Ui,r and Si,r, respectively. However, while the update rules for i and Si,r follow immediately from the respective derivatives, care must be taken in order to keep Ui,r and Vi,r orthogonal during optimization. This can be achieved through re-orthogonalization of these matrices after each gradient step.
In our current implementation, however, we perform those updates subject to the so-called Stiefel constraints [11]. Intuitively, this amounts to rotating (rather than translating) the columns of Ui,r and Vi,r, which ensures that the resulting matrices are still orthogonal, i.e. lie on the Stiefel manifold. Let S(h, r) be the manifold of orthogonal h  r matrices, the (h, r)-Stiefel manifold. Further, let U denote an orthogonal basis for the orthogo-                                                      i,r nal complement of the subspace spanned by the columns of Ui,r. Now, given the 'free' gradient G = /Ui,r we compute the 'constrained' gradient
                                  ^                                    G = G - U                       G U                                                             i,r           i,r ,              (18)

which is the projection of G onto the tangent space of S(h, r) at Ui,r. The desired rotation is then given [11] by the (matrix) exponential of the h  h skew-symmetric matrix
                                       ^                                            G U                                      )                            A = t                    i,r      -( ^                                                                     G U                                                                              i,r                                            ^                                                 (19)                                            G U                       0                                                      i,r

where t is a user-defined step size parameter. For details, see [11]. A Matlab library is available at [12].
5      Experiments
This section shows the results of two experiments. The first part illustrates the behavior of rank deficient approximations for a face detection SVM in terms of the convergence rate and classification accuracy for different values of r. In the second part, we show how an actual face detection system, similar to that presented in [5], can be sped up using rank deficient RSVs. In both experiments we used the same training and validation set. It consisted of 19  19 gray level image patches containing 16081 manually collected faces (3194 of them kindly provided by Sami Romdhani) and 42972 non-faces automatically collected from a set of 206 background scenes. Each patch was normalized to zero mean and unit variance. The set was split into a training set (13331 faces and 35827 non-faces) and a validation set (2687 faces and 7145 non-faces). We trained a 1-norm soft margin SVM on the training set using a Gaussian kernel with  = 10. The regularization constant C was set to 1. The resulting decision function (1) achieved a hit rate of 97.3% at 1.0% false positives on the validation set using m = 6910 SVs. This solution served as the approximation target  (see equation (2)) during the experiments described below.
5.1    Rank deficient faces
In order to see how m and r affect the accuracy of our approximations, we compute rank deficient reduced sets for m = 1 . . . 32 and r = 1 . . . 3 (the left array in Figure 1 illustrates the actual appearance of rank deficient RSVs for the m = 6 case). Accuracy of the re- sulting decision functions is measured in ROC score (the area under the ROC curve) on the validation set. For the full SVM, this amounts to 0.99. The results for our approximations are depicted in Figure 2. As expected, we need a larger number of rank deficient RSVs than unconstrained RSVs to obtain similar classification accuracies, especially for small r. Nevertheless, the experiment points out two advantages of our method. First, a rank as
                                                 m' = 6                                                                    m' = 1

                                                                                                                     =         +     +     + ...          r=full                                                                                  r=full



                                                                                                       r=3           =         +     +                     r=3



                                                                                      r=2                            =         +    r=2



                      r=1                                                                                     r=1    =

Figure 1: Rank deficient faces. The left array shows the RSVs (Zi) of the unconstrained (top row) and constrained (r decreases from 3 to 1 down the remaining rows) approxi- mations for m = 6. Interestingly, the r = 3 RSVs are already able to capture face-like structures. This supports the fact that the classification accuracy for r = 3 is similar to that of the unconstrained approximations (cf. Figure 2, left plot). The right array shows the m = 1 RSVs (r = full, 3, 2, 1, top to bottom row) and their decomposition into rank one matrices according to (10). For the unconstrained RSV (first row) it shows an approximate (truncated) expansion based on the three leading singular vectors. While for r = 3 the decomposition is indeed similar to the truncated SVD, note how this similarity decreases for r = 2, 1. This illustrates that the approach is clearly different from simply finding un- constrained RSVs and then imposing the rank constraint via SVD (in fact, the norm (4) is smaller for the r = 1 RSV than for the leading singular vector of the r = full RSV).
low as three seems already sufficient for our face detection SVM in the sense that for equal sizes m there is no significant loss in accuracy compared to the unconstrained approxima- tion (at least for m > 2). The associated speed benefit over unconstrained RSVs is shown in the right plot of Figure 2: the rank three approximations achieve accuracies similar to the unconstrained functions, while the number of operations reduces to less than a third. Second, while for unconstrained RSVs there is no solution with a number of operations smaller than h  w = 361 (in the right plot, this is the region beyond the left end of the solid line), there exist rank deficient functions which are not only much faster than this, but yield considerably higher accuracies. This property will be exploited in the next experiment.
5.2                                A cascade-based face detection system
In this experiment we built a cascade-based face detection system similar to [5, 6], i.e. a cascade of RSV approximations of increasing size m . As the benefit of a cascaded classifier heavily depends on the speed of the first classifier which has to be evaluated on the whole image [5, 6], our system uses a rank deficient approximation as the first stage. Based on the previous experiment, we chose the m = 3, r = 1 classifier. Note that this function yields an ROC score of 0.9 using 114 multiply-adds, whereas the simplest possible unconstrained approximation m = 1, r = full needs 361 multiply-adds to achieve a ROC score of only 0.83 (cf. Figure 2). In particular, if the threshold of the first stage is set to yield a hit rate of 95% on the validation set, scanning the MIT+CMU set (130 images, 507 faces) with m = 3, r = 1 discards 91.5% of the false positives, whereas the m = 1, r = full can only reject 70.2%. At the same time, when scanning a 320  240 image3, the three separable convolutions plus nonlinearity require 55ms, whereas the single, full kernel evaluation takes 208ms on a Pentium 4 with 2.8 GHz. Moreover, for the unconstrained
                             3For multi-scale processing the detectors are evaluated on an image pyramid with 12 different scales using a scale decay of 0.75. This amounts to scanning 140158 patches for a 320  240 image.



           1                                                            1



          0.9                                                          0.9



          0.8                                                          0.8

ROC score                                                    ROC score                                              r=1                                                                    r=1               0.7                            r=2                           0.7                                      r=2                                              r=3                                                                    r=3                                              r=full                                                                 r=full               0.6                                                          0.6                      0                  1                                          2                   3                      4                10                 10                                              10                10                     10
                      #RSVs (m')                                                    #operations (m'r(h+w))

Figure 2: Effect of the rank parameter r on classification accuracies. The left plots shows the ROC score of the rank deficient RSV approximations (cf. Section 4) for varying set sizes (m = 1 . . . 32, on a logarithmic scale) and ranks (r = 1 . . . 3). Additionally, the solid line shows the accuracy of the RSVs without rank constraint (cf. Section 2), here denoted by r = full. The right plot shows the same four curves, but plotted against the number of operations needed for the evaluation of the corresponding decision function when scanning large images (i.e. m  r  (h + w) with h = w = 19), also on a logarithmic scale.
                                                   Figure 3: A sample output from our demonstra-                                                        tion system (running at 14 frames per second).                                                        In this implementation, we reduced the number                                                        of false positives by adjusting the threshold of                                                        the final classifier. Although this reduces the                                                        number of detections as well, the results are still                                                        satisfactory. This is probably due to the fact that                                                        the MIT+CMU set contains several images of                                                        very low quality that are not likely to occur in                                                        our setting, using a good USB camera.

cascade to catch up in terms of accuracy, the (at least) m = 2, r = full classifier (also with an ROC score of roughly 0.9) should be applied afterwards, requiring another 0.3  2  208 ms  125ms.
The subsequent stages of our system consist of unconstrained RSV approximations of size m = 4, 8, 16, 32, respectively. These sizes were chosen such that the number of false positives roughly halves after each stage, while the number of correct detections remains close to 95% on the validation set (with the decision thresholds adjusted accordingly). To eliminate redundant detections, we combine overlapping detections via averaging of posi- tion and size if they are closer than 0.15 times the estimated patch size. This system yields 93.1% correct detections and 0.034% false positives on the MIT+CMU set. The current system was incorporated into a demo application (Figure 3). For optimal performance, we re-compiled our system using the Intel compiler (ICC). The application now classifies a 320x240 image within 54ms (vs. 238ms with full rank RSVs only) on a 2.8 GHz PC. To further reduce the number of false positives, additional bootstrapped (as in [5]) stages need to be added to the cascade. Note that this will not significantly affect the speed of our sys- tem (currently 14 frames per second) since 0.034% false positives amounts to merely 47 patches to be processed by subsequent classifiers."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/f35a2bc72dfdc2aae569a0c7370bd7f5-Abstract.html,A Temporal Kernel-Based Model for Tracking Hand Movements from Neural Activities,"Lavi Shpigelman, Koby Crammer, Rony Paz, Eilon Vaadia, Yoram Singer","We devise and experiment with a dynamical kernel-based system for          tracking hand movements from neural activity. The state of the system          corresponds to the hand location, velocity, and acceleration, while the          system's input are the instantaneous spike rates. The system's state dy-          namics is defined as a combination of a linear mapping from the previous          estimated state and a kernel-based mapping tailored for modeling neural          activities. In contrast to generative models, the activity-to-state mapping          is learned using discriminative methods by minimizing a noise-robust          loss function. We use this approach to predict hand trajectories on the          basis of neural activity in motor cortex of behaving monkeys and find          that the proposed approach is more accurate than both a static approach          based on support vector regression and the Kalman filter.
1     Introduction
The paper focuses on the problem of tracking hand movements, which constitute smooth spatial trajectories, from spike trains of a neural population. We do so by devising a dynam- ical system which employs a tailored kernel for spike trains along with a linear mapping corresponding to the states' dynamics. Consider a situation where a subject performs free hand movements during a task that requires accurate space and time precision. In the lab, it may be a constrained reaching task while in real life it may be an every day task such as eating. We wish to track the hand position given only spike trains from a recorded neural population. The rationale of such an undertaking is two fold. First, this task can be viewed as a stem towards the development of a Brain Machine Interface (BMI) which gradually and rapidly become a possible future solution for the motor disabled patients. Recent studies of BMIs [13, 3, 10] (being on-line and feedback enabled) show that a relatively small number of cortical units can be used to move a cursor or a robot effectively, even without genera- tion of hand movements and that training of the subjects improves the overall success of the BMIs. Second, an open loop (off-line) movement decoding (see e.g. [7, 1, 15, 11, 8]), while inappropriate for BMIs, is computationally less expensive, easier to implement and allows repeated analysis thus providing a handle to understandings of neural computations in the brain.
Early studies [6] showed that the direction of arm movement is reflected by the population vector of preferred directions weighted by current firing ra tes, suggesting that intended
movement is encoded in the firing rate which, in turn, is modulated by the angle between a unit's preferred direction (PD) and the intended direction. This linear regression approach is still prevalent and is applied, with some variation of the learning methods, in closed and open loop settings. There is relatively little work on the development of dedicated nonlinear methods.
Both movement and neural activity are dynamic and can therefore be naturally modeled by dynamical systems. Filtering methods often employ generative probabilistic models such as the well known Kalman filter [16] or more neurally specialized models [1] in which a cortical unit's spike count is generated by a probability function of its underlying firing rate which is tuned to movement parameters. The movement, being a smooth trajectory, is modeled as a linear transition with (typically additive Gaussian) noise. These methods have the advantage of being aware of the smooth nature of movement and provide models of what neurons are tuned to. However, the requirement of describing a neural population's firing probability as a function of movement state is hard to satisfy without making costly assumptions. The most prominent is the assumption of statistical independence of cells given the movement.
Kernel based methods have been shown to achieve state of the art results in many applica- tion domains. Discriminative kernel methods, such as Support Vector Regression (SVR) forgo the task of modeling neuronal tuning functions. Furthermore, the construction of kernel induced feature spaces, lends itself to efficient implementation of distance measures over spike trains that are better suited to comparing two neural population trajectories than the Euclidean distance in the original space of spike counts per bins [11, 5]. However, SVR is a ""static"" method that does not take into account the smooth dynamics of the pre- dicted movement trajectory which imposes a statistical dependency between consecutive examples.
This paper introduces a kernel based regression method that incorporates linear dynamics of the predicted trajectories. In Sec. 2 we formally describe the problem setting. We intro- duce the movement tracking model and the associated learning framework in Sec. 3. The resulting learning problem yields a new kernel for linear dynamical systems. We provide an efficient calculation of this kernel and describe our dual space optimization method for solving the learning problem. The experimental method is presented in Sec. 4. Results, underscoring the merits of our algorithm are provided in Sec. 5 and conclusions are given in Sec. 6.
2    Problem Setting
Our training set contains m trials. Each trial (typically indexed by i or j) consists of a pair                                                                                             ti of movement and neural recordings, designated by                  Yi, Oi . Yi =       yi     end                                                                                        t            is a time                                                                                             t=1 series of movement state values and yi                                                t           Rd is the movement state vector at time t in trial i. We are interested in reconstructing position, however, for better modeling, yit may be a vector of position, velocity and acceleration (as is the case in Sec. 4). This trajectory is
observed during model learning and is the inference target. Oi = {ot}tiend                                                                                     t=1 is a time series of neural spike counts and oi                                t         Rq is a vector of spike counts from q cortical units at time t. We wish to learn a function zi = f Oi                                    t               1:t    that is a good estimate (in a sense formalized in the sequel) of the movement yit. Thus, f is a causal filtering method.
We confine ourselves to a causal setting since we plan to apply the proposed method in a closed loop scenario where real-time output is required. The partition into separate trajecto- ries is a natural one in a setting where a session is divided into many trials, each consisting of one attempt at accomplishing the basic task (such as reaching movements to displayed targets). In tasks that involve no hitting of objects, hand movements are typically smooth.
End point movement in small time steps is loosely approximated as having constant ac- celeration. On the other hand, neural spike counts (which are typically measured in bins of 50 - 100ms) vary greatly from one time step to the next. In summary, our goal is to devise a dynamic mapping from sequences of neural activities ending at a given time to the instantaneous hand movement characterization (location, velocity, and acceleration).
3    Movement Tracking Algorithm
Our regression method is defined as follows: given a series O  Rqtend of observations and, possibly, an initial state y0, the predicted trajectory Z  Rdtend is,                           zt    = Azt-1 + W (ot) , tend  t > 0 ,                                              (1)
where z0 = y0, A  Rdd is a matrix describing linear movement dynamics and W       Rdq is a weight matrix.  (ot) is a feature vector of the observed spike trains at time t and is later replaced by a kernel operator (in the dual formulation to follow). Thus, the state transition is a linear transformation of the previous state with the addition of a non-linear effect of the observation. Note that unfolding the recursion in Eq. (1) yields zt = Aty0 +                        t           At-kW (o                                                                                        k=1                   k )    . Assuming that A describes stable dynamics (the real parts of the eigenvalues of A are les than 1), then the current prediction depends, in an exponentially decaying manner, on the previous observations. We further assume that A is fixed and wish to learn W (we describe our choice of A in Sec. 4). In addition, ot may also encompass a series of previous spike counts in a window ending at time t (as is the case in Sec. 4). Also, note that this model (in its non-kernelized version) has an algebraic form which is similar to the Kalman filter (to which we compare our results later).
Primal Learning Problem:              The optimization problem presented here is identical to the standard SVR learning problem (see, for example [12]) with the exception that zit is defined as in Eq. (1) while in standard SVR, zt = W (ot) (i.e. without the linear dynamics). Given a training set of fully observed trials          Yi, Oi m           we define the learning problem                                                                    i=1 to be
                                                  ti                                  1               m     end    d                     min               W 2 + c                      zi          - yi           .                                                                     t             t                             (2)                      W           2                                        s            s                                                   i=1 t=1 s=1

Where W 2 =                 (W)2 (is the Forbenius norm). The second term is a sum of training                      a,b        ab errors (in all trials, times and movement dimensions). |  | is the  insensitive loss and is defined as |v| = max {0, |v| - }. The first term is a regularization term that promotes                small weights and c is a fixed constant providing a tradeoff between the regularization term and the training error. Note that to compensate for different units and scales of the movement dimensions one could either define a different s and cs for each dimension of the movement or, conversely, scale the sth movement dimension. The tracking method, combined with the optimization specified here, defines the complete algorithm. We name this method the Discriminative Dynamic Tracker or DDT in short.
A Dual Solution:         The derivation of the dual of the learning problem defined in Eq. (2) is rather mundane (e.g. [12]) and is thus omitted. Briefly, we replace the -loss with pairs of slack variables. We then write a Lagrangian of the primal problem and replace zit with its (less-standard) definition. We then differentiate the Lagrangian with respect to the slack variables and W and obtain a dual optimization problem. We present the dual dual problem in a top-down manner, starting with the general form and finishing with a kernel definition. The form of the dual is
      max      - 1 ( - )T G ( - ) + ( - )T y - ( + )T                     2           ,

                                        s.t. ,   [0, c]                                         .       (3)

Note that the above expression conforms to the dual form of SVR. Let equal the size of the movement space (d), multiplied by the total number of time steps in all the training trajecto- ries. ,   R are vectors of Lagrange multipliers, y  R is a column concatenation of                                                                                             T                        T         T all the training set movement trajectories                                          y11             ym                                                                                                         tm                            ,    = [, . . . , ]T  R                                                                                                               end
and G  R  is a Gram matrix (vT denotes transposition). One obvious difference be- tween our setting and the standard SVR lies within the size of the vectors and Gram matrix. In addition, a major difference is the definition of G. We define G here in a hierarchical manner. Let i, j  {1, . . . , m} be trajectory (trial) indexes. G is built from blocks indexed by Gij , which are in turn made from basic blocks, indexed by Kij                                                                                                                                tq as follows
           G11  G1m                                                                      Kij11  Kij1tj                                                                                                          .                                          .        G =           .                                   .                                                                .                . ... .                                                       , Gij =                   ..                     . .                 ..                 ,                .                                        .                                                                                                                      Gm1                             Gmm                                           Kij  Kij                                                                                                         ti       1                                                                                                                                                         end                                  ti          tj                                                                                                                                               end end

where block Gij refers to a pair of trials (i and j). Finally Each basic block, Kij                                                                                                                                                                tq refers to a
pair of time steps t and q in trajectories i and j respectively. ti                                                            , tj          are the time lengths                                                                                                                      end              end of trials i and j. Basic blocks are defined as                                                                t         q
                                          Kij =                                At-r kij Aq-s T ,                                                    tq                                            rs                                                                         (4)                                                               r=1 s=1

where kij = k oi , oj         rs                r         s    is a (freely chosen) basic kernel between the two neural observa- tions oir and ojs at times r and s in trials i and j respectively. For an explanation of kernel operators we refer the reader to [14] and mention that the kernel operator can be viewed as computing  oi   oj                                r                   s     where  is a fixed mapping to some inner product space. The choice of kernel (being the choice of feature space) reflects a modeling decision that specifies how similarities between neural patterns are measured. The resulting dual form of the tracker is zt =                                                                  k     k Gtk where Gt is the Gram matrix row of the new example.
It is therefore clear from Eq. (4) that the linear dynamic characteristics of DDT results in a Gram matrix whose entries depend on previous observations. This dependency is ex- ponentially decaying as the time difference between events in the trajectories grow. Note that solution of the dual optimization problem in Eq. (3) can be calculated by any stan- dard quadratic programming optimization tool. Also, note that direct calculation of G is inefficient. We describe an efficient method in the sequel.
Efficient Calculation of the Gram Matrix                                                    Simple, straight-forward calculation of the Gram matrix is time consuming. To illustrate this, suppose each trial is of length ti                                                                                      = n,                                                                                                                                                                   end then calculation of each basic block would take (n2) summation steps. We now describe a procedure based on dynamic-programming method for calculating the Gram matrix in a constant number of operations for each basic block.
Omitting the indexing over trials to ease notation, we are interested in calculating the basic block Ktq. First, define Btq =                                       t         k                                                                     k=1       kq At-k . the basic block Ktq can be recursively calculated in three different ways:                      Ktq                 = Kt(q-1)AT + Btq                                                                                                                  (5)
                 Ktq                 = AK(t-1)q + (Bqt)T                                                                                                                (6)

                 Ktq                 = AK(t-1)(q-1)AT + (Bqt)T + Btq - ktq .                                                                                            (7) Thus, by adding Eq. (5) to Eq. (6) and subtracting Eq. (7) we get               Ktq         = AK(t-1)q + Kt(q-1)AT - AK(t-1)(q-1)AT + ktqI . Btq (and the entailed summation) is eliminated in exchange for a 2D dynamic program with initial conditions: K11 = k11I , K1q = K1(q-1)AT + k1qI , Kt1 = AK(t-1)1 + kt1I.

Table 1: Mean R2, MAE & MSE (across datasets, folds, hands and directions) for each algorithm.                                                                               
                                                                                   R2                                            MAE                                     MSE                                       Algorithm                              pos.     vel.              accl.          pos.               vel.         accl.         pos.     vel.             accl.                                       Kalman filter                           0.64     0.58              0.30           0.40               0.15         0.37          0.78     0.27             1.16                                       DDT-linear                             0.59     0.49              0.17           0.63               0.41         0.58          0.97     0.50             1.23                                       SVR-Spikernel                          0.61     0.64              0.37           0.44               0.14         0.34          0.76     0.20             0.98                                       DDT-Spikernal                          0.73     0.67              0.40           0.37               0.14         0.34          0.50     0.16             0.91

                                 1



                                0.8

  Scores 2                                     0.6



                                0.4                                                                                                                                                                                       left hand, X dir.

                                                                                                                                                                                  left hand, Y dir.                                     0.2                 DDT-Spikernel, R                                                                                                                                                      right hand, X dir.

                                                                                                                                                                                  right hand, Y dir.

                                 00           0.2    0.4     0.6          0.8     1       0           0.2         0.4          0.6      0.8        1        0      0.2    0.4       0.6         0.8     1                                                    Kalman filter, R2 Scores                                DDT-linear, R2 Scores                                       SVR-Spikernel, R2 Scores

Figure 1: Correlation coefficients (R2, of predicted and observed hand positions) comparisons of the DDT-Spikernel versus the Kalman filter (left), DDT-linear (center) and SVR-Spikernel (right). Each data point is the R2 values obtained by the DDT-Spikernel and by another method in one fold of one of the datasets for one of the two axes of movement (circle / square) and one of the hands (filled/non-filled). Results above the diagonals are cases were the DDT-Spikernel outperformes.
Suggested Optimization Method.                                                                           One possible way to solve the optimization problem (essentially, a modification of the method described in [4] for classification) is to sequen- tially solve a reduced problem with respect to a single constraint at a time. Define:
                                         i =                  -                                           -           min                              -                                .                                                                         j          j Gij - yi                                                         j           j Gij - yi                                                                                                                       i,[0,c]                                                             j                                                                i                    j

Then i is the amount of -insensitive error that can be corrected for example i by keeping                                            ()                                                     () all                                               constant and changing                                 . Optimality is reached by iteratively choosing the                                            j=i                                                     i
example with the largest i and changing its () within the [0, c] limits to minimize the                                                                                                                               i error for this example.
4                                          Experimental Setting
The data used in this work was recorded from the primary motor cortex of a Rhesus (Macaca Mulatta) monkey (~4.5 kg). The monkey sat in a dark chamber, and up to 8 electrodes were introduced into MI area of each hemisphere. The electrode signals were amplified, filtered and sorted. The data used in this report was recorded on 8 different days and includes hand positions, sampled at 500Hz, spike times of single units (isolated by sig- nal fit to a series of windows) and of multi units (detection by threshold crossing) sampled at 1ms precision. The monkey used two planar-movement manipulanda to control 2 cur- sors on the screen to perform a center-out reaching task. Each trial began when the monkey centered both cursors on a central circle. Either cursor could turn green, indicating the hand to be used in the trial. Then, one of eight targets appeared ('go signal'), the center circle disappeared and the monkey had to move and reach the target to receive liquid reward. The number of multi-unit channels ranged from 5 to 15, the number of single units was 20-27 and the average total was 34 units per dataset. The average spike rate per channel was 8.2 spikes/sec. More information on the recordings can be found in [9].
                                              DDT (Spikernel)        DDT (Spikernel)                                                                                 DDT (Spikernel)


                         88.1%                          75%                                                  78.7%

             100%        Kalman Filter        SVR (Spikernel)         87.5%                        SVR (Spikernel)          91.88%

   100%                63.75%               99.4%            80.0%                              98.7%             86.3%

      SVR (Spikernel)             78.12%              96.3%      Kalman Filter                             95.6%      Kalman Filter


               62.5%                                             86.8%                                                84.4%


       DDT (Linear)                          DDT (Linear)                                         DDT (Linear)

Figure 2: Comparison of R2-performance between algorithms. Each algorithm is represented by a vertex. The weight of an edge between two algorithms is the fraction of tests in which the algorithm on top achieves higher R2 score than the other. A bold edge indicates a fraction higher than 95%. Graphs from left to right are for position, velocity, and acceleration respectively.
The results that we present here refer to prediction of instantaneous hand movements during the period from 'Go Signal' to 'Target Reach' times of both hands in successful trials. Note that some of the trials required movement of the left hand while keeping the right hand steady and vise versa. Therefore, although we considered only movement periods of the trials, we had to predict both movement and non-movement for each hand. The cumulative time length of all the datasets was about 67 minutes. Since the correlation between the movements of the two hands tend to zero - we predicted movement for each hand separately, choosing the movement space to be [x, y, vx, vy, ax, ay]T for each of the hands (preliminary results using only [x, y, vx, vy]T were less accurate).
We preprocessed the spike trains into spike counts in a running windows of 100ms (choice of window size is based on previous experience [11]). Hand position, velocity and acceler- ation were calculated using the 500Hz recordings. Both spike counts and hand movement were then sampled at steps of 100ms (preliminary results with step size 50ms were negli- gibly different for all algorithms). A labeled example yi, oi                                                                                       t    t    for time t in trial i consisted of the previous 10 bins of population spike counts and the state, as a 6D vector for the left or right hand. Two such consecutive examples would than have 9 time bins of spike count overlap. For example, the number of cortical units q in the first dataset was 43 (27 single and 16 multiple) and the total length of all the trials that were used in that dataset is 529 seconds. Hence in that session there are 5290 consecutive examples where each is a 4310 matrix of spike counts along with two 6D vectors of end point movement.
In order to run our algorithm we had to choose base kernels, their parameters, A and c (and , to be introduced below). We used the Spikernel [11], a kernel designed to be used with spike rate patterns, and the simple dot product (i.e. linear regression). Kernel parmeters and c were chosen (and subsequently held fixed) by 5 fold cross validation over half of the first dataset only. We compared DDT with the Spikernel and with the linear kernel to standard SVR using the Spikernel and the Kalman filter. We also obtained tracking results using both DDT and SVR with the standard exponential kernel. These results were slightly less accurate on average than with the Spikernel and are therefore omitted here. The Kalman filter was learned assuming the standard state space model (yt = Ayt-1 +  ,                                                           ot = Hyt +, where ,  are white Gaussian noise with appropriate correlation matrices) such as in [16]. y belonged to the same 6D state space as described earlier. To ease the comparison - the same matrix A that was learned for the Kalman filter was used in our algorithm (though we show that it is not optimal for DDT), multiplied by a scaling parameter . This parameter was selected to produce best position results on the training set. The selected  value is 0.8.
The figures that we show in Sec. 5 are of test results in 5 fold cross validation on the rest of the data. Each of the 8 remaining datasets was divided into 5 folds. 4/5 were used for
                                                                                                                               X              Y                                               R2        MAE    MSE    # Support


                                                                               14K

                                                                                       position      Position                                                                                    12K                                                                                                                                         Actual                                                                                                                                         DDT-Spikernel                                                                                                                                         SVR-Spikernel                                                                                    10K                  Velocity                                                                              velocity

                                                                                8K



                                                                                6K                              Acceleration                                                                                                                acceleration

Figure 3: Effect of  on R2, MAE ,MSE and                                                 Figure 4: Sample of tracking with the DDT-                                                                       
number of support vectors.                                                                Spikernel and the SVR-Spikernel.
training (with the parameters obtained previously and the remaining 1/5 as test set). This process was repeated 5 times for each hand. Altogether we had 8sets  5folds  2hands = 80 folds.
5                                            Results We begin by showing average results across all datasets, folds, hands and X/Y directions for the four algorithms that are compared. Table. 1 shows mean Correlation Coefficients (R2, between recorded and predicted movement values), Mean  insensitive Absolute Errors (MAE) and Mean Square Errors (MSE). R2 is a standard performance measure, MAE is the error minimized by DDT (subject to the regularization term) and MSE is minimized by the Kalman filter. Under all the above measures the DDT-Spikernel outperforms the rest with the SVR-Spikernel and the Kalman Filter alternating in second place.
To understand whether the performance differences are significant we look at the distribu- tion of position (X and Y) R2 values at each of the separate tests (160 altogether). Figure 1 shows scatter plots of R2 results for position predictions. Each plot compares the DDT- Spikernel (on the Y axis) with one of the other three algorithms (on the X axes). It is clear that in spite large differences in accuracy across datasets, the algorithm pairs achieve similar success with the DDT-Spikernel achieving a better R2 score in almost all cases.
To summarize the significance of R2 differences we computed the number of tests in which one algorithm achieved a higher R2 value than another algorithm (for all pairs, in each of the position, velocity and acceleration categories). The results of this tournament between the algorithms are presented in Figure 2 as winning percentages. The graphs produce a ranking of the algorithms and the percentages are the significances of the ranking between pairs. The DDT-Spikernel is significantly better then the rest in tracking position.
The matrix A in use is not optimal for our algorithm. The choice of  scales its effect. When  = 0 we get the standard SVR algorithm (without state dynamics). To illustrate the effect of  we present in Figure 3 the mean (over 5 folds, X/Y direction and hand) R2 results on the first dataset as a function of . It is clear that the value chosen to minimize position error is not optimal for minimizing velocity and acceleration errors. Another important effect of  is the number of the support patterns in the learned model, which drops considerably (by about one third) when the effect of the dynamics is increased. This means that more training points fall strictly within the -tube in training, suggesting that the kernel which tacitly results from the dynamical model is better suited for the problem. Lastly, we show a sample of test tracking results for the DDT-Spikernel and SVR-Spikernel in Figure 4. Note that the acceleration values are not smooth and are, therefore, least aided by the dynamics of the model. However, adding acceleration to the model improves the prediction of position.
6    Conclusion We described and reported experiments with a dynamical system that combines a linear state mapping with a nonlinear observation-to-state mapping. The estimation of the sys- tem's parameters is transformed to a dual representation and yields a novel kernel for tem- poral modelling. When a linear kernel is used, the DDT system has a similar form to the Kalman filter as t  . However, the system's parameters are set so as to minimize the regularized -insensitive 1 loss between state trajectories. DDT also bares similarity to SVR, which employs the same loss yet without the state dynamics. Our experiments indi- cate that by combining a kernel-induced feature space, linear state dynamics, and using a robust loss we are able to leverage the trajectory prediction accuracy and outperform com- mon approaches. Our next step toward an accurate brain-machine interface for predicting hand movements is the development of a learning procedure for the state dynamic mapping A and further developments of neurally motivated and compact representations.
Acknowledgments        This study was partly supported by a center of excellence grant (8006/00) administered by the ISF, BMBF-DIP, by the U.S. Israel BSF and by the IST Programme of the Eu- ropean Community, under the PASCAL Network of Excellence, IST-2002-506778. L.S. is supported by a Horowitz fellowship."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/f6185f0ef02dcaec414a3171cd01c697-Abstract.html,On-Chip Compensation of Device-Mismatch Effects in Analog VLSI Neural Networks,"Miguel Figueroa, Seth Bridges, Chris Diorio","Device mismatch in VLSI degrades the accuracy of analog arithmetic           circuits and lowers the learning performance of large-scale neural net-           works implemented in this technology. We show compact, low-power           on-chip calibration techniques that compensate for device mismatch. Our           techniques enable large-scale analog VLSI neural networks with learn-           ing performance on the order of 10 bits. We demonstrate our techniques           on a 64-synapse linear perceptron learning with the Least-Mean-Squares           (LMS) algorithm, and fabricated in a 0.35m CMOS process."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/f8da71e562ff44a2bc7edf3578c593da-Abstract.html,At the Edge of Chaos: Real-time Computations and Self-Organized Criticality in Recurrent Neural Networks,"Nils Bertschinger, Thomas Natschläger, Robert A. Legenstein","In this paper we analyze the relationship between the computational ca-            pabilities of randomly connected networks of threshold gates in the time-            series domain and their dynamical properties. In particular we propose            a complexity measure which we find to assume its highest values near            the edge of chaos, i.e. the transition from ordered to chaotic dynamics.            Furthermore we show that the proposed complexity measure predicts the            computational capabilities very well: only near the edge of chaos are            such networks able to perform complex computations on time series. Ad-            ditionally a simple synaptic scaling rule for self-organized criticality is            presented and analyzed.
1      Introduction
It has been proposed that extensive computational capabilities are achieved by systems whose dynamics is neither chaotic nor ordered but somewhere in between order and chaos. This has led to the idea of ""computation at the edge of chaos"". Early evidence for this hypothesis has been reported e.g. in [1]. The results of numerous computer simulations carried out in these studies suggested that there is a sharp transition between ordered and chaotic dynamics. Later on this was confirmed by Derrida and others [2]. They used ideas from statistical physics to develop an accurate mean-field theory which allowed to determine the critical parameters analytically. Because of the physical background, this theory focused on the autonomous dynamics of the system, i.e. its relaxation from an initial state (the input) to some terminal state (the output) without any external influences. In contrast to such ""off-line"" computations, we will focus in this article on time-series computations, i.e. mappings, also called filters, from a time-varying input signal to a time- varying output signal. Such ""online"" or real-time computations describe more adequately the input to output relation of systems like animals or autonomous robots which must react in real-time to a continuously changing stream of sensory input.
The purpose of this paper is to analyze how the computational capabilities of randomly connected recurrent neural networks in the domain of real-time processing and the type of dynamics induced by the underlying distribution of synaptic weights are related to each other. In particular, we will show that for the types of neural networks considered in this pa- per (defined in Sec. 2) there also exists a transition from ordered to chaotic dynamics. This phase transition is determined using an extension of the mean-field approach described in [3] and [4] (Sec. 3). As the next step we propose a novel complexity measure (Sec. 4) which
                                                                     input


                                                            network activity


           10


           20

       neuron # 30

                      0     20          40             0           20           40       0                             20        40                                timesteps                           timesteps                              timesteps

                                        0.4

                                        0.2                                                   0.8

                                         0                                                                                                  0.6                                            -0.2                   critical

                                       -0.4     ordered                       chaotic         0.4     mean activity                                            -0.6

                                                0.1           1                10                                                                        2

Figure 1: Networks of randomly connected threshold gates can exhibit ordered, critical and chaotic dynamics. In the upper row examples of the temporal evolution of the network state xt are shown (black: xi,t = 1, white: xi,t = 0, input as indicated above) for three different networks with parameters taken from the ordered, critical and chaotic regime, respectively. Parameters: K = 5, N = 500,                                                     u = -0.5, r = 0.3 and  and 2 as indicated in the phase plot below. The background of the phase plot shows the mean activity a (see Sec. 3) of the networks depending on the parameters  and 2.
can be calculated using the mean-field theory developed in Sec. 3 and serves as a predic- tor for the computational capability of a network in the time-series domain. Employing a recently developed framework for analyzing real-time computations [5, 6] we investigate in Sec. 5 the relationship between network dynamics and the computational capabilities in the time-series domain. In Sec. 6 of this paper we propose and analyze a synaptic scaling rule for self-organized criticality (SOC) for the types of networks considered here. In con- trast to previous work [7], we do not only check that the proposed rule shows adaptation towards critical dynamics, but also show that the computational capabilities of the network are actually increased if the rule is applied.
Relation to previous work: In [5], the so-called liquid state machine (LSM) approach was proposed and used do analyze the computational capabilities in the time-series domain of randomly connected networks of biologically inspired network models (composed of leaky integrate-and-fire neurons). We will use that approach to demonstrate that only near the edge of chaos, complex computations can be performed (see Sec. 5). A similar analysis for a restricted case (zero mean of synaptic weights) of the network model considered in this paper can be found in [4].
2    The Network Model and its Dynamics We consider input driven recurrent networks consisting of N threshold gates with states xi  {0, 1}. Each node i receives nonzero incoming weights wij from exactly K randomly chosen nodes j. Each nonzero connection weight wij is randomly drawn from a Gaussian distribution with mean  and variance 2. Furthermore, the network is driven by an exter- nal input signal u() which is injected into each node. Hence, in summary, the update of the network state xt = (x1,t, . . . , xN,t) is given by xi,t = (                                        N                  w                                                                                                          j=1                     ij  xj,t-1 + ut-1) which is applied to all neurons in parallel and where (h) = 1 if h  0 and (h) = 0 otherwise. In the following we consider a randomly drawn binary input signal u(): at each
time step ut assumes the value                                                  u + 1 with probability r and the value                                                                                          u with probability 1 - r. This network model is similar to the one we have considered in [4]. However it differs in two important aspects: a) By using states xi  {0, 1} we emphasis the asymmet- ric information encoding by spikes prevalent in biological neural systems and b) it is more general in the sense that the Gaussian distribution from which the non-zero weights are drawn is allowed to have an arbitrary mean   R. This implies that the network activity a                  N      t =    1             x             N      i=1         i,t can vary considerably for different parameters (compare Fig. 1) and enters all the calculations discussed in the rest of the paper.
The top row of Fig. 1 shows typical examples of ordered, critical and chaotic dynamics (see the next section for a definition of order and chaos). The system parameters corresponding to each type of dynamics are indicated in the lower panel (phase plot). We refer to the (phase) transition from the ordered to the chaotic regime as the critical line (shown as the solid line in the phase plot). Note that increasing the variance 2 of the weights consistently leads to chaotic behavior.
3      The Critical Line: Order and Fading Memory versus Chaos
To define the chaotic and ordered phase of an input driven network we use an approach which is similar to that proposed by Derrida and Pomeau [2] for autonomous systems: consider two (initial) network states with a certain (normalized) Hamming distance. These states are mapped to their corresponding successor states (using the same weight matrix) with the same input in each case and the change in the Hamming distance is observed. If small distances tend to grow this is a sign of chaos whereas if the distance tends to decrease this is a signature of order.
Following closely the arguments in [4, 3] we developed a mean-field theory (see [8] for all details) which allows to calculate the update dt+1 = f (dt, at, ut) of the normalized Hamming distance dt = |{i : xi,t = ~                                                        xi,t}|/N between two states xt and ~                                                                                             xt as well as the update at+1 = A(at, ut) of the network activity in one time step. Note that dt+1 depends on the input ut (in contrast to [3]) and also on the activity at (in contrast to [4]). Hence the two-dimensional map Fu(dt, at) := (dt+1, at+1) = (f (dt, at, ut), A(at, ut)) describes the time evolution of dt and at given the input times series u().
Let us consider the steady state of the averaged Hamming distance f  as well as the steady state of the averaged network activity a, i.e. (f , a) = limt F tu .1 If f  = 0 we know that any state differences will eventually die out and the network is in the ordered phase. If on the other hand a small difference is amplified and never dies out we have f  = 0 and the network is in the chaotic phase. Whether f  = 0 or f  = 0 can be decided by looking at the slope of the function f (, , ) at its fixed point f  = 0. Since at does not depend on dt we calculate the averaged steady state activity a and determine the slope  of the map rf (d, a,                                      u + 1) + (1 - r)f (d, a,                                                                u) at the point (d, a) = (0, a). Accordingly we say that the network is in the ordered, critical or chaotic regime if  < 1,  = 1 or  > 1 respectively. In [8] it is shown that the so called critical line  = 1 where the phase transition from ordered to chaotic behavior occurs is given by
             K-1    K - 1                                                                           1  Pbf =                                 an(1 - a)K-1-n(rQ(1, n,                                                                       u + 1) + (1 - r)Q(1, n,                                                                                                 u)) =         (1)                                n                                                                        K                  n=0

where Pbf denotes the probability (averaged over the inputs and the network activity) that a node will change its output if a single out of its K input bits is flipped.2 Examples of
  1F tu denotes t-fold composition of the map Fu(, ) where in the k-th iteration the input uk is applied and  denotes the average over all possible initial conditions and all input signals with a given statistics determined by                                             u and r.       2The actual single bit-flip probability Q depends on the number n of inputs which are 1 and the



                     K = 5                                                                   K = 10        0.4                                                                        0.4                                                 0.1        0.3                                                                        0.3                                   0.07

   0.2                                                                        0.2                                                 0.08                                                                    0.06

   0.1                                                                        0.1                                                                                                                         0.05           0                                                                         0                                                 0.06       -0.1                                                                      -0.1                                  0.04

  -0.2                                      0.04                             -0.2                                   0.03                                                         NM-Separation                                                             NM-Separation       -0.3                                                                       -0.3                                                                                                                         0.02       -0.4                                      0.02                             -0.4                                                                                                                         0.01       -0.5                                                                       -0.5

  -0.6                                      0                                -0.6                                   0                   0.1      1          10                                                  0.1      1          10                           2                                                                       2

Figure 2: N M -separation assumes high values on the critical line. The gray coded image shows the N M -separation in dependence on  and 2 for K denoted in the panels, r = 0.3,
u = -0.5 and b = 0.1. The solid line marks the critical values for  and 2.
critical lines that were calculated from this formula (marked by the solid lines) can be seen in Fig. 2 for K = 5 and K = 10.3
The ordered phase can also be described by using the notion of fading memory (see [5] and the references therein). Intuitively speaking in a network with fading memory a state xt is fully determined by a finite history ut-T , ut-T +1, . . . , ut-1, ut of the input u(). A slight reformulation of this property (see [6] and the references therein) shows that it is equivalent to the requirement that all state differences vanish, i.e. being in the ordered phase. Fading memory plays an important role in the ""liquid state machine"" framework [5] since together with the separation property (see below) it would in principle allow an appropriate readout function to deduce the recent input, or any function of it, from the network state. If on the other hand the network does not have fading memory (i.e. is in the chaotic regime) a given network state xt also contains ""spurious"" information about the initial conditions and hence it is hard or even impossible to deduce any features of the recent input.
4              NM-Separation as a Predictor for Computational Power
The already mentioned separation property [5] is especially important if a network is to be useful for computations on input time-series: only if different input signals separate the network state, i.e. different inputs result in different states, it is possible for a readout function to respond differently. Hence it is necessary that any two different input time series for which the readout function should produce different outputs drive the recurrent network into two sufficiently different states.
The mean field theory we have developed (see [8]) can be extended to describe the update dt+1 = s(dt, ...) of the Hamming distance that result from applying differ- ent inputs u() and ~                                   u() with a mean distance of b := Pr {ut = ~                                                                                                         ut}, i.e. the separa- tion. In summary the three-dimensional map Su,~u(dt, at, ~                                                                                            at) := (dt+1, at+1, ~at+1) = (s(dt, at, ~at, ut, ~                          ut), A(at, ut), A(~at, ~                                                      ut)) fully describes the time evolution of the Ham- ming distance and the network activities. Again we consider the steady state of the averaged Hamming distance s and the network activities a, ~                                                                                  a, i.e. (s, a, ~                                                                                                   a) = limt St                                  .                                                                                                                         u,~                                                                                                                              u
The overall separation for a given input statistics (determined by                                                                                                   u, r, and b) is then given by s. However, this overall separation measure can not be directly related to the computa-
external input u and is given by Q(1, n, u) =                            -u    (, n, n2) 1 - (-u - , , 2) d +                                                                          -
     (, n, n2)(-u - , , 2)d where ,  denote the Gaussian density and cumulative den-  -u

sity respectively (see [8] for a detailed explanation).         3For each value of  = -0.6 + k  0.01, k = 0 . . . 100 a search was conducted to find the value for 2 such that  = 1. Numerical iterations of the function A were used to determine a.
      A                                                        B                                                                 C                          3bit parity (K = 5)                                          3bit parity (K = 10)                                5bit random boolean functions            0.4                                                               5                                                                                                                                                                                  0.8            0.2                                                                                                       4                                                               4

                                                                                                                                                                             0.6                0                                                                                                     3                                                               3                 -0.2                                                     MC (MI)                                           2    MC (MI)                                                0.4    mean MI                                                               2


      -0.4                                                                                                                                                                   0.2                                                               1                                                      1


      -0.6                                                0                                                      0                                                           0                0.01      0.1      1           10       100                    0.01    0.1      1       10     100                    0.01       0.1     1           10    100                                  2                                                           2                                                       2

Figure 3: Real-time computation at the edge of chaos. A The gray coded image (an in- terpolation between the data points marked with open diamonds) shows the performance of trained networks in dependence of the parameters  and 2 for the delayed 3-bit par- ity task. Performance is measured as the memory capacity M C =                                                                                               I(v, y()) where                                                                                                                                                         I(v, y()) is the mutual information between the classifier output v() and the target func- tion y() = PARITY(u                     t                                 t- , ut- -1, ut- -2) measured on a test set. B Same as panel A but for K = 10. C Same as panel A but for an average over 50 randomly drawn Boolean functions f of the last 5 time steps, i.e. yt = f (ut, ut-1, ..., ut-4).
tional power since chaotic networks separate even minor differences in the input to a very high degree. The part of this separation that is caused by the input distance b and not by the distance of some initial state is therefore given by s - f  because f  measures the state distance that is caused by differences in the initial states and remains even after long runs with the same inputs (see Sec. 3). Note that f  is always zero in the ordered phase and non-zero in the chaotic phase.
Since we want the complexity measure, which we will call N M -separation, to be a predictor for computational power we correct s - f  by a term which accounts for the separation due to an all-dominant input drive. A suitable measure for this ""imme- diate separation"" i is the average increase in the Hamming distance if the system is run for a long time (t  ) with equal inputs u() = ~                                                                                                                                       u() and then a single step with an input pair (v, ~                                                      v) with an average difference of b = Pr {v, = ~                                                                                                                                                                v} is applied: i = lim                               1                          t                        rv(1-r)1-vb|v-~v|(1-b)1-|v-~v| s(, , , v, ~                                                                                                                                                 v)  St             -f . Hence                                        v,~                                             v=0                                                                                                              u,u a measure of the network mediated separation N Msep due to input differences is given by
                                                                          N Msep = s - f  - i                                                                             (2)

In Fig. 2 the N M -separation resulting from an input difference of b = 0.1 is shown in dependence of the network parameters  and 2.4 Note that the N M -separation peaks very close to the critical line. Because of the computational importance of the separation property this also suggests that the computational capabilities of the networks will peak at the onset of chaos, which is confirmed in the next section.
5              Real-Time Computations at the Edge of Chaos
To access the computational power of a network we make use of the so called ""liquid state machine"" framework which was proposed by Maass et.al. [5] and independently by Jaeger [6]. They put forward the idea that any complex time-series computation can be imple- mented by composing a system which consists of two conceptually different parts: a) a
       4For each value of  = -0.6 + k  0.05, k = 0 . . . 20, 10 values for 2 where chosen near the critical line and 10 other values where equally spaced (on a logarithmic scale) over the interval [0.02,50]. For each such pair (, 2) extensive numerical iterations of the map S where performed to obtain accurate estimates of s, f  and i. Hopefully these numerical estimates can be replaced by analytic results in the future.

properly chosen general-purpose recurrent network with ""rich"" dynamics and b) a read- out function that is trained to map the network state to the desired outputs (see [5, 6, 4] for more details). This approach is potentially successful if the general-purpose network encodes the relevant features of the input signal in the network state in such a way that the readout function can easily extract it. We will show that near the critical line the net- works considered in this paper encode the input in such a way that a simple linear classifier C(xt) = (w  xt + w0) suffices to implement a broad range of complex nonlinear fil- ters. Note that in order to train the network for a given task only the parameters w  RN , w0  R of the linear classifier are adjusted such that the actual network output vt = C(xt) is as close as possible to the target values yt.
To access the computational power in a principled way networks with different parameters were tested on a delayed 3-bit parity task for increasing delays and on randomly drawn Boolean functions of the last 5 input bits. Note that these tasks are quite complex for the networks considered here since most of them are not linear separable (i.e. the parity function) and require memory. Hence to achieve good performance it is necessary that a state xt contains information about several input bits ut , t < t in a nonlinear transformed form such that a linear classifier C is sufficient to perform the nonlinear computations.
The results are summarized in Fig. 3 where the performance (measured in terms of mutual information) on a test set between the network output and the target signal is shown for various parameter settings (for details see [4]). The highest performance is clearly achieved for parameter values close to the critical line where the phase transition occurs. This has been noted before [1]. In contrast to these previous results the networks used here are not optimized for any specific task but their computational capabilities are assessed by evaluating them for many different tasks. Therefore a network that is specifically designed for a single task will not show a good performance in this setup. These considerations suggest the following hypotheses regarding the computational function of generic recurrent neural circuits: to serve as a general-purpose temporal integrator, and simultaneously as a kernel (i.e., nonlinear projection into a higher dimensional space) to facilitate subsequent (linear) readout of information whenever it is needed.
6    Self-Organized Criticality via Synaptic Scaling Since the computational capabilities of a network depend crucially on having almost critical dynamics an adaptive system should be able to adjust its dynamics accordingly.
Equ. (1) states that critical dynamics are achieved if the probability Pbf that a single bit- flip in the input shows up in the output should on average (over the external and internal input statistics given by                                 u, r and a respectively) be equal to 1 . To allow for a rule that                                                                                  K can adjust the weights of each node a local estimate of Pbf must be available. This can be accomplished by estimating Pbf from the margin of each node, i.e. the distance of the internal activation from the firing threshold. Intuitively a node with an activation that is much higher or lower than its firing threshold is rather unlikely to change its output if a single bit in its input is flipped. Formally P i of node i is given by the average (over the                                                              bf internal and external input statistics) of the following quantity:
                1           N                                          (w                     K                           ij (1 - 2xj,t-1)(1 - 2xi,t) - mi,t)             (3)                           j=1,wij =0

where mi,t =       N      w                    j=1         ij xj,t-1 + ut-1 denotes the margin of node i (see [8] for details). Each node now applies synaptic scaling to adjust itself towards the critical line. Accord- ingly we arrive at the following SOC-rule:
                                            1     w                        (t) > 1                    w                       1+              ij     if P esti                                                                        bf             K                         ij (t + 1) =                                                            (4)                                            (1 + )  wij(t) if P esti(t) < 1                                                                        bf             K

A
                            50


                          100

        neuron #                               150


                          200                                                            100                 200            300                   400                 500            600            700                                                                                                      timesteps

B                                                                                                                  C                               1.5                                                                                         1.5
                           1                                                                                           1

                    bf                                                                                          bf

 K*P                                                                                                     K*P                               0.5                                                                                         0.5




                           0                                                                                           0                                 0           100     200      300      400       500    600    700                           0    100     200    300    400    500    600     700                                                                   timesteps                                                                      timesteps

Figure 4: Self-organized criticality. A Time evolution of the network state xt starting in a chaotic regime while the SOC-rule (4) is active (black: xi,t = 1, white: xi,t = 0). Parameters: N = 500, K = 5,                                                                                         u = -0.5, r = 0.3,  = 0 and initial 2 = 100. B Estimated Pbf . The dotted line shows how the node averaged estimate of Pbf evolves over time for the network shown in A. The running average of this estimate (thick black line) as used by the SOC-rule clearly shows that Pbf approaches its critical value (dashed line). C Same as B but for K = 10 and initial 2 = 0.01 in the ordered regime.
where 0 <                                                  1 is the learning rate and P esti(t) is a running average of the formula                                                                                                        bf in Equ. (3) to estimate P i . Applying this rule in parallel to all nodes of the network is                                                                         bf then able to adjust the network dynamics towards criticality as shown in Fig. 45. The upper row shows the time evolution of the network states xt while the SOC-rule (4) is running. It is clearly visible how the network dynamics changes from chaotic (the initial network had the parameters K = 5,  = 0 and 2 = 100) to critical dynamics that respect the input signal. The lower row of Fig. 4 shows how the averaged estimated bit-flip probability 1                                    N      P esti(t) approaches its critical value for the case of the above network and one N                                    i=1      bf that started in the ordered regime (K = 10,  = 0, 2 = 0.01). Since critical dynamics are better suited for information processing (see Fig. 3) it is ex- pected that the performance on the 3-bit parity task improves due to SOC. This is con- firmed in Fig. 5 which shows how the memory capacity M C (defined in Fig. 3) grows for networks that were initialized in the chaotic and ordered regime respectively. Note that the performance reached by these networks using the SOC-rule (4) is as high as for networks where the critical value for 2 is chosen apriori and stays at this level. This shows that rule (4) is stable in the sense that it keeps the dynamics critical and does not destroy the computational capabilities.
7                              Discussion
We developed a mean-field theory for input-driven networks which allows to determine the position of the transition line between ordered and chaotic dynamics with respect to the
                          5Here a learning rate of  = 0.01 and an exponentially weighted running average with a time constant of 15 time steps were used.

A                                                                        B                          K = 5, start 2 = 100 (chaotic)                                        K = 10, start 2 = 0.01 (ordered)
  5

                                                                                  4       4

                                                                                  3       3

MC [bits] 2                                                                          2                                                                          MC [bits]
  1                                                                               1


  0                                                                               0                 0         500         1000            1500      2000                       0       500         1000             1500       2000                                    SOC steps                                                                 SOC steps

Figure 5: Time evolution of the performance with activated SOC-rule. A The plot shows the memory capacity M C (see Fig. 3) on the 3-bit parity task averaged over 25 networks ( standard deviation as error-bars) evaluated at the indicated time steps. At each evaluation time step the network weights were fixed and the M C was measured as in Fig. 3 by training the corresponding readouts from scratch. The networks were initialized in the chaotic regime. B Same as in A but for K = 10 and networks initialized in the ordered regime.
parameters controlling the network connectivity and input statistics. Based on this theory we proposed a complexity measure (called N M -separation) which assumes its highest values at the critical line and shows a clear correlation with the computational power for real-time time-series processing. These results provide further evidence for the idea of ""computation at the edge of chaos"" [1] and support the hypothesis that dynamics near the critical line are expected to be a general property of input driven dynamical systems which support complex real-time computations. Therefore our analysis and the proposed complexity measure provide a new approach towards discovering dynamical principles that enable biological systems to do sophisticated information processing.
Furthermore we have shown that a local rule for synaptic scaling is able to adjust the weights of a network towards critical dynamics. Additionally networks adjusting them- selves by this rule have been found to exhibit enhanced computational capabilities. Thereby systems can combine task-specific optimization provided by (supervised) learning rules with self-organization of its dynamics towards criticality. This provides an explanation how specific information can be processed while still being able to react to incoming sig- nals in a flexible way.
Acknowledgement                        This work was supported in part by the PASCAL project #IST-2002-506778 of the European Community."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/fa2e8c4385712f9a1d24c363a2cbe5b8-Abstract.html,Trait Selection for Assessing Beef Meat Quality Using Non-linear SVM,"Juan Coz, Gustavo F. Bayón, Jorge Díez, Oscar Luaces, Antonio Bahamonde, Carlos Sañudo","In this paper we show that it is possible to model sensory impressions          of consumers about beef meat. This is not a straightforward task; the          reason is that when we are aiming to induce a function that maps object          descriptions into ratings, we must consider that consumers' ratings are          just a way to express their preferences about the products presented in          the same testing session. Therefore, we had to use a special purpose          SVM polynomial kernel. The training data set used collects the ratings of          panels of experts and consumers; the meat was provided by 103 bovines          of 7 Spanish breeds with different carcass weights and aging periods.          Additionally, to gain insight into consumer preferences, we used feature          subset selection tools. The result is that aging is the most important trait          for improving consumers' appreciation of beef meat."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/fb4ab556bc42d6f0ee0f9e24ec4d1af0-Abstract.html,Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes,"Yee W. Teh, Michael I. Jordan, Matthew J. Beal, David M. Blei","We propose the hierarchical Dirichlet process (HDP), a nonparametric Bayesian model for clustering problems involving multiple groups of data. Each group of data is modeled with a mixture, with the number of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing dependencies across groups to be modeled effectively as well as conferring generaliza- tion to new groups. Such grouped clustering problems occur often in practice, e.g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing the effective and superior performance of the HDP over previous models."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/fb89fd138b104dcf8e2077ad2a23954d-Abstract.html,Spike-timing Dependent Plasticity and Mutual Information Maximization for a Spiking Neuron Model,"Taro Toyoizumi, Jean-pascal Pfister, Kazuyuki Aihara, Wulfram Gerstner","We derive an optimal learning rule in the sense of mutual information maximization for a spiking neuron model. Under the assumption of small ﬂuctuations of the input, we ﬁnd a spike-timing dependent plas- ticity (STDP) function which depends on the time course of excitatory postsynaptic potentials (EPSPs) and the autocorrelation function of the postsynaptic neuron. We show that the STDP function has both positive and negative phases. The positive phase is related to the shape of the EPSP while the negative phase is controlled by neuronal refractoriness."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/fc03d48253286a798f5116ec00e99b2b-Abstract.html,Approximately Efficient Online Mechanism Design,"David C. Parkes, Dimah Yanovsky, Satinder P. Singh","Online mechanism design (OMD) addresses the problem of sequential            decision making in a stochastic environment with multiple self-interested            agents. The goal in OMD is to make value-maximizing decisions despite            this self-interest. In previous work we presented a Markov decision pro-            cess (MDP)-based approach to OMD in large-scale problem domains.            In practice the underlying MDP needed to solve OMD is too large and            hence the mechanism must consider approximations. This raises the pos-            sibility that agents may be able to exploit the approximation for selfish            gain. We adopt sparse-sampling-based MDP algorithms to implement -            efficient policies, and retain truth-revelation as an approximate Bayesian-            Nash equilibrium. Our approach is empirically illustrated in the context            of the dynamic allocation of WiFi connectivity to users in a coffeehouse.
1      Introduction
Mechanism design (MD) is concerned with the problem of providing incentives to im- plement desired system-wide outcomes in systems with multiple self-interested agents. Agents are assumed to have private information, for example about their utility for differ- ent outcomes and about their ability to implement different outcomes, and act to maximize their own utility. The MD approach to achieving multiagent coordination supposes the ex- istence of a center that can receive messages from agents and implement an outcome and collect payments from agents. The goal of MD is to implement an outcome with desired system-wide properties in a game-theoretic equilibrium.
Classic mechanism design considers static systems in which all agents are present and a one-time decision is made about an outcome. Auctions, used in the context of resource- allocation problems, are a standard example. Online mechanism design [1] departs from this and allows agents to arrive and depart dynamically requiring decisions to be made with uncertainty about the future. Thus, an online mechanism makes a sequence of decisions without the benefit of hindsight about the valuations of the agents yet to arrive. Without the issue of incentives, the online MD problem is a classic sequential decision problem.
In prior work [6], we showed that Markov decision processes (MDPs) can be used to define an online Vickrey-Clarke-Groves (VCG) mechanism [2] that makes truth-revelation by the agents (called incentive-compatibility) a Bayesian-Nash equilibrium [5] and implements a policy that maximizes the expected summed value of all agents. This online VCG model
differs from the line of work in online auctions, introduced by Lavi and Nisan [4] in that it assumes that the center has a model and it handles a general decision space and any decision horizon. Computing the payments and allocations in the online VCG mechanism involves solving the MDP that defines the underlying centralized (ignoring self-interest) decision making problem. For large systems, the MDPs that need to be solved exactly become large and thus computationally infeasible.
In this paper we consider the case where the underlying centralized MDPs are indeed too large and thus must be solved approximately, as will be the case in most real applications. Of course, there are several choices of methods for solving MDPs approximately. We show that the sparse-sampling algorithm due to Kearns et al. [3] is particularly well suited to online MD because it produces the needed local approximations to the optimal value and action efficiently. More challengingly, regardless of our choice the agents in the system can exploit their knowledge of the mechanism's approximation algorithm to try and ""cheat"" the mechanism to further their own selfish interests. Our main contribution is to demonstrate that our new approximate online VCG mechanism has truth-revelation by the agents as an -Bayesian-Nash equilibrium (BNE). This approximate equilibrium supposes that each agent is indifferent to within an expected utility of , and will play a truthful strategy in best- response to truthful strategies of other agents if no other strategy can improve its utility by more than . For any , our online mechanism has a run-time that is independent of the number of states in the underlying MDP, provides an -BNE, and implements a policy with expected value within     of the optimal policy's value.
Our approach is empirically illustrated in the context of the dynamic allocation of WiFi con- nectivity to users in a coffeehouse. We demonstrate the speed-up introduced with sparse- sampling (compared with policy calculation via value-iteration), as well as the economic value of adopting an MDP-based approach over a simple fixed-price approach.
2    Preliminaries
Here we formalize the multiagent sequential decision problem that defines the online mech- anism design (OMD) problem. The approach is centralized. Each agent is asked to report its private information (for instance about its value and its capabilities) to a central planner or mechanism upon arrival. The mechanism implements a policy based on its view of the state of the world (as reported by agents). The policy defines actions in each state, and the assumption is that all agents acquiesce to the decisions of the planner. The mechanism also collects payments from agents, which can themselves depend on the reports of agents.
Online Mechanism Design          We consider a finite-horizon problem with a set T of time points and a sequence of decisions k = {k1, . . . , kT }, where kt  Kt and Kt is the set of feasible decisions in period t. Agent i  I arrives at time ai  T , departs at time li  T , and has value vi(k)  0 for a sequence of decisions k. By assumption, an agent has no value for decisions outside of interval [ai, li]. Agents also face payments, which can be col- lected after an agent's departure. Collectively, information i = (ai, li, vi) defines the type of agent i with i  . Agent types are sampled i.i.d. from a probability distribution f (), assumed known to the agents and to the central mechanism. Multiple agents can arrive and depart at the same time. Agent i, with type i, receives utility ui(k, p; i) = vi(k; i) - p, for decisions k and payment p. Agents are modeled as expected-utility maximizers.
Definition 1 (Online Mechanism Design) The OMD problem is to implement the sequence of decisions that maximizes the expected summed value across all agents in equilibrium, given self-interested agents with private information about valuations.
In economic terms, an optimal (value-maximizing) policy is the allocatively-efficient, or simply the efficient policy. Note that nothing about the OMD models requires centralized
execution of the joint plan. Rather, the agents themselves can have capabilities to perform actions and be asked to perform particular actions by the mechanism. The agents can also have private information about the actions that they are able to perform.
Using MDPs to Solve Online Mechanism Design.                                                                   In the MDP-based approach to solv- ing the OMD problem the sequential decision problem is formalized as an MDP with the state at any time encapsulating both the current agent population and constraints on current decisions as reflected by decisions made previously. The reward function in the MDP is simply defined to correspond with the total reported value of all agents for all sequences of decisions.
Given types i  f () we define an MDP, Mf , as follows. Define the state of the MDP at time t as the history-vector ht = (1, . . . , t; k1, . . . , kt-1), to include the reported types up to and including period t and the decisions made up to and including period t - 1.1 The set of all possible states at time t is denoted Ht. The set of all possible states across all time is H =       T +1 H           t=1          t. The set of decisions available in state ht is Kt(ht). Given a decision kt  Kt(ht) in state ht, there is some probability distribution Prob(ht+1|ht, kt) over possible next states ht+1. In the setting of OMD, this probability distribution is determined by the uncertainty on new agent arrivals (as represented within f ()), together with departures and the impact of decision kt on state.
The payoff function for the induced MDP is defined to reflect the goal of maximizing the total expected reward across all agents. In particular, payoff Ri(ht, kt) = vi(kt; i) - vi(kt-1; i) becomes available from agent i upon taking action kt in state ht. With this, we have                 Ri(h                  t=1             t, kt) = vi(k ; i), for all periods  to provide the required cor- respondence with agent valuations. Let R(ht, kt) =                                                                                 Ri(h                                                                                                                               i            t, kt), denote the payoff obtained from all agents at time t. Given a (nonstationary) policy  = {1, 2, . . . , T } where t : Ht  Kt, an MDP defines an MDP-value function V  as follows: V (ht) is the expected value of the summed payoff obtained from state ht onwards under policy , i.e., V (ht) = E{R(ht, (ht)) + R(ht+1, (ht+1)) +    + R(hT , (hT ))}. An optimal policy  is one that maximizes the MDP-value of every state in H.
The optimal MDP-value function V  can be computed by value-iteration, and is defined so that V (h) = maxkK                                                                                       P rob(h |h, k)V (h )] for t = T -                                                t (h) [R(h, k) +                     h Ht+1 1, T - 2, . . . , 1 and all h  Ht, with V (h  HT ) = maxkKT (h) R(h, k). Given the optimal MDP-value function, the optimal policy is derived as follows: for t < T , policy (h  Ht) chooses action arg maxkK                                                                                                         P rob(h |h, k)V (h )]                                                                               t (h) [R(h, k) +                                     h Ht+1
and (h  HT ) = arg maxkKT (h) R(h, k). Let ^                                                                                                               t denote reported types up to and including period t . Let Ri                         (^                                                                                                t             t ; ) denote the total reported reward to agent i up to and including period t . The commitment period for agent i is defined as the first period, mi, for which t  mi, Ri                         (^                                                               ; ) = Ri (^                                                                                                                            ; )                                                                                                                                    , for any types                         still to                                     m               m                                   m                                          i                i                   t               i                   >mi                                                >mi arrive. This is the earliest period in which agent i's total value is known with certainty.
Let ht (^           t ; ) denote the state in period t given reports ^                                                                                                                          t . Let ^                                                                                                                                              t \i = ^                                                                                                                                                                 t \ ^                                                                                                                                                                              i.
Definition 2 (Online VCG mechanism) Given history h  H, mechanism Mvcg = (; , pvcg) implements policy  and collects payment,
pvcg(^                       ; )    =         Ri              (^                                                                          ; ) - V (h (^                                                                                                                              ; )) - V (h (^                                                                                                                                                                  i          mi                           m               m                                   ^                                                                                                     a              ^                                                                                                                     a                            ^                                                                                                                                                  a              ^                                                                                                                                                                  a                                                     i                i                                   i               i                            i               i \i ; ))    (1)
from agent i in some period t  mi.
1Using histories as state will make the state space very large. Often, there will be some function g for which g(h) is a sufficient statistic for all possible states h. We ignore this possibility here.
Agent i's payment is equal to its reported value discounted by the expected marginal value that it will contribute to the system as determined by the MDP-value function for the policy in its arrival period. The incentive-compatibility of the Online VCG mechanism requires that the MDP satisfies stalling which requires that the expected value from the optimal policy in every state in which an agent arrives is at least the expected value from following the optimal action in that state as though the agent had never arrived and then returning to the optimal policy. Clearly, property Kt(ht)  Kt(ht \ i) in any period t in which i has just arrived is sufficient for stalling. Stalling is satisfied whenever an agent's arrival does not force a change in action on a policy.
Theorem 1 (Parkes & Singh [6]) An online VCG mechanism, Mvcg = (; , pvcg), based on an optimal policy  for a correct MDP model that satisfies stalling is Bayesian- Nash incentive compatible and implements the optimal MDP policy.
3    Solving Very Large MDPs Approximately
From Equation 1, it can be seen that making outcome and payment decisions at any point in time in an online VCG mechanism does not require a global value function or a global policy. Unlike most methods for approximately solving MDPs that compute global approx- imations, the sparse-sampling methods of Kearns et al. [3] compute approximate values and actions for a single state at a time. Furthermore, sparse-sampling methods provide approx- imation guarantees that will be important to establish equilibrium properties -- they can compute an -approximation to the optimal value and action in a given state in time inde- pendent of the size of the state space (though polynomial in 1 and exponential in the time horizon). Thus, sparse-sampling methods are particularly suited to approximating online VCG and we adopt them here.
The sparse-sampling algorithm uses the MDP model Mf as a generative model, i.e., as a black box from which a sample of the next-state and reward distributions for any given state-action pair can be obtained. Given a state s and an approximation parameter , it computes an -accurate estimate of the optimal value for s as follows. We make the param- eterization on explicit by writing sparse-sampling( ). The algorithm builds out a depth-T sampled tree in which each node is a state and each node's children are obtained by sam- pling each action in that state m times (where m is chosen to guarantee an approximation to the optimal value of s), and each edge is labeled with the sample reward for that transi- tion. The algorithm computes estimates of the optimal value for nodes in the tree working backwards from the leaves as follows. The leaf-nodes have zero value. The value of a node is the maximum over the values for all actions in that node. The value of an action in a node is the summed value of the m rewards on the m outgoing edges for that action plus the summed value of the m children of that node. The estimated optimal value of state s is the value of the root node of the tree. The estimated optimal action in state s is the action that leads to the largest value for the root node in the tree.
Lemma 1 (Adapted from Kearns, Mansour & Ng [3]) The sparse-sampling( ) algorithm, given access to a generative model for any n-action MDP M , takes as input any state s  S and any > 0, outputs an action, and satisfies the following two conditions:
    (Running Time) The running time of the algorithm is O((nC)T ), where C =          f (n, 1 , Rmax, T ) and f is a polynomial function of the approximation parameter           1 , the number of actions n, the largest expected reward in a state Rmax and the          horizon T . In particular, the running time has no dependence on the number of          states.

    (Near-Optimality) The value function of the stochastic policy implemented by the          sparse-sampling( ) algorithm, denoted V ss, satisfies |V (s) - V ss(s)|         si-



               multaneously for all states s  S.

It is straightforward to derive the following corollary from the proof of Lemma 1 in [3].
Corollary 1 The value function computed by the sparse-sampling( ) algorithm, denoted ^ V ss, is near-optimal in expectation, i.e., |V (s) - E{ ^                                                                                                                 V ss(s)}|                  simultaneously for all states s  S and where the expectation is over the randomness introduced by the sparse- sampling( ) algorithm.
4         Approximately Efficient Online Mechanism Design
In this section, we define an approximate online VCG mechanism and consider the effect on incentives of substituting the sparse-sampling( ) algorithm into the online VCG mech- anism. We model agents as indifferent between decisions that differ by at most a utility of      > 0, and consider an approximate Bayesian-Nash equilibrium. Let vi(; ) denote the final value to agent i after reports  given policy .
Definition 3 (approximate BNE) Mechanism Mvcg = (, , pvcg) is -Bayesian-Nash in- centive compatible if
  E|          {vi(; ) - pvcg(; )} +                                           E                     {vi(-i, ^                                                                                                                                       i; ) - pvcg(-i, ^                                                                                                                                                                           i; )}(2)                                                                                              |               t                              i                                                       t                                        i

where agent i with type i arrives in period t , and with the expectation taken over future types given current reports t .
In particular, when truth-telling is an -BNE we say that the mechanism is -BNE incentive compatible and no agent can improve its expected utility by more than                                                                               > 0, for any type, as long as other agents are bidding truthfully. Equivalently, one can interpret an -BNE as an exact equilibrium for agents that face a computational cost of at least                                                                                    to compute the exact BNE.
Definition 4 (approximate mechanism) A sparse-sampling( ) based approximate online VCG mechanism, Mvcg( ) = (; ~                                                                    , ~                                                                              pvcg), uses the sparse-sampling( ) algorithm to imple- ment stochastic policy ~                                              and collects payment
 ~      pvcg(^                          ; ~                            )          =     Ri              (^                                                                             ; ~                                                                               ) - ^                                                                                          V ss(h (^                                                                                                                               ; ~                                                                                                                                 )) - ^                                                                                                                                           V ss(h (^                                                                                                                                                                      i            mi                            m              m                                 ^                                                                                                      a              ^                                                                                                                      a                              ^                                                                                                                                                     a              ^                                                                                                                                                                     a                                                         i               i                                 i               i                              i               i \i ; ~                                                                                                                                                                               ))

from agent i in some period t  mi, for commitment period mi.
Our proof of incentive-compatibility first demonstrates that an approximate delayed VCG mechanism [1, 6] is -BNE. With this, we demonstrate that the expected value of the pay- ments in the approximate online VCG mechanism is within 3 of the payments in the approximate delayed VCG mechanism. The delayed VCG mechanism makes the same decisions as the online VCG mechanism, except that payments are delayed until the final period and computed as:
                             pDvcg(^                                             ; ) = Ri (^                                                                              ; ) - R                                   i                                T                         T ( ^                                                                                                            ; ) - RT (^                                                                                                                                           -i; )                                    (3)

where the discount is computed ex post, once the effect of an agent on the system value is known. In an approximate delayed-VCG mechanism, the role of the sparse-sampling algorithm is to implement an approximate policy, as well as counterfactual policies for the worlds -i without each agent i in turn. The total reported reward to agents = i over this counterfactual series of states is used to define the payment to agent i.
Lemma 2 Truthful bidding is an -Bayesian-Nash equilibrium in the sparse-sampling( ) based approximate delayed-VCG mechanism.
Proof: Let ~                            denote the approximate policy computed by the sparse-sampling algorithm. Assume agents = i are truthful. Now, if agent i bids truthfully its expected utility is
                              E|                  {v                                       Rj (; ~                                                                                                                         ) -               Rj (                                              a               i(; ~                                                                      ) +                              T                                       T      -i; ~                                                                                                                                                                    )}                                (4)                                                    i

                                                                                  j=i                                       j=i

where the expectation is over both the types yet to be reported and the random- ness introduced by the sparse-sampling( ) algorithm.                                                                                       Substituting R
                                          V (ha (                          ; ~                                                                                   )) - V ss(h (                                                                 i         ai                                            ai    ai\i; ~                                                                                                                                             )) -                                                     (5)

because V ss(ha (                                 ; ~                                                     ))  V (h (                                           ; ~                                                                                                                   )) -              by Lemma 1. Now, ignore term                              i          ai                                           ai              ai RT (-i; ~                     ) in Equation (4), which is independent of agent i's bid ^                                                                                                                                                                    i, and consider the maximal expected utility to agent i from some non-truthful bid. The effect of ^                                                                                                                                                                                i on the first two terms is indirect, through a change in the policy for periods  ai. An agent can change the policy only indirectly, by changing the center's view of the state by misreporting its type. By definition, the agent can do no better than selecting optimal policy , which is defined to maximize the expected value of the first two terms. Putting this together, the expected utility from ^                                               i is at most V (ha (                                              ; ~                                                                                                                         )) - V ss(h (                                                                                                  i          ai                                  ai         ai\i; ~                                                                                                                                                                          )) and at most better than that from bidding truthfully.
Theorem 2 Truthful bidding is an 4 -Bayesian-Nash equilibrium in the sparse- sampling( ) based approximate online VCG mechanism.
Proof:         Assume agents = i bid truthfully, and consider report ^                                                                                                                                                              i. Clearly, the policy implemented in the approximate online-VCG mechanism is the same as in the delayed- VCG mechanism for all ^                                                         i. Left to show is that the expected value of the payments are within 3 for all ^                                        i. From this we conclude that the expected utility to agent i in the approximate-VCG mechanism is always within 3 of that in the approximate delayed-VCG mechanism, and therefore 4 -BNE by Lemma 2. The expected payment in the approximate online VCG mechanism is
 E|                 {Ri (^                                         ; ~                                               )} - E{ ^                                                                            V ss(h (^                                                                                                                   ; ~                                                                                                                         )} - E{ ^                                                                                                                                            V ss(h (^                                                                                                                                                                               a           T                                                              ^                                                                                             ai              ^                                                                                                              ai                                        ^                                                                                                                                                        ai          ^                                                                                                                                                                     ai\i; ~                                                                                                                                                                             )}                      i

The value function computed by the sparse-sampling( ) algorithm is a random variable to agent i at the time of bidding, and the second and third expectations are over the random- ness introduced by the sparse-sampling( ) algorithm. The first term is the same as in the payment in the approximate delayed-VCG mechanism. By Corollary 1, the value function estimated in the sparse-sampling( ) is near-optimal in expectation and the total of the sec- ond two terms is at least V (h^a (^                                                                                                                                     (^                                                                                                                                                 ; )) - 2 . Ignoring the first                                                                      i          ^                                                                                  ai\i; )) - V (h^                                                                                                                                ai         ^                                                                                                                                            ai
term in pDvcg, the expected payment in the approximate delayed-VCG mechanism is no                i more than V (h^a (^                                                                                                       (^                                                                                                                        ; )) - ) because of the near-optimality                                   i          ^                                               ai\i; )) - (V (h^                                                                                                  ai         ^                                                                                                                   ai of the value function of the stochastic policy (Lemma 1). Putting this together, we have a maximum difference in expected payments of 3 . Similar analysis yields a maximum dif- ference of 3 when an upper-bound is taken on the payment in the online VCG mechanism and compared with a lower-bound on the payment in the delayed mechanism.
Theorem 3 For any parameter                                                 > 0, the sparse-sampling( ) based approximate online VCG mechanism has -efficiency in an 4 -BNE.
5    Empirical Evaluation of Approximate Online VCG
The WiFi Problem.                                  The WiFi problem considers a fixed number of channels C with a random number of agents (max A) that can arrive per period.                                                                                                            The time horizon
T = 50. Agents demand a single channel and arrive with per-unit value, distributed i.i.d. V = {v1, . . . , vk} and duration in the system, distributed i.i.d. D = {d1, . . . , dl}. The value model requires that any allocation to agent i must be for contiguous periods, and be made while the agent is present (i.e., during periods [t, ai + di], for arrival ai and duration di). An agent's value for an allocation of duration x is vix where vi is its per-unit value. Let dmax denote the maximal possible allocated duration. We define the following MDP components: State space: We use the following compact, sufficient, statistic of history: a resource schedule is a (weakly non-decreasing) vector of length dmax that counts the number of channels available in the current period and next dmax - 1 periods given previous actions (C channels are available after this); an agent vector of size (k  l) that provides a count of the number of agents present but not allocated for each possible per-unit value and each possible duration (the duration is automatically decremented when an agent remains in the system for a period without receiving an allocation); the time remaining until horizon T . Action space: The policy can postpone an agent allocation, or allocate an agent to a chan- nel for the remaining duration of the agent's time in the system if a channel is available, and the remaining duration is not greater than dmax. Payoff function: The reward at a time step is the summed value obtained from all agents for which an allocation is made in this time step. This is the total value such an agent will receive before it departs. Transition probabilities: The change in resource schedule, and in the agent vector that relates to agents currently present, is deterministic. The random new additions to the agent vector at each step are unaffected by the actions and also independent of time.
Mechanisms.       In the exact online VCG mechanism we compute an optimal policy, and optimal MDP values, offline using finite-horizon value iteration [7]. In the sparse- sampling( ) approach, we define a sampling tree depth L (perhaps < T ) and sample each state m times. This limited sampling depth places a lower-bound on the best possible ap- proximation accuracy from the sparse-sampling algorithm. We also employ agent pruning, with the agent vector in the state representation pruned to remove dominated agents: con- sider agent type with duration d and value v and remove all but C - N agents where N is the number of agents that either have remaining duration  d and value > v or duration < d and value  v. In computing payments we use factoring, and only determine VCG payments once for each type of agent to arrive. We compare performance with a simple fixed-price allocation scheme that given a particular problem, computes off-line a fixed number of periods and a fixed price (agents are queued and offered the price at random as resources become available) that yields the maximum expected total value.
Results    In the default model, we set C = 5, A = 5, define the set of values V = {1, 2, 3}, define the set of durations D = {1, 2, 6}, with lookahead L = 4 and sampling width m = 6. All results are averaged over at least 10 instances, and experiments were performed on a 3GHz P4, with 512 MB RAM. Value and revenue is normalized by the total value demanded by all agents, i.e. the value with infinite capacity.2 Looking first at economic properties, Figure 1(A) shows the effect of varying the number of agents from 2 to 12, comparing the value and revenue between the approximate online VCG mechanism and the fixed price mechanism. Notice that the MDP method dominates the price-based scheme for value, with a notable performance improvement over fixed price when demand is neither very low (no contention) nor very high (lots of competition). Revenue is also generally better from the MDP-based mechanism than in the fixed price scheme. Fig. 1(B) shows the similar effect of varying the number of channels from 3 to 10.
Turning now to computational properties, Figure 1 (C) illustrates the effectiveness of sparse-sampling, and also agent pruning, sampled over 100 instances. The model is very
2This explains why the value appears to drop as we scale up the number of agents-- the total available value is increasing but supply remains fixed.
                                                                                                                          100                                                                                     value:mdp                                                                   value:mdp                                  80                                                 rev:mdp                                                                     rev:mdp                                                                                     value:fixed                                                     80          value:fixed                                                                                     rev:fixed                                                                   rev:fixed


                             60                                                                                                                                                     60

                       %                                                                                                  %

                             40                                                                                                                 40



                                                                                                                                                20                                  20


                              2         4             6            8               10          12                                                03         4           5         6      7         8          9    10                                                        Number of agents                                                                                                          Number of channels


                             98                                                                 1.0                                                                                                                                                     600                                                  value:pruning                                                                                                       vs. #agents                                                                                                                                                                      time:pruning                                                                                                                                                                      vs. #agents (no pruning)                                  96              value:no pruning                                   0.8                                             500              time:no pruning                                                                                                                                                                      vs. #channels


                                                                                                                                                400                                  94                                                                 0.6


                                                                                                                                                300

                             92                                                                 0.4                                                                                                                                    Run time (s)        % of Exact Value                                                                                    % of Exact Time                          200

                             90                                                                 0.2                                                                             time:pruning                                                            100                                                                             time:no pruning

                             88                                                                   0                                                   0                                        2          4               6               8                10                                                      2           4              6           8         10         12

                                                     Sampling Width                                                                                                           Number of Agents

Figure 1: (A) Value and Revenue vs. Number of Agents. (B) Value and Revenue vs. Number of Channels. (C) Effect of Sampling Width. (D) Pruning speed-up.
small: A = 2, C = 2, D = {1, 2, 3}, V = {1, 2, 3} and L = 4, to allow a compari- son with the compute time for an optimal policy. The sparse-sampling method is already running in less than 1% of the time for optimal value-iteration (right-hand axis), with an accuracy as high as 96% of the optimal. Pruning provides an incremental speed-up, and actually improves accuracy, presumably by making better use of each sample. Figure 1 (D) shows that pruning is extremely useful computationally (in comparison with plain sparse- sampling), for the default model parameters and as the number of agents is increased from 2 to 12. Pruning is effective, removing around 50% of agents (summed across all states in the lookahead tree) at 5 agents.
Acknowledgments.                                               David Parkes was funded by NSF grant IIS-0238147. Satinder Singh was funded by NSF grant CCF 0432027 and by a grant from DARPA's IPTO program."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/fecf2c550171d3195c879d115440ae45-Abstract.html,"Comparing Beliefs, Surveys, and Random Walks","Erik Aurell, Uri Gordon, Scott Kirkpatrick","Survey propagation is a powerful technique from statistical physics that          has been applied to solve the 3-SAT problem both in principle and in          practice. We give, using only probability arguments, a common deriva-          tion of survey propagation, belief propagation and several interesting hy-          brid methods. We then present numerical experiments which use WSAT          (a widely used random-walk based SAT solver) to quantify the complex-          ity of the 3-SAT formulae as a function of their parameters, both as ran-          domly generated and after simplication, guided by survey propagation.          Some properties of WSAT which have not previously been reported make          it an ideal tool for this purpose  its mean cost is proportional to the num-          ber of variables in the formula (at a xed ratio of clauses to variables) in          the easy-SAT regime and slightly beyond, and its behavior in the hard-          SAT regime appears to reect the underlying structure of the solution          space that has been predicted by replica symmetry-breaking arguments.          An analysis of the tradeoffs between the various methods of search for          satisfying assignments shows WSAT to be far more powerful than has          been appreciated, and suggests some interesting new directions for prac-          tical algorithm development."
2004,https://papers.nips.cc/paper_files/paper/2004,https://papers.nips.cc/paper_files/paper/2004/hash/ff2cc3b8c7caeaa068f2abbc234583f5-Abstract.html,Theories of Access Consciousness,"Michael D. Colagrosso, Michael Mozer","Theories of access consciousness address how it is that some mental states but          not others are available for evaluation, choice behavior, and verbal report. Farah,          O'Reilly, and Vecera (1994) argue that quality of representation is critical; De-          haene, Sergent, and Changeux (2003) argue that the ability to communicate rep-          resentations is critical. We present a probabilistic information transmission or          PIT model that suggests both of these conditions are essential for access con-          sciousness. Having successfully modeled data from the repetition priming litera-          ture in the past, we use the PIT model to account for data from two experiments          on subliminal priming, showing that the model produces priming even in the ab-          sence of accessibility and reportability of internal states. The model provides a          mechanistic basis for understanding the dissociation of priming and awareness.
Philosophy has made many attempts to identify distinct aspects of consciousness. Perhaps the most famous effort is Block's (1995) delineation of phenomenal and access conscious- ness. Phenomenal consciousness has to do with ""what it is like"" to experience chocolate or a pin prick. Access consciousness refers to internal states whose content is ""(1) inferen- tially promiscuous, i.e., poised to be used as a premise in reasoning, (2) poised for control of action, and (3) poised for rational control of speech."" (p. 230) The scientific study of con- sciousness has exploded in the past six years, and an important catalyst for this explosion has been the decision to focus on the problem of access consciousness: how is it that some mental states but not others become available for evaluation, choice behavior, verbal report, and storage in working memory. Another reason for the recent explosion of consciousness research is the availability of functional imaging techniques to explore differences in brain activation between conscious and unconscious states, as well as the development of clever psychological experiments that show that a stimulus that is not consciously perceived can nonetheless influence cognition, which we describe shortly.
1    Subliminal Priming
The phenomena we address utilize an experimental paradigm known as repetition priming. Priming refers to an improvement in efficiency in processing a stimulus item as a result of previous exposure to the item. Efficiency is defined in terms of shorter response times, lower error rates, or both. A typical long-term perceptual priming experiment consists of a study phase during which participants are asked to read aloud a list of words, and a test phase during which participants must name or categorize a series of words, presented one at a time. Reaction time is lower and/or accuracy is higher for test words that were also on the study list. Repetition priming occurs without strategic effort on the part of participants, and therefore appears to be a low level mechanism of learning, which likely serves as the mechanism underlying the refinement of cognitive skills with practice.
In traditional studies, priming is supraliminal--the prime is consciously perceived. In the studies we model here, primes are subliminal. Subliminal priming addresses fundamental issues concerning conscious access: How is it that a word or image that cannot be identified, detected, or even discriminated in forced choice can nonetheless influence the processing of a subsequent stimulus word? Answering this question in a computational framework would be a significant advance toward understanding the nature of access consciousness.
2    Models of Conscious and Unconscious Processing
In contrast to the wealth of experimental data, and the large number of speculative and philosophical papers on consciousness, concrete computational models are rare. The do- main of consciousness is particularly ripe for theoretical perspectives, because it is a sig- nificant contribution to simply provide an existence proof of a mechanism that can explain specific experimental data. Ordinarily, a theorist faces skepticism when presenting a model; it often seems that hundreds of alternative, equally plausible accounts must exist. However, when addressing data deemed central to issues of consciousness, simply providing a con- crete handle on the phenomena serves to demystify consciousness and bring it into the realm of scientific understanding.
We are familiar with only three computational models that address specific experimental data in the domain of consciousness. We summarize these models, and then present a novel model and describe its relationship to the previous efforts. Farah, O'Reilly, and Ve- cera (1994) were the first to model specific phenomena pertaining to consciousness in a computational framework. The phenomena involve prosopagnosia, a deficit of overt face recognition following brain damage. Nonetheless, prosopagnosia patients exhibit residual covert recognition by a variety of tests. For example, when patients are asked to categorize names as famous or nonfamous, their response times are faster to a famous name when the name is primed by a picture of a semantically related face (e.g., the name ""Bill Clinton"" when preceded by a photograph of Hillary), despite the fact that they could not identify the related face. Farah et al. model face recognition in a neural network, and show that when the network is damaged, it loses the ability to perform tasks requiring high fidelity represen- tations (e.g., identification) but not tasks requiring only coarse information (e.g., semantic priming). They argue that conscious perception is associated with a certain minimal quality of representation.
Dehaene and Naccache (2001) outline a framework based on Baars' (1989) notion of con- scious states as residing in a global workspace. They describe the workspace as a ""dis- tributed neural system...with long-distance connectivity that can potentially interconnect multiple specialized brain areas in a coordinated, though variable manner."" (p. 13) De- haene, Sergent, and Changeaux (2003) implement this framework in a complicated archi- tecture of integrate-and-fire neurons and show that the model can qualitatively account for the attentional blink phenomenon. The attentional blink is observed in experiments where participants are shown a rapid series of stimuli, which includes two targets (T1 and T2). If T2 appears shortly after T1, the ability to report T2 drops, as if attention is distracted. Dehane et al. explain this phenomenon as follows. When T1 is presented, its activation propagates to frontal cortical areas (the global workspace). Feedback connections lead to a resonance between frontal and posterior areas, which strengthen T1 but block T2 from entering the workspace. If the T1-T2 lag is sufficiently great, habituation of T1 sufficiently weakens the representation such that T2 can enter the workspace and suppress T1. In this account, conscious access is achieved via resonance between posterior and frontal areas.
Although the Farah et al. and Dehaene et al. models might not seem to have much in common, they both make claims concerning what is required to achieve functional con- nectivity between perceptual and response systems. Farah et al. focus on aspects of the representation; Dehaene et al. focus on a pathway through which representations can be
communicated. These two aspects are not incompatible, and in fact, a third model incorpo- rates both. Mathis and Mozer (1996) describe an architecture with processing modules for perceptual and response processes, implemented as attractor neural nets. They argue that in order for a representation in some perceptual module to be assured of influencing a response module, (a) it must have certain characteristicstemporal persistence and well-formedness which is quite similar to Farah et al.'s notion of quality, and (b) the two modules must be interconnected--which is the purpose of Dehaene et al.'s global workspace. The model has two limitations that restrict its value as a contemporary account of conscious access. First, it addressed classical subliminal priming data, but more reliable data has recently been reported. Second, like the other two models, Mathis and Mozer used a complex neural network architecture with arbitrary assumptions built in, and the sensitivity of the model's behavior to these assumptions is far from clear. In this paper, we present a model that em- bodies the same assumptions as Mathis and Mozer, but overcomes its two limitations, and explains subliminal-priming data that has yet to be interpreted via a computational model.
3    The Probabilistic Information Transmission (PIT) Framework
Our model is based on the probabilistic information transmission or PIT framework of Mozer, Colagrosso, and Huber (2002, 2003). The framework characterizes the transmission of information from perceptual to response systems, and how the time course of informa- tion transmission changes with experience (i.e., priming). Mozer et al. used this framework to account for a variety of facilitation effects from supraliminal repetition priming.
The framework describes cognition in terms of a collection of information-processing path- ways, and supposes that any act of cognition involves coordination among multiple path- ways. For example, to model a letter-naming task where a letter printed in upper or lower case is presented visually and the letter must be named, the framework would assume a perceptual pathway that maps the visual input to an identity representation, and a response pathway that maps a identity representation to a naming response. The framework is for- malized as a probabilistic model: the pathway input and output are random variables and microinference in a pathway is carried out by Bayesian belief revision.
The framework captures the time course of information processing for a single experi- mental trial. To elaborate, consider a pathway whose input at time t is a discrete random variable, denoted X(t), which can assume values x1, x2, x3, . . . , xn corresponding to al-                                                                        x ternative input states. Similarly, the output of the pathway at time t is a discrete random variable, denoted Y (t), which can assume values y1, y2, y3, . . . , yn . For example, in the                                                                       y letter-naming task, the input to the perceptual pathway would be one of nx = 52 visual patterns corresponding to the upper- and lower-case letters of the alphabet, and the output is one of ny = 26 letter identities. To present a particular input alternative, say xi, to the model for T time steps, we specify X(t) = xi for t = 1 . . . T , and allow the model to compute P(Y (t) | X(1) . . . X(t)).
A pathway is modeled as a dynamic Bayes network; the minimal version of the model used in the present simulations is simply a hidden Markov model, where the X(t) are observations and the Y (t) are inferred state (see Figure 1a). In typical usage, an HMM is presented with a sequence of distinct inputs, whereas we maintain the same input for many successive time steps; and an HMM transitions through a sequence of distinct hidden states, whereas we attempt to converge with increasing confidence on a single state.
Figure 1b illustrates the time course of inference in a single pathway with 52 input and 26 output alternatives and two-to-one associations. The solid line in the Figure shows, as a function of time t, P(Y (t) = yi | X(1) = x2i . . . X(t) = x2i), i.e., the probability that input i (say, the visual pattern of an upper case O) will produce its target output (the letter identity). Evidence for the target output accumulates gradually over time, yielding a speed-accuracy curve that relates the number of iterations to the accuracy of identification.
                                                                    1

                                                              0.8        Y          Y             0          1     Y2          Y3                                                                   0.6                     O

                                                              0.4                                                                P(Output) 0.2              Q                   X          X                        1          2      X3                             0 (a)                                                     (b)                                    Time

Figure 1: (a) basic pathway architecture--a hidden Markov model; (b) time course of inference in a pathway when the letter O is presented, causing activation of both O and the visually similar Q.
The exact shape of the speed-accuracy curve--the pathway dynamics--are determined by three probability distributions, which embody the knowledge and past experience of the model. First, P(Y (0)) is the prior distribution over outputs in the absence of any informa- tion about the input. Second, P(Y (t) | Y (t - 1)) characterizes how the pathway output evolves over time. We assume the transition probability matrix serves as a memory with diffusion, i.e., P(Y (t) = yi|Y (t - 1) = yj) = (1 - )ij + P(Y (0) = yi), where  is the diffusion constant and ij is the Kronecker delta. Third, P(X(t) | Y (t)) characterizes the strength of association between inputs and outputs. The greater the association strength, the more rapidly that information about X will be communicated to Y . We parameterize this distribution as P(X(t) = xi|Y (t) = yj)  1 +                                                                                       k               ik kj , where ij indicates the frequency of experience with the association between states xi and yj, and ik specifies the similarity between states xi and xk. (Although the representation of states is localist, the  terms allow us to design in the similarity structure inherent in a distributed representation.) These association strengths are highly constrained by the task structure and the similarity structure and familiarity of the inputs.
Fundamental to the framework is the assumption that with each experience, a pathway be- comes more efficient at processing an input. Efficiency is reflected by a shift in the speed- accuracy curve to the left. In Mozer, Colagrosso, and Huber (2002, 2003), we propose two distinct mechanisms to model phenomena of supraliminal priming. First, the association frequencies, ij, are increased following a trial in which xi leads to activation of yj, re- sulting in more efficient transmission of information, corresponding to an increased slope of the solid line in Figure 1b. The increase is Hebbian, based on the maximum activation achieved by xi and yj: ij =  maxt P(X(t) = xi)P(Y (t) = yj), where  is a step size. Second, the priors, which serve as a model of the environment, are increased to indicate a greater likelihood of the same output occurring again in the future. In modeling data from supraliminal priming, we found that the increases to association frequencies are long last- ing, but the increases to the priors decay over the course of a few minutes or a few trials. As a result, the prior updating does not play into the simulation we report here; we refer the reader to Mozer, Colagrosso, and Huber (2003) for details.
4      Access Consciousness and PIT
We have described the operation of a single pathway, but to model any cognitive task, we require a series of pathways in cascade. For a simple choice task, we use a percpet- ual pathway cascaded to a response pathway. The interconnection between the pathways is achieved by copying the output of the perceptual pathway, Y p(t), to the input of the response pathway, Xr(t), at each time t.
This multiple-pathway architecture allows us to characterize the notion of access con- sciousness. Considering the output of the perceptual pathway, access is achieved when: (1) the output representation is sufficient to trigger the correct behavior in the response pathway, and (2) the perceptual and response pathways are functionally interconnected. In more general terms, access for a perceptual pathway output requires that these two condi-
tions be met not just for a specific response pathway, but for arbitrary response pathways (e.g., pathways for naming, choice, evaluation, working memory, etc.). In Mozer and Co- lagrosso (in preparation) we characterize the sufficiency requirements of condition 1; they involve a representation of low entropy that stays active for long enough that the represen- tation can propagate to the next pathway.
As we will show, a briefly presented stimulus fails to achieve a representation that supports choice and naming responses. Nonetheless, the stimulus evokes activity in the perceptual pathway. Because perceptual priming depends on the magnitude of the activation in the perceptual pathway, not on the activation being communicated to response pathways, the framework is consistent with the notion of priming occurring in the absence of awareness.
4.1    Simulation of Bar and Biederman (1998)
Bar and Biederman (1998) presented a sequence of masked line drawings of objects and asked participants to name the objects, even if they had to guess. If the guess was incorrect, participants were required to choose the object name from a set of four alternatives. Unbe- knownst to the participant, some of the drawings in the series were repeated, and Bar and Biederman were interested in whether participants would benefit from the first presenta- tion even if it could not be identified. The repeated objects could be the same or a different exemplar of the object, and it could appear in either the same or a different display position.
Participants were able to name 13.5% of drawings on presentation 1, but accuracy jumped to 34.5% on presentation 2. Accuracy did improve, though not as much, if the same shape was presented in a different position, but not if a different drawing of the same object was presented, suggesting a locus of priming early in the visual stream. The improvement in accuracy is not due to practice in general, because accuracy rose only 4.0% for novel control objects over the course of the experiment. The priming is firmly subliminal, because participants were not only unable to name objects on the first presentation, but their four- alternative forced choice (4AFC) performance was not much above chance (28.5%).
To model these phenomena, we created a response pathway with fifty states representing names of objects that are used in the experiment, e.g., chair and lamp. We also created a perceptual pathway with states representing visual patterns that correspond to the names in the response pathway. Following the experimental design, every object identity was instantiated in two distinct shapes, and every shape could be in one of nine different visual- field positions, leading to 900 distinct states in the perceptual pathway to model the possible visual stimuli. The following parameters were fit to the data. If two perceptual states, xi and xk are the same shape in different positions, they are assigned a similarity coefficient ik = 0.95; all other similarity coefficients are zero. The association frequency, , for valid associations in the perceptual pathway was 22, and the response pathway 18. Other parameters were p = .05, r = .01, and  = 1.0.
The PIT model achieves a good fit to the human experimental data (Figure 2). Specifi- cally, priming is greatest for the same shape in the same position, some priming occurs for the same shape in a different position, and no substantial priming occurs for the different shape. Figure 3a shows the time course of activation of a stimulus representation in the perceptual pathway when the stimulus is presented for 50 iterations, on both the first and third presentations. The third presentation was chosen instead of the second to make the effect of priming clearer.
Even though a shape cannot be named on the first presentation, partial information about the shape may nonetheless be available for report. The 4AFC test of Bar and Bieder- man provides a more sensitive measure of residual stimulus information. In past work, we modeled forced-choice tasks using a response pathway with only the alternatives under consideration. However, in this experiment, forced-choice performance must be estimated conditional on incorrect naming. In PIT framework, we achieve this using naming and
                        40                                                                                                                   40

                                   First Block                                                                                                         First Block                             35                                                         Second Block                                              35                                                        Second Block


                        30                                                                                                                   30


                        25                                                                                                                   25


                        20                                                                                                                   20


                        15                                                                                                                   15

                        10   Percent Correct Naming                                                                                                                         10                                                                                                                        Percent Correct Naming

                         5                                                                                                                    5


                         0                                                                                                                    0                                    Control    Prime    SHAPE:    Same     Same        Different Different Second                                       Control    Prime    SHAPE:    Same     Same        Different Different Second                                   Objects     Objects POSITION: Same     Different     Same    Different    Control                                    Objects    Objects POSITION: Same     Different     Same    Different    Control

Figure 2: (left panel) Data from Bar and Biederman (1998) (right panel) Simulation of PIT. White bar: accuracy on first presentation of a prime object. Black bars: the accuracy when the object is repeated, either with the same or different shape, and in the same or different position. Grey bars: accuracy for control objects at the beginning and the end of the experiment.
forced-choice output pathways having output distributions N (t) and F (t), which are linked via the perceptual state, Y p(t). F (t) must be reestimated with the evidence that N (t) is not the target state. This inference problem is intractable. We therefore used a shortcut in which a single response pathway is used, augmented with a simple three-node belief net (Figure 3b) to capture the dependence between naming and forced choice. The belief net has a response pathway node Y r(t) connected to F (t) and N (t), with conditional distribu- tion P (N (t) = ni|Y r(t) = yj) = ij + (1 - )/|Y r|, and an analogous distribution for P (F (t) = fi|Y r(t) = yj). The free parameter  determines how veridically naming and forced-choice actions reflect response-pathway output. Over a range of ,  < 1, the model obtains forced-choice performance near chance on the first presentation when the naming response is incorrect. For example, with  = 0.72, the model produces a forced-choice accuracy on presentation 1 of 26.1%. (Interestingly, the model also produces below chance performance on presentation 2 if the object is not named correctly--23.5%--which is also found in the human data--20.0%.) Thus, by the stringent criterion of 4AFC, the model shows no access consciousness, and therefore illustrates a dissociation between priming and access consciousness. In our simulation, we followed the procedure of Bar and Bieder- man by including distractor alternatives with visual and semantic similarity to the target. These distractors are critical: with unrelated distractors, the model's 4AFC performance is significantly above chance, illustrating that a perceptual representation can be adequate to support some responses but not others, as Farah et al. (1994) also argued.
4.2                               Simulation of Abrams and Greenwald (2000)
During an initial phase of the experiment, participants categorized 24 clearly visible target words as pleasant (e.g., HUMOR) or unpleasant (e.g., SMUT). They became quite familiar with the task by categorizing each word a total of eight times. In a second phase, partici- pants were asked to classify the same targets and were given a response deadline to induce errors. The targets were preceded by masked primes that could not be identified. Of interest is the effective valence (or EV) of the target for different prime types, defined as the error rate difference between unpleasant and pleasant targets. A positive (negative) EV indicates that responses are biased toward a pleasant (unpleasant) interpretation by the prime. As one would expect, pleasant primes resulted in a positive EV, unpleasant primes in a negative EV. Of critical interest is the finding that a nonword prime formed by recombining two pleas- ant targets (e.g., HULIP from HUMOR and TULIP) or unpleasant targets (e.g., BIUT from BILE and SMUT) also served to bias the targets. More surprising, a positive EV resulted from unpleasant prime words formed by recombining two pleasant targets (TUMOR from TULIP and HUMOR), indicating that subliminal priming arises from word fragments, not words as unitary entities, and providing further evidence for an early locus of subliminal priming. Note that the results depend critically on the first phase of the experiment, which gave participants extensive practice on a relatively small set of words that were then used as and recombined to form primes. Words not studied in the first phase (orphans) provided
                     0.6                                                                                 object, first presentation                          0.5                                                   object, third presentation                          0.4                                                                                                      N(t)             F(t)                                                                                           different object                          0.3                          0.2          Probability     0.1                           0 1                                                                                                             Yr(t)   (a)                            50                                                                            1000      (b)                                                                         Time (msec) Figure 3: (a) Activation of the perceptual representation in PIT as a function of processing iterations on the first (thin solid line) and third (thick solid line) presentations of target. (b) Bayes net for performing 4AFC conditional on incorrect naming response.

                                            0.4                                                                      Experiment                                                 0.3                                                                      Model                                   alence                                                 0.2

                                            0.1                                             fective VEf 0                                                              targets        hulip-type           tumor-type            orphans

Figure 4: Effective valence of primes in the Abrams and Greenwald (2000) experiment for human subjects (black bars) and PIT model (grey bars). HULIP-type primes are almost as strong as target repetitions, and TUMOR-type primes have a positive valence, contrary to the meaning of the word.
no significant EV effect when used as primes.
In this simulation, we used a three pathway model: a perceptual pathway that maps vi- sual patterns to orthography with 200 input states corresponding both to words, nonwords, and nonword recombinations of words; a semantic pathway that maps to 100 distinct lex- ical/semantic states; and a judgement pathway that maps to two responses, pleasant and unpleasant. In the perceptual pathway, similarity structure was based on letter overlap, so that HULIP was similar to both TULIP and HUMOR, with  = 0.837. No similarity was assumed in the semantic state representation; consistent with the previous simulation, p = .05, s = .01, j = .01, and  = .01. At the outset of the simulation,  frequencies for correct associations were 15, 19, and 25 in the perceptual, semantic, and judgement pathways. The initial phase of the experiment was simulated by repeated supraliminal presentation of words, which increased the association frequencies in all three pathways through the ij learning rule.
Long-term supraliminal priming is essential in establishing the association strengths, as we'll explain. Short-term subliminal priming also plays a key role in the experiment. Dur- ing the second phase of the experiment, residual activity from the prime--primarily in the judgement pathway--biases the response to the target. Residual activation of the prime is present even if the representation of the prime does not reach sufficient strength that it could be named or otherwise reported.
The outcome of the simulation is consistent with the human data (Figure 4). When a HULIP-type prime is presented, HUMOR and TULIP become active in the semantic path- way because of their visual similarity to HULIP. Partial activation of these two practiced words pushes the judgement pathway toward a pleasant response, resulting in a positive EV. When a TUMOR-type prime is presented, three different words become active in the semantic pathway: HUMOR, TULIP, and TUMOR itself. Although TUMOR is more active, it was not one of the words studied during the initial phase of the experiment, and as a result, it has a relatively weak association to the unpleasant judgement, in contrast to the other two words which have strong associations to the pleasant judgement. Orphan primes have little effect because they were not studied during the initial phase of the experiment, and consequently their association to pleasant and unpleasant judgements is also weak. In sum- mary, activation of the prime along a critical, well-practiced pathway may not be sufficient to support an overt naming response, yet it may be sufficient to bias the processing of the
immediately following target."
