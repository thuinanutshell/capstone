year,proceeding_link,paper_link,title,authors,abstract
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/03004620ea802b9118dd44d69f07af56-Abstract.html,Synchronization in Neural Nets,"Jacques J. Vidal, John Haggerty","The  paper  presents  an  artificial  neural  network  concept  (the  Synchronizable Oscillator Networks)  where the instants of individual  firings  in  the  form  of  point  processes  constitute  the  only  form  of  information  transmitted  between  joining  neurons.  This  type  of  communication contrasts with  that which  is  assumed  in most  other  models  which  typically  are  continuous  or  discrete  value-passing  networks.  Limiting the messages received  by each processing unit to  time  markers that signal  the firing  of other units  presents  significant  implemen tation advantages. 
In  our  model,  neurons  fire  spontaneously  and  regularly  in  the  absence of perturbation.  When interaction is  present,  the scheduled  firings  are  advanced  or  delayed  by  the firing  of neighboring  neurons.  Networks  of  such  neurons  become  global  oscillators  which  exhibit  multiple  synchronizing  attractors.  From  arbitrary  initial  states,  energy  minimization  learning  procedures  can  make  the  network  converge  to  oscillatory  modes  that  satisfy  multi-dimensional  constraints  Such  networks  can  directly  represent  routing  and  scheduling problems  that conSist of ordering sequences of events."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/0316d8d63a0c252a3ec57921d7d2429b-Abstract.html,Static and Dynamic Error Propagation Networks with Application to Speech Coding,"A J Robinson, F Fallside","Error propagation nets have been shown to be able to learn a variety of tasks in  which a static input pattern is mapped outo a static output pattern. This paper  presents a generalisation of these nets to deal with time varying, or dynamic  patterns, and three possible architectures are explored. As an example, dynamic  nets are applied to tbe problem of speech coding, in which a time sequence of  speech data are coded by one net and decoded by another. The use of dynamic  nets gives a better signal to noise ratio than that achieved using static nets."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/03f8faa47a2e9614453e50aaf8d7f278-Abstract.html,PATTERN CLASS DEGENERACY IN AN UNRESTRICTED STORAGE DENSITY MEMORY,"Christopher L. Scofield, Douglas L. Reilly, Charles Elbaum, Leon N. Cooper",in
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/069a002768bcb31509d4901961f23b3c-Abstract.html,Introduction to a System for Implementing Neural Net Connections on SIMD Architectures,Sherryl Tomboulian,"Neural  networks  have  attracted  much  interest  recently,  and  using  parallel 
architectures  to simulate  neural  networks  is  a  natural  and  necessary  applica(cid:173) tion.  The  SIMD  model  of parallel  computation is  chosen,  because systems  of  this  type  can  be  built  with  large  numbers  of processing  elements.  However,  such systems are not naturally suited to generalized communication.  A method  is  proposed  that  allows  an implementation of neural  network  connections  on  massively parallel SIMD  architectures.  The key to this system is an algorithm  that allows  the formation  of arbitrary  connections  between  the ""neurons"".  A  feature  is  the ability  to add  new  connections  quickly.  It also  has error  recov(cid:173) ery  ability and  is  robust  over a  variety  of network  topologies.  Simulations  of  the general  connection system, and its implementation on the Connection Ma(cid:173) chine,  indicate  that  the  time and  space  requirements  are  proportional  to  the  product of the  average number of connections per neuron and the diameter of  the interconnection network."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/099268c3121d49937a67a052c51f865d-Abstract.html,The Capacity of the Kanerva Associative Memory is Exponential,P. A. Chou,"The  capacity  of  an  associative  memory  is  defined  as  the  maximum 
number  of  vords  that  can  be  stored  and  retrieved  reliably  by  an  address  vithin  a  given  sphere  of  attraction.  It  is  shown  by  sphere  packing  arguments  that  as  the  address  length  increases.  the  capacity  of  any  associati ve  memory  is  limited  to  an  exponential  grovth  rate  of  1 - h2 ( 0).  vhere  h2(0)  is  the  binary  entropy  function  in  bits.  and  0  is  the  radius  of  the  sphere  of  attraction.  This  exponential  grovth  in  capacity  can  actually  be  achieved  by  the  Kanerva  associative  memory.  if  its  parameters  are  optimally  set .  Formulas  for  these  op.timal  values  are  provided.  The  exponential  grovth  in  capacity  for  the  Kanerva  associative  memory  contrasts  sharply  vith  the  sub-linear  grovth  in  capacity  for  the  Hopfield  associative  memory. 
ASSOCIATIVE  MEMORY  AND  ITS  CAPACITY 
Our  model  of  an  associative  memory  is  the  folloving.  Let  ()(,Y)  be 
an  (address.  datum)  pair.  vhere  )(  is  a  vector  of  n  ±ls  and  Y  is  a  vector  of  m  ±ls.  and  let  ()(l),y(I)), ... ,()(M) , y(M)).  be  M  (address,  datum)  pairs  stored  in  an  associative  memory.  is  presented  at  the  input  vith  an  address  )(  that  is  close  to  some  stored  address  )(W.  then  it should  produce  at  the  output  a  vord  Y  that  is  close  to  the  corresponding  contents  y(j).  To  be  specific,  let  us  say  that  an  associative  memory  can  correct fraction  0  errors  if  an  )(  vi thin  Hamming  distance  no  of  )((j)  retrieves  Y  equal  to  y(j).  The  Hamming  sphere  around  each  )(W  vill  be  called  the  sphere  of  attraction,  and  0  viII  be  called  the  radius  of  attraction. 
If  the  associative  memory 
One  notion  of  the  capacity  of  this  associative  memory  is  the 
maximum  number  of  vords  that  it  can  store  vhile  correcting  fraction  0  errors .  Unfortunately.  this  notion  of  capacity  is  ill-defined.  because  it  depends  on  exactly  vhich  (address.  datum)  pairs  have  been  stored.  Clearly.  no  associative  memory  can  correct  fraction  0  errors  for  every  sequence  of  stored  (address,  datum)  pairs.  Consider.  for  example,  a  sequence  in  vhich  several  different  vords  are  vritten  to  the  same  address .  No  memory  can  reliably  retrieve  the  contents  of  the  overvritten  vords.  At  the  other  extreme.  any  associative  memory ' can  store  an  unlimited  number  of  vords  and  retrieve  them  all  reliably.  if  their  contents  are  identical. 
A useful  definition  of  capacity  must  lie  somevhere  betveen  these 
tvo  extremes.  that  for  most  sequences  of  addresses  XU), .. . , X(M)  and  most  sequences  of  data  y(l), ... , y(M).  the  memory  can  correct  fraction  0  errors.  We  define 
In  this  paper.  ve  are  interested  in  the  largest  M  such 
IThis  vork  vas  supported  by  the  National  Science  Foundation  under  NSF 
grant  IST-8509860  and  by  an  IBM  Doctoral  Fellovship. 
© American Institute of Physics 1988 
185 
I  most  sequences'  in  a  probabilistic  sense,  as  some  set  of  sequences  yi th  total  probability  greater  than  say,  .99.  When  all  sequences  are  equiprobab1e,  this  reduces  to  the  deterministic  version:  sequences."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/09c653c3ae9d116e5f288ff988283a06-Abstract.html,How Neural Nets Work,"Alan Lapedes, Robert Farber","There is  presently great interest in the abilities of neural networks to mimic 
""qualitative reasoning""  by manipulating neural incodings of symbols.  Less work 
has  been performed on using neural networks to process floating  point numbers 
and it is  sometimes stated that neural networks are somehow inherently inaccu(cid:173)
rate  and  therefore  best  suited  for  ""fuzzy""  qualitative reasoning.  Nevertheless, 
the  potential  speed  of massively  parallel  operations  make  neural  net  ""number 
crunching""  an interesting topic  to explore.  In this paper we  discuss some of our 
work in which we  demonstrate that for  certain applications neural networks can 
achieve  significantly  higher  numerical  accuracy  than  more  conventional  tech(cid:173)
niques.  In  particular,  prediction  of future  values  of a  chaotic  time  series  can 
be  performed  with  exceptionally  high  accuracy.  We  analyze  how  a  neural  net 
is  able  to do  this  ,  and in  the process  show  that  a  large class  of functions  from 
Rn.  ~ Rffl  may  be  accurately  approximated  by  a  backpropagation  neural  net 
with just two  ""hidden""  layers.  The network  uses  this functional  approximation 
to perform either interpolation (signal processing applications)  or extrapolation 
(symbol processing applicationsJ.  Neural nets therefore use quite familiar meth(cid:173)
ods to perform. their tasks.  The geometrical viewpoint advocated here seems to 
be a  useful  approach  to analyzing  neural  network  operation  and  relates  neural 
networks  to well  studied topics  in  functional  approximation."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/0a5209deb79dc584d8ddb41a792d8549-Abstract.html,A Trellis-Structured Neural Network,"Thomas Petsche, Bradley W. Dickinson","We have developed a neural network which consists of cooperatively inter(cid:173)
connected Grossberg on-center off-surround subnets and which can be used to  optimize a function related to the log likelihood function for decoding convolu(cid:173) tional codes or more general FIR signal deconvolution problems. Connections in  the network are confined to neighboring subnets, and it is representative of the  types of networks which lend themselves to VLSI implementation. Analytical and  experimental results for convergence and stability of the network have been found.  The structure of the network can be used for distributed representation of data  items while allowing for fault tolerance and replacement of faulty units. 
1"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/0cafb7890f6a7d4de65507d5bb7e0187-Abstract.html,Connecting to the Past,Bruce A. MacDonald,"Recently  there  has  been  renewed  interest  in  neural-like  processing  systems,  evidenced  for  ex(cid:173) ample in  the two volumes  Parallel Distributed Processing edited by Rumelhart and McClelland,  and  discussed  as  parallel  distributed  systems,  connectionist  models,  neural  nets,  value  passing  systems  and  multiple  context  systems.  Dissatisfaction  with  symbolic  manipulation  paradigms  for  artificial  intelligence seems  partly  responsible  for  this  attention, encouraged by  the promise  of massively  parallel  systems  implemented  in  hardware.  This  paper  relates  simple  neural-like  systems  based  on  multiple  context  to  some  other  well-known  formalisms-namely  production  systems, k-Iength sequence prediction, finite-state  machines and Turing machines-and presents  earlier sequence  prediction  results  in  a  new  light."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/0e65cd5d77a5e8b2d1f9ab31ba50b49b-Abstract.html,Phase Transitions in Neural Networks,Joshua Chover,"Various  simulat.ions  of  cort.ical  subnetworks  have  evidenced  something  like  phase  transitions  with  respect  to  key  parameters.  We  demonstrate  that.  such  transi t.ions  must.  indeed  exist.  in  analogous  infinite  array  models.  For  related  finite  array  models  classical  phase  transi t.ions  (which  describe  steady-state  behavior)  may  not.  exist.,  but.  there  can  be  distinct. quali tative  changes  in  (""metastable"")  transient  behavior  as  key  system  parameters  pass  through  crit.ical  values ."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/1ccc3bfa05cb37b917068778f3c4523a-Abstract.html,MURPHY: A Robot that Learns by Doing,Bartlett W. Mel,"MURPHY consists of a camera looking at a robot arm, with a connectionist network  architecture situated in between. By moving its arm through a small, representative  sample of the 1 billion possible joint configurations, MURPHY learns the relationships,  backwards and forwards, between the positions of its joints and the state of its visual field.  MURPHY can use its internal model in the forward direction to ""envision"" sequences  of actions for planning purposes, such as in grabbing a visually presented object, or in  the reverse direction to ""imitate"", with its arm, autonomous activity in its visual field.  Furthermore, by taking explicit advantage of continuity in the mappings between visual  space and joint space, MURPHY is able to learn non-linear mappings with only a single  layer of modifiable weights."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/1eb34d662b67a14e3511d0dfd78669be-Abstract.html,Centric Models of the Orientation Map in Primary Visual Cortex,"William Baxter, Bruce Dow","In  the  visual  cortex  of  the  monkey  the  horizontal  organization  of  the  preferred  orientations of orientation-selective  cells  follows  two opposing  rules:  1) neighbors  tend  to  have similar orientation preferences, and  2) many different orientations are  observed  in a  local  region.  Several  orientation  models  which satisfy these  constraints are found  to  differ  in  the spacing  and  the  topological  index  of their singularities.  Using  the  rate  of orientation change as  a  measure,  the  models  are  compared to published experimental  results."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/20a5e2274bcb5ea174f03f9ce1487900-Abstract.html,A Computer Simulation of Olfactory Cortex with Functional Implications for Storage and Retrieval of Olfactory Information,"Matthew A. Wilson, James M. Bower","Based  on  anatomical  and  physiological  data,  we  have  developed  a  computer  simulation  of piri(cid:173) form  (olfactory)  cortex  which  is  capable  of reproducing  spatial  and  temporal  patterns  of actual  cortical  activity  under  a  variety  of conditions.  Using  a  simple  Hebb-type  learning  rule  in  conjunc(cid:173) tion  with  the  cortical  dynamics  which  emerge  from  the  anatomical  and  physiological  organiza(cid:173) tion  of  the  model,  the  simulations  are  capable  of establishing  cortical  representations  for  differ(cid:173) ent  input  patterns.  The  basis  of  these  representations  lies  in  the  interaction  of  sparsely  distribut(cid:173) ed,  highly  divergent/convergent  interconnections  between  modeled  neurons.  We  have  shown  that  different  representations  can  be  stored  with  minimal  interference.  and  that  following  learning  these  representations  are  resistant  to  input  degradation,  allowing  reconstruction  of  a  representa(cid:173) tion  following  only  a  partial  presentation  of  an  original  training  stimulus.  Further,  we  have  demonstrated  that  the  degree  of  overlap  of  cortical  representations  for  different  stimuli  can  also  be  modulated.  For  instance  similar  input  patterns  can  be  induced  to generate  distinct  cortical  representations  (discrimination).  while  dissimilar  inputs  can  be  induced  to  generate  overlapping  representations  (accommodation).  Both  features  are  presumably  important  in  classifying  olfacto(cid:173) ry stimuli."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/21ac0a2e346b1e5b77b3b61ee63eaae3-Abstract.html,Learning a Color Algorithm from Examples,"Anya C. Hurlbert, Tomaso A. Poggio","A lightness algorithm that separates surface reflectance from illumination in a  Mondrian world is synthesized automatically from a set of examples, pairs of input  (image irradiance) and desired output (surface reflectance). The algorithm, which re(cid:173) sembles a new lightness algorithm recently proposed by Land, is approximately equiva(cid:173) lent to filtering the image through a center-surround receptive field in individual chro(cid:173) matic channels. The synthesizing technique, optimal linear estimation, requires only  one assumption, that the operator that transforms input into output is linear. This  assumption is true for a certain class of early vision algorithms that may therefore be  synthesized in a similar way from examples. Other methods of synthesizing algorithms  from examples, or ""learning"", such as backpropagation, do not yield a significantly dif(cid:173) ferent or better lightness algorithm in the Mondrian world. The linear estimation and  backpropagation techniques both produce simultaneous brightness contrast effects. 
The problems that a visual system must solve in decoding two-dimensional images  into three-dimensional scenes (inverse optics problems) are difficult: the information  supplied by an image is not sufficient by itself to specify a unique scene. To reduce  the number of possible interpretations of images, visual systems, whether artificial  or biological, must make use of natural constraints, assumptions about the physical  properties of surfaces and lights. Computational vision scientists have derived effective  solutions for some inverse optics problems (such as computing depth from binocular  disparity) by determining the appropriate natural constraints and embedding them in  algorithms. How might a visual system discover and exploit natural constraints on its  own? We address a simpler question: Given only a set of examples of input images and  desired output solutions, can a visual system synthesize. or ""learn"", the algorithm that  converts input to output? We find that an algorithm for computing color in a restricted  world can be constructed from examples using standard techniques of optimal linear  estimation. 
The computation of color is a prime example of the difficult problems of inverse  optics. We do not merely discriminate betwN'n different wavelengths of light; we assign 
@ American Institute of Physics 1988 
623 
roughly constant colors to objects even though the light signals they send to our eyes  change as the illumination varies across space and chromatic spectrum. The compu(cid:173) tational goal underlying color constancy seems to be to extract the invariant surface  spectral reflectance properties from the image irradiance, in which reflectance and iI-""  lumination are mixed 1 • 
Lightness algorithms 2-8, pioneered by Land, assume that the color of an object 
can be specified by its lightness, or relative surface reflectance, in each of three inde(cid:173) pendent chromatic channels, and that lightness is computed in the same way in each  channel. Computing color is thereby reduced to extracting surface reflectance from the  image irradiance in a single chromatic channel. 
The image irra.diance, s', is proportional to the product of the illumination inten(cid:173)
sity e' and the surface reflectance r' in that channel: 
(1 )  This form of the image intensity equation is true for a Lambertian reflectance model,  in which the irradiance s' has no specular components, and for appropriately chosen  color channels 9. Taking the logarithm of both sides converts it to a sum: 
s' (x, y) = r' (x, y )e' (x, y). 
s(x, y) = rex, y) + e(x,y), 
(2) 
where s = loges'), r = log(r') and e = log(e'). 
Given s(x,y) alone, the problem of solving Eq. 2 for r(x,y) is underconstrained.  Lightness algorithms constrain the problem by restricting their domain to a world of  Mondrians, two-dimensional surfaces covered with patches of random colors2 and by  exploiting two constraints in that world: (i) r'(x,y) is unifonn within patches but  has sharp discontinuities at edges between patches and (ii) e' (x, y) varies smoothly  across the Mondrian. Under these constraints, lightness algorithms can recover a good  approximation to r( x, y) and so can recover lightness triplets that label roughly constant  colors 10. 
We ask whether it is possible to synthesize from examples an algorithm that ex· 
tracts reflectance from image irradiance. and whether the synthesized algorithm will re(cid:173) semble existing lightness algorithms derived from an explicit analysis of the constraints.  We make one assumption, that the operator that transforms irradiance into reflectance  is linear. Under that assumption, motivated by considerations discussed later, we use  optimal linear estimation techniques to synthesize an operator from examples. The  examples are pairs of images: an input image of a Mondrian under illumination that  varies smoothly across space and its desired output image that displays the reflectance  of the Mondrian without the illumination. The technique finds the linear estimator  that best maps input into desired output. in the least squares sense. 
For computational convenience we use one-dimensional ""training vectors"" that  represent vertical scan lines across the ~londrian images (Fig. 1). We generate many"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/25a3192c804d6b1c7d309c0155d3aa1a-Abstract.html,Phasor Neural Networks,André J. Noest,"A novel  network  type  is  introduced  which  uses  unit-length  2-vectors 
for  local  variables.  As  an  example  of  its  applications,  associative  memory  nets  are  defined  and  their  performance  analyzed.  Real  systems  corresponding  to  such  'phasor'  models  can  be  e.g.  (neuro)biological  networks  of  limit-cycle  oscillators  or  optical  resonators  that  have  a  hologram  in  their  feedback  path."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/2c929ca7263d51b38312797c9a9a6358-Abstract.html,Cycles: A Simulation Tool for Studying Cyclic Neural Networks,Michael T. Gately,"A computer program has been designed and implemented to allow a researcher 
to  analyze the oscillatory behavior of simulated neural networks with  cyclic  con(cid:173) nectivity.  The  computer  program,  implemented  on  the  Texas  Instruments  Ex(cid:173) plorer / Odyssey system, and the results of numerous experiments are discussed. 
The program, CYCLES, allows a user to construct, operate, and inspect neural 
networks containing cyclic connection paths with  the aid of a  powerful graphics(cid:173) based interface.  Numerous cycles have been studied, including cycles with one or  more activation points, non-interruptible cycles, cycles with variable path lengths,  and interacting cycles.  The final  class,  interacting cycles,  is  important due to its  ability to implement time-dependent goal processing in neural networks."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/2f9b1b6b29361118f630783c19891ea0-Abstract.html,A Method for the Design of Stable Lateral Inhibition Networks that is Robust in the Presence of Circuit Parasitics,"J.L. WYATT, Jr, D.L. STANDLEY","In  the  analog  VLSI  implementation  of  neural  systems,  it is 
sometimes  convenient  to  build  lateral  inhibition  networks  by  using  a  locally  connected  on-chip  resistive  grid.  A  serious  problem  of  unwanted  spontaneous  oscillation  often  arises  with  these  circuits  and  renders  them  unusable  in  practice.  This  paper  reports  a  design  approach  that  guarantees  such  a  system  will  be  stable,  even  though  the  values  of  designed  elements  and  parasitic  elements  in  the  resistive  grid  may  be  unknown.  The  method  is  based  on  a  rigorous,  somewhat  novel  mathematical  analysis  using  Tellegen's  theorem  and  the  idea  of  Popov  multipliers  from  control  theory.  It  is  thoroughly  practical  because  the  criteria  are  local  in  the  sense  that  no  overall  analysis  of  the  interconnected  system  is  required,  empirical  in  the  sense  that  they  involve  only  measurable  frequency  response  data  on  the  individual  cells,  and  robust  in  the  sense  that  unmodelled  parasitic  resistances  and  capacitances  in  the  inter(cid:173) connection  network  cannot  affect  the  analysis. 
I."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/2ff175df3b37981e4ea5aab357043d82-Abstract.html,Encoding Geometric Invariances in Higher-Order Neural Networks,"C.L. Giles, R.D. Griffin, T. Maxwell","We  describe  a  method  of  constructing  higher-order  neural 
networks  that  respond  invariantly  under  geometric  transformations  on  the  input  space.  By  requiring  each  unit  to  satisfy  a  set  of  constraints  on  the  interconnection  weights,  a  particular  structure  is  imposed  on  the  network.  A  network  built  using  such  an  architecture  maintains  its  invariant  performance  independent  of  the  values  the  weights  assume,  of  the  learning  rules  used,  and  of  the  form  of  the  nonlinearities  in  the  network.  The  invariance  exhibited  by  a  first(cid:173) order  network  is  usually  of  a  trivial  sort,  e.g.,  responding  only  to  the  average  input  in  the  case  of  translation  invariance,  whereas  higher-order  networks  can  perform  useful  functions  and  still  exhibit  the  invariance.  We  derive  the  weight  constraints  for  translation,  rotation,  scale,  and  several  combinations  of  these  transformations,  and  report  results  of  simulation  studies."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/317fd294bfd5c40816ce48bae30b1d4c-Abstract.html,Scaling Properties of Coarse-Coded Symbol Memories,"Ronald Rosenfeld, David S. Touretzky","Abstract:  Coarse-coded symbol memories have appeared in several neural network 
symbol  processing  models.  In  order  to  determine  how  these  models  would  scale,  one 
must  first  have  some  understanding  of the  mathematics  of coarse-coded  representa(cid:173)
tions.  We  define  the  general  structure  of coarse-coded  symbol  memories  and  derive 
mathematical relationships among  their essential parameters:  memory 8ize,  8ymbol-8et 
size and capacity.  The computed capacity of one of the schemes agrees well with actual 
measurements oC  tbe coarse-coded working memory of DCPS, Touretzky and Hinton's 
distributed connectionist production system."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/35ab33f5f9a61426560675e75c14cc0b-Abstract.html,"HOW THE CATFISH TRACKS ITS PREY: AN INTERACTIVE ""PIPELINED"" PROCESSING SYSTEM MAY DIRECT FORAGING VIA RETICULOSPINAL NEURONS",Jagmeet S. Kanwal,of
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/3c5882a72c072a68dc50d4091eae11df-Abstract.html,Capacity for Patterns and Sequences in Kanerva's SDM as Compared to Other Associative Memory Models,James D. Keeler,"The  information  capacity  of Kanerva's  Sparse,  Distributed Memory  (SDM)  and  Hopfield-type  neural networks  is  investigated.  Under  the  approximations  used here,  it  is shown  that  the  to(cid:173) tal  information  stored in  these  systems  is proportional  to  the  number  connections  in  the  net(cid:173) work.  The  proportionality  constant  is  the  same  for  the  SDM  and  HopJreld-type  models  in(cid:173) dependent  of  the  particular  model,  or  the  order  of  the  model.  The  approximations  are  checked  numerically.  This  same  analysis  can  be  used  to  show  that  the  SDM  can  store  se(cid:173) quences  of spatiotemporal patterns,  and  the  addition  of time-delayed  connections  allows  the  retrieval  of context  dependent  temporal  patterns.  A  minor  modification  of the  SDM  can  be  used to store correlated patterns."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/3d8edd573b5b21fede5d98ecee0f6382-Abstract.html,A NEURAL NETWORK CLASSIFIER BASED ON CODING THEORY,"Tzi-Dar Chiueh, Rodney Goodman","The new neural network classifier we propose transforms the  classification problem into the coding theory problem of decoding a noisy  codeword. An input vector in the feature space is transformed into an internal  representation which is a codeword in the code space, and then error correction  decoded in this space to classify the input feature vector to its class. Two classes  of codes which give high performance are the Hadamard matrix code and the  maximal length sequence code. We show that the number of classes stored in an  N-neuron system is linear in N and significantly more than that obtainable by  using the Hopfield type memory as a classifier."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/3ed923f9f88108cb066c6568d3df2666-Abstract.html,Probabilistic Characterization of Neural Model Computations,Richard M. Golden,"Information  retrieval  in  a  neural  network  is  viewed  as  a  procedure  in 
which  the  network  computes  a  ""most  probable""  or  MAP estimate  of the  unk(cid:173) nown information.  This viewpoint allows the class of probability distributions,  P, the neural network can acquire to be explicitly  specified. Learning algorithms  for the  neural  network  which  search  for  the  ""most probable""  member of P can  then  be  designed.  Statistical  tests  which  decide if the  ""true""  or  environmental  probability  distribution  is in  P can also  be developed.  Example applications of  the theory to  the  highly nonlinear back-propagation learning algorithm,  and  the  networks of Hopfield and Anderson are discussed."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/48152e80d1b9822ce18941eb437f1cba-Abstract.html,A Novel Net that Learns Sequential Decision Process,"G.Z. SUN, Y.C. LEE, H.H. CHEN","We propose a  new  scheme  to construct  neural networks  to classify  pat(cid:173)
terns.  The new scheme has several novel features  : 

We focus  attention on the important attributes of patterns in ranking  order.  Extract  the  most  important ones  first  and  the less  important  ones later. 
In  training we  use  the  information  as  a  measure  instead of the  error"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/4878a0527a93d431d2637338f51cb4af-Abstract.html,An Adaptive and Heterodyne Filtering Procedure for the Imaging of Moving Objects,"F. H. Schuling, H. A. K. Mastebroek, W. H. Zaagman","Recent experimental work on the stimulus velocity dependent time resolving  power of the neural units, situated in the highest order optic ganglion of the  blowfly, revealed the at first sight amazing phenomenon that at this high level of  the fly visual system, the time constants of these units which are involved in the  processing of neural activity evoked by moving objects, are -roughly spoken(cid:173) inverse proportional to the velocity of those objects over an extremely wide range.  In this paper we will discuss the implementation of a two dimensional heterodyne  adaptive filter construction into a computer simulation model. The features of this  simulation model include the ability to account for the experimentally observed  stimulus-tuned adaptive temporal behaviour of time constants in the fly visual  system. The simulation results obtained, clearly show that the application of such  an adaptive processing procedure delivers an improved imaging technique of  moving patterns in the high velocity range. 
A FEW REMARKS ON THE FLY VISUAL SYSTEM"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/4a420924d20bc025ebb37849169e6ebd-Abstract.html,Strategies for Teaching Layered Networks Classification Tasks,"Ben S. Wittner, John S. Denker","There is  a widespread misconception  that the delta-rule is in some sense guaranteed to  work  on  networks  without hidden units.  As  previous authors have mentioned,  there is  no such guarantee for  classification tasks.  We  will begin by  presenting explicit counter(cid:173) examples illustrating two different interesting ways in which  the delta rule can fail.  We  go on  to provide conditions  which  do guarantee  that gradient  descent  will  successfully  train  networks  without  hidden  units  to  perform  two-category  classification  tasks.  We  discuss  the  generalization  of our  ideas  to  networks  with  hidden  units  and  to  multi(cid:173) category classification  tasks. 
The Classification Task 
Consider  networks  of the form  indicated  in  figure  1.  We  discuss  various  methods  for  training such  a  network,  that is  for  adjusting its weight  vector,  w.  If we  call  the input  v, the output is  g(w· v), where 9 is  some function. 
The classification  task we  wish  to train the network  to perform is  the following.  Given  two finite sets of vectors,  Fl  and F2, output a number greater than zero when a vector in  Fl  is input, and output a number less than zero when a  vector in  F2  is input.  Without  significant loss of generality, we  assume that 9 is odd (Le.  g( -s) ==  -g( s».  In that case,  the task can be reformulated  as follows.  Define  2 
F  :==  Fl  U {-v such  that v  E F2} 
(1) 
and  output  a  number  greater  than  zero  when  a  vector  in  F  is  input.  The  former  formulation is  more natural in some sense,  but the later formulation is  somewhat  more  convenient  for  analysis  and is  the one we  use.  We  call  vectors in  F,  training  vectors. 
A  Class  of Gradient  Descent  Algorithms 
We  denote  the solution set  by 
W  :==  {w such  that g(w· v) > 0 for  all v  E F}, 
(2) 
lCurrently at  NYNEX  Science  and Technology,  500  Westchester  Ave.,  White  Plains,  NY  10604  2 We  use  both  A  := Band B  =: A  to denote  ""A  is  by definition  B"". 
@  American Institute of Physics 1988"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/4b613e438a6b3842f77056e5b4c9b42e-Abstract.html,Neuromorphic Networks Based on Sparse Optical Orthogonal Codes,"Mario P. Vecchi, Jawad A. Salehi",Abstract Unavailable
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/4dd1a7279a8cfeea2660fbc34f02a2bc-Abstract.html,New Hardware for Massive Neural Networks,"D. D. Coon, A. G. U. Perera","Transient phenomena associated with forward biased silicon p + - n - n + struc(cid:173) tures at 4.2K show remarkable similarities with biological neurons.  The devices  play  a  role  similar to the  two-terminal switching elements in  Hodgkin-Huxley  equivalent  circuit  diagrams.  The  devices  provide simpler  and  more  realistic  neuron  emulation  than  transistors  or op-amps.  They  have  such  low  power  and  current  requirements  that  they  could  be  used  in  massive  neural  networks.  Some  observed  properties  of  simple  circuits  containing  the  devices  include  action  potentials,  refractory  periods,  threshold behavior, excitation, inhibition, summation over synaptic inputs, synaptic  weights,  temporal  integration, memory,  network  connectivity modification  based  on  experience, pacemaker activity, firing  thresholds, coupling to sensors with graded sig(cid:173) nal  outputs  and  the  dependence  of firing  rate  on input  current.  Transfer functions  for simple artificial neurons with spiketrain inputs and spiketrain outputs have been  measured  and correlated with input coupling."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/5190e987c46a346974e351f96997d640-Abstract.html,Mathematical Analysis of Learning Behavior of Neuronal Models,"JOHN Y. CHEUNG, MASSOUD OMIDVAR","In  this  paper,  we  wish  to  analyze  the  convergence  behavior  of a  number  of neuronal plasticity models.  Recent neurophysiological research suggests that  the neuronal behavior is adaptive.  In particular, memory stored within a neuron  is  associated with the synaptic weights which are varied or adjusted to achieve  learning.  A  number  of adaptive  neuronal  models  have  been  proposed  in  the  literature.  Three specific models will be analyzed in this paper, specifically the  Hebb model, the Sutton-Barto model, and the most recent trace model.  In this  paper we  will  examine  the conditions  for  convergence,  the  position  of conver(cid:173) gence and the rate at convergence,  of these models  as they applied to classical  conditioning.  Simulation results  are also presented to verify the analysis."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/523f87e9d08e6071a3bbd150e6da40fb-Abstract.html,Bit-Serial Neural Networks,"Alan F. Murray, Anthony V. W. Smith, Zoe F. Butler","A  bit  - serial  VLSI  neural  network  is  described  from  an  initial  architecture  for  a  synapse array through to silicon layout and board design.  The issues surrounding bit  - serial  computation,  and  analog/digital  arithmetic  are  discussed  and  the  parallel  development  of  a  hybrid  analog/digital  neural  network  is  outlined.  Learning  and  recall  capabilities  are  reported  for  the  bit  - serial  network  along  with  a  projected  specification  for  a  64  - neuron,  bit  - serial  board  operating  at 20 MHz.  This tech(cid:173) nique  is  extended  to  a  256  (2562  synapses)  network  with  an  update  time  of 3ms,  using  a  ""paging""  technique  to  time  - multiplex  calculations  through  the  synapse  array."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/58b244eaff03de43ae7db44c4e787105-Abstract.html,The Sigmoid Nonlinearity in Prepyriform Cortex,Frank H. Eeckman,Abstract Unavailable
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/5b47430e24a5a1f9fe21f0e8eb814131-Abstract.html,Neural Net and Traditional Classifiers,"William Y. Huang, Richard P. Lippmann","Abstract.  Previous  work on  nets  with  continuous-valued inputs  led  to generative 
procedures  to construct convex decision  regions with two-layer perceptrons (one hidden 
layer) and  arbitrary decision  regions  with  three-layer perceptrons  (two hidden layers). 
Here we demonstrate that two-layer perceptron classifiers trained with back propagation 
can  form  both  convex  and  disjoint  decision  regions.  Such  classifiers  are  robust,  train 
rapidly,  and  provide  good  performance  with  simple  decision  regions.  When  complex 
decision  regions  are  required,  however,  convergence  time  can  be  excessively  long  and 
performance is  often no better than that of k-nearest  neighbor classifiers.  Three neural 
net  classifiers  are  presented  that  provide  more  rapid  training  under  such  situations. 
Two use  fixed  weights in  the  first  one  or  two layers and  are  similar  to classifiers  that 
estimate probability density functions using histograms.  A third ""feature map classifier"" 
uses  both  unsupervised  and  supervised  training.  It provides  good  performance  with 
little supervised training in situations such as speech recognition where much unlabeled 
training data is  available.  The architecture of this classifier can  be  used  to implement 
a  neural net  k-nearest  neighbor classifier."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/6270a15843a2e06a95d3e3ad8b489e4b-Abstract.html,Time-Sequential Self-Organization of Hierarchical Neural Networks,"Ronald H. Silverman, Andrew S. Noetzel","Self-organization  of  multi-layered  networks  can  be  realized  time-sequential  organization  of  successive  neural  layers.  by  Lateral  inhibition  operating  in  the  surround  of  firing  cells  in  each  for  unsupervised  capture  of  excitation  patterns  presented  by  the  previous  layer.  By  presenting  patterns  of  organization,  higher  implicit  in  the  pattern  set. 
in  co-ordination  with  network  self(cid:173)
the  hierarchy  capture  concepts 
increasing  complexity,"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/650e2245aa3513ed517f4cf1b3d58e06-Abstract.html,Using Neural Networks to Improve Cochlear Implant Speech Perception,Manoel F. Tenorio,"-
An  increasing  number  of  profoundly  deaf  patients  suffering  from  sen- sorineural  deafness  are  using  cochlear  implants  as  prostheses.  Mter  the  implant,  sound  can  be  detected  through  the  electrical  stimulation  of  the  remaining  peripheral  auditory  nervous  system.  Although  great  progress  has  been  achieved  in  this  area,  no  useful  speech  recognition  has  been  attained  with either single  or multiple  channel cochlear implants. 
Coding  evidence  suggests  that  it  is  necessary  for  any  implant  which 
would  effectively  couple  with  the  natural  speech  perception  system  to simu(cid:173) late  the  temporal  dispersion  and  other  phenomena  found  in  the  natural  receptors,  and  currently  not  implemented  in  any  cochlear  implants.  To  this  end,  it  is  presented  here  a  computational  model  using  artificial  neural  net(cid:173) works  cochlear. 
the  natural  phenomena"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/676638b91bc90529e09b22e58abb01d6-Abstract.html,Learning in Networks of Nondeterministic Adaptive Logic Elements,Richard C. Windecker,from
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/678e209691cd37f145a5502695378bac-Abstract.html,Learning on a General Network,Amir F. Atiya,"This paper generalizes the backpropagation method to a  general network containing feed(cid:173)
back t;onnections.  The network model considered consists of interconnected groups of neurons,  where each  group could be fully  interconnected  (it could  have feedback connections, with pos(cid:173) sibly  asymmetric weights),  but no loops between the  groups are  allowed.  A stochastic descent  algorithm  is  applied,  under  a  certain inequality constraint  on each  intra-group weight  matrix  which  ensures for  the  network  to  possess  a  unique equilibrium  state for  every input."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/68ba979a6eef19c1fa7771e6582185bc-Abstract.html,Analysis of Distributed Representation of Constituent Structure in Connectionist Systems,Paul Smolensky,"A  general  method, the  tensor product representation, is described for  the distributed representation of  value/variable bindings.  The method allows the fully distributed representation of symbolic structures:  the roles  in  the structures, as well as the fillers  for  those roles, can be arbitrarily non-local.  Fully and  partially localized  special cases reduce to existing cases of connectionist representations of structured  data;  the  tensor  product  representation  generalizes  these  and  the  few  existing  examples  of  fuUy  distributed  representations  of structures.  The  representation  saturates  gracefully  as  larger  structures  are  represented;  it penn its  recursive  construction  of complex  representations  from  simpler  ones;  it  respects  the  independence  of the  capacities  to generate and  maintain  multiple bindings in  parallel;  it  extends naturally to continuous structures and continuous representational patterns; it pennits values to  also  serve  as  variables;  it  enables  analysis  of  the  interference  of  symbolic  structures  stored  in  associative  memories;  and  it  leads  to characterization  of optimal  distributed representations  of roles  and a recirculation algorithm for learning them."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/6ad10c3a760cfad3e5d1fa1ddaefdec2-Abstract.html,Generalization of Back propagation to Recurrent and Higher Order Neural Networks,Fernando J. Pineda,"A general method for deriving backpropagation algorithms for networks 
with recurrent and higher order networks is introduced.  The propagation of activation  in these networks is determined by dissipative differential equations.  The error signal  is backpropagated by integrating an associated differential equation.  The method is  introduced by applying it to the recurrent generalization of the feedforward  backpropagation network.  The method is extended to the case of higher order  networks and to a constrained dynamical system for training a content addressable  memory.  The essential feature of the adaptive algorithms is that adaptive equation has  a simple outer product form. 
Preliminary experiments suggest that learning can occur very rapidly in 
networks with recurrent connections.  The continuous formalism makes the new  approach more suitable for implementation in VLSI."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/6c44dc73014d66ba49b28d483a8f8b0d-Abstract.html,HIGH DENSITY ASSOCIATIVE MEMORIES,"Amir Dembo, Ofer Zeitouni","from a description of desired properties 
A class of high dens ity assoc iat ive memories 
is constructed,  starting  should  exhib it. These propert ies include high capac ity, controllable bas ins  of attraction and fast speed of convergence. Fortunately enough, the  resulting memory is implementable by an artificial Neural Net."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/71996f80223a3e89a5bd0139908097db-Abstract.html,A Computer Simulation of Cerebral Neocortex: Computational Capabilities of Nonlinear Neural Networks,"Alexander Singer, John P. Donoghue",Abstract Unavailable
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/77cdf4ffbd2afd02541e02533ec56820-Abstract.html,A Neural-Network Solution to the Concentrator Assignment Problem,"Gene A. Tagliarini, Edward W. Page",Networks  of simple analog  processors  having  neuron-like properties have  been  employed  to  compute  good  solutions  to  a  variety  of optimization  prob(cid:173) lems.  This  paper presents  a  neural-net solution to  a  resource allocation prob(cid:173) lem that arises  in  providing  local  access  to  the  backbone of a  wide-area  com(cid:173) munication  network.  The  problem is  described in  terms of an energy function  that can be  mapped onto an analog computational network.  Simulation results  characterizing  the  performance  of the  neural  computation  are  also  presented.
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/79c36fe64e04b80fc44845bb9fe73242-Abstract.html,Correlational Strength and Computational Algebra of Synaptic Connections Between Neurons,Eberhard E. Fetz,"Intracellular  recordings  in  spinal  cord  motoneurons  and  cerebral  cortex neurons have provided new evidence on the correlational strength of  monosynaptic  connections,  and  the  relation  between  the  shapes  of  postsynaptic  potentials  and  the  associated  increased  firing  probability.  In  these  cells,  excitatory  postsynaptic  potentials  (EPSPs)  produce  cross(cid:173) correlogram peaks  which resemble  in large part the derivative of  the EPSP.  Additional  synaptic  noise broadens  the peak,  but the  peak  area  -- i.e.,  the  number of above-chance firings triggered per EPSP -- remains proportional to  the EPSP  amplitude.  A typical EPSP of 100  ~v triggers about .01  firings per  EPSP.  The  consequences  of  these  data  for  information  processing  by  polysynaptic connections is discussed.  The effects of sequential polysynaptic  links  can  be  calculated  by  convolving  the  effects  of  the  underlying  monosynaptic connections.  The net effect of parallel pathways is the sum of  the individual contributions."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/7cc2b68825046ade9b4a2f3b1dbd675b-Abstract.html,The Connectivity Analysis of Simple Association,Dan Hammerstrom,"The efficient realization, using current silicon technology, of Very Large Connection  Networks (VLCN) with more than a billion connections requires that these networks exhibit  a high degree of communication locality. Real neural networks exhibit significant locality,  yet most connectionist/neural network models have little. In this paper, the connectivity  requirements of a simple associative network are analyzed using communication theory.  Several techniques based on communication theory are presented that improve the robust(cid:173) ness of the network in the face of sparse, local interconnect structures. Also discussed are  some potential problems when information is distributed too widely."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/8081be7b20e6ddb1a26c2786da98e137-Abstract.html,Temporal Patterns of Activity in Neural Networks,Paolo Gaudiano,"Patterns  of activity  over  real  neural  structures  are  known  to  exhibit  time(cid:173)
dependent  behavior.  It  would  seem  that  the  brain  may  be  capable  of utilizing  temporal  behavior of activity in neural  networks as  a  way  of performing functions  which cannot otherwise be easily implemented.  These might include the origination  of sequential behavior and  the  recognition  of time-dependent  stimuli.  A  model  is  presented  here  which  uses  neuronal  populations  with  recurrent  feedback  connec(cid:173) tions in an attempt to observe and describe  the resulting time-dependent behavior.  Shortcomings and problems inherent  to  this model are  discussed.  Current models  by other  researchers  are  reviewed  and their similarities and differences  discussed. 
METHODS  /  PRELIMINARY RESULTS 
In previous  papers,[2,3]  computer models  were  presented  that  simulate a  net  con(cid:173)
sisting of two spatially organized populations of realistic neurons.  The populations are  richly  interconnected  and  are  shown  to  exhibit  internally  sustained  activity.  It  was  shown that if the neurons have response times significantly shorter than the typical unit  time characteristic of the input  patterns  (usually  1 msec),  the populations will exhibit  time-dependent behavior.  This will typically result in the net falling into a  limit cycle.  By a limit cycle, it is meant that the population falls into activity patterns during which  all of the active cells  fire  in  a  cyclic,  periodic  fashion.  Although  the period of firing  of  the individual  cells  may be  different,  after  a  fixed  time  the  overall population activity  will  repeat  in  a  cyclic,  periodic  fashion.  For  populations  organized  in  7x7  grids,  the  limit  cycle  will  usually  start  20~200 msec  after  the input  is  turned off,  and  its period  will be in  the order of 20-100  msec. 
The point ofinterest is that ifthe net is allowed to undergo synaptic modifications by  means  of a  modified  hebbian learning rule  while being presented with a  specific  spatial  pattern (i.e., cells at  specific  spatial locations within the net are externally stimulated),  subsequent  presentations  of the  same  pattern  with  different  temporal  characteristics  will cause the population to recall patterns which are spatially identical (the same cells  will be active)  but which have different  temporal qualities.  In other words, the net can  fall  into a  different  limit  cycle.  These limit cycles  seem  to behave as  attractors in that  similar input  patterns will result  in  the  same limit cycle,  and hence  each distinct limit  cycle  appears  to have a  basin of attraction.  Hence a  net  which can  only learn a  small 
© American Institute of Physics 1988 
298 
number  of spatially  distinct  patterns  can recall  the  patterns in a  number  of temporal  modes.  If it were possible to quantitatively discriminate between such temporal modes,  it  would  seem  reasonable  to  speculate  that  different  limit  cycles  could  correspond  to  different  memory traces.  This would  significantly increase estimates on the  capacity of  memory  storage in the net. 
It has  also been shown that a net being presented with a  given pattern will fall and  stay  into  a  limit  cycle  until another  pattern is  presented  which  will  cause  the  system  to fall  into a  different  basin of attraction.  If no other  patterns  are presented,  the  net  will remain in  the  same limit  cycle  indefinitely.  Furthermore, the net  will fall  into the  same  limit  cycle  independently  of the  duration  of the  input  stimulus,  so  long  as  the  input stimulus is  presented for  a  long enough time to raise the population activity level  beyond  a  minimum necessary  to  achieve  self-sustained  activity.  Hence,  if we  suppose  that  the  net  ""recognizes""  the input  when  it  falls  into the  corresponding  limit  cycle,  it  follows  that the net will recognize a string of input patterns regardless of the duration of  each input pattern, so long as each input is presented long enough for the net  to fall into  the  appropriate limit  cycle.  In particular,  our  system is  capable of falling  into  a  limit  cycle within some tens of milliseconds.  This can be fast enough to encode, for example, a  string of phonemes as would typically be found in continuous speech.  It may be possible,  for  instance,  to create a  model  similar  to Rumelhart  and McClelland's  1981  model  on  word recognition by appropriately connecting multiple layers  of these networks.  If the  response  time  of the  cells  were  increased  in  higher  layers,  it may  be  possible  to have  the  lowest  level  respond  to  stimuli  quickly  enough  to  distinguish  phonemes  (or  some  sub-phonemic basic linguistic unit), then have populations from this first  level feed  into  a  slower,  word-recognizing population layer,  and  so  On.  Such a  model  may  be  able  to  perform word  recognition from an input  consisting of continuous phoneme strings even  when  the phonemes  may vary in duration of presentation."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/853f7b3615411c82a2ae439ab8c4c96e-Abstract.html,An Artificial Neural Network for Spatio-Temporal Bipolar Patterns: Application to Phoneme Classification,"Toshiteru Homma, Les E. Atlas, Robert J. Marks II","An  artificial  neural  network  is  developed  to  recognize  spatio-temporal  bipolar patterns  associatively.  The  function  of a formal  neuron is  generalized by  replacing  multiplication  with  convolution,  weights  with  transfer  functions,  and  thresholding  with  nonlinear  transform  following  adaptation.  The Hebbian  learn(cid:173) ing  rule  and  the  delta  learning  rule  are  generalized  accordingly,  resulting  in  the  learning  of weights  and  delays.  The  neural  network  which  was  first  developed  for  spatial  patterns  was  thus  generalized  for  spatio-temporal  patterns.  It  was  tested  using  a  set  of bipolar input patterns  derived from  speech  signals,  showing  robust classification of 30 model phonemes."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/875578931a159790107a9184e39a67a4-Abstract.html,Computing Motion Using Resistive Networks,"Christof Koch, Jin Luo, Carver Mead, James Hutchinson",Abstract Unavailable
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/8af873095176650d4c5e738e35383498-Abstract.html,On Properties of Networks of Neuron-Like Elements,"Pierre Baldi, Santosh S. Venkatesh","The  complexity  and  computational  capacity  of multi-layered,  feedforward  neural networks is examined.  Neural networks for special purpose (structured)  functions  are examined  from  the  perspective of circuit  complexity.  Known  re(cid:173) sults in  complexity theory are applied to the special instance of neural network  circuits,  and  in  particular,  classes  of functions  that  can  be  implemented  in  shallow circuits characterised.  Some conclusions are  also  drawn about learning  complexity,  and some  open problems raised.  The dual  problem of determining  the computational capacity of a  class of multi-layered  networks with  dynamics  regulated  by  an  algebraic  Hamiltonian  is  considered.  Formal  results  are  pre(cid:173) sented  on  the  storage  capacities  of programmed  higher-order  structures,  and  a  tradeoff between ease  of programming  and  capacity is  shown.  A  precise  de(cid:173) termination is  made of the static fixed  point structure of random higher-order  constructs,  and phase-transitions (0-1  laws)  are  shown."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/91fc23ceccb664ebb0cf4257e1ba9c51-Abstract.html,Analysis and Comparison of Different Learning Algorithms for Pattern Association Problems,J. Bernasconi,"We 
investigate the behavior of different learning algorithms 
for networks of neuron-like units. As test cases we use simple pat(cid:173) tern association problems, such as the XOR-problem and symmetry de(cid:173) tection problems. The algorithms considered are either versions of  the Boltzmann machine learning rule or based on the backpropagation  of errors. We also propose and analyze a generalized delta rule for  linear threshold units. We  find that the performance of a given  learning algorithm depends strongly on the type of units used. In  particular, we observe that networks with ±1 units quite generally  exhibit a significantly better learning behavior than the correspon(cid:173) ding 0,1 versions. We also demonstrate that an adaption of the  weight-structure to  the symmetries of the problem can lead to a  drastic increase in learning speed."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/935ad074f32d1e8f085a143449894cdc-Abstract.html,Optimal Neural Spike Classification,"Amir F. Atiya, James M. Bower","Being able  to record the electrical activities of a  number of neurons simultaneously is  likely  to be important in  the study of the functional organization of networks of real neurons.  Using  one  extracellular  microelectrode  to  record  from  several neurons  is  one  approach  to  studying  the  response  properties  of sets  of  adjacent  and  therefore  likely  related  neurons.  However,  to  do  this,  it  is  necessary  to  correctly  classify  the  signals  generated  by  these  different  neurons.  This paper considers  this problem of classifying the  signals  in  such an  extracellular recording,  based upon their shapes, and specifically considers the classification of signals in the case when  spikes overlap  temporally."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/9412531719be7ccf755c4ff98d0969dc-Abstract.html,An Optimization Network for Matrix Inversion,"Ju-Seog Jang, Soo-Young Lee, Sang-Yung Shin","Inverse  matrix  calculation  can  be  considered  as  an  optimization.  We  have  demonstrated  that  this  problem  can  be  rapidly  solved  by  highly  interconnected  simple  neuron-like  analog  processors.  A  network  for  matrix  inversion  based  on  the  concept  of  Hopfield's  neural  network  was  designed,  and  implemented  with  electronic  hardware.  With  slight  modifications,  the  network  is  readily  applicable  to  solving  a  linear  simultaneous  equation  efficiently.  Notable  features  of  this  circuit  are  potential  speed  due  to  parallel  processing,  and  robustness  against  variations  of  device  parameters."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/98ae8cb22462884b1b8c0b351779292b-Abstract.html,On the Power of Neural Networks for Solving Hard Problems,"Jehoshua Bruck, Joseph W. Goodman","This  paper deals  with a  neural network model in  which each neuron  performs a  threshold logic function.  An important property of the model  is  that  it always  converges  to a  stable state  when  operating in  a  serial  mode [2,5].  This property is  the basis of the potential applications of the  model such as associative memory devices and combinatorial optimization  [3,6].  One of the motivations for use of the model for solving hard combinatorial  problems  is  the fact  that it can  be implemented  by optical devices  and  thus operate at a  higher speed  than conventional electronics.  The main theme in this work is  to investigate the power of the model for  solving NP-hard problems  [4,8],  and to understand  the relation  between  speed of operation and the size of a  neural network.  In particular, it will  be  shown  that  for  any  NP-hard  problem  the  existence  of a  polynomial  size  network  that  solves  it  implies  that  NP=co-NP.  Also,  for  Traveling  Salesman  Problem  (TSP), even  a  polynomial  size  network  that  gets  an  €-approximate  solution does  not exist unless  P=NP. 
The above results  are of great  practical interest,  because  right  now it  is  possible  to build neural networks  which will  operate fast  but are limited  in  the number of neurons."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/9db64c20dee0011899dfdf200e61ef35-Abstract.html,Programmable Synaptic Chip for Electronic Neural Networks,"A. Moopenn, H. Langenbacher, A.P. Thakoor, S.K. Khanna","A binary  synaptic  matrix  chip  has  been  developed  for  electronic  neural  networks.  The  matrix  chip  contains  a  programmable  32X32  array  of  ""long  channel""  NMOSFET  binary  connection  elements  imple(cid:173) mented  in  a  3-um  bulk  CMOS  process.  Since  the  neurons  are  kept  off(cid:173) chip,  the  synaptic  chip  serves  as  a  ""cascadable""  building  block  for  a  multi-chip  synaptic  network  as  large  as  512X512  in  size.  As  an  alternative  the  programmable  NMOSFET  (long  channel)  connection  elements,  tailored  thin  film  resistors  are  deposited,  in  series  with  FET  switches,  on  some  CMOS  test  chips,  to  obtain  the  weak  synaptic  connections.  Although  deposition  and  patterning  of  the  resistors  require  additional  they  promise  substantial  savings  in  silcon  area.  The  performance  of  a  synaptic  chip  in  a  32- neuron  breadboard  system  in  an  associative  memory  test  application  is  discussed. 
processing  steps, 
to"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/9fccde8a665c57cbde962b405ee6a44b-Abstract.html,Hierarchical Learning Control - An Approach with Neuron-Like Associative Memories,"E. Ersü, H. Tolle",Advances
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/a05a7b15b2006378156a777243d58818-Abstract.html,SPONTANEOUS AND  INFORMATION-TRIGGERED SEGMENTS OF SERIES OF HUMAN BRAIN ELECTRIC FIELD MAPS,"D. Lehmann, D. Brandeis, A. Horst, H. Ozaki, I. Pal","The brain works in a state-dependent manner: processin9 
strate9ies and access to stored information depends on the momentary  functional state which is continuously re-adjusted. The state is  manifest as spatial confi9uration of the brain electric field.  Spontaneous and information-tri9gered brain electric activity is a  series of momentary field maps. Adaptive segmentation of spontaneous  series into spatially stable epochs (states) exhibited 210 msec mean  segments, discontinuous changes. Different maps imply different  active neural populations, hence expectedly different effects on  information processing: Reaction time differred between map classes  at stimulus arrival. Segments might be units of brain information  processin9 (content/mode/step), possibly operationalizin9  consciousness time. Related units (e.9. tri9gered by stimuli durin9  fi9ure perception and voluntary attention) mi9ht specify brain sub(cid:173) mechanisms of information treatment. 
BRAIN FUNCTIONAL STATES AND THEIR CHANGES 
The momentary functional state of the brain is reflected by the 
confi9uration of the brain's electro-ma9netic field. The state  manifests the strate9Y, mode, step and content of brain information  processing, and the state constrains the choice of strate9ies and  modes and the access to memory material available for processin9 of  incoming information (1). The constraints include the available  range of changes of state in PAVLOV's classical ·orienting reaction""  as response to new or important informations. Different states mi9ht  be viewed as different functional connectivities between the neural  elements. 
The orienting reaction (see 1,2) is the result of the first  (Mpre-attentiveM) stage of information processing. This stage  operates automatically (no involvement of consciousness) and in a  parallel mode, and quickly determines whether (a) the information is  important or unknown and hence requires increased attention and  alertness, i.e. an orienting reaction which means a re-adjustment of  functional state in order to deal adequately with the information  invokin9 consciousness for further processing, or whether (b) the  information is known or unimportant and hence requires no re(cid:173) adjustment of state, i.e. that it can be treated further with well-

Present addresses: D.B. at Psychiat. Dept., V.A. Med. Center, San  Francisco CA 94121; H.O. at lab. Physiol. for the Developmentally  Handicapped, Ibaraki Univ., Mito, Japan 310; I.P. at Biol09ic  Systems Corp., Mundelein Il 60060. 

© American Institute of Physics 1988 
468 
established (·automatic·) strategies. Conscious strategies are slow  but flexible (offer wide choice), automatic strategies are fast but  rigid. 
Examples for functional states on a gross scale are wakefulness,  drowsin.ss and sleep in adults, or developmental stages as infancy,  childhood and adolesc.nce, or drug states induced by alcohol or  other psychoactive agent •• The different states are associated with  distinctly different ways of information processing. For example, in  normal adults, reality-close, abstracting strategies based on causal  relationships predominate during wakefulness, whereas in drowsiness  and sleep (dreams), reality-remote, visualizing, associative  concatenations of contents are used. Other well-known examples are  drug states. 
HUMAN BRAIN ELECTRIC FIELD DATA AND STATES 
While alive, the brain produces an ever-changing el.ctromagnetic 
fi.ld, which very sensitively reflects global and local states as  effected by spontaneous activity, incoming information, metabolism,  drugs, and diseases. The .lectric component of the brain~s electro(cid:173) magnetic field as non-invasively measured from the intact human  scalp shows voltages between 0.1 and 250 microVolts, temporal  fr.quencies between 0.1 and 30, 100 or 3000 Hz depending on the  examined function, and spatial frequencies up to 0.2 cycles/em. 
Brain electric field data are traditionally viewed as time series 
of potential differences betwe.n two scalp locations (the  electroencephalogram or EE6). Time series analysis has offered an  effective way to class different gross brain functional states,  typically using EE6 power spectral values. Differences between power  spectra during different gross states typically are greater than  between different locations. States of lesser functional complexity  such as childhood vs adult states, sleep vs wakefulness, and many  drug-state. vs non-drug states tend to increased power in slower  frequencies (e.g. 1,4). 
Time series analyses of epochs of intermediate durations between 
30 and 10 seconds have demonstrated (e.g. 1,5,6) that there are  significant and reliable relations between spectral power or  coh.rency values of EE6 and characteristics of human mentation  (reality-close thoughts vs free associations, visual vs non-visual  thoughts, po.itive vs negative ~otions). 
Viewing brain electric field data as series of momentary field 
maps (7,8) opens the possibility to investigate the temporal  microstructure of brain functional states in the sub-second range.  The rationale is that the momentary configuration of activated  neural elements represents a given brain functional state, and that  the spatial pattern of activation is reflected by the momentary  brain electric field which is recordable on the scalp as a momentary  field map. Different configurations of activation (different field  maps) are expected to be associated with different modes,  strategies, steps and contents of information processing. 
SE(J1ENTATI~ OF BRAIN ELECTRIC HAP SERIES INTO STABLE SE(J1ENTS 
469 
When Viewing brain electric activity as series of maps of 
momentary potential distributions, changes of functional state are  recognizable as changes of the ·electric landscapes· of these maps.  Typically, several successive maps show similar landscapes, then  quickly change to a new configuration which again tends to persist  for a number of successive maps, suggestive of stable states  concatenated by non-linear transitions (9,10). Stable map landscapes  might be hypothesized to indicate the basic building blocks of  information processing in the brain, the -atoms of thoughts·. Thus,  the task at hand is the recognition of the landscape configurations;  this leads to the adaptive segmentation of time series of momentary  maps into segments of stable landscapes during varying durations. 
We have proposed and used a method which describes the 
configuration of a momentary map by the locations of its maximal and  minimal potential values, thus invoking a dipole model. The goal  here is the phenomenological recognition of different momentary  functional states using a very limited number of major map features  as classifiers, and we suggest conservative interpretion of the data  as to real brain locations of the generating processes which always  involve millions of neural elements. 
We have studied (11) map series recorded from 16 scalp locations 
over posterior skull areas from normal subjects during relaxation  with closed eyes. For adaptive segmentation, the maps at the times  of maximal map relief were selected for optimal signal/nOise  conditions. The locations of the maximal and minimal (extrema)  potentials were extracted in each map as descriptors of the  landscape; taking into account the basically periodic nature of  spontaneous brain electric activity (Fig. 1), extrema locations were  treated disregarding polarity information. If over time an extreme  left its pre-set spatial window (say, one electrode distance), the  segment was terminated. The map series showed stable map  configurations for varying durations (Fig. 2), and discontinuous,  step-wise changes. Over 6 subjects, resting alpha-type EEG showed  210 msec mean segment duration; segments longer than 323 msec  covered 50% of total time; the most prominent segment class (1.5% of  all classes) covered 20% of total time (prominence varied strongly  over classes; not all possible classes occurred). Spectral power and  phase of averages of adaptive and pre-determined segments  demonstrated the adequacy of the strategy and the homogeneity of  adaptive segment classes by their reduced within-class variance.  Segmentation using global map dissimilarity (sum of Euklidian  difference vs average reference at all measured points) emulates the  results of the extracted-characteristics-strategy. 
FUNCTIONAL SIGNIFICANCE OF MOMENTARY MICRO STATES 
Since different maps of momentary EEG fields imply activity of 
different neural populations, different segment classes must  manifest different brain functional states with expectedly different"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/a0a81eed87dd44d6504fed5f81f6de5a-Abstract.html,High Order Neural Networks for Efficient Associative Memory Design,"I. GUYON, L. PERSONNAZ, J. P. NADAL, G. DREYFUS",We  propose  learning  rules  for  recurrent  neural  networks  with  high-order  interactions  between  some or all  neurons.  The designed  networks  exhibit the  desired associative  memory  function: perfect  storage  and  retrieval  of pieces  of information and/or sequences of information of any complexity.
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/a1126573153ad7e9f44ba80e99316482-Abstract.html,Constrained Differential Optimization,"John C. Platt, Alan H. Barr","Many optimization models of neural  networks need constraints to restrict the space of outputs to  a subspace which satisfies external criteria.  Optimizations using energy methods yield ""forces"" which  act upon  the  state of the  neural  network.  The penalty method, in which quadratic  energy  constraints  are  added  to  an  existing  optimization  energy,  has  become  popular  recently,  but  is  not  guaranteed  to satisfy  the  constraint conditions  when  there  are  other forces  on  the  neural  model  or when  there  are  multiple constraints.  In this paper, we present the basic differential multiplier method (BDMM),  which  satisfies constraints exactly;  we  create forces  which gradually apply  the constraints over time,  using ""neurons"" that estimate Lagrange multipliers. 
The  basic  differential  multiplier  method  is  a  differential  version  of the  method  of multipliers  from  Numerical Analysis.  We  prove  that the differential  equations locally converge  to  a constrained  minimum. 
Examples of applications of the differential method of multipliers include enforcing permutation  codewords in the analog decoding problem and enforcing valid tours in the traveling salesman problem."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/ae5bb54885c12561e43ac87f6f89e934-Abstract.html,REFLEXIVE ASSOCIATIVE MEMORIES,Hendricus G. Loos,"In the synchronous discrete model, the average memory capacity of  bidirectional associative memories (BAMs) is compared with that of  Hopfield memories, by means of a calculat10n of the percentage of good  recall for 100 random BAMs of dimension 64x64, for different numbers  of stored vectors. The memory capac1ty Is found to be much smal1er than  the Kosko upper bound, which Is the lesser of the two dimensions of the  BAM. On the average, a 64x64 BAM has about 68 % of the capacity of the  corresponding Hopfield memory with the same number of neurons. Ortho(cid:173) normal coding of the BAM Increases the effective storage capaCity by  only 25 %. The memory capacity limitations are due to spurious stable  states, which arise In BAMs In much the same way as in Hopfleld  memories. Occurrence of spurious stable states can be avoided by  replacing the thresholding in the backlayer of the BAM by another  nonl1near process, here called ""Dominant Label Selection"" (DLS). The  simplest DLS is the wlnner-take-all net, which gives a fault-sensitive  memory. Fault tolerance can be improved by the use of an orthogonal or  unitary transformation. An optical application of the latter is a Fourier  transform, which is implemented simply by a lens."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/afacc5db3e0e85b446e6c7727cd7dca5-Abstract.html,Learning Representations by Recirculation,"Geoffrey E. Hinton, James L. McClelland","We describe a new learning procedure for networks that contain groups of non(cid:173)
linear units  arranged in a closed loop.  The  aim of the learning is  to  discover codes  that  allow  the  activity  vectors  in  a  ""visible""  group  to  be  represented  by  activity  vectors  in  a  ""hidden""  group.  One  way  to  test  whether  a  code  is  an  accurate  representation is to try to reconstruct the visible vector from the hidden vector.  The  difference  between  the  original  and  the  reconstructed  visible  vectors  is  called  the  reconstruction  error,  and  the  learning  procedure  aims  to  minimize  this  error.  The  learning procedure has  two  passes.  On the  fust pass,  the  original  visible  vector is  passed around the loop,  and on the second pass an average of the original vector and  the reconstructed vector is passed around the  loop.  The learning procedure  changes  each weight by  an  amount proportional  to  the product of the  ""presynaptic""  activity  and the difference in the post-synaptic activity on the two passes.  This procedure is  much  simpler  to  implement  than  methods  like  back-propagation.  Simulations  in  simple networks  show that it usually converges rapidly on a good set of codes,  and  analysis  shows  that  in  certain  restricted  cases  it  performs  gradient  descent  in  the  squared reconstruction error."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/afe8a4577080504b8bec07bbe4b2b9cc-Abstract.html,A Mean Field Theory of Layer IV of Visual Cortex and Its Application to Artificial Neural Networks,Christopher L. Scofield,"A  single  cell  theory  for  the  development  of  selectivity  and  ocular  dominance  in  visual  cortex  has  been  presented  previously  by  Bienenstock,  Cooper  and  Munrol.  This  has  been  extended  to  a  network  applicable  to  layer  IV  of  visual  cortex2 .  In  this  paper  we  present  a  mean  field  approximation  that  captures  in  a  fairly  transparent  manner  the  quantitative,  results  of  the  network  theory.  Finally,  we  consider  the  application  of  this  theory  to  artificial  neural  networks  and  show  that  a  significant  reduction  in  architectural  complexity  is  possible. 
the  qualitative,"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/b2801c6afc7057f3a3d331a71a028611-Abstract.html,Stability Results for Neural Networks,"A. N. Michel, J. A. Farrell, W. Porod","In the present paper we survey and utilize results from the qualitative theory of large  scale interconnected dynamical systems in order to develop  a  qualitative theory for  the  Hopfield model of neural networks.  In our approach we  view such networks as  an inter(cid:173) connection of many single neurons.  Our results  are  phrased in  terms of the  qualitative  properties of the individual neurons and in terms of the properties of the interconnecting  structure of the neural  networks.  Aspects of neural networks which  we  address include  asymptotic stability,  exponential stability,  and instability  of an  equilibrium;  estimates  of trajectory bounds; estimates of the domain of attraction of an asymptotically stable  equilibrium;  and stability of neural networks  under structural perturbations."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/b328b0417abe3085884133503d2bc151-Abstract.html,Simulations Suggest Information Processing Roles for the Diverse Currents in Hippocampal Neurons,Lyle J. Borg-Graham,"A computer model of the hippocampal pyramidal cell (HPC) is  described 
which  integrates  data from  a  variety  of sources  in order  to  develop  a  con(cid:173) sistent description for  this cell  type.  The model  presently includes  descrip(cid:173) tions  of eleven non-linear somatic currents of the HPC, and the electrotonic  structure of the neuron is modelled with a soma/short-cable approximation.  Model simulations qualitatively or quantitatively reproduce a  wide range of  somatic electrical behavior i~ HPCs, and demonstrate possible  roles  for the  various currents in information  processing. 
1  The  Computational Properties of Neurons 
There  are  several  substrates  for  neuronal  computation,  including  connec(cid:173) tivity, synapses,  morphometries of dendritic  trees,  linear parameters  of cell  membrane, as well as non-linear, time-varying membrane conductances, also  referred  to as  currents or channels.  In  the classical  description  of neuronal  function,  the contribution  of membrane channels  is  constrained  to  that  of  generating the action potential, setting firing  threshold, and establishing the  relationship  between (steady-state)  stimulus  intensity and  firing  frequency.  However,  it is  becoming clear that  the role  of these channels  may  be much  more complex, resulting in a variety of novel ""computational operators"" that  reflect  the information  processing occurring in  the biological neural  net. 
© American Institute of Physics 1988 
83 
2  Modelling  Hippocampal Neurons 
Over the  past  decade  a  wide  variety of non-linear  ion channels,  have  been  described  for  many  excitable  cells,  in  particular  several  kinds  of neurons.  One  such  neuron  is  the  hippocampal  pyramidal  cell  (HPC).  HPC  chan(cid:173) nels  are  marked  by  their  wide  range  of temporal,  voltage-dependent,  and  chemical-dependent characteristics,  which  results  in very complex  behavior  or  responses  of these  stereotypical  cortical  integrating cells.  For example,  some HPC channels are activated (opened)  transiently and quickly, thus pri(cid:173) marily affecting  the action  potential shape.  Other channels  have longer  ki(cid:173) netics, modulating the response of HPCs over hundreds of milliseconds.  The  measurement  these  channels  is  hampered  by  various  technical  constraints,  including the small size and extended electrotonic structure of HPCs and the  diverse  preparations  used  in experiments.  Modelling the electrical  behavior  of HPCs  with computer simulations is  one method  of integrating data from  a  variety of sources in order to develop  a  consistent description for  this  cell  type. 
In the model referred to here putative mechanisms for  voltage-dependent 
and  calcium-dependent  channel gating  have  been  used  to generate  simula(cid:173) tions of the somatic electrical behavior of HPCs, and to suggest mechanisms  for  information processing at  the single cell  level.  The model  has  also  been  used  to suggest experimental protocols  designed  to test  the validity of sim(cid:173) ulation results.  Model simulations qualitatively or quantitatively reproduce  a  wide  range of somatic electrical behavior in HPCs, and  explicitly  demon(cid:173) strate possible functional  roles  for  the various currents [1]. 
The model  presently includes  descriptions  of eleven  non-linear somatic  currents,  including  three  putative  N a+  currents  - INa-trig,  INa-rep,  and  INa-tail;  six  K+  currents  that  have  been  reported  in  the  literature - IDR  (Delayed  Rectifier),  lA,  Ie,  IAHP  (After-hyperpolarization),  1M,  and  IQ;  and two Ca2+ currents, also reported  previously - lea  and  leas. 
The  electrotonic  structure  of the  HPC  is  modelled  with  a  soma/short(cid:173)
cable approximation, and  the dendrites are assumed to be linear.  While the  conditions  for  reducing  the dendritic  tree  to a  single  cable are  not  met  for  HPC  (the so-called Rall conditions  [3]),  the  Zin  of the cable is  close  to that  of the tree.  In addition, although HPC dendrites have non-linear membrane,  it  assumed  that  as  a  first  approximation  the contribution of currents  from  this membrane may be ignored in the somatic response  to somatic stimulus.  Likewise,  the  model structure assumes  that axon-soma current  under these  conditions can be lumped into the soma circuit. 
84 
In  part  this  paper  will  address  the  following  question:  if  neural  nets 
are realizable using elements that  have simple integrative all-or-nothing re(cid:173) sponses,  connected  to each other  with  regenerative  conductors,  then  what  is  the function  for all  the channels observed experimentally in real neurons?  The results  of this  HPC  model study suggest  some  purpose for  these com(cid:173) plexities, and in this  paper we  shall investigate some of the possible roles  of  non-linear channels in neuronal information processing.  However, given the  speculative  nature  of many of the  currents  that  we  have  presented  in  the  model, it is  important to view  results  based  on  the interaction of the many  model  elements as  preliminary. 
3  Defining Neural Information Coding is the First 
Step  in  Describing  Biological Computations 
Determination of computational properties  of neurons  requires  a  priori as(cid:173) sumptions  as  to how information is  encoded in  neuronal  output.  The clas(cid:173) sical  description  assumes  that  information  is  encoded  as  spike  frequency.  However,  a  single  output variable,  proportional to firing  frequency,  ignores  other potentially information-rich degrees  of freedom,  including: 
•  Relative phase of concurrent inputs. 
•  Frequency modulation during single  bursts. 
•  Cessation of firing  due to intrinsic mechanisms."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/b3cd73d353d39e5cf6f6e9ff8d14c87f-Abstract.html,Speech Recognition Experiments with Perceptrons,D. J. Burr,"Artificial  neural  networks  (ANNs)  are  capable  of accurate  recognition  of  simple speech  vocabularies such  as  isolated  digits  [1].  This paper looks  at two  more  difficult  vocabularies,  the  alphabetic  E-set  and  a  set  of  polysyllabic  words.  The  E-set  is  difficult  because  it  contains  weak  discriminants  and  polysyllables  are  difficult  because  of  timing  variation.  Polysyllabic  word  recognition  is  aided  by a  time  pre-alignment technique  based on  dynamic pro(cid:173) gramming  and  E-set  recognition  is  improved  by  focusing  attention.  Recogni(cid:173) tion  accuracies  are  better  than  98%  for  both  vocabularies  when  implemented  with a single  layer perceptron."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/b422680f3db0986ddd7f8f126baaf0fa-Abstract.html,"Network Generality, Training Required, and Precision Required","John S. Denker, Ben S. Wittner, Leon Cooper","We  show  how  to estimate  (1)  the  number  of functions  that  can  be implemented  by  a  particular  network  architecture,  (2)  how  much  analog  precision  is  needed  in  the  con(cid:173) nections in the network, and (3) the number of training examples the network must see  before it can  be expected  to form  reliable  generalizations. 
Generality versus Training  Data Required 
Consider  the following  objectives:  First, the network  should be very  powerful and ver(cid:173) satile,  i.e.,  it  should  implement  any  function  (truth  table)  you  like,  and  secondly,  it  should learn easily, forming  meaningful generalizations from  a small number of training  examples.  Well, it is  information-theoretically impossible to create such a  network.  We  will  present  here a  simplified  argument; a  more complete and sophisticated version can  be found  in  Denker et al.  (1987). 
It is  customary to regard learning as  a  dynamical process:  adjusting the weights  (etc.)  in  a  single  network.  In  order  to  derive  the  results  of  this  paper,  however,  we  take  a  different  viewpoint,  which  we  call  the  ensemble  viewpoint.  Imagine  making  a  very  large  number of replicas of the network.  Each  replica has  the same architecture as  the  original,  but  the  weights  are  set  differently  in  each  case.  No  further  adjustment  takes  place;  the  ""learning process""  consists  of winnowing the ensemble of replicas,  searching  for  the one( s)  that satisfy our requirements. 
Training proceeds as follows:  We  present each item in  the training set to every network  in  the  ensemble.  That  is,  we  use  the  abscissa of the  training  pattern  as  input  to  the  network,  and  compare  the  ordinate of the  training  pattern  to see  if it  agrees  with  the  actual output  of the  network.  For  each  network,  we  keep  a  score  reflecting  how  many  times (and how badly) it disagreed with a  training item.  Networks with the lowest score  are  the  ones  that  agree  best  with  the  training data.  If we  had  complete  confidence  in 
lCurrently  at  NYNEX  Science  and Technology,  500  Westchester Ave.,  White  Plains,  NY  10604 
@)  American Institute of Physics 1988 
220 
the reliability of the training set, we  could at each step simply throwaway all  networks  that disagree. 
For definiteness, let us  consider a typical network architecture, with  No input wires and  Nt  units in  each processing layer  I, for  I  E {I·· ·L}.  For simplicity  we  assume NL  =  1.  We  recognize  the  importance of networks  with  continuous-valued  inputs  and  outputs,  but  we  will  concentrate  for  now  on  training  (and  testing)  patterns  that  are  discrete,  with N  ==  No  bits of abscissa and N L =  1 bit of ordinate.  This allows  us to classify  the  networks  into  bins  according  to  what  Boolean  input-output  relation  they  implement,  and simply  consider the ensemble of bins. 
If the  network  architecture  is  completely  general  and  There  are  22N  jossible  bins.  powerful,  all  22  functions  will  exist  in  the ensemble of bins.  On  average,  one  expects  that each  training item  will  throwaway  at  most  half of the  bins.  Assuming  maximal  efficiency,  if m  training items are  used,  then  when  m  ~ 2N  there  will  be only one  bin  remaining,  and  that  must  be  the  unique  function  that  consistently  describes  all  the  data.  But there  are  only  2N  possible abscissas  using N  bits.  Therefore a  truly general  network cannot possibly exhibit meaningful generalization - 100% of the possible data  is  needed for  training. 
N ow  suppose that the network is not  completely general, so that even  with all  possible  settings of the weights we can only create functions in 250  bins, where So  < 2N.  We call  So  the initial entropy of the network.  A more formal  and general  definition is  given  in  Denker et al.  (1987).  Once again, we  can use the training data to winnow the ensemble,  and when  m  ~ So,  there will be only one remaining bin.  That function  will presumably  generalize correctly to the remaining 2N - m  possible patterns.  Certainly that function  is  the best we can do with the network architecture and the training data we were given. 
The  usual  problem  with  automatic  learning  is  this:  If the  network  is  too  general,  So  will  be large, and an inordinate amount of training data will be required.  The required  amount of data may be simply unavailable, or it may be so large that training would be  prohibitively time-consuming.  The shows the critical importance of building a  network  that is  not more general  than  necessary. 
Estimating the Entropy 
In real engineering situations, it is  important to be able to estimate the initial entropy  of various proposed designs, since that determines the amount of training data that will  be required.  Calculating So  directly from  the definition is prohibitively difficult, but we  can  use the definition to derive useful  approximate expressions.  (You  wouldn't want to  calculate the thermodynamic entropy of a  bucket of water directly  from  the definition,  either. ) 
221 
Suppose  that  the  weights  in  the  network  at  each  connection  i  were  not  continuously  adjustable real numbers, but rather were specified by a  discrete code with bi  bits.  Then  the total number of bits required  to specify  the configuration  of the network is 
(1) 
Now the total number offunctions that could possibly be implemented by such a network  architecture would  be at most 2B.  The actual number will  always be smaller than this,  since there are various ways in which different settings of the weights can lead to identical  functions  (bins).  For one  thing, for  each  hidden layer 1 E {1··· L-1}, the numbering of  the  hidden units can be permuted, and the polarity of the hidden units can be flipped,  which  means  that  250  is  less  than  2B  by  a  factor  (among  others)  of III Nl! 2N ,.  In  addition,  if there  is  an  inordinately  large  number  of bits  bi  at  each  connection,  there  will  be  many  settings  where  small  changes  in  the connection  will  be immaterial.  This  will  make 2so  smaller by an additional factor.  We expect  aSO/abi  ~ 1 when bi is small,  and aSO/abi  ~ 0 when  bi  is  large;  we  must now  figure  out  where the crossover occurs. 
The number of ""useful and significant"" bits of precision, which we designate b, typically  scales like the logarithm  of number of connections  to the unit in question.  This can  be  understood as follows:  suppose there are N  connections into a  given  unit, and an input  signal  to  that  unit  of some  size  A  is  observed  to  be  significant  (the  exact  value  of  A  drops  out of the  present  calculation).  Then  there is  no  point  in  having  a  weight  with  magnitude  much  larger  than  A,  nor  much  smaller  than  A/N.  That  is,  the  dynamic  range should be comparable to the number of connections.  (This argument is not exact,  and it is easy to devise exceptions, but the conclusion remains useful.)  If only a fraction  1/ S  of the units in the previous layer are active (nonzero) at a time, the needed dynamic  range is  reduced.  This implies  b  ~ log(N/S). 
Note:  our  calculation  does  not  involve  the  dynamics  of  the  learning  process.  Some  numerical methods (including versions of back propagation) commonly require a number  of temporary  ""guard bits""  on  each  weight,  as  pointed  out  by  llichard  Durbin  (private  communication).  Another log N  bits ought  to suffice.  These bits  are  not  needed  after  learning is  complete, and do not contribute to So. 
If we combine these ideas and apply them to a  network with N  units in each layer, fully  connected,  we  arrive  at  the  following  expression  for  the  number  of  different  Boolean  functions  that  can be implemented by such a  network:"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/b559156047e50cf316207249d0b5a6c5-Abstract.html,Teaching Artificial Neural Systems to Drive: Manual Training Techniques for Autonomous Systems,"J. F. Shepanski, S. A. Macy",Abstract Unavailable
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/b95b58ff6d46d4b7ef2b3e2fd0ddb24c-Abstract.html,Distributed Neural Information Processing in the Vestibulo-Ocular System,"Clifford Lau, Vicente Honrubia","A new distributed neural information-processing 
model is proposed to explain the response characteristics  of the vestibulo-ocular system and to reflect more  accurately the latest anatomical and neurophysiological  data on the vestibular afferent fibers and vestibular nuclei.  In this model, head motion is sensed topographically by hair  cells in the semicircular canals. Hair cell signals are then  processed by multiple synapses in the primary afferent  neurons which exhibit a continuum of varying dynamics. The  model is an application of the concept of ""multilayered""  neural networks to the description of findings in the  bullfrog vestibular nerve, and allows us to formulate  mathematically the behavior of an assembly of neurons  whose physiological characteristics vary according to their  anatomical properties."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/b9799a12d683d136cc817f94b73a8938-Abstract.html,Stochastic Learning Networks and their Electronic Implementation,"Joshua Alspector, Robert B. Allen, Victor Hu, Srinagesh Satyanarayana",Abstract Unavailable
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/baf570e47e7f4e314a9ffb72c4a5459c-Abstract.html,On Tropistic Processing and Its Applications,Manuel F. Fernández,"The  interaction  of  a  set  of  tropisms  is  sufficient  in  many 
cases  to  explain  the  seemingly  complex  behavioral  responses  exhibited  by  varied  classes  of  biological  systems  to  combinations  of  stimuli.  It  can  be  shown  that  a  straightforward  generalization  of  the  tropism  phenomenon  allows  the  efficient  implementation  of  effective  algorithms  which  appear  to  respond  ""intelligently""  to  changing  environmental  conditions.  Examples  of  the  utilization  of  tropistic  processing  techniques  will  be  presented  in  this  paper  in  applications  entailing  simulated  behavior  synthesis,  path-planning,  pattern  analysis  (clustering),  and  engineering  design  optimization."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/bbf0dfe4064e04b4d24bce800ea5abba-Abstract.html,Minkowski-r Back-Propagation: Learning in Connectionist Models with Non-Euclidian Error Signals,"Stephen José Hanson, David J. Burr","Many connectionist learning models are implemented using a gradient descent  in a least squares error function of the output and teacher signal.  The present model  Fneralizes. in particular. back-propagation [1]  by using Minkowski-r power metrics.  For  small  r's  a  ""city-block""  error  metric  is  approximated  and  for  large  r's  the  ""maximum"" or ""supremum""  metric is  approached.  while  for r=2  the  standard  back(cid:173) propagation  model  results.  An  implementation  of Minkowski-r back-propagation  is  described.  and  several  experiments  are  done  which  show  that  different values  of r  may be desirable for various purposes. Different r values may be appropriate for the  reduction  of  the  effects  of outliers  (noise).  modeling  the  input  space  with  more  compact clusters. or modeling  the statistics of a particular domain more naturally or  in a way that may be more perceptually or psychologically meaningful (e.g. speech or  vision)."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/bc19061f88f16e9ed4a18f0bbd47048a-Abstract.html,Experimental Demonstrations of Optical Neural Computers,"Ken Hsu, David Brady, Demetri Psaltis",We  describe  two  expriments  in  optical  neural  computing.  In  the  first  a  closed  optical  feedback  loop  is  used  to  implement  auto-associative  image  recall.  In the second a perceptron-Iike learning algorithm is  implemented with  photorefractive holography.
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/c0987e6b6da2428e8cd43efa74790ccb-Abstract.html,"Discovering Structure from Motion in Monkey, Man and Machine",Ralph M. Siegel,"The ability to obtain three-dimensional structure from visual motion is 
important for survival of human and non-human primates. Using a parallel process(cid:173) ing model, the current work explores how the biological visual system might solve  this problem and how the neurophysiologist might go about understanding the  solution."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/c0d16b623be8b439d9c075eb5a97efd1-Abstract.html,The Performance of Convex Set Projection Based Neural Networks,"Robert J. Marks II, Les E. Atlas, Seho Oh, James A. Ritcey",and
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/c48e820389ae2420c1ad9d5856e1e41c-Abstract.html,Microelectronic Implementations of Connectionist Neural Networks,"Stuart Mackie, Hans P. Graf, Daniel B. Schwartz, John S. Denker","In  this  paper  we  discuss  why  special  purpose  chips  are  needed  for  useful  implementations  of connectionist  neural  networks  in  such  applications  as  pattern  recognition  and  classification.  Three  chip  designs  are  described:  a  hybrid  digital/analog programmable  connection  matrix,  an  analog  connection  matrix  with  adjustable connection strengths, and a digital pipe lined best-match chip.  The common  feature  of the designs  is the distribution of arithmetic processing power amongst the  data storage to minimize data movement."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/ca8d99f92af40381763ab9fd7f926a57-Abstract.html,Connectivity Versus Entropy,Yaser S. Abu-Mostafa,"How  does  the  connectivity  of a  neural  network  (number  of synapses  per  neuron)  relate  to  the complexity  of the  problems  it  can  handle  (measured  by  the entropy)?  Switching theory would suggest no relation at all, since all Boolean  functions  can be  implemented  using  a  circuit  with very  low  connectivity  (e.g.,  using  two-input  NAND  gates).  However,  for  a  network  that  learns  a  problem  from  examples  using  a  local  learning  rule,  we  prove  that  the  entropy  of  the  problem becomes  a  lower  bound for  the connectivity of the network."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/cd3a9a55f7f3723133fa4a13628cdf03-Abstract.html,Invariant Object Recognition Using a Distributed Associative Memory,"Harry Wechsler, George Lee Zimmerman","This  paper  describes  an  approach  to  2-dimensional  object  recognition.  Complex-log  con(cid:173) formal  mapping  is  combined  with  a  distributed  associative  memory  to  create  a  system  which  recognizes  objects  regardless  of  changes  in  rotation  or  scale.  Recalled  information  from  the  memorized  database  is  used  to  classify  an object,  reconstruct  the  memorized  ver(cid:173) sion  of the  object,  and estimate  the  magnitude of changes in  scale  or rotation.  The system  response  is  resistant  to  moderate  amounts of noise  and occlusion.  Several experiments,  us(cid:173) ing real,  gray scale images,  are  presented to show  the feasibility  of our approach."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/cdb1bbbe8f246aa0942da408d79f19ca-Abstract.html,Ensemble' Boltzmann Units have Collective Computational Properties like those of Hopfield and Tank Neurons,"Mark Derthick, Joe Tebelskis",Abstract Unavailable
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/d0b9a3081f811b2a307c38ad457a487c-Abstract.html,Performance Measures for Associative Memories that Learn and Forget,Anthony Kuh,"Recently,  many  modifications  to  the  McCulloch/Pitts  model  have  been  proposed  where  both  learning  and  forgetting  occur.  Given  that  the  network  never saturates  (ceases  to  function  effectively  due  to  an  overload  of  information),  the  learning  updates  can  con(cid:173) tinue indefinitely.  For these networks,  we  need  to introduce  performance  measmes in  addi(cid:173) tion  to  the  information  capacity  to  evaluate  the  different  networks.  We  mathematically  define  quantities such  as  the  plasticity  of  a  network,  the  efficacy  of an  information  vector,  and the  probability  of network  saturation.  From  these  quantities  we  analytically  compare  different networks."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/d0e33d1c7888753180d9ccc28c983685-Abstract.html,Optimization with Artificial Neural Network Systems: A Mapping Principle and a Comparison to Gradient Based Methods,Harrison MonFook Leong,"General  formulae  for  mapping  optimization  problems  into  systems  of  ordinary  differential 
equations  associated with artificial  neural  networks  are  presented.  A comparison is  made  to  optim(cid:173) ization using gradient-search methods.  The perfonnance  measure  is  the  settling time  from  an  initial  state  to  a  target  state.  A  simple  analytical  example  illustrates  a situation  where  dynamical  systems  representing  artificial  neural  network  methods  would  settle  faster  than  those  representing  gradient(cid:173) search.  Settling  time  was  investigated  for  a  more  complicated  optimization  problem  using  com(cid:173) puter  simulations.  The  problem  was  a  simplified  version  of a problem  in  medical  imaging:  deter(cid:173) mining  loci  of cerebral  activity  from  electromagnetic  measurements  at  the  scalp.  The  simulations  showed  that  gradient  based  systems  typically  settled  50  to  100  times  faster  than  systems  based  on  current neural  network optimization methods."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/d162c44a3b7215f415cfe881329ed408-Abstract.html,Towards an Organizing Principle for a Layered Perceptual Network,Ralph Linsker,"An information-theoretic optimization principle is  proposed for  the development  of  each  processing  stage  of  a  multilayered  perceptual  network.  This  principle  of  ""maximum information preservation""  states that the signal transformation that is to be  realized at each stage is one that maximizes the information that the output signal values  (from that stage) convey about the input signals values (to that stage), subject to certain  constraints and in  the presence of processing noise.  The quantity being maximized is  a  Shannon information rate.  I provide motivation for this principle and -- for some simple  model cases -- derive some of its consequences, discuss an algorithmic implementation,  and  show  how  the  principle  may  lead  to  biologically  relevant  neural  architectural  features  such  as  topographic  maps,  map  distortions,  orientation  selectivity,  and  extraction of spatial and temporal signal correlations.  A  possible  connection between  this  information-theoretic principle  and  a  principle  of minimum  entropy production in  nonequilibrium thermodynamics is suggested."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/d59a1dc497cf2773637256f50f492723-Abstract.html,A 'Neural' Network that Learns to Play Backgammon,"G. Tesauro, T. J. Sejnowski","We describe a class of connectionist networks that have learned to play back(cid:173)
gammon  at  an  intermediate-to-advanced  level.  TIle  networks  were  trained  by  a  supervised  learning  procedure  on  a  large  set  of sample  positions evaluated  by  a  human  expert.  In  actual  match  play  against  humans  and  conventional  computer  programs, the networks demonstrate substantial ability to generalize on the basis of  expert knowledge.  Our study touches on some of the most important issues in net(cid:173) work  learning  theory,  including the  development  of efficient coding schemes  and  training procedures, scaling,  generalization,  the  use  of real-valued inputs  and out(cid:173) puts,  and  techniques  for  escaping  from  local  minima.  Practical  applications  in  games and other domains are also discussed."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/d8e3068a1b4ad91b2066d3e1780593ee-Abstract.html,Supervised Learning of Probability Distributions by Neural Networks,"Eric B. Baum, Frank Wilczek","We propose that the back propagation algorithm for super(cid:173)
vised learning can be generalized, put on a satisfactory conceptual 
footing, and very likely made more efficient by defining the val(cid:173)
ues of the output and input neurons as probabilities and varying 
the synaptic weights in the gradient direction of the log likelihood, 
rather than the 'error'. 
In the past thirty years many researchers have studied the 
question of supervised learning in 'neural'-like networks. Recently 
a learning algorithm called 'back propagation H - 4 or the 'general(cid:173)
ized delta-rule' has been applied to numerous problems including 
the mapping of text to phonemes 5 , the diagnosis of illnesses6 and 
the classification of sonar targets 7 • In these applications, it would 
often be natural to consider imperfect, or probabilistic informa(cid:173)
tion. We believe that by considering supervised learning from this 
slightly larger perspective, one can not only place back propaga-"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/d9cd83bc91b8c36a0c7c0fcca59228f2-Abstract.html,PARTITIONING OF SENSORY DATA BY A CORTICAL NETWORK,"Richard Granger, José Ambros-Ingerson, Howard Henry, Gary Lynch",Abstract Unavailable
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/d9e5bd751997cffa6bc2d0e31ebdc048-Abstract.html,A Dynamical Approach to Temporal Pattern Processing,"W. Scott Stornetta, Tad Hogg, B. A. Huberman","Recognizing  patterns  with  temporal  context  is  important for  such  tasks  as  speech  recognition,  motion  detection  and  signature  verification.  We  propose  an  architecture  in  which  time  serves as its  own representation, and temporal context is encoded in the state of the  nodes. We contrast this with the approach of replicating portions of the  architecture to represent time. 
As one example of these ideas, we demonstrate an architecture  with  capacitive  inputs  serving  as  temporal  feature  detectors  in  an  otherwise  standard  back  propagation  model.  Experiments  involving  motion  detection  and  word  discrimination  serve  to  illustrate  novel  features  of the system.  Finally, we discuss possible  extensions of the  architecture."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/de081105cd68393144944696d3fb6778-Abstract.html,Neural Network Implementation Approaches for the Connection Machine,"Nathan H. Brown, Jr.","The SIMD parallelism of the Connection Machine (eM) allows the construction of 
neural network simulations by the use of simple data and control structures.  Two 
approaches are described which allow parallel computation of a model's nonlinear 
functions, parallel modification of a model's weights, and parallel propagation of a 
model's activation and error.  Each approach also allows a model's interconnect 
structure to be physically dynamic.  A Hopfield model is implemented with each 
approach at six sizes over the same number of CM processors to provide a performance 
comparison."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/e09d45e14e9ece7142217550ddd3c4d0-Abstract.html,Spatial Organization of Neural Networks: A Probabilistic Modeling Approach,"A. Stafylopatis, M. Dikaiakos, D. Kontoravdis","The  aim  of  this  paper  is  to  explore  the  spatial  organization  of  neural  networks  under  Markovian  assumptions,  in  what  concerns  the be(cid:173) haviour  of  individual  cells  and  the  interconnection  mechanism.  Space(cid:173) organizational  properties  of  neural  nets  are  very  relevant  in  image  modeling  and  pattern  analysis,  where  spatial  computations  on  stocha(cid:173) stic  two-dimensional  image  fields  are  involved.  As  a  first  approach  we  develop  a  random  neural  network  model,  based  upon  simple  probabi(cid:173) listic  assumptions,  whose  organization  is  studied  by  means  of  dis(cid:173) crete-event  simulation.  We  then  investigate  the  possibility  of  ap(cid:173) proXimating  the  random  network's  behaviour  by  using  an  analytical  ap(cid:173) proach  originating  from  the  theory  of  general  product-form  queueing  networks.  The  neural  network  is  described  by  an  open  network  of  no(cid:173) des,  in  which  customers  moving  from  node  to  node  represent  stimula(cid:173) tions  and  connections  between  nodes  are  expressed  in  terms  of  sui(cid:173) tably  selected  routing  probabilities.  We  obtain  the  solution  of  the  model  under  different  disciplines  affecting  the  time  spent  by  a  sti(cid:173) mulation  at  each  node  visited.  Results  concerning  the  distribution  of  excitation  in  the  network  as  a  function  of  network  topology  and  external  stimulation  arrival  pattern  are  compared  with  measures  ob(cid:173) tained  from  the  simulation  and  validate  the  approach  followed."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/e8a21a93f244b29e4da50ccbe409c28f-Abstract.html,Self-Organization of Associative Database and Its Applications,"Hisashi Suzuki, Suguru Arimoto","An  efficient  method  of self-organizing  associative  databases  is  proposed  together  with  applications  to  robot  eyesight  systems.  The  proposed  databases  can  associate  any  input  with  some  output.  In  the  first  half part  of discussion,  an  algorithm of self-organization  is  proposed.  From  an  aspect  of  hardware,  it  produces  a  new  style  of  neural  network.  In  the  latter half part, an applicability to handwritten letter recognition and that to an autonomous  mobile  robot system are demonstrated."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/e8adc089e27840d852bf8d13951090b4-Abstract.html,Neural Networks for Template Matching: Application to Real-Time Classification of the Action Potentials of Real Neurons,"Yiu-fai Wong, Jashojiban Banik, James M. Bower","Much  experimental study  of  real  neural  networks  relies  on  the  proper  classification  of 
extracellulary  sampled  neural  signals  (i .e.  action  potentials)  recorded  from  the  brains  of ex(cid:173) perimental animals.  In  most  neurophysiology  laboratories this classification  task  is  simplified  by  limiting  investigations  to  single,  electrically  well-isolated  neurons recorded  one  at  a  time.  However, for those interested in sampling the activities of many single neurons simultaneously,  waveform  classification  becomes  a  serious  concern.  In  this  paper  we  describe  and  constrast  three  approaches  to  this  problem  each  designed  not  only  to  recognize  isolated  neural  events,  but also  to separately classify temporally overlapping events in real time.  First we  present two  formulations  of  waveform  classification  using  a  neural network  template  matching  approach.  These  two  formulations  are  then  compared  to  a  simple  template  matching  implementation.  Analysis with real neural signals reveals  that simple template matching is  a  better solution to  this  problem  than either neural network  approach."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/eca986d585a03890a412587a2f5ccb43-Abstract.html,LEARNING BY STATE RECURRENCE DETECTION,"Bruce E. Rosen, James M. Goodwin, Jacques J. Vidal","This research investigates a new technique for unsupervised learning of nonlinear  control problems. The approach is applied both to Michie and Chambers BOXES  algorithm and to Barto, Sutton and Anderson's extension, the ASE/ACE system, and  has significantly improved the convergence rate of stochastically based learning  automata. 
Recurrence learning is a new nonlinear reward-penalty algorithm. It exploits  information found during learning trials to reinforce decisions resulting in the  recurrence of nonfailing states. Recurrence learning applies positive reinforcement  during the exploration of the search space, whereas in the BOXES or ASE algorithms,  only negative weight reinforcement is applied, and then only on failure. Simulation  results show that the added information from recurrence learning increases the learning  rate. 
Our empirical results show that recurrence learning is faster than both basic failure  driven learning and failure prediction methods. Although recurrence learning has only  been tested in failure driven experiments, there are goal directed learning applications  where detection of recurring oscillations may provide useful information that reduces  the learning time by applying negative, instead of positive reinforcement. 
Detection of cycles provides a heuristic to improve the balance between evidence 
gathering and goal directed search."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/ee5cc652ce2fdd25562a9411809f6d8f-Abstract.html,Schema for Motor Control Utilizing a Network Model of the Cerebellum,James C. Houk,"This  paper  outlines  a  schema  for  movement  control 
based  on  two  stages  of  signal  processing.  The  higher  stage  is  a  neural  network  model  that  treats  the  cerebellum  as  an  array  of  adjustable  motor  pattern  generators.  This  network  uses  sensory  input  to  preset  and  to  trigger  elemental  pattern  generators  and  to  evaluate  their  performance.  The  actual  patterned  outputs,  however,  are  produced  by  intrin(cid:173) sic  circuitry  that  includes  recurrent  loops  and  is  thus  capable  of  self-sustained  activity.  These  patterned  outputs  are  sent  as  motor  commands  to  local  feedback  systems  called  motor  servos.  The  latter  control  the  forces  and  lengths  of  individual  muscles.  Overall  control  is  thus  achieved  in  two  stages:  (1)  an  adaptive  cerebellar  network  generates  an  array  of  feedforward  motor  commands  and  (2)  a  set  of  local  feedback  systems  translates  these  commands  into  actual  movements."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/f67b34cb0f0d24b6226178aa6a649cc4-Abstract.html,Presynaptic Neural Information Processing,L. R. Carley,"The  potential  for  presynaptic  information  processing  within  the  arbor  of a  single  axon  will  be  discussed  in  this  paper.  Current  knowledge  about  the  activity  dependence  of  the  firing  threshold,  the  conditions  required  for  conduction  failure,  and  the  similarity  of  nodes  along  a  single  axon  will  be  reviewed.  An  electronic  circuit  model  for  a  site  of low  conduction  safety  in  an  axon  will  be  presented.  In  response to  single  frequency  stimulation  the  electronic circuit acts  as  a  lowpass filter."
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/f77e8ab11fcfa3f83e20d59b0a87f374-Abstract.html,The Hopfield Model with Multi-Level Neurons,Michael Fleisher,"The  Hopfield  neural  network.  model  for  associative  memory  is  generalized.  The  generalization 
replaces  two  state  neurons by neurons taking a  richer set of values.  Two  classes  of neuron  input output 
relations are developed guaranteeing convergence to stable states.  The first is a class of ""continuous"" rela-
tions and the second is a class of allowed quantization rules for the neurons.  The information capacity for 
networks from  the second class is fOWld  to be of order N 3 bits for a network with N  neurons. 
A generalization of the sum of outer products learning rule is developed and investigated as well. 
© American Institute of Physics 1988 
279"
1987,https://papers.nips.cc/paper_files/paper/1987,https://papers.nips.cc/paper_files/paper/1987/hash/fea47a8aa372e42f3c84327aec9506cf-Abstract.html,Basins of Attraction for Electronic Neural Networks,"C. M. Marcus, R. M. Westervelt","We  have  studied  the  basins  of  attraction  for  fixed  point  and  oscillatory  attractors  in  an  electronic  analog  neural  network.  Basin  measurement  circuitry  periodically  opens  the  network  feedback  loop,  loads  raster-scanned  initial  conditions  and  examines  the  resulting  attractor.  Plotting  the  basins  for  fixed  points  (memories),  we  show  that  overloading  an  associative  memory  network  leads  to  irregular  basin  shapes.  The  network  also  includes  analog  time  delay  circuitry,  and  we  have  shown  that  delay  in  symmetric  networks  can  introduce  basins  for  oscillatory  attractors.  Conditions  leading  to  oscillation  are  related  to  the  presence  of  frustration;  reducing  frustration  by  diluting  the  connections  can  stabilize  a  delay  network."
