year,proceeding_link,paper_link,title,authors,abstract
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/0004d0b59e19461ff126e3a08a814c33-Abstract.html,"Means, Correlations and Bounds","Martijn Leisink, Bert Kappen","The  partition function  for  a  Boltzmann  machine  can  be  bounded  from  above  and  below.  We  can  use  this  to  bound  the  means  and  the  correlations.  For  networks  with  small  weights,  the  values  of  these statistics can be restricted to non-trivial regions  (i.e.  a subset  of  [-1 , 1]).  Experimental  results  show  that  reasonable  bounding  occurs  for  weight sizes  where  mean field  expansions  generally  give  good  results."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/0070d23b06b1486a538c0eaa45dd167a-Abstract.html,Fragment Completion in Humans and Machines,"David Jacobs, Bas Rokers, Archisman Rudra, Zili Liu","Partial information can trigger a complete memory. At the same time, human memory is not perfect. A cue can contain enough information to specify an item in memory, but fail to trigger that item. In the context of word memory, we present experiments that demonstrate some basic patterns in human memory errors. We use cues that consist of word frag- ments. We show that short and long cues are completed more accurately than medium length ones and study some of the factors that lead to this behavior. We then present a novel computational model that shows some of the Ô¨Çexibility and patterns of errors that occur in human memory. This model iterates between bottom-up and top-down computations. These are tied together using a Markov model of words that allows memory to be accessed with a simple feature set, and enables a bottom-up process to compute a probability distribution of possible completions of word frag- ments, in a manner similar to models of visual perceptual completion."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/008bd5ad93b754d500338c253d9c1770-Abstract.html,Eye movements and the maturation of cortical orientation selectivity,"Antonino Casile, Michele Rucci","Neural activity appears to be a crucial component for shaping the recep- tive Ô¨Åelds of cortical simple cells into adjacent, oriented subregions alter- nately receiving ON- and OFF-center excitatory geniculate inputs. It is known that the orientation selective responses of V1 neurons are reÔ¨Åned by visual experience. After eye opening, the spatiotemporal structure of neural activity in the early stages of the visual pathway depends both on the visual environment and on how the environment is scanned. We have used computational modeling to investigate how eye movements might affect the reÔ¨Ånement of the orientation tuning of simple cells in the pres- ence of a Hebbian scheme of synaptic plasticity. Levels of correlation be- tween the activity of simulated cells were examined while natural scenes were scanned so as to model sequences of saccades and Ô¨Åxational eye movements, such as microsaccades, tremor and ocular drift. The speciÔ¨Åc patterns of activity required for a quantitatively accurate development of simple cell receptive Ô¨Åelds with segregated ON and OFF subregions were observed during Ô¨Åxational eye movements, but not in the presence of saccades or with static presentation of natural visual input. These re- sults suggest an important role for the eye movements occurring during visual Ô¨Åxation in the reÔ¨Ånement of orientation selectivity."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/0189caa552598b845b29b17a427692d1-Abstract.html,Orientation-Selective aVLSI Spiking Neurons,"Shih-Chii Liu, J√∂rg Kramer, Giacomo Indiveri, Tobi Delbr√ºck, Rodney J. Douglas","We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-Ô¨Åre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip ap- proximates a cortical microcircuit. The neurons can be conÔ¨Ågured for different computational properties by the virtual connections of a se- lected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication pro- tocol that uses asynchronous digital pulses, similar to spikes in a neu- ronal system. We used the multi-chip spike-based system to synthe- size orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computa- tional time does not scale with the size of the neuronal network."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/01931a6925d3de09e5f87419d9d55055-Abstract.html,On the Generalization Ability of On-Line Learning Algorithms,"Nicol√≤ Cesa-bianchi, Alex Conconi, Claudio Gentile","In this paper we show that on-line algorithms for classiÔ¨Åcation and re- gression can be naturally used to obtain hypotheses with good data- dependent tail bounds on their risk. Our results are proven without re- quiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/02b1be0d48924c327124732726097157-Abstract.html,Analysis of Sparse Bayesian Learning,"Anita C. Faul, Michael E. Tipping","The recent introduction of the 'relevance vector machine' has effec(cid:173) tively  demonstrated  how  sparsity  may  be obtained  in  generalised  linear  models  within  a  Bayesian  framework.  Using  a  particular  form  of Gaussian  parameter prior,  'learning'  is  the  maximisation,  with respect  to hyperparameters, of the  marginal  likelihood  of the  data.  This  paper  studies  the  properties  of  that  objective  func(cid:173) tion,  and  demonstrates  that  conditioned  on  an  individual  hyper(cid:173) parameter,  the  marginal  likelihood  has  a  unique  maximum which  is  computable in closed form.  It is  further  shown that if a  derived  'sparsity criterion'  is  satisfied,  this maximum is  exactly  equivalent  to  'pruning' the corresponding parameter from  the model."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/03e7d2ebec1e820ac34d054df7e68f48-Abstract.html,A Hierarchical Model of Complex Cells in Visual Cortex for the Binocular Perception of Motion-in-Depth,"Silvio P. Sabatini, Fabio Solari, Giulia Andreani, Chiara Bartolozzi, Giacomo M. Bisio","A cortical model for  motion-in-depth selectivity of complex cells in  the  visual  cortex  is  proposed.  The  model  is  based  on  a  time  ex(cid:173) tension of the phase-based techniques for  disparity estimation.  We  consider  the  computation  of the  total  temporal  derivative  of the  time-varying disparity through the combination of the responses of  disparity energy units.  To take into account the physiological plau(cid:173) sibility, the model  is  based on the  combinations of binocular cells  characterized by different  ocular dominance indices.  The resulting  cortical units  of the model show  a  sharp selectivity for  motion-in(cid:173) depth that has been compared with that reported in the literature  for  real cortical cells."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/051928341be67dcba03f0e04104d9047-Abstract.html,Motivated Reinforcement Learning,Peter Dayan,"The  standard reinforcement learning view  of the involvement  of neuromodulatory systems in instrumental conditioning in(cid:173) cludes  a  rather  straightforward  conception  of  motivation  as  prediction of sum future reward.  Competition between actions  is based on the motivating characteristics of their consequent  states in this sense. Substantial, careful, experiments reviewed  in Dickinson & Balleine, 12,13 into the neurobiology and psychol(cid:173) ogy of motivation shows that this view is incomplete. In many  cases, animals are faced with the choice not between many dif(cid:173) ferent actions  at a given state, but rather whether a single re(cid:173) sponse is  worth  executing  at  all.  Evidence  suggests  that  the  motivational process underlying  this choice has different psy(cid:173) chological  and neural properties from  that underlying  action  choice.  We  describe  and  model  these  motivational  systems,  and consider the way they interact."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/05546b0e38ab9175cd905eebcc6ebb76-Abstract.html,Categorization by Learning and Combining Object Parts,"Bernd Heisele, Thomas Serre, Massimiliano Pontil, Thomas Vetter, Tomaso Poggio","We describe an algorithm for automatically learning discriminative com- ponents of objects with SVM classiÔ¨Åers. It is based on growing image parts by minimizing theoretical bounds on the error probability of an SVM. Component-based face classiÔ¨Åers are then combined in a second stage to yield a hierarchical SVM classiÔ¨Åer. Experimental results in face classiÔ¨Åcation show considerable robustness against rotations in depth and suggest performance at signiÔ¨Åcantly better level than other face detection systems. Novel aspects of our approach are: a) an algorithm to learn component-based classiÔ¨Åcation experts and their combination, b) the use of 3-D morphable models for training, and c) a maximum operation on the output of each component classiÔ¨Åer which may be relevant for bio- logical models of visual recognition."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/05a5cf06982ba7892ed2a6d38fe832d6-Abstract.html,Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway,"Gal Chechik, Amir Globerson, M. J. Anderson, E. D. Young, Israel Nelken, Naftali Tishby","The  way  groups  of auditory  neurons  interact  to  code  acoustic  in(cid:173) formation is  investigated using an information theoretic approach.  We develop measures of redundancy among groups of neurons, and  apply  them  to  the  study  of collaborative  coding  efficiency  in  two  processing stations in the auditory pathway:  the inferior colliculus  (IC)  and the primary auditory cortex (AI).  Under two schemes for  the  coding  of the  acoustic  content,  acoustic  segments  coding  and  stimulus  identity  coding,  we  show  differences  both in  information  content and group redundancies between IC and AI neurons.  These  results  provide for  the first  time  a  direct  evidence for  redundancy  reduction  along  the  ascending  auditory  pathway,  as  has  been  hy(cid:173) pothesized for  theoretical considerations  [Barlow  1959,2001].  The  redundancy effects under the single-spikes coding scheme are signif(cid:173) icant only for  groups larger than ten cells, and cannot be revealed  with  the  redundancy  measures  that  use  only  pairs  of cells.  The  results  suggest  that the  auditory system transforms low  level  rep(cid:173) resentations that contain redundancies due to the statistical struc(cid:173) ture of natural stimuli, into a  representation in which cortical neu(cid:173) rons extract rare and independent component of complex acoustic  signals,  that are useful  for  auditory scene analysis."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/06964dce9addb1c5cb5d6e3d9838f733-Abstract.html,Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex,"Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue","Statistical learning and probabilistic inference techniques are used to in- fer the hand position of a subject from multi-electrode recordings of neu- ral activity in motor cortex. First, an array of electrodes provides train- ing data of neural Ô¨Åring conditioned on hand kinematics. We learn a non- parametric representation of this Ô¨Åring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned Ô¨Åring models of multiple cells are used to deÔ¨Åne a non- Gaussian likelihood term which is combined with a prior probability for the kinematics. A particle Ô¨Åltering method is used to represent, update, and propagate the posterior distribution over time. The approach is com- pared with traditional linear Ô¨Åltering methods; the results suggest that it may be appropriate for neural prosthetic applications."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/06b1338ba02add2b5d2da67663b19ebe-Abstract.html,The Noisy Euclidean Traveling Salesman Problem and Learning,"Mikio L. Braun, Joachim M. Buhmann","We  consider  noisy  Euclidean  traveling  salesman  problems  in  the  plane,  which  are  random combinatorial problems  with  underlying  structure. Gibbs sampling is  used to compute average trajectories,  which  estimate  the  underlying structure common  to all  instances.  This procedure requires identifying the exact relationship between  permutations and tours.  In  a  learning setting,  the  average trajec(cid:173) tory  is  used  as  a  model  to  construct  solutions  to  new  instances  sampled from the same source. Experimental results show that the  average trajectory can in fact estimate the underlying structure and  that overfitting effects occur if the trajectory adapts too closely to  a  single instance."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/07811dc6c422334ce36a09ff5cd6fe71-Abstract.html,Incorporating Invariances in Non-Linear Support Vector Machines,"Olivier Chapelle, Bernhard Sch√∂lkopf","The  choice  of an  SVM  kernel  corresponds  to  the  choice  of a  rep(cid:173) resentation  of  the  data  in  a  feature  space  and,  to  improve  per(cid:173) formance,  it  should  therefore incorporate prior knowledge  such  as  known  transformation invariances.  We  propose  a  technique which  extends earlier work and aims at incorporating invariances in  non(cid:173) linear  kernels.  We  show  on  a  digit  recognition  task that the  pro(cid:173) posed  approach is  superior to the Virtual Support Vector method,  which  previously had been the method of choice."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/07cb5f86508f146774a2fac4373a8e50-Abstract.html,Sampling Techniques for Kernel Methods,"Dimitris Achlioptas, Frank Mcsherry, Bernhard Sch√∂lkopf","We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained ap- proximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function  by a ‚Äúrandomized kernel‚Äù which behaves like"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/08f90c1a417155361a5c4b8d297e0d78-Abstract.html,Reinforcement Learning and Time Perception -- a Model of Animal Experiments,"Jonathan L. Shapiro, J. Wearden","Animal  data on delayed-reward conditioning experiments  shows  a  striking property - the  data for  different  time intervals  collapses  into  a  single  curve  when  the  data is  scaled  by  the  time  interval.  This is  called the scalar property of interval timing.  Here a  simple  model of a  neural  clock is  presented and shown to give  rise  to the  scalar  property.  The model  is  an accumulator consisting of noisy,  linear  spiking  neurons.  It  is  analytically  tractable  and  contains  only three parameters.  When coupled with reinforcement learning  it simulates peak procedure experiments, producing both the scalar  property and the pattern of single  trial covariances."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/093b60fd0557804c8ba0cbf1453da22f-Abstract.html,Hyperbolic Self-Organizing Maps for Semantic Navigation,"Jorg Ontrup, Helge Ritter","We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a ‚Äúhyper- bolic SOM‚Äù (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/0ae3f79a30234b6c45a6f7d298ba1310-Abstract.html,Probabilistic Abstraction Hierarchies,"Eran Segal, Daphne Koller, Dirk Ormoneit","Many domains are naturally organized in an abstraction hierarchy or taxonomy, where the instances in ‚Äúnearby‚Äù classes in the taxonomy are similar. In this pa- per, we provide a general probabilistic framework for clustering data into a set of classes organized as a taxonomy, where each class is associated with a prob- abilistic model from which the data was generated. The clustering algorithm simultaneously optimizes three things: the assignment of data instances to clus- ters, the models associated with the clusters, and the structure of the abstraction hierarchy. A unique feature of our approach is that it utilizes global optimization algorithms for both of the last two steps, reducing the sensitivity to noise and the propensity to local maxima that are characteristic of algorithms such as hierarchi- cal agglomerative clustering that only take local steps. We provide a theoretical analysis for our algorithm, showing that it converges to a local maximum of the joint likelihood of model and data. We present experimental results on synthetic data, and on real data in the domains of gene expression and text."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/0b1ec366924b26fc98fa7b71a9c249cf-Abstract.html,Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade,"Paul Viola, Michael Jones","This paper develops a new approach for extremely fast detection in do- mains where the distribution of positive and negative examples is highly skewed (e.g. face detection or database retrieval). In such domains a cascade of simple classiÔ¨Åers each trained to achieve high detection rates and modest false positive rates can yield a Ô¨Ånal detector with many desir- able features: including high detection rates, very low false positive rates, and fast performance. Achieving extremely high detection rates, rather than low error, is not a task typically addressed by machine learning al- gorithms. We propose a new variant of AdaBoost as a mechanism for training the simple classiÔ¨Åers used in the cascade. Experimental results in the domain of face detection show the training algorithm yields sig- niÔ¨Åcant improvements in performance over conventional AdaBoost. The Ô¨Ånal face detection system can process 15 frames per second, achieves over 90% detection, and a false positive rate of 1 in a 1,000,000."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/1113d7a76ffceca1bb350bfe145467c6-Abstract.html,Agglomerative Multivariate Information Bottleneck,"Noam Slonim, Nir Friedman, Naftali Tishby","The information bottleneck method  is  an  unsupervised  model  independent data  organization  technique.  Given  a joint distribution  peA, B),  this  method  con(cid:173) structs a new variable T  that extracts partitions, or clusters, over the values of A  that are informative about B.  In  a recent paper,  we introduced a general princi(cid:173) pled framework for multivariate extensions of the information bottleneck method  that allows us to consider multiple systems of data partitions that are inter-related.  In this  paper,  we  present  a  new  family  of simple  agglomerative  algorithms  to  construct such systems of inter-related clusters.  We analyze the behavior of these  algorithms and apply them to several real-life datasets."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/1359aa933b48b754a2f54adb688bfa77-Abstract.html,K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms,"Pascal Vincent, Yoshua Bengio","Guided by an initial idea of building a complex (non linear) decision surface with maximal local margin in input space, we give a possible geometrical intuition as to why K-Nearest Neighbor (KNN) algorithms often perform more poorly than SVMs on classiÔ¨Åcation tasks. We then propose modiÔ¨Åed K-Nearest Neighbor algorithms to overcome the per- ceived problem. The approach is similar in spirit to Tangent Distance, but with invariances inferred from the local neighborhood rather than prior knowledge. Experimental results on real world classiÔ¨Åcation tasks sug- gest that the modiÔ¨Åed KNN algorithms often give a dramatic improve- ment over standard KNN and perform as well or better than SVMs."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/16ba72172e6a4f1de54d11ab6967e371-Abstract.html,Using Vocabulary Knowledge in Bayesian Multinomial Estimation,"Thomas L. Griffiths, Joshua B. Tenenbaum","Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks. Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity. We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates. We demonstrate these improvements in applications to text compres(cid:173) sion and estimating distributions over words in newsgroup data."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/194cf6c2de8e00c05fcf16c498adc7bf-Abstract.html,Bayesian time series classification,"Peter Sykacek, Stephen J. Roberts","This paper proposes an approach to classiÔ¨Åcation of adjacent segments of a time series as being either of  classes. We use a hierarchical model that consists of a feature extraction stage and a generative classiÔ¨Åer which is built on top of these features. Such two stage approaches are often used in signal and image processing. The novel part of our work is that we link these stages probabilistically by using a latent feature space. To use one joint model is a Bayesian requirement, which has the advantage to fuse information according to its certainty. The classiÔ¨Åer is implemented as hidden Markov model with Gaussian and Multinomial observation distributions deÔ¨Åned on a suitably chosen representation of autoregressive models. The Markov dependency is mo- tivated by the assumption that successive classiÔ¨Åcations will be corre- lated. Inference is done with Markov chain Monte Carlo (MCMC) tech- niques. We apply the proposed approach to synthetic data and to classi- Ô¨Åcation of EEG that was recorded while the subjects performed different cognitive tasks. All experiments show that using a latent feature space results in a signiÔ¨Åcant improvement in generalization accuracy. Hence we expect that this idea generalizes well to other hierarchical models."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/1a0a283bfe7c549dee6c638a05200e32-Abstract.html,Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks,M. Schmitt,"Recurrent  neural networks of analog units are computers for  real(cid:173) valued functions.  We  study the time complexity of real computa(cid:173) tion  in  general  recurrent  neural  networks.  These  have  sigmoidal,  linear,  and  product  units  of unlimited  order  as  nodes  and  no  re(cid:173) strictions on the weights.  For networks operating in discrete time,  we  exhibit  a  family  of functions  with  arbitrarily high  complexity,  and we derive almost tight bounds on the time required to compute  these functions.  Thus, evidence is  given of the computational lim(cid:173) itations  that  time-bounded  analog  recurrent  neural  networks  are  subject to."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/1b36ea1c9b7a1c3ad668b8bb5df7963f-Abstract.html,Playing is believing: The role of beliefs in multi-agent learning,"Yu-Han Chang, Leslie Pack Kaelbling","We propose a new classiÔ¨Åcation for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classiÔ¨Åcation, we review the optimality of ex- isting algorithms, including the case of interleague play. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the long- run against fair opponents."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/1b5230e3ea6d7123847ad55a1e06fffd-Abstract.html,Probabilistic principles in unsupervised learning of visual structure: human data and a model,"Shimon Edelman, Benjamin P. Hiles, Hwajin Yang, Nathan Intrator","To Ô¨Ånd out how the representations of structured visual objects depend on the co-occurrence statistics of their constituents, we exposed subjects to a set of composite images with tight control exerted over (1) the condi- tional probabilities of the constituent fragments, and (2) the value of Bar- low‚Äôs criterion of ‚Äúsuspicious coincidence‚Äù (the ratio of joint probability to the product of marginals). We then compared the part veriÔ¨Åcation re- sponse times for various probe/target combinations before and after the exposure. For composite probes, the speedup was much larger for tar- gets that contained pairs of fragments perfectly predictive of each other, compared to those that did not. This effect was modulated by the sig- niÔ¨Åcance of their co-occurrence as estimated by Barlow‚Äôs criterion. For lone-fragment probes, the speedup in all conditions was generally lower than for composites. These results shed light on the brain‚Äôs strategies for unsupervised acquisition of structural information in vision."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/1e4d36177d71bbb3558e43af9577d70e-Abstract.html,Predictive Representations of State,"Michael L. Littman, Richard S. Sutton","We show that states of a dynamical system can be usefully repre(cid:173) sented by multi-step, action-conditional predictions of future ob(cid:173) servations. State representations that are grounded in data in this way may be easier to learn, generalize better, and be less depen(cid:173) dent on accurate prior models than, for example, POMDP state representations. Building on prior work by Jaeger and by Rivest and Schapire, in this paper we compare and contrast a linear spe(cid:173) cialization of the predictive approach with the state representa(cid:173) tions used in POMDPs and in k-order Markov models. Ours is the first specific formulation of the predictive idea that includes both stochasticity and actions (controls). We show that any system has a linear predictive state representation with number of predictions no greater than the number of states in its minimal POMDP model.
In predicting or controlling a sequence of observations, the concepts of state and state estimation inevitably arise. There have been two dominant approaches. The generative-model approach, typified by research on partially observable Markov de(cid:173) cision processes (POMDPs), hypothesizes a structure for generating observations and estimates its state and state dynamics. The history-based approach, typified by k-order Markov methods, uses simple functions of past observations as state, that is, as the immediate basis for prediction and control. (The data flow in these two ap(cid:173) proaches are diagrammed in Figure 1.) Of the two, the generative-model approach is more general. The model's internal state gives it temporally unlimited memory(cid:173) the ability to remember an event that happened arbitrarily long ago--whereas a history-based approach can only remember as far back as its history extends. The bane of generative-model approaches is that they are often strongly dependent on a good model of the system's dynamics. Most uses of POMDPs, for example, assume a perfect dynamics model and attempt only to estimate state. There are algorithms for simultaneously estimating state and dynamics (e.g., Chrisman, 1992), analogous to the Baum-Welch algorithm for the uncontrolled case (Baum et al., 1970), but these are only effective at tuning parameters that are already approximately cor(cid:173) rect (e.g., Shatkay & Kaelbling, 1997).
observations (and actions)"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/1f36c15d6a3d18d52e8d493bc8187cb9-Abstract.html,Discriminative Direction for Kernel Classifiers,Polina Golland,"In many scientific and engineering applications, detecting and under- standing differences between two groups of examples can be reduced to a classical problem of training a classifier for labeling new examples while making as few mistakes as possible. In the traditional classifi- cation setting, the resulting classifier is rarely analyzed in terms of the properties of the input data captured by the discriminative model. How- ever, such analysis is crucial if we want to understand and visualize the detected differences. We propose an approach to interpretation of the sta- tistical model in the original feature space that allows us to argue about the model in terms of the relevant changes to the input vectors. For each point in the input space, we define a discriminative direction to be the direction that moves the point towards the other class while introducing as little irrelevant change as possible with respect to the classifier func- tion. We derive the discriminative direction for kernel-based classifiers, demonstrate the technique on several examples and briefly discuss its use in the statistical shape analysis, an application that originally motivated this work. 1 Introduction
Once a classifier is estimated from the training data, it can be used to label new examples, and in many application domains, such as character recognition, text classification and oth- ers, this constitutes the final goal of the learning stage. The statistical learning algorithms are also used in scientific studies to detect and analyze differences between the two classes when the ``correct answer'' is unknown, and the information we have on the differences is represented implicitly by the training set. Example applications include morphologi- cal analysis of anatomical organs (comparing organ shape in patients vs. normal controls), molecular design (identifying complex molecules that satisfy certain requirements), etc. In such applications, interpretation of the resulting classifier in terms of the original feature vectors can provide an insight into the nature of the differences detected by the learning algorithm and is therefore a crucial step in the analysis. Furthermore, we would argue that studying the spatial structure of the data captured by the classification function is important in any application, as it leads to a better understanding of the data and can potentially help in improving the technique. This paper addresses the problem of translating a classifier into a different representation
that allows us to visualize and study the differences between the classes. We introduce and derive a so called discriminative direction at every point in the original feature space with respect to a given classifier. Informally speaking, the discriminative direction tells us how to change any input example to make it look more like an example from another class without introducing any irrelevant changes that possibly make it more similar to other examples from the same class. It allows us to characterize differences captured by the classifier and to express them as changes in the original input examples. This paper is organized as follows. We start with a brief background section on kernel- based classification, stating without proof the main facts on kernel-based SVMs necessary for derivation of the discriminative direction. We follow the notation used in [3, 8, 9]. In Section 3, we provide a formal definition of the discriminative direction and explain how it can be estimated from the classification function. We then present some special cases, in which the computation can be simplified significantly due to a particular structure of the kernel. Section 4 demonstrates the discriminative direction for different kernels, followed by an example from the problem of statistical analysis of shape differences that originally motivated this work."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/1f4fe6a4411edc2ff625888b4093e917-Abstract.html,Contextual Modulation of Target Saliency,Antonio Torralba,"The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. fu this paper we show that including contextual information in object detection pro(cid:173) cedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/1f71e393b3809197ed66df836fe833e5-Abstract.html,On Kernel-Target Alignment,"Nello Cristianini, John Shawe-Taylor, Andr√© Elisseeff, Jaz S. Kandola","We  introduce the notion of kernel-alignment, a  measure of similar(cid:173) ity  between two  kernel functions  or between a  kernel and a  target  function.  This quantity captures the degree of agreement between  a  kernel  and a  given  learning task,  and has  very  natural interpre(cid:173) tations  in  machine  learning,  leading also  to  simple  algorithms for  model selection and learning.  We analyse its theoretical properties,  proving that it  is  sharply concentrated around its expected value,  and  we  discuss  its  relation  with  other  standard  measures  of  per(cid:173) formance.  Finally we  describe  some  of the algorithms that  can be  obtained within this framework,  giving experimental results show(cid:173) ing that adapting the kernel  to improve  alignment  on the labelled  data  significantly  increases  the  alignment  on  the  test  set,  giving  improved  classification  accuracy.  Hence,  the  approach  provides  a  principled method of performing transduction. 
Keywords:  Kernels,  alignment, eigenvectors, eigenvalues, transduction"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/23d2e1578544b172cca332ff74bddf5f-Abstract.html,Escaping the Convex Hull with Extrapolated Vector Machines,Patrick Haffner,"Maximum  margin  classifiers  such  as  Support  Vector  Machines  (SVMs)  critically  depends  upon  the  convex  hulls  of the  training  samples  of each  class,  as  they  implicitly  search for  the  minimum  distance  between the convex hulls.  We  propose Extrapolated Vec(cid:173) tor  Machines  (XVMs)  which  rely  on  extrapolations  outside  these  convex hulls.  XVMs improve SVM generalization very significantly  on  the  MNIST  [7]  OCR  data.  They  share  similarities  with  the  Fisher  discriminant:  maximize  the  inter-class  margin  while  mini(cid:173) mizing the intra-class disparity."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/24f0d2c90473b2bc949ad962e61d9bcb-Abstract.html,Algorithmic Luckiness,"Ralf Herbrich, Robert C. Williamson","In  contrast  to  standard  statistical  learning  theory  which  studies  uniform bounds on the expected error we present a framework that  exploits  the  specific  learning  algorithm  used.  Motivated  by  the  luckiness framework  [8]  we  are also  able to exploit the serendipity  of the training sample.  The main difference to previous approaches  lies  in  the  complexity  measure;  rather  than covering  all  hypothe(cid:173) ses  in  a  given  hypothesis  space  it  is  only  necessary  to  cover  the  functions  which  could  have  been  learned  using  the  fixed  learning  algorithm.  We  show  how  the  resulting  framework  relates  to  the  VC, luckiness and compression frameworks.  Finally,  we  present an  application  of this  framework  to  the  maximum  margin  algorithm  for  linear classifiers which results in a  bound that exploits both the  margin and the distribution of the data in feature  space."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/253614bbac999b38b5b60cae531c4969-Abstract.html,Optimising Synchronisation Times for Mobile Devices,"Neil D. Lawrence, Antony I. T. Rowstron, Christopher M. Bishop, Michael J. Taylor","With the increasing number of users of mobile computing devices  (e.g.  personal digital assistants) and the advent of third generation  mobile phones, wireless communications are becoming increasingly  important.  Many  applications  rely  on  the  device  maintaining  a  replica  of a  data-structure which  is  stored on  a  server,  for  exam(cid:173) ple news databases, calendars and e-mail.  ill this paper we explore  the question of the optimal strategy for synchronising such replicas.  We utilise probabilistic models to represent how the data-structures  evolve  and to model  user behaviour.  We  then formulate objective  functions which can be minimised with respect to the synchronisa(cid:173) tion timings.  We demonstrate, using two real world data-sets, that  a user can obtain more up-to-date information using our approach."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/2557911c1bf75c2b643afb4ecbfc8ec2-Abstract.html,Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds,"B. D. Wright, Kamal Sen, William Bialek, A. J. Doupe","In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. Attempts to study the neu- ral coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. For zebra Ô¨Ånches, naturalistic stimuli can be deÔ¨Åned as sounds that they encounter in a colony of conspeciÔ¨Åc birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra Ô¨Ånches, and then ana- lyzed the response of single neurons in the songbird central auditory area (Ô¨Åeld L) to continuous playback of long segments from this ensemble. Following methods developed in the Ô¨Çy visual system, we measured the information that spike trains provide about the acoustic stimulus with- out any assumptions about which features of the stimulus are relevant. Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still be- ing revealed as time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike patterns. Initial results show that in Ô¨Åeld L, temporal patterns give at least
 % extra information. Thus, single central auditory neurons can pro-
vide an informative representation of naturalistic sounds, in which spike timing may play a signiÔ¨Åcant role."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/26f5bd4aa64fdadf96152ca6e6408068-Abstract.html,A Variational Approach to Learning Curves,"D√∂rthe Malzahn, Manfred Opper","We combine the replica approach from statistical physics with a varia- tional approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive ap- proximative relations between empirical error measures, the generaliza- tion error and the posterior variance."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/277a78fc05c8864a170e9a56ceeabc4c-Abstract.html,Why Neuronal Dynamics Should Control Synaptic Learning Rules,"Jesper Tegn√©r, √Åd√°m Kepecs","Hebbian learning rules are generally formulated as static rules. Un(cid:173) der changing condition (e.g. neuromodulation, input statistics)  most rules are sensitive to parameters. In particular, recent work  has focused on two different formulations of spike-timing-dependent  plasticity rules. Additive STDP [1] is remarkably versatile but  also very fragile, whereas multiplicative STDP [2, 3] is more ro(cid:173) bust but lacks attractive features such as synaptic competition and  rate stabilization. Here we address the problem of robustness in  the additive STDP rule. We derive an adaptive control scheme,  where the learning function is under fast dynamic control by post(cid:173) synaptic activity to stabilize learning under a variety of conditions.  Such a control scheme can be implemented using known biophysical  mechanisms of synapses. We show that this adaptive rule makes  the addit ive STDP more robust. Finally, we give an example how  meta plasticity of the adaptive rule can be used to guide STDP  into different type of learning regimes."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/296472c9542ad4d4788d543508116cbc-Abstract.html,Latent Dirichlet Allocation,"David M. Blei, Andrew Y. Ng, Michael I. Jordan","We propose a generative model for text and other collections of dis(cid:173) crete data that generalizes or improves on several previous models  including naive Bayes/unigram, mixture of unigrams  [6],  and Hof(cid:173) mann's  aspect  model,  also  known  as  probabilistic latent  semantic  indexing  (pLSI)  [3].  In  the  context  of text  modeling,  our  model  posits  that  each  document  is  generated  as  a  mixture  of  topics,  where  the  continuous-valued  mixture  proportions  are  distributed  as  a  latent  Dirichlet  random  variable.  Inference  and  learning  are  carried out efficiently  via variational  algorithms.  We  present  em(cid:173) pirical  results  on  applications  of this  model  to  problems  in  text  modeling,  collaborative filtering,  and text classification."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/2b0f658cbffd284984fb11d90254081f-Abstract.html,A Bayesian Network for Real-Time Musical Accompaniment,Christopher Raphael,"We  describe  a  computer  system  that  provides  a  real-time  musi(cid:173) cal  accompaniment  for  a  live  soloist  in  a  piece  of non-improvised  music for soloist and accompaniment.  A Bayesian network is devel(cid:173) oped  that  represents  the joint  distribution  on  the  times  at  which  the  solo  and  accompaniment  notes  are  played,  relating  the  two  parts through a  layer of hidden variables.  The network is first  con(cid:173) structed using  the rhythmic  information  contained in the musical  score.  The network is then trained to capture the musical interpre(cid:173) tations of the soloist and accompanist in an off-line rehearsal phase.  During live accompaniment the learned distribution of the network  is  combined  with a  real-time  analysis  of the  soloist's  acoustic  sig(cid:173) nal,  performed with  a  hidden Markov  model,  to  generate  a  musi(cid:173) cally principled accompaniment  that  respects all available  sources  of knowledge.  A live  demonstration will  be provided."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/2cad8fa47bbef282badbb8de5374b894-Abstract.html,Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms,"Roni Khardon, Dan Roth, Rocco A. Servedio","We study online learning in Boolean domains using kernels which cap- ture feature expansions equivalent to using conjunctions over basic fea- tures. We demonstrate a tradeoff between the computational efÔ¨Åciency with which these kernels can be computed and the generalization abil- ity of the resulting classiÔ¨Åer. We Ô¨Årst describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efÔ¨Åciently run the Percep- tron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple func- tions. We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. While known upper bounds imply that Winnow can learn DNF formulae with a polynomial mistake bound in this setting, we prove that it is computationally hard to simulate Win- now‚Äôs behavior for learning DNF over such a feature set, and thus that such kernel functions for Winnow are not efÔ¨Åciently computable."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/2cd4e8a2ce081c3d7c32c3cde4312ef7-Abstract.html,Prodding the ROC Curve: Constrained Optimization of Classifier Performance,"Michael Mozer, Robert Dodier, Michael D. Colagrosso, Cesar Guerra-Salcedo, Richard Wolniewicz","When designing a two-alternative classiÔ¨Åer, one ordinarily aims to maximize the classiÔ¨Åer‚Äôs ability to discriminate between members of the two classes. We describe a situation in a real-world business application of machine-learning prediction in which an additional constraint is placed on the nature of the solu- tion: that the classiÔ¨Åer achieve a speciÔ¨Åed correct acceptance or correct rejection rate (i.e., that it achieve a Ô¨Åxed accuracy on members of one class or the other). Our domain is predicting churn in the telecommunications industry. Churn refers to customers who switch from one service provider to another. We pro- pose four algorithms for training a classiÔ¨Åer subject to this domain constraint, and present results showing that each algorithm yields a reliable improvement in performance. Although the improvement is modest in magnitude, it is nonethe- less impressive given the difÔ¨Åculty of the problem and the Ô¨Ånancial return that it achieves to the service provider.
When designing a classiÔ¨Åer, one must specify an objective measure by which the classi- Ô¨Åer‚Äôs performance is to be evaluated. One simple objective measure is to minimize the number of misclassiÔ¨Åcations. If the cost of a classiÔ¨Åcation error depends on the target and/ or response class, one might utilize a risk-minimization framework to reduce the expected loss. A more general approach is to maximize the classiÔ¨Åer‚Äôs ability to discriminate one class from another class (e.g., Chang & Lippmann, 1994).
An ROC curve (Green & Swets, 1966) can be used to visualize the discriminative performance of a two-alternative classiÔ¨Åer that outputs class posteriors. To explain the ROC curve, a classiÔ¨Åer can be thought of as making a positive/negative judgement as to whether an input is a member of some class. Two different accuracy measures can be obtained from the classiÔ¨Åer: the accuracy of correctly identifying an input as a member of the class (a correct acceptance or CA), and the accuracy of correctly identifying an input as a nonmember of the class (a correct rejection or CR). To evaluate the CA and CR rates, it is necessary to pick a threshold above which the classiÔ¨Åer‚Äôs probability estimate is inter- preted as an ‚Äúaccept,‚Äù and below which is interpreted as a ‚Äúreject‚Äù‚Äîcall this the criterion. The ROC curve plots CA against CR rates for various criteria (Figure 1a). Note that as the threshold is lowered, the CA rate increases and the CR rate decreases. For a criterion of 1, the CA rate approaches 0 and the CR rate 1; for a criterion of 0, the CA rate approaches 1"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/2d00f43f07911355d4151f13925ff292-Abstract.html,Natural Language Grammar Induction Using a Constituent-Context Model,"Dan Klein, Christopher D. Manning","This paper presents a novel approach to the unsupervised learning of syn- tactic analyses of natural language text. Most previous work has focused on maximizing likelihood according to generative PCFG models. In con- trast, we employ a simpler probabilistic model over trees based directly on constituent identity and linear context, and use an EM-like iterative procedure to induce structure. This method produces much higher qual- ity analyses, giving the best published results on the ATIS dataset. 1 Overview
To enable a wide range of subsequent tasks, human language sentences are standardly given tree-structure analyses, wherein the nodes in a tree dominate contiguous spans of words called constituents, as in figure 1(a). Constituents are the linguistically coherent units in the sentence, and are usually labeled with a constituent category, such as noun phrase (NP) or verb phrase (VP). An aim of grammar induction systems is to figure out, given just the sentences in a corpus S, what tree structures correspond to them. In this sense, the grammar induction problem is an incomplete data problem, where the complete data is the corpus of trees T , but we only observe their yields S. This paper presents a new approach to this problem, which gains leverage by directly making use of constituent contexts. It is an open problem whether entirely unsupervised methods can produce linguistically accurate parses of sentences. Due to the difficulty of this task, the vast majority of statis- tical parsing work has focused on supervised learning approaches to parsing, where one uses a treebank of fully parsed sentences to induce a model which parses unseen sentences [7, 3]. But there are compelling motivations for unsupervised grammar induction. Building supervised training data requires considerable resources, including time and linguistic ex- pertise. Investigating unsupervised methods can shed light on linguistic phenomena which are implicit within a supervised parser's supervisory information (e.g., unsupervised sys- tems often have difficulty correctly attaching subjects to verbs above objects, whereas for a supervised parser, this ordering is implicit in the supervisory information). Finally, while the presented system makes no claims to modeling human language acquisition, results on whether there is enough information in sentences to recover their structure are important data for linguistic theory, where it has standardly been assumed that the information in the data is deficient, and strong innate knowledge is required for language acquisition [4]."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/2d405b367158e3f12d7c1e31a96b3af3-Abstract.html,Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference,"Nicolas Chapados, Yoshua Bengio, Pascal Vincent, Joumana Ghosn, Charles Dugas, Ichiro Takeuchi, Linyan Meng","Estimating insurance premia from data is a difficult regression problem for several reasons: the large number of variables, many of which are .discrete, and the very peculiar shape of the noise distri(cid:173) bution, asymmetric with fat tails, with a large majority zeros and a few unreliable and very large values. We compare several machine learning methods for estimating insurance premia, and test them on a large data base of car insurance policies. We find that func(cid:173) tion approximation methods that do not optimize a squared loss, like Support Vector Machines regression, do not work well in this context. Compared methods include decision trees and generalized linear models. The best results are obtained with a mixture of experts, which better identifies the least and most risky contracts, and allows to reduce the median premium by charging more to the most risky customers."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/2d579dc29360d8bbfbb4aa541de5afa9-Abstract.html,Classifying Single Trial EEG: Towards Brain Computer Interfacing,"Benjamin Blankertz, Gabriel Curio, Klaus-Robert M√ºller","Driven by the progress in the Ô¨Åeld of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming Ô¨Ånger movements in a natural keyboard typing condition and predicts their lat- erality. This can be done on average 100‚Äì230 ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classiÔ¨Åcation accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classiÔ¨Åers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable reg- ularization properties for dealing with high noise cases (inter-trial vari- ablity)."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/2eace51d8f796d04991c831a07059758-Abstract.html,Kernel Logistic Regression and the Import Vector Machine,"Ji Zhu, Trevor Hastie","The support vector machine (SVM) is known for its good performance in binary classiÔ¨Åcation, but its extension to multi-class classiÔ¨Åcation is still an on-going research issue. In this paper, we propose a new approach for classiÔ¨Åcation, called the import vector machine (IVM), which is built on kernel logistic regression (KLR). We show that the IVM not only per- forms as well as the SVM in binary classiÔ¨Åcation, but also can naturally be generalized to the multi-class case. Furthermore, the IVM provides an estimate of the underlying probability. Similar to the ‚Äúsupport points‚Äù of the SVM, the IVM model uses only a fraction of the training data to index kernel basis functions, typically a much smaller fraction than the SVM. This gives the IVM a computational advantage over the SVM, especially when the size of the training data set is large.
is qualitative and assumes values in a Ô¨Ånite set 
  , where the output  from 
. We , to it. Usually it is assumed that the training data are an independently and identically distributed sample from an unknown probability distribution"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/312351bff07989769097660a56395065-Abstract.html,A Model of the Phonological Loop: Generalization and Binding,"Randall C. O'Reilly, R. Soto","We present a  neural network model that shows how the prefrontal  cortex, interacting with the basal ganglia, can maintain a sequence  of  phonological  information  in  activation-based  working  memory  (i.e.,  the  phonological  loop).  The  primary function  of this  phono(cid:173) logical  loop  may  be  to  transiently  encode  arbitrary  bindings  of  information  necessary  for  tasks  - the  combinatorial  expressive  power  of language  enables  very  flexible  binding  of essentially  ar(cid:173) bitrary  pieces  of information.  Our  model  takes  advantage  of the  closed-class nature of phonemes, which allows  different  neural rep(cid:173) resentations of all possible phonemes at each sequential position to  be encoded.  To  make this work,  we  suggest that the basal ganglia  provide a  region-specific update signal that allocates phonemes to  the appropriate sequential coding slot.  To  demonstrate that flexi(cid:173) ble,  arbitrary binding of novel  sequences can be supported by this  mechanism,  we  show  that  the  model  can  generalize  to  novel  se(cid:173) quences after  moderate amounts of training."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/325995af77a0e8b06d1204a171010b3a-Abstract.html,Thin Junction Trees,"Francis R. Bach, Michael I. Jordan","We present an algorithm that induces a class of models with thin junction trees‚Äîmodels that are characterized by an upper bound on the size of the maximal cliques of their triangulated graph. By ensuring that the junction tree is thin, inference in our models remains tractable throughout the learning process. This allows both an efÔ¨Åcient implementation of an iterative scaling parameter estimation algorithm and also ensures that inference can be performed efÔ¨Åciently with the Ô¨Ånal model. We illustrate the approach with applications in handwritten digit recognition and DNA splice site detection."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/3683af9d6f6c06acee72992f2977f67e-Abstract.html,Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field,"O. Donchin, Reza Shadmehr","Reaching movements require the brain to generate motor com- mands that rely on an internal model of the task's dynamics. Here we consider the errors that subjects make early in their reaching trajectories to various targets as they learn an internal model. Us- ing a framework from function approximation, we argue that the sequence of errors should reflect the process of gradient descent. If so, then the sequence of errors should obey hidden state transitions of a simple dynamical system. Fitting the system to human data, we find a surprisingly good fit accounting for 98% of the variance. This allows us to draw tentative conclusions about the basis ele- ments used by the brain in transforming sensory space to motor commands. To test the robustness of the results, we estimate the shape of the basis elements under two conditions: in a traditional learning paradigm with a consistent force field, and in a random sequence of force fields where learning is not possible. Remarkably, we find that the basis remains invariant. 1 Introduction
It appears that in constructing the motor commands to guide the arm toward a target, the brain relies on an internal model (IM) of the dynamics of the task that it learns through practice [1]. The IM is presumably a system that transforms a desired limb trajectory in sensory coordinates to motor commands. The motor commands in turn create the complex activation of muscles necessary to cause action. A major issue in motor control is to infer characteristics of the IM from the actions of subjects. Recently, we took a first step toward mathematically characterizing the IM's rep- resentation in the brain [2]. We analyzed the sequence of errors made by subjects on successive movements as they reached to targets while holding a robotic arm. The robot produced a force field and subjects learned to compensate for the field (presumably by constructing an IM) and eventually produced straight movements within the field. Our analysis sought to draw conclusions about the structure of the IM from the sequence of errors generated by the subjects. For instance, in a
velocity-dependent force field (such as the fields we use), the IM must be able to encode velocity in order to anticipate the upcoming force. We hoped that the e#ect of errors in one direction on subsequent movements in other directions would give information about the width of the elements which the IM used in encoding velocity. For example, if the basis elements were narrow, then movements in a given direction would result in little or no change in performance in neighboring directions. Wide basis elements would mean appropriately larger e#ects. We hypothesized that an estimate of the width of the basis elements could be cal- culated by fitting the time sequence of errors to a set of equations representing a dynamical system. The dynamical system assumed that error in a movement resulted from a di#erence between the IM's approximation and the actual environ- ment, an assumption that has recently been corroborated [3]. The error in turn changed the IM, a#ecting subsequent movements:"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/36ac8e558ac7690b6f44e2cb5ef93322-Abstract.html,A Parallel Mixture of SVMs for Very Large Scale Problems,"Ronan Collobert, Samy Bengio, Yoshua Bengio","Support Vector Machines  (SVMs)  are currently the state-of-the-art models for  many classification problems but they suffer from the complexity of their train(cid:173) ing algorithm which is at least quadratic with respect to the number of examples.  Hence,  it is  hopeless  to try to solve  real-life  problems  having more than a  few  hundreds  of  thousands  examples  with  SVMs.  The  present  paper  proposes  a  new  mixture  of SVMs  that  can  be  easily  implemented  in  parallel  and  where  each SVM is  trained on a  small  subset of the whole  dataset.  Experiments on a  large benchmark dataset  (Forest)  as well  as  a  difficult  speech database, yielded  significant  time  improvement  (time  complexity  appears  empirically  to  locally  grow linearly with the number of examples) .  In addition, and that is a surprise,  a  significant improvement in  generalization was  observed on Forest."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/378a063b8fdb1db941e34f4bde584c7d-Abstract.html,Switch Packet Arbitration via Queue-Learning,Timothy X. Brown,"In packet switches, packets queue at switch inputs and contend for out- puts. The contention arbitration policy directly affects switch perfor- mance. The best policy depends on the current state of the switch and current trafÔ¨Åc patterns. This problem is hard because the state space, possible transitions, and set of actions all grow exponentially with the size of the switch. We present a reinforcement learning formulation of the problem that decomposes the value function into many small inde- pendent value functions and enables an efÔ¨Åcient action selection."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/3875115bacc48cca24ac51ee4b0e7975-Abstract.html,Multi Dimensional ICA to Separate Correlated Sources,"Roland Vollgraf, Klaus Obermayer","We present a new method for the blind separation of sources, which  do not fulfill the independence assumption.  In contrast to standard  methods  we  consider  groups  of  neighboring  samples  (""patches"")  within the observed mixtures.  First  we  extract independent  features  from  the  observed  patches.  It turns out that the average dependencies  between these features  in  different  sources  is  in  general  lower  than  the  dependencies  be(cid:173) tween  the  amplitudes  of different  sources.  We  show  that it  might  be  the  case  that  most  of  the  dependencies  is  carried  by  only  a  small  number  of features.  Is  this  case  - provided  these  features  can  be  identified  by  some  heuristic  - we  project  all  patches  into  the subspace  which  is  orthogonal to  the subspace  spanned by  the  ""correlated""  features.  Standard ICA is then performed on the elements of the transformed  patches  (for  which  the  independence  assumption  holds)  and  ro(cid:173) bustly yields a  good estimate of the mixing matrix."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/3937230de3c8041e4da6ac3246a888e8-Abstract.html,Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes,"Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer","Inferior temporal cortex (IT) neurons have large receptive Ô¨Åelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive Ô¨Åelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniÔ¨Åcation factor than the peripheral visual Ô¨Åeld. Further- more, we show that top-down object bias can increase the receptive Ô¨Åeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive Ô¨Åeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/39dcaf7a053dc372fbc391d4e6b5d693-Abstract.html,A kernel method for multi-labelled classification,"Andr√© Elisseeff, Jason Weston","This article presents a Support Vector Machine (SVM) like learning sys- tem to handle multi-label problems. Such problems are usually decom- posed into many two-class problems but the expressive power of such a system can be weak [5, 7]. We explore a new direct approach. It is based on a large margin ranking system that shares a lot of common proper- ties with SVMs. We tested it on a Yeast gene functional classiÔ¨Åcation problem with positive results."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/3a824154b16ed7dab899bf000b80eeee-Abstract.html,Learning Lateral Interactions for Feature Binding and Sensory Segmentation,Heiko Wersing,"We present a new approach to the supervised learning of lateral inter- actions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning prob- lem is formulated as a convex quadratic optimization problem in the lat- eral interaction weights. An efÔ¨Åcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis inter- actions. We show the successful application of the method to a medical image segmentation problem of Ô¨Çuorescence microscope cell images."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/3b92d18aa7a6176dd37d372bc2f1eb71-Abstract.html,Very loopy belief propagation for unwrapping phase images,"Brendan J. Frey, Ralf Koetter, Nemanja Petrovic","Since  the  discovery  that  the  best  error-correcting  decoding  algo(cid:173) rithm can be viewed  as belief propagation in a  cycle-bound graph,  researchers  have  been  trying  to  determine  under  what  circum(cid:173) stances ""loopy belief propagation"" is effective for probabilistic infer(cid:173) ence.  Despite several theoretical advances in our understanding of  loopy belief propagation, to our knowledge,  the only  problem that  has  been  solved  using  loopy  belief propagation is  error-correcting  decoding on Gaussian channels.  We  propose a  new  representation  for  the two-dimensional  phase unwrapping problem,  and  we  show  that loopy belief propagation produces results that are superior to  existing techniques.  This is  an important result, since  many imag(cid:173) ing techniques, including magnetic resonance imaging and interfer(cid:173) ometric synthetic aperture radar,  produce phase-wrapped images.  Interestingly,  the  graph  that  we  use  has  a  very  large  number  of  very short cycles,  supporting evidence that a  large minimum cycle  length is  not needed for  excellent results  using  belief propagation."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/3d863b367aa379f71c7afc0c9cdca41d-Abstract.html,Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies,"A. D'avella, M. C. Tresch","The question of whether the nervous system produces movement through the combination of a few discrete elements has long been central to the study of motor control. Muscle synergies, i.e. coordinated patterns of muscle activity, have been proposed as possible building blocks. Here we propose a model based on combinations of muscle synergies with a spe- ciÔ¨Åc amplitude and temporal structure. Time-varying synergies provide a realistic basis for the decomposition of the complex patterns observed in natural behaviors. To extract time-varying synergies from simultane- ous recording of EMG activity we developed an algorithm which extends existing non-negative matrix factorization techniques."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/3e9e39fed3b8369ed940f52cf300cf88-Abstract.html,The Method of Quantum Clustering,"David Horn, Assaf Gottlieb","We propose a novel clustering method that is an extension of ideas inher- ent to scale-space clustering and support-vector clustering. Like the lat- ter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scale- space probability function. The novelty of our approach is the study of an operator in Hilbert space, represented by the Schr¬®odinger equation of which the probability function is a solution. This Schr¬®odinger equation contains a potential function that can be derived analytically from the probability function. We associate minima of the potential with cluster centers. The method has one variable parameter, the scale of its Gaussian kernel. We demonstrate its applicability on known data sets. By limiting the evaluation of the Schr¬®odinger potential to the locations of data points, we can apply this method to problems in high dimensions."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/3f088ebeda03513be71d34d214291986-Abstract.html,Generating velocity tuning by asymmetric recurrent connections,"Xiaohui Xie, Martin A. Giese","Asymmetric lateral connections are one possible mechanism that can ac- count for the direction selectivity of cortical neurons. We present a math- ematical analysis for a class of these models. Contrasting with earlier theoretical work that has relied on methods from linear systems theory, we study the network‚Äôs nonlinear dynamic properties that arise when the threshold nonlinearity of the neurons is taken into account. We show that such networks have stimulus-locked traveling pulse solutions that are appropriate for modeling the responses of direction selective cortical neurons. In addition, our analysis shows that outside a certain regime of stimulus speeds the stability of this solutions breaks down giving rise to another class of solutions that are characterized by speciÔ¨Åc spatio- temporal periodicity. This predicts that if direction selectivity in the cor- tex is mainly achieved by asymmetric lateral connections lurching activ- ity waves might be observable in ensembles of direction selective cortical neurons within appropriate regimes of the stimulus speed."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/405e28906322882c5be9b4b27f4c35fd-Abstract.html,Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM,"Shai Fine, Katya Scheinberg","We propose a framework based on a parametric quadratic program(cid:173) ming  (QP)  technique  to  solve  the  support vector  machine  (SVM)  training problem.  This framework, can be specialized to obtain two  SVM  optimization  methods.  The first  solves  the  fixed  bias  prob(cid:173) lem,  while  the  second  starts  with  an  optimal  solution  for  a  fixed  bias problem and adjusts the bias until the optimal value is found.  The later method can be applied in conjunction with any other ex(cid:173) isting technique which obtains a fixed  bias solution.  Moreover, the  second  method  can  also  be  used  independently  to  solve  the  com(cid:173) plete SVM training problem.  A combination of these two methods  is  more  flexible  than  each  individual  method  and,  among  other  things,  produces an incremental algorithm which exactly  solve the  1-Norm Soft  Margin  SVM  optimization problem.  Applying  Selec(cid:173) tive  Sampling techniques may further  boost convergence."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/44cd7a8f7f9f85129b9953950665064d-Abstract.html,Bayesian Predictive Profiles With Applications to Retail Transaction Data,"Igor V. Cadez, Padhraic Smyth","Massive transaction data sets are recorded in a routine manner in telecommunications, retail commerce, and Web site management. In this paper we address the problem of inferring predictive in- dividual proÔ¨Çles from such historical transaction data. We de- scribe a generative mixture model for count data and use an an approximate Bayesian estimation framework that eÔ¨Åectively com- bines an individual‚Äôs speciÔ¨Çc history with more general population patterns. We use a large real-world retail transaction data set to illustrate how these proÔ¨Çles consistently outperform non-mixture and non-Bayesian techniques in predicting customer behavior in out-of-sample data."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/4a3e00961a08879c34f91ca0070ea2f5-Abstract.html,A Rational Analysis of Cognitive Control in a Speeded Discrimination Task,"Michael Mozer, Michael D. Colagrosso, David E. Huber","We are interested in the mechanisms by which individuals monitor and adjust their performance of simple cognitive tasks. We model a speeded discrimination task in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). Response conÔ¨Çict arises when one stimulus class is infrequent relative to another, resulting in more errors and slower reaction times for the infrequent class. How do control pro- cesses modulate behavior based on the relative class frequencies? We explain performance from a rational perspective that casts the goal of individuals as minimizing a cost that depends both on error rate and re- action time. With two additional assumptions of rationality‚Äîthat class prior probabilities are accurately estimated and that inference is optimal subject to limitations on rate of information transmission‚Äîwe obtain a good Ô¨Åt to overall RT and error data, as well as trial-by-trial variations in performance.
Consider the following scenario: While driving, you approach an intersection at which the trafÔ¨Åc light has already turned yellow, signaling that it is about to turn red. You also notice that a car is approaching you rapidly from behind, with no indication of slowing. Should you stop or speed through the intersection? The decision is difÔ¨Åcult due to the presence of two conÔ¨Çicting signals. Such response conÔ¨Çict can be produced in a psychological labo- ratory as well. For example, Stroop (1935) asked individuals to name the color of ink on which a word is printed. When the words are color names incongruous with the ink color‚Äî e.g., ‚Äúblue‚Äù printed in red‚Äîreaction times are slower and error rates are higher. We are in- terested in the control mechanisms underlying performance of high-conÔ¨Çict tasks. ConÔ¨Çict requires individuals to monitor and adjust their behavior, possibly responding more slowly if errors are too frequent.
In this paper, we model a speeded discrimination paradigm in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). The stimuli are letters of the alphabet, A‚ÄìZ, presented in rapid succession. In a choice task, individuals are asked to press one response key if the letter is an X or another response key for any letter other than X (as a shorthand, we will refer to non-X stimuli as Y). In a go/no-go task, individuals
 
are asked to press a response key when X is presented and to make no response otherwise. We address both tasks because they elicit slightly different decision-making behavior. In both tasks, Jones and Braver (2001) manipulated the relative frequency of the X and Y stimuli; the ratio of presentation frequency was either 17:83, 50:50, or 83:17. Response conÔ¨Çict arises when the two stimulus classes are unbalanced in frequency, resulting in more errors and slower reaction times. For example, when X‚Äôs are frequent but Y is presented, individuals are predisposed toward producing the X response, and this predisposition must be overcome by the perceptual evidence from the Y.
Jones and Braver (2001) also performed an fMRI study of this task and found that anterior cingulate cortex (ACC) becomes activated in situations involving response conÔ¨Çict. Specif- ically, when one stimulus occurs infrequently relative to the other, event-related fMRI re- sponse in the ACC is greater for the low frequency stimulus. Jones and Braver also ex- tended a neural network model of Botvinick, Braver, Barch, Carter, and Cohen (2001) to account for human performance in the two discrimination tasks. The heart of the model is a mechanism that monitors conÔ¨Çict‚Äîthe posited role of the ACC‚Äîand adjusts response biases accordingly. In this paper, we develop a parsimonious alternative account of the role of the ACC and of how control processes modulate behavior when response conÔ¨Çict arises.
1 A RATIONAL ANALYSIS
Our account is based on a rational analysis of human cognition, which views cognitive processes as being optimized with respect to certain task-related goals, and being adaptive to the structure of the environment (Anderson, 1990). We make three assumptions of ratio- nality: (1) perceptual inference is optimal but is subject to rate limitations on information transmission, (2) response class prior probabilities are accurately estimated, and (3) the goal of individuals is to minimize a cost that depends both on error rate and reaction time.
The heart of our account is an existing probabilistic model that explains a variety of fa- cilitation effects that arise from long-term repetition priming (Colagrosso, in preparation; Mozer, Colagrosso, & Huber, 2000), and more broadly, that addresses changes in the na- ture of information transmission in neocortex due to experience. We give a brief overview of this model; the details are not essential for the present work.
The model posits that neocortex can be characterized by a collection of information- processing pathways, and any act of cognition involves coordination among pathways. To model a simple discrimination task, we might suppose a perceptual pathway to map the visual input to a semantic representation, and a response pathway to map the semantic representation to a response. The choice and go/no-go tasks described earlier share a per- ceptual pathway, but require different response pathways. The model is framed in terms of probability theory: pathway inputs and outputs are random variables and microinference in a pathway is carried out by Bayesian belief revision."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/4afd521d77158e02aed37e2274b90c9c-Abstract.html,Learning Spike-Based Correlations and Conditional Probabilities in Silicon,"Aaron P. Shon, David Hsu, Chris Diorio",Abstract Unavailable
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/4b86abe48d358ecf194c56c69108433e-Abstract.html,A Natural Policy Gradient,Sham M. Kakade,"We provide a  natural gradient method that represents the steepest  descent  direction based on the underlying structure of the param(cid:173) eter space.  Although gradient methods cannot make large changes  in  the  values  of the  parameters,  we  show  that  the  natural  gradi(cid:173) ent is moving toward choosing a greedy optimal action rather than  just a  better action.  These greedy optimal  actions  are those  that  would  be  chosen  under  one  improvement  step  of  policy  iteration  with  approximate,  compatible  value  functions,  as  defined  by  Sut(cid:173) ton  et al.  [9].  We  then show drastic performance improvements in  simple MDPs and in the more challenging MDP of Tetris."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/4ba29b9f9e5732ed33761840f4ba6c53-Abstract.html,Spectral Kernel Methods for Clustering,"Nello Cristianini, John Shawe-Taylor, Jaz S. Kandola","In this paper we introduce new algorithms for unsupervised learn(cid:173) ing based on the use of a kernel matrix. All the information re(cid:173) quired by such algorithms is contained in the eigenvectors of the  matrix or of closely related matrices. We use two different but re(cid:173) lated cost functions, the Alignment and the 'cut cost'. The first  one is discussed in a companion paper [3], the second one is based  on graph theoretic concepts. Both functions measure the level of  clustering of a labeled dataset, or the correlation between data clus(cid:173) ters and labels. We state the problem of unsupervised learning as  assigning labels so as to optimize these cost functions. We show  how the optimal solution can be approximated by slightly relaxing  the corresponding optimization problem, and how this corresponds  to using eigenvector information. The resulting simple algorithms  are tested on real world data with positive results."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/4ba3c163cd1efd4c14e3a415fa0a3010-Abstract.html,Batch Value Function Approximation via Support Vectors,"Thomas G. Dietterich, Xin Wang","We present three ways of combining linear programming with the kernel trick to find value function approximations for reinforcement learning. One formulation is based on SVM regression; the second is based on the Bellman equation; and the third seeks only to ensure that good moves have an advantage over bad moves. All formu(cid:173) lations attempt to minimize the number of support vectors while fitting the data. Experiments in a difficult, synthetic maze problem show that all three formulations give excellent performance, but the advantage formulation is much easier to train. Unlike policy gradi(cid:173) ent methods, the kernel methods described here can easily 'adjust the complexity of the function approximator to fit the complexity of the value function."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/4c144c47ecba6f8318128703ca9e2601-Abstract.html,PAC Generalization Bounds for Co-training,"Sanjoy Dasgupta, Michael L. Littman, David A. McAllester","The rule-based bootstrapping introduced by Yarowsky, and its co- training variant by Blum and Mitchell, have met with considerable em- pirical success. Earlier work on the theory of co-training has been only loosely related to empirically useful co-training algorithms. Here we give a new PAC-style bound on generalization error which justiÔ¨Åes both the use of conÔ¨Ådences ‚Äî partial rules and partial labeling of the unlabeled data ‚Äî and the use of an agreement-based objective function as sug- gested by Collins and Singer. Our bounds apply to the multiclass case, i.e., where instances are to be assigned one of"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/4d6b3e38b952600251ee92fe603170ff-Abstract.html,Face Recognition Using Kernel Methods,Ming-Hsuan Yang,"Principal Component Analysis and Fisher Linear Discriminant methods have demonstrated their success in face detection, recog(cid:173) nition, and tracking. The representation in these subspace methods is based on second order statistics of the image set, and does not address higher order statistical dependencies such as the relation(cid:173) ships among three or more pixels. Recently Higher Order Statistics and Independent Component Analysis (ICA) have been used as in(cid:173) formative low dimensional representations for visual recognition. In this paper, we investigate the use of Kernel Principal Compo(cid:173) nent Analysis and Kernel Fisher Linear Discriminant for learning low dimensional representations for face recognition, which we call Kernel Eigenface and Kernel Fisherface methods. While Eigenface and Fisherface methods aim to find projection directions based on the second order correlation of samples, Kernel Eigenface and Ker(cid:173) nel Fisherface methods provide generalizations which take higher order correlations into account. We compare the performance of kernel methods with Eigenface, Fisherface and ICA-based meth(cid:173) ods for face recognition with variation in pose, scale, lighting and expression. Experimental results show that kernel methods pro(cid:173) vide better representations and achieve lower error rates for face recognition.
1 Motivation and Approach
Subspace methods have been applied successfully in numerous visual recognition tasks such as face localization, face recognition, 3D object recognition, and tracking. In particular, Principal Component Analysis (PCA) [20] [13] ,and Fisher Linear Dis(cid:173) criminant (FLD) methods [6] have been applied to face recognition with impressive results. While PCA aims to extract a subspace in which the variance is maximized (or the reconstruction error is minimized), some unwanted variations (due to light(cid:173) ing, facial expressions, viewing points, etc.) may be retained (See [8] for examples). It has been observed that in face recognition the variations between the images of the same face due to illumination and viewing direction are almost always larger than image variations due to the changes in face identity [1]. Therefore, while the PCA projections are optimal in a correlation sense (or for reconstruction"" from a low dimensional subspace), these eigenvectors or bases may be suboptimal from the
classification viewpoint.
Representations of Eigenface [20] (based on PCA) and Fisherface [6] (based on FLD) methods encode the pattern information based on the second order dependencies, i.e., pixelwise covariance among the pixels, and are insensitive to the dependencies among multiple (more than two) pixels in the samples. Higher order dependencies in an image include nonlinear relations among the pixel intensity values, such as the relationships among three or more pixels in an edge or a curve, which can cap(cid:173) ture important information for recognition. Several researchers have conjectured that higher order statistics may be crucial to better represent complex patterns. Recently, Higher Order Statistics (HOS) have been applied to visual learning prob(cid:173) lems. Rajagopalan et ale use HOS of the images of a target object to get a better approximation of an unknown distribution. Experiments on face detection [16] and vehicle detection [15] show comparable, if no better, results than other PCA-based methods.
The concept of Independent Component Analysis (ICA) maximizes the degree of statistical independence of output variables using contrast functions such as Kullback-Leibler divergence, negentropy, and cumulants [9] [10]. A neural net(cid:173) work algorithm to carry out ICA was proposed by Bell and Sejnowski [7], and was applied to face recognition [3]. Although the idea of computing higher order mo(cid:173) ments in the ICA-based face recognition method is attractive, the assumption that the face images comprise of a set of independent basis images (or factorial codes) is not intuitively clear. In [3] Bartlett et ale showed that ICA representation out(cid:173) perform PCA representation in face recognition using a subset of frontal FERET face images. However, Moghaddam recently showed that ICA representation does not provide significant advantage over PCA [12]. The experimental results suggest that seeking non-Gaussian and independent components may not necessarily yield better representation for face recognition.
In [18], Sch6lkopf et ale extended the conventional PCA to Kernel Principal Com(cid:173) ponent Analysis (KPCA). Empirical results on digit recognition using MNIST data set and object recognition using a database of rendered chair images showed that Kernel PCA is able to extract nonlinear features and thus provided better recog(cid:173) nition results. Recently Baudat and Anouar, Roth and Steinhage, and Mika et ale applied kernel tricks to FLD and proposed Kernel Fisher Linear Discriminant (KFLD) method [11] [17] [5]. Their experiments showed that KFLD is able to ex(cid:173) tract the most discriminant features in the feature space, which is equivalent to extracting the most discriminant nonlinear features in the original input space.
In this paper we seek a method that not only extracts higher order statistics of samples as features, but also maximizes the class separation when we project these features to a lower dimensional space for efficient recognition. Since much of the important information may be contained in the high order dependences among the pixels of a: face image, we investigate the use of Kernel PCA and Kernel FLD for face recognition, which we call Kernel Eigenface and Kernel Fisherface methods, and compare their performance against the standard Eigenface, Fisherface and ICA methods. In the meanwhile, we explain why kernel methods are suitable for visual recognition tasks such as face recognition.
2 Kernel Principal Component Analysis
== Given a set of m centered (zero mean, unit variance) samples Xk, Xk [Xkl, ... ,Xkn]T ERn, PCA aims to find the projection directions that maximize the variance, C, which is equivalent to finding the eigenvalues from the covariance"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/4d8556695c262ab91ff51a943fdd6058-Abstract.html,Active Portfolio-Management based on Error Correction Neural Networks,"Hans-Georg Zimmermann, Ralph Neuneier, Ralph Grothmann","This paper deals with a neural network architecture which establishes a portfolio management system similar to the Black / Litterman approach. This allocation scheme distributes funds across various securities or Ô¨Å- nancial markets while simultaneously complying with speciÔ¨Åc allocation constraints which meet the requirements of an investor. The portfolio optimization algorithm is modeled by a feedforward neural network. The underlying expected return forecasts are based on error correction neural networks (ECNN), which utilize the last model error as an auxiliary input to evaluate their own misspeciÔ¨Åcation. The portfolio optimization is implemented such that (i.) the allocations comply with investor‚Äôs constraints and that (ii.) the risk of the portfo- lio can be controlled. We demonstrate the proÔ¨Åtability of our approach by constructing internationally diversiÔ¨Åed portfolios across  different Ô¨Ånancial markets of the G7 contries. It turns out, that our approach is superior to a preset benchmark portfolio.
1 Introduction: Portfolio-Management
We integrate the portfolio optimization algorithm suggested by Black / Litterman [1] into a neural network architecture. Combining the mean-variance theory [5] with the capital asset pricing model (CAPM) [7], this approach utilizes excess returns of the CAPM equilibrium to deÔ¨Åne a neutral, well balanced benchmark portfolio. Deviations from the benchmark allocation are only allowed within preset boundaries. Hence, as an advantage, there are no unrealistic solutions (e. g. large short positions, huge portfolio changes). Moreover, there is no need of formulating return expectations for all assets.
In contrast to Black / Litterman, excess return forecasts are estimated by time-delay recur- rent error correction neural networks [8]. Investment decisions which comply with given allocation constraints are derived from these predictions. The risk exposure of the portfolio is implicitly controlled by a parameter-optimizing task over time (sec. 3 and 5).
Our approach consists of the following three steps: (i.) Construction of forecast models
              on the basis of error correction neural networks (ECNN) for all  assets (sec. 2).  To whom correspondence should be addressed: Georg.Zimmermann@mchp.siemens.de.
(sec. 3 and 4). By this, the proÔ¨Åtability of an asset with respect to all others is measured.
(ii.) Computation of excess returns  (iii.) Optimization of the investment proportions     Allocation constraints ensure, that the investment proportions 
 by a higher-level feedforward network  on the basis of the excess returns.  may deviate from a given
benchmark only within predeÔ¨Åned intervals (sec. 3 and 4).
Finally, we apply our neural network based portfolio management system to an asset allo- cation problem concerning the G7 countries (sec. 6).

2 Forecasting by Error Correction Neural Networks
Most dynamical systems are driven by a superposition of autonomous development and external inÔ¨Çuences [8]. For discrete time grids, such a dynamics can be described by a
recurrent state transition

 and an output equation  (Eq. 1). 

state transition eq. output eq."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/4e62e752ae53fb6a6eebd0f6146aa702-Abstract.html,Generalization Performance of Some Learning Problems in Hilbert Functional Spaces,T. Zhang,"We investigate the generalization performance of some learning prob- lems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/51174add1c52758f33d414ceaf3fe6ba-Abstract.html,Analog Soft-Pattern-Matching Classifier using Floating-Gate MOS Technology,"Toshihiko Yamasaki, Tadashi Shibata","A flexible pattern-matching analog classifier is presented in con- junction with a robust image representation algorithm called Prin- cipal Axes Projection (PAP). In the circuit, the functional form of  matching is configurable in terms of the peak position, the peak height  and the sharpness of the similarity evaluation. The test chip was fabri- cated in a 0.6-m m CMOS technology and successfully applied to  hand-written pattern recognition and medical radiograph analysis using  PAP as a feature extraction pre-processing step for robust image coding.  The separation and classification of overlapping patterns is also ex- perimentally demonstrated."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/519c84155964659375821f7ca576f095-Abstract.html,Generalizable Relational Binding from Coarse-coded Distributed Representations,"Randall C. O'Reilly, R. S. Busby","We present a model of binding of relationship information in a spatial domain (e.g., square above triangle) that uses low-order coarse-coded
conjunctive representations instead of more popular temporal synchrony mechanisms. Supporters of temporal synchrony argue that conjunctive representations lack both efficiency (i.e., combinatorial numbers of units are required) and systematicity (i.e., the resulting representations are overly specific and thus do not support generalization to novel exem- plars). To counter these claims, we show that our model: a) uses far fewer hidden units than the number of conjunctions represented, by us- ing coarse-coded, distributed representations where each unit has a broad tuning curve through high-dimensional conjunction space, and b) is ca- pable of considerable generalization to novel inputs. 1 Introduction
The binding problem as it is classically conceived arises when different pieces of infor- mation are processed by entirely separate units. For example, we can imagine there are neurons that separately code for the shape and color of objects, and we are viewing a scene having a red triangle and a blue square (Figure 1). Because color and shape are encoded separately in this system, the internal representations do not discriminate this situation from one where we are viewing a red square and a blue triangle. This is the problem. Broadly speaking, there are two solutions to it. Perhaps the most popular solution is to imagine that binding is encoded by some kind of transient signal, such as temporal synchrony (e.g., von der Malsburg, 1981; Gray, Engel, Konig, & Singer, 1992; Hummel & Holyoak, 1997). Under this solution, the red and triangle units should fire together, as should the blue and square units, with each group firing out of phase with the other. The other solution can be construed as solving the problem by questioning its fundamental assumption --- that information is encoded completely separately in the first place (which is so seductive that it typically goes unnoticed). Instead, one can imagine that color and shape information are encoded together (i.e., conjunctively). In the red-triangle blue-square example, some neurons encode the conjunction of red and triangle, while others encode the conjunction of blue and square. Because these units are explicitly sensitive to these
Red Blue Square Triangle ? ? Red Blue Square Triangle a) Input activates features b) But rest of brain doesn't know which features go with each other Figure 1: Illustration of the binding problem. a) Visual inputs (red triangle, blue square) activate separate representations of color and shape properties. b) However, just the mere activation of these features does not distinguish for the rest of the brain the alternative scenario of a blue triangle and a red square. Red is indicated by dashed outline and blue by a dotted outline. RC RC GS GS obj1 obj2 R G B S C T BT obj1 obj2 R G B S C T BT RS GC 1 1 0 1 1 0 0 RT GC 1 1 0 0 1 1 0 RC GS 1 1 0 1 1 0 1 RC BT 1 0 1 0 1 1 1 RS GT 1 1 0 1 0 1 0 RT BC 1 0 1 0 1 1 0 RT GS 1 1 0 1 0 1 1 GS BC 0 1 1 1 1 0 1 RS BC 1 0 1 1 1 0 0 GC BS 0 1 1 1 1 0 0 RC BS 1 0 1 1 1 0 1 GS BT 0 1 1 1 0 1 1 RS BT 1 0 1 1 0 1 1 GT BS 0 1 1 1 0 1 0 RT BS 1 0 1 1 0 1 0 GC BT 0 1 1 0 1 1 1 RC GT 1 1 0 0 1 1 1 GT BC 0 1 1 0 1 1 0
Table 1: Solution to the binding problem by using representations that encode combinations of input features (i.e., color and shape), but achieve greater efficiency by representing multiple such combinations. Obj1 and obj2 show the features of the two objects (R = Red, G = Green, B = Blue, S = Square, C = Circle, T = Triangle), and remaining columns show 6 localist units and one coarse- coded conjunctive unit. Adding this one conjunctive unit is enough to disambiguate the inputs. conjunctions, they will not fire to a red square or a blue triangle, and thereby avoid the binding problem. The obvious problem with this solution, and one reason it has been largely rejected in the literature, is that it would appear to require far too many units to cover all of the possible conjunctions that need to be represented --- a combinatorial explosion. However, the combinatorial explosion problem is predicated on another seductive notion --- that separate units are used for each possible conjunction. In short, both the binding prob- lem itself and the problem with the conjunctive solution derive from localist assumptions about neural coding. In contrast, these problems can be greatly reduced by simply thinking in terms of distributed representations, where each unit encodes some possibly-difficult to describe amalgam of input features, such that individual units are active at different levels for different inputs, and many such units are active for each input (Hinton, McClelland, & Rumelhart, 1986). Therefore, the input is represented by a complex distributed pattern of activation over units, and each unit can exhibit varying levels of sensitivity to the featural conjunctions present in the input. The binding problem is largely avoided because a differ- ent pattern of activation will be present for a red-triangle, blue-square input as compared to a red-square, blue-triangle input. These kinds of distributed representations can be difficult to understand. This is probably a significant reason why the ability of distributed representations to resolve the binding problem goes under-appreciated. However, we can analyze special cases of these repre- sentations to gain some insight. One such special case is shown in Table 1 from O'Reilly and Munakata (2000). Here, we add one additional distributed unit to an otherwise localist
featural encoding like that shown in Figure 1. This unit has a coarse-coded conjunctive representation, meaning that instead of coding for a single conjunction, it codes for several possible conjunctions. The table shows that if this set of conjunctions is chosen wisely, this single unit can enable the distributed pattern of activation across all units to distinguish between any two possible combinations of stimulus inputs. A more realistic system will have a larger number of partially redundant coarse-coded conjunctive units that will not require such precise representations from each unit. A similar demonstration was recently provided by Mel and Fiser (2000) in an analysis of distributed, low-order conjunctive rep- resentations (resembling ``Wickelfeatures''; Wickelgren, 1969; Seidenberg & McClelland, 1989) in the domain of textual inputs. However, they did not demonstrate that a neural net- work learning mechanism would develop these representations, or that they could support systematic generalization to novel inputs. 2 Learning Generalizable Relational Bindings
We present here a series of models that test the ability of existing neural network learning mechanisms to develop low-order coarse-coded conjunctive representations in a challeng- ing binding domain. Specifically, we focus on the problem of relational binding, which provides a link to higher-level cognitive function, and speaks to the continued use of struc- tured representations in these domains. Furthermore, we conduct a critical test of these models in assessing their ability to generalize to novel inputs after moderate amounts of training. This is important because conjunctive representations might appear to limit gen- eralization as these representations are more specific than purely localist representations. Indeed the inability to generalize is considered by some the primary limitation of conjunc- tive binding mechanisms (Holyoak & Hummel, 2000).
2.1 Relational Binding, Structured Representations, and Higher-level Cognition
A number of existing models rely on structured representations because they are regarded as essential for encoding complex relational information and other kinds of data structures that are used in symbolic models (e.g., lists, trees, sequences) (e.g., Touretzky, 1986; Shas- tri & Ajjanagadde, 1993; Hummel & Holyoak, 1997). A canonical example of a structured representation is a propositional encoding (e.g., LIKES cats milk) that has a main relational term (LIKES) that operates on a set of slot-like arguments that specify the items enter- ing into the relationship. The primary advantages of such a representation are that it is transparently systematic or productive (anything can be put in the slots), and it is typically easy to compose more elaborate structures from these individual propositions (e.g., this proposition can have other propositions in its slots instead of just basic symbols). The fundamental problem with structured representations, regardless of what implements them, is that they cannot be easily learned. To date, there have been no structured rep- resentation models that exhibit powerful learning of the form typically associated with neural networks. There are good reasons to believe that this reflects basic tradeoffs between complex structured representations and powerful learning mechanisms (Elman, 1991; St John & McClelland, 1990; O'Reilly & Munakata, 2000). Essentially, structured representations are discrete and fragile, and therefore do not admit to gradual changes over learning. In contrast, neural networks employ massively-parallel, graded processing that can search out many possible solutions at the same time, and optimize those that seem to make graded improvements in performance. In contrast, the discrete character of structured representations requires exhaustive combinatorial search in high-dimensional spaces. To provide an alternative to these structured representations, our models test a simple ex- ample of relational encoding, focusing on easily-visualized spatial relationships, which can be thought of in propositional terms as for example (LEFT-OF square triangle).
Input Location Question Hidden where? what? relation-obj? relation-loc? Relation Object right left above below Figure 2: Spatial relationship binding model. Objects are represented by distributed patterns of activation over 8 features per location within a 4x4 array of locations. Inputs have two objects, arranged vertically or horizontally. The network answers questions posed by the Question input (what'',where'', and ``what relationship?'') --- the answers require binding of object, location, and relationship information. 3 Spatial Relationship Binding Model
The spatial relationship binding model is shown in Figure 2. The overall framework for training the network is to present it with input patterns containing objects in different loca- tions, and ask it various questions about these input displays. These questions ask about the identity and location of objects (i.e., what?'' andwhere?''), and the relationships between the two objects (e.g., where is object1 relative to object2?''). To answer these questions correctly, the hidden layer must bind object, location, and relationship information accu- rately in the hidden layer. Otherwise, it will confuse the two objects and their locations and relationships. Furthermore, we encoded the objects using distributed representations over features, so these features must be correctly bound into the same object. Specifically, objects are represented by distributed patterns of activation over 8 features per location, in a 4x4 location array. Inputs have two different objects, arranged either vertically or horizontally. The network answers different questions about the objects posed by the Question input. For thewhat?'' question, the location of one of the objects is activated as an input in the Location layer, and the network must produce the correct object features for the object in that location. We also refer to this target object as the agent object. For the where?'' question, the object features for the agent object are activated in the Object layer, and the network must produce the correct location activation for that object. For therelation-obj?'' question, the object features for the agent object are activated, and the network must activate the relationship between this object and the other object (referred to as the patient object), in addition to activating the location for the agent object. For the ``relation-loc?'' question, the location of the agent object is activated, and the network must activate the relationship between this object and the patient object, in addition to activating the object features for the agent object. This network architecture has a number of nice properties. For example, it has only one object and location encoding layer, both of which can act as either an input or an output. This is better than an alternative architecture having separate slots representing the agent and patient objects, because such slot-based encodings solve the binding problem by having separate role-specific units, which becomes implausible as the number of different roles and objects multiply. Note that supporting the dual input/output roles requires an interactive (recurrent, bidirectionally-connected) network (O'Reilly, 2001, 1998).
a) b) Input Location Question Hidden Object Relation R L A B What? Where? Rel-Obj? Rel-Loc? Input Location Question Hidden Object Relation R L A B What? Where? Rel-Obj? Rel-Loc? c) d) Input Location Question Hidden Object Relation R L A B What? Where? Rel-Obj? Rel-Loc? Input Location Question Hidden Object Relation R L A B What? Where? Rel-Obj? Rel-Loc? Figure 3: Hidden unit representations (values are weights into a hidden unit from all other layers) showing units (a & b) that bind object, location, & relationship information via low-order conjunc- tions, and other units that have systematic representations of location (c) and object features (d). There are four levels of questions we can ask about this network. First, we can ask if stan- dard neural network learning mechanisms are capable of solving this challenging binding problem. They are. Second, we can ask whether the network actually develops coarse- coded distributed representations. It does. Third, we can ask if these networks can gener- alize to novel inputs (both novel objects and novel locations for existing objects). They can. Finally, we can ask whether there are differences in how well different kinds of learning algorithms generalize, specifically comparing the Leabra algorithm with purely error-driven networks, as was recently done in other generalization tests with interactive networks (O'Reilly, 2001). This paper showed that interactive networks generalize signif- icantly worse than comparable feedforward networks, but that good generalization can be achieved by adding additional biases or constraints on the learning mechanisms in the form of inhibitory competition and Hebbian learning in the Leabra algorithm. These results are replicated here, with Leabra generalization being roughly twice as good as other interactive algorithms.
0 10 20 30 40 No. of Patients Per Agent, Location 0.0 0.2 0.4 0.6 0.8 1.0 Generalization Error Spat Rel Generalization (Fam Objs) 200 Agent, Locs 300 Agent, Locs 400 Agent, Locs a) 0 10 20 30 40 No. of Patients Per Agent, Location 0.0 0.2 0.4 0.6 0.8 1.0 Error Spat Rel Generalization (Nov Objs) 200 Agent, Locs 300 Agent, Locs 400 Agent, Locs b) Figure 4: Generalization results (proportion errors on testing set) for the spatial relationship binding model using the Leabra algorithm as a function of the number of training items, specified as number of agent, location combinations and number of patient, locations per each agent, location. a) shows results for testing on familiar objects in novel locations. b) shows results for testing on novel objects that were never trained before. 3.1 Detailed Results
First, we examined the representations that developed in the network's hidden layer (Fig- ure 3). Many units encoded low-order combinations (conjunctions) of object, location, and relationship features (Figure 3a & b). This is consistent with our hypothesis. Other units also encoded more systematic representations of location without respect to objects (Figure 3c) and object features without respect to location (Figure 3d). To test the generalization capacity of the networks, we trained on only 26 of the 28 possible objects that can be composed out of 8 features with two units active, and only a subset of all 416 possible agent object x location combinations. We trained on 200, 300, and 400 such combinations. For each agent object-location input, there are 150 different patient object-location combinations per agent object-location, and we trained on 4, 10, 20, and 40, selected at random, for each different level of agent object-location combination training. At the most (400x40) there were a total of 16000 unique inputs trained out of a total possible of 62400, which amounts to about 1/4 of the training space. At the least (200x4) only roughly 1.3% of the training space was covered. The ability of the network to generalize to the 26 familiar objects in novel locations was tested by measuring performance on a random sample of 640 of the untrained agent object- location combinations. The results for the Leabra algorithm are shown in Figure 4a. As one would expect, the number of training patterns improves generalization in a roughly propor- tional manner. Importantly, the network is able to generalize to a high level of performance, getting roughly 95% correct after training on only 25% of the training space (400x40), and achieving roughly 80% correct after training on only roughly 10% of the space (300x20). The ability of the network to generalize to novel objects was tested by simply presenting the two novel objects as agents in all possible locations, with a random sampling of 20 different patients (which were the familiar objects), for a total of 640 different testing items (Figure 4b). Generalization on these novel objects was roughly comparable to the famil- iar objects, except there was an apparent ceiling point at roughly 15% generalization error where the generalization did not improve even with more training. Overall, the network performed remarkably well on these novel objects, and future work will explore general- ization with fewer training objects. To evaluate the extent to which the additional biologically-motivated biases in the Leabra algorithm are contributing to these generalization results, we ran networks using the con- trastive Hebbian learning algorithm (CHL) and the Almeida-Pineda (AP) recurrent back-
10 20 No. of Patients Per Agent, Location 0.00 0.10 0.20 0.30 0.40 Generalization Error Spat Rel Generalization (Fam Objs) Leabra CHL Figure 5: Generalization results for different algorithms on the spatial relationship binding task (see previous figure for details on measures) in the 400 x 10 or 20 conditions. propagation algorithm, as in O'Reilly (2001). Both of these algorithms work in interactive, bidirectionally-connected networks, which are required for this task. Standard AP was unable to learn the task, we suspected because it does not preserve the symmetry of the weights as is required for stable settling. Attempts to to rectify this problem by enforc- ing symmetric weight changes did not succeed either. The results for CHL (Figure 5) replicated earlier results (O'Reilly, 2001) in showing that the additional biases in Leabra produced roughly twice as good of generalization performance compared to CHL. 4 Discussion
These networks demonstrate that existing, powerful neural network learning algorithms can learn representations that perform complex relational binding of information. Specifically, these networks had to bind together object identity, location, and relationship informa- tion to answer a number of questions about input displays containing two objects. This supports our contention that rich distributed representations containing coarse-coded con- junctive encodings can effectively perform binding. It is critical to appreciate that these distributed representations are highly efficient, encoding over 62400 unique input config- urations with only 200 hidden units. Furthermore, these representations are systematic,
in that they support generalization to novel inputs after training on a fraction of the input space. Despite these initial successes, more work needs to be done to extend this approach to other kinds of domains that require binding. One early example of such an application is the St John and McClelland (1990) sentence gestalt model, which was able to sequen- tially process words in a sentence and construct a distributed internal representation of the meaning of the sentence (the sentence gestalt). This model was limited in that it required extremely large numbers of training trials and an elaborate training control mechanism. However, these limitations were eliminated in a recent replication of this model based on the Leabra algorithm (O'Reilly & Munakata, 2000). We plan to extend this model to handle a more complex corpus of sentences to more fully push the relational binding capacities of the model. Finally, it is important to emphasize that we do not think that these low-order conjunctive representations are entirely sufficient to resolve the binding problems that arise in the cor- tex. One important additional mechanism is the use of selective attention to focus neural processing on coherent subsets of information present in the input (e.g., on individual ob- jects, people, or conversations). The interaction between such a selective attentional system and a complex object recognition system was modeled in O'Reilly and Munakata (2000). In this model, selective attention was an emergent process deriving from excitatory interac-
tions between a spatial processing pathway and the object processing pathway, combined with surround inhibition as implemented by inhibitory interneurons. The resulting model was capable of sequentially processing individual objects when multiple such objects were simultaneously present in the input."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/5227b6aaf294f5f027273aebf16015f2-Abstract.html,On the Concentration of Spectral Properties,"John Shawe-Taylor, Nello Cristianini, Jaz S. Kandola","We  consider  the  problem  of  measuring  the  eigenvalues  of a  ran(cid:173) domly drawn sample of points.  We  show  that these  values  can be  reliably  estimated  as  can the  sum  of the  tail  of eigenvalues.  Fur(cid:173) thermore,  the  residuals  when  data is  projected  into a  subspace  is  shown to be reliably estimated on  a  random sample.  Experiments  are presented that confirm the theoretical results."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/52dbb0686f8bd0c0c757acf716e28ec0-Abstract.html,Small-World Phenomena and the Dynamics of Information,Jon M. Kleinberg,"The problem of searching for information in networks like the World Wide Web can
be approached in a variety of ways, ranging from centralized indexing schemes to
decentralized mechanisms that navigate the underlying network without knowledge
of its global structure. The decentralized approach appears in a variety of settings:
in the behavior of users browsing the Web by following hyperlinks; in the design of
focused crawlers [4, 5, 8] and other agents that explore the Web's links to gather
information; and in the search protocols underlying decentralized peer-to-peer sys-
tems such as Gnutella [10], Freenet [7], and recent research prototypes [21, 22, 23],
through which users can share resources without a central server."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/5352696a9ca3397beb79f116f3a33991-Abstract.html,Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines,"Roman Genov, Gert Cauwenberghs","A mixed-signal paradigm is presented for high-resolution parallel inner- product computation in very high dimensions, suitable for efÔ¨Åcient im- plementation of kernels in image processing. At the core of the externally digital architecture is a high-density, low-power analog array performing binary-binary partial matrix-vector multiplication. Full digital resolution is maintained even with low-resolution analog-to-digital conversion, ow- ing to random statistics in the analog summation of binary products. A random modulation scheme produces near-Bernoulli statistics even for highly correlated inputs. The approach is validated with real image data, and with experimental results from a CID/DRAM analog array prototype in 0.5"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/54ff9e9e3a2ec0300d4ce11261f5169f-Abstract.html,The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay,"Michael Kositsky, Andrew G. Barto","Tangential hand velocity proÔ¨Åles of rapid human arm movements of- ten appear as sequences of several bell-shaped acceleration-deceleration phases called submovements or movement units. This suggests how the nervous system might efÔ¨Åciently control a motor plant in the presence of noise and feedback delay. Another critical observation is that stochastic- ity in a motor control problem makes the optimal control policy essen- tially different from the optimal control policy for the deterministic case. We use a simpliÔ¨Åed dynamic model of an arm and address rapid aimed arm movements. We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. Using a simpliÔ¨Åed model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay. The optimal policy in this situation is to drive the arm‚Äôs end point close to the target by one fast submovement and then apply a few slow submovements to accu- rately drive the arm‚Äôs end point into the target region. In our simulations, the controller sometimes generates corrective submovements before the initial fast submovement is completed, much like the predictive correc- tions observed in a number of psychophysical experiments."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/5531a5834816222280f20d1ef9e95f69-Abstract.html,Pranking with Ranking,"Koby Crammer, Yoram Singer","We  discuss  the  problem  of ranking  instances.  In  our  framework  each  instance  is  associated  with  a  rank  or  a  rating,  which  is  an  integer from  1 to k.  Our goal is  to find  a  rank-prediction rule that  assigns  each  instance  a  rank  which  is  as  close  as  possible  to  the  instance's true rank.  We  describe  a  simple  and efficient  online  al(cid:173) gorithm, analyze its performance in the mistake bound model, and  prove  its  correctness.  We  describe  two  sets  of experiments,  with  synthetic  data and  with  the  EachMovie  dataset  for  collaborative  filtering.  In  the experiments we  performed,  our algorithm outper(cid:173) forms  online algorithms for  regression and classification applied to  ranking."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/584b98aac2dddf59ee2cf19ca4ccb75e-Abstract.html,Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning,"Evan Greensmith, Peter L. Bartlett, Jonathan Baxter","We consider the use of two additive control variate methods to reduce the variance of performance gradient estimates in reinforcement learn- ing problems. The Ô¨Årst approach we consider is the baseline method, in which a function of the current state is added to the discounted value estimate. We relate the performance of these methods, which use sam- ple paths, to the variance of estimates based on iid data. We derive the baseline function that minimizes this variance, and we show that the vari- ance for any baseline is the sum of the optimal variance and a weighted squared distance to the optimal baseline. We show that the widely used average discounted value baseline (where the reward is replaced by the difference between the reward and its expectation) is suboptimal. The second approach we consider is the actor-critic method, which uses an approximate value function. We give bounds on the expected squared error of its estimates. We show that minimizing distance to the true value function is suboptimal in general; we provide an example for which the true value function gives an estimate with positive variance, but the op- timal value function gives an unbiased estimate with zero variance. Our bounds suggest algorithms to estimate the gradient of the performance of parameterized baseline or value functions. We present preliminary exper- iments that illustrate the performance improvements on a simple control problem.
1 Introduction, Background, and Preliminary Results
In reinforcement learning problems, the aim is to select a controller that will maximize the average reward in some environment. We model the environment as a partially ob- servable Markov decision process (POMDP). Gradient ascent methods (e.g., [7, 12, 15]) estimate the gradient of the average reward, usually using Monte Carlo techniques to cal-
(cid:3)Most of this work was performed while the authors were with the Research School of Information
Sciences and Engineering at the Australian National University.
culate an average over a sample path of the controlled POMDP. However such estimates tend to have a high variance, which means many steps are needed to obtain a good esti- mate. GPOMDP [4] is an algorithm for generating an estimate of the gradient in this way. Compared with other approaches, it is suitable for large systems, when the time between visits to a state is large but the mixing time of the controlled POMDP is short. However, it can suffer from the problem of producing high variance estimates. In this paper, we investi- gate techniques for variance reduction in GPOMDP. One generic approach to reducing the variance of Monte Carlo estimates of integrals is to use an additive control variate (see, for example, [6]). Suppose we wish to estimate the integral of f : X ! R, and we know the
integral of another function ‚Äô : X ! R. Since RX f = RX (f (cid:0) ‚Äô) +RX ‚Äô, the integral
of f (cid:0) ‚Äô can be estimated instead. Obviously if ‚Äô = f then the variance is zero. More generally, Var(f (cid:0) ‚Äô) = Var(f ) (cid:0) 2Cov(f; ‚Äô) + Var(‚Äô), so that if (cid:30) and f are strongly correlated, the variance of the estimate is reduced.
In this paper, we consider two approaches of this form. The Ô¨Årst (Section 2) is the technique of adding a baseline. We Ô¨Ånd the optimal baseline and we show that the additional variance of a suboptimal baseline can be expressed as a weighted squared distance from the optimal baseline. Constant baselines, which do not depend on the state or observations, have been widely used [13, 15, 9, 11]. In particular, the expectation over all states of the discounted value of the state is a popular constant baseline (where, for example, the reward at each step is replaced by the difference between the reward and the expected reward). We give bounds on the estimation variance that show that, perhaps surprisingly, this may not be the best choice.
The second approach (Section 3) is the use of an approximate value function. Such actor- critic methods have been investigated extensively [3, 1, 14, 10]. Generally the idea is to minimize some notion of distance between the Ô¨Åxed value function and the true value function. In this paper we show that this may not be the best approach: selecting the Ô¨Åxed value function to be equal to the true value function is not always the best choice. Even more surprisingly, we give an example for which the use of a Ô¨Åxed value function that is different from the true value function reduces the variance to zero, for no increase in bias. We give a bound on the expected squared error (that is, including the estimation variance) of the gradient estimate produced with a Ô¨Åxed value function. Our results suggest new algorithms to learn the optimum baseline, and to learn a Ô¨Åxed value function that minimizes the bound on the error of the estimate. In Section 5, we describe the results of preliminary experiments, which show that these algorithms give performance improvements.
POMDP with Reactive, Parameterized Policy
A partially observable Markov decision process (POMDP) consists of a state space, S, a control space, U, an observation space, Y, a set of transition probability matrices fP(u) : u 2 Ug, each with components pij (u) for i; j 2 S; u 2 U, an observation pro- cess (cid:23) : S ! PY , where PY is the space of probability distributions over Y, and a reward function r : S ! R. We assume that S; U; Y are Ô¨Ånite, although all our re- sults extend easily to inÔ¨Ånite U and Y, and with more restrictive assumptions can be extended to inÔ¨Ånite S. A reactive, parameterized policy for a POMDP is a set of map- pings f(cid:22)((cid:1); (cid:18)) : Y ! PU j(cid:18) 2 RKg. Together with the POMDP, this deÔ¨Ånes the con- trolled POMDP (S; U; Y; P ; (cid:23); r; (cid:22)). The joint state, observation and control process, fXt; Yt; Utg, is Markov. The state process, fXtg, is also Markov, with transition prob-
abilities pij ((cid:18)) = Py2Y;u2U (cid:23)y(i)(cid:22)u(y; (cid:18))pij (u), where (cid:23)y(i) denotes the probability of
observation y given the state i, and (cid:22)u(y; (cid:18)) denotes the probability of action u given pa- rameters (cid:18) and observation y. The Markov chain M((cid:18)) = (S; P((cid:18))) then describes the behaviour of the process fXtg.
Assumption 1 The controlled POMDP (S; U; Y; P ; (cid:23); r; (cid:22)) satisÔ¨Åes: For all (cid:18) 2 RK there exists a unique stationary distribution satisfying (cid:25) 0((cid:18)) P((cid:18)) = (cid:25)0((cid:18)). There is an R < 1 such that, for all i 2 S, jr(i)j (cid:20) R. There is a B < 1 such that, for all u 2 U, y 2 Y and (cid:18) 2 RK the deriva- tives @(cid:22)u(y; (cid:18))=@(cid:18)k (1 (cid:20) k (cid:20) K) exist, and the vector of these derivatives satisÔ¨Åes kr(cid:22)u(y; (cid:18))=(cid:22)u(y; (cid:18))k (cid:20) B, where k (cid:1) k denotes the Euclidean norm on RK.
implies that this limit exists, and does not depend on the start state X0. The aim is to select a policy to maximize this quantity. DeÔ¨Åne the discounted value function, J(cid:12)(i; (cid:18)) def=
We consider the average reward, (cid:17)((cid:18)) def= limT !1 Eh 1 t=0 r(Xt)i. Assumption 1 X0 = i i. Throughout the rest of the paper, dependences limT !1 EhPT (cid:0)1 Var(A) = Eh(A (cid:0) E [A])2i, where a2 denotes a0a, and a0 denotes the transpose of the
upon (cid:18) are assumed, and dropped in the notation. For a random vector A, we denote
t=0 (cid:12)tr(Xt)(cid:12)(cid:12)(cid:12)"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/5a1e3a5aede16d438c38862cac1a78db-Abstract.html,Asymptotic Universality for Learning Curves of Support Vector Machines,"Manfred Opper, Robert Urbanczik","Using  methods  of  Statistical  Physics,  we  investigate  the  rOle  of  model  complexity  in  learning  with  support  vector  machines  (SVMs).  We  show  the  advantages  of  using  SVMs  with  kernels  of infinite  complexity  on  noisy  target  rules,  which,  in  contrast  to  common  theoretical  beliefs,  are found  to  achieve  optimal  general(cid:173) ization error although the training error  does  not  converge to the  generalization error.  Moreover,  we  find  a  universal  asymptotics of  the  learning curves  which  only  depend  on the target  rule  but not  on the  SVM  kernel."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/5a7f963e5e0504740c3a6b10bb6d4fa5-Abstract.html,Active Information Retrieval,"Tommi Jaakkola, Hava T. Siegelmann","In classical large information retrieval systems, the system responds  to a  user initiated query with a  list of results ranked by relevance.  The  users  may further  refine  their  query  as  needed.  This  process  may  result  in  a  lengthy  correspondence  without  conclusion.  We  propose  an  alternative  active  learning  approach,  where  the  sys(cid:173) tem responds to the initial user's query by successively probing the  user for  distinctions at multiple levels of abstraction.  The system's  initiated  queries  are  optimized  for  speedy  recovery  and  the  user  is  permitted to respond with multiple selections  or may reject the  query.  The information is in each case unambiguously incorporated  by the system and the subsequent queries are adjusted to minimize  the  need  for  further  exchange.  The system's  initiated  queries  are  subject  to resource  constraints pertaining to the  amount of infor(cid:173) mation that can be presented  to the user  per iteration."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/5ec829debe54b19a5f78d9a65b900a39-Abstract.html,Tempo tracking and rhythm quantization by sequential Monte Carlo,"Ali Taylan Cemgil, Bert Kappen","We present a probabilistic generative model for timing deviations in expressive music. performance. The structure of the proposed model is equivalent to a switching state space model. We formu(cid:173) late two well known music recognition problems, namely tempo tracking and automatic transcription (rhythm quantization) as fil(cid:173) tering and maximum a posteriori (MAP) state estimation tasks. The inferences are carried out using sequential Monte Carlo in(cid:173) tegration (particle filtering) techniques. For this purpose, we have derived a novel Viterbi algorithm for Rao-Blackwellized particle fil(cid:173) ters, where a subset of the hidden variables is integrated out. The resulting model is suitable for realtime tempo tracking and tran(cid:173) scription and hence useful in a number of music applications such as adaptive automatic accompaniment and score typesetting."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/6351bf9dce654515bf1ddbd6426dfa97-Abstract.html,Learning a Gaussian Process Prior for Automatically Generating Music Playlists,"John C. Platt, Christopher J. C. Burges, Steven Swenson, Christopher Weare, Alice Zheng","This paper presents AutoDJ: a system for automatically generating mu- sic playlists based on one or more seed songs selected by a user. AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. For playlist generation, AutoDJ learns a kernel from a large set of albums. This learned kernel is shown to be more effective at predicting users‚Äô playlists than a reasonable hand-designed kernel."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/6492d38d732122c58b44e3fdc3e9e9f3-Abstract.html,Exact differential equation population dynamics for integrate-and-fire neurons,"Julian Eggert, Berthold B√§uml","In  our  previous  work,  integral  equation  formulations  for 
Mesoscopical,  mathematical  descriptions  of  dynamics  of  popula(cid:173) tions of spiking neurons  are getting increasingly important for  the  understanding  of large-scale  processes  in  the  brain  using  simula(cid:173) tions.  population dynamics  have been derived for  a  special  type of spik(cid:173) ing neurons.  For  Integrate- and- Fire type neurons, these formula(cid:173) tions  were  only  approximately  correct.  Here,  we  derive  a  math(cid:173) ematically  compact,  exact  population  dynamics  formulation  for  Integrate- and- Fire type  neurons.  It can  be  shown  quantitatively  in  simulations that the numerical  correspondence with  microscop(cid:173) ically modeled neuronal populations is  excellent. 
1 
Introduction and  motivation 
The goal of the population dynamics approach is to model the time course of the col(cid:173) lective activity of entire populations of functionally and dynamically similar neurons  in a compact way,  using a  higher descriptionallevel than that of single neurons and  spikes.  The usual observable at the level of neuronal populations is  the population(cid:173) averaged instantaneous firing  rate  A(t),  with  A(t)6.t being the number of neurons  in the population that release a spike in an interval [t, t+6.t).  Population dynamics  are formulated  in  such  a  way, that they match quantitatively the time  course  of a  given  A(t), either gained experimentally or by microscopical, detailed simulation. 
At  least  three  main  reasons  can  be  formulated  which  underline  the  importance  of  the  population  dynamics  approach  for  computational  neuroscience.  First,  it  enables the simulation of extensive networks involving a massive number of neurons 
and connections, which is  typically the case when dealing with biologically realistic  functional  models  that go  beyond  the single neuron level.  Second,  it increases the  analytical understanding of large-scale neuronal dynamics, opening the way towards  better control and predictive capabilities when dealing with large networks.  Third,  it enables  a  systematic  embedding  of the  numerous  neuronal  models  operating at  different  descriptional scales into a  generalized theoretic framework, explaining the  relationships,  dependencies  and derivations of the respective models. 
Early efforts on population dynamics approaches date back as early as 1972, to the  work of Wilson  and  Cowan  [8]  and  Knight  [4],  which  laid  the  basis  for  all  current  population-averaged graded-response  models  (see  e.g.  [6]  for  modeling  work  using  these models).  More recently, population-based approaches for spiking neurons were  developed,  mainly by Gerstner [3, 2]  and Knight  [5].  In our own previous work  [1],  we  have  developed  a  theoretical framework  which enables to systematize and sim(cid:173) ulate  a  wide  range  of models  for  population-based  dynamics.  It  was  shown  that  the equations of the framework produce results that agree quantitatively well  with  detailed  simulations  using  spiking  neurons,  so  that  they  can  be  used  for  realistic  simulations  involving  networks  with  large  numbers  of spiking  neurons.  Neverthe(cid:173) less,  for  neuronal populations composed of Integrate-and-Fire (I&F)  neurons, this  framework was only correct in an approximation.  In this paper, we  derive the exact  population  dynamics  formulation  for  I&F  neurons.  This  is  achieved  by  reducing  the  I&F  population  dynamics  to  a  point  process  and  by  taking  advantage  of the  particular properties of I&F neurons. 
2  Background:  Integrate-and-Fire dynamics 
2.1  Differential form 
We  start  with  the  standard Integrate- and- Fire  (I&F)  model  in  form  of the  well(cid:173) known differential equation [7] 
(1) 
which  describes  the  dynamics  of the  membrane  potential  Vi  of a  neuron  i  that  is  modeled  as  a  single  compartment  with  RC  circuit  characteristics.  The  membrane  relaxation time is in this case T  =  RC with R being the membrane resistance and C  the  membrane  capacitance.  The resting  potential  v R est  is  the  stationary potential  that is  approached in  the no-input  case.  The input arriving from  other neurons  is  described in form  of a  current ji. 
In  addition  to  eq.  (1),  which  describes  the  integrate  part  of the  I&F  model,  the  neuronal  dynamics  are  completed  by  a  nonlinear  step.  Every  time  the  membrane  potential Vi  reaches a fixed  threshold ()  from  below,  Vi  is  lowered by a fixed  amount  Ll  > 0,  and from the new value of the membrane potential integration according to  eq.  (1)  starts again. 
if Vi(t)  =  ()  (from  below)  . 
(2) 
At  the  same  time,  it  is  said  that  the  release  of a  spike  occurred  (i.e.,  the  neuron  fired),  and  the  time  ti  =  t  of this  singular  event  is  stored.  Here  ti  indicates  the  time of the  most recent spike.  Storing all the last firing  times, we  gain the sequence  of spikes  {t{}  (spike ordering index  j, neuronal index i). 
2.2"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/654ad60ebd1ae29cedc37da04b6b0672-Abstract.html,Improvisation and Learning,Judy A Franklin,"This article presents a 2-phase computational learning model and appli- cation. As a demonstration, a system has been built, called CHIME for Computer Human Interacting Musical Entity. In phase 1 of training, re- current back-propagationtrains the machine to reproduce 3 jazz melodies. The recurrent network is expanded and is further trained in phase 2 with a reinforcement learning algorithm and a critique produced by a set of basic rules for jazz improvisation. After each phase CHIME can interactively improvise with a human in real time."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/65d2ea03425887a717c435081cfc5dbb-Abstract.html,Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons,"Shun-ichi Amari, Hyeyoung Park, Tomoko Ozeki","Singularities are ubiquitous  in the parameter space of hierarchical  models such as multilayer perceptrons.  At singularities, the Fisher  information  matrix  degenerates,  and  the  Cramer-Rao  paradigm  does no more hold, implying that the classical model selection the(cid:173) ory  such  as  AIC  and  MDL  cannot  be  applied.  It is  important  to  study the relation between the generalization error and the training  error  at  singularities.  The  present  paper  demonstrates  a  method  of analyzing these errors both for  the maximum likelihood estima(cid:173) tor and the  Bayesian  predictive distribution  in  terms  of Gaussian  random fields,  by using simple models."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/6a508a60aa3bf9510ea6acb021c94b48-Abstract.html,A Sequence Kernel and its Application to Speaker Recognition,William M. Campbell,"A novel approach for comparing sequences of observations using an explicit-expansion kernel is demonstrated. The kernel is derived using the assumption of the independence of the sequence of observations and a mean-squared error training criterion. The use of an explicit expan- sion kernel reduces classiÔ¨Åer model size and computation dramatically, resulting in model sizes and computation one-hundred times smaller in our application. The explicit expansion also preserves the computational advantages of an earlier architecture based on mean-squared error train- ing. Training using standard support vector machine methodology gives accuracy that signiÔ¨Åcantly exceeds the performance of state-of-the-art mean-squared error training for a speaker recognition task."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/6f2688a5fce7d48c8d19762b88c32c3b-Abstract.html,Convergence of Optimistic and Incremental Q-Learning,"Eyal Even-dar, Yishay Mansour","Vie sho,v the convergence of tV/O deterministic variants of Q(cid:173) learning. The first is the widely used optimistic Q-learning, which initializes the Q-values to large initial values and then follows a greedy policy with respect to the Q-values. We show that setting the initial value sufficiently large guarantees the converges to an E(cid:173) optimal policy. The second is a new and novel algorithm incremen(cid:173) tal Q-learning, which gradually promotes the values of actions that are not taken. We show that incremental Q-learning converges, in the limit, to the optimal policy. Our incremental Q-learning algo(cid:173) rithm can be viewed as derandomization of the E-greedy Q-learning."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/6f4920ea25403ec77bee9efce43ea25e-Abstract.html,Rao-Blackwellised Particle Filtering via Data Augmentation,"Christophe Andrieu, Nando D. Freitas, Arnaud Doucet","EE Engineering 
University of Melbourne  Parkville,  Victoria 3052"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/71e09b16e21f7b6919bbfc43f6a5b2f0-Abstract.html,Boosting and Maximum Likelihood for Exponential Models,"Guy Lebanon, John D. Lafferty","We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between mini- mizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normal- ized to form a conditional probability distribution over labels. In addi- tion to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood. Experiments on UCI datasets support our theoretical analy- sis and give additional insight into the relationship between boosting and logistic regression."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/798cebccb32617ad94123450fd137104-Abstract.html,Iterative Double Clustering for Unsupervised and Semi-Supervised Learning,"Ran El-Yaniv, Oren Souroujon","We present a powerful meta-clustering technique called Iterative Dou- ble Clustering (IDC). The IDC method is a natural extension of the recent Double Clustering (DC) method of Slonim and Tishby that ex- hibited impressive performance on text categorization tasks [12]. Us- ing synthetically generated data we empirically Ô¨Çnd that whenever the DC procedure is successful in recovering some of the structure hidden in the data, the extended IDC procedure can incrementally compute a signiÔ¨Çcantly more accurate classiÔ¨Çcation. IDC is especially advan- tageous when the data exhibits high attribute noise. Our simulation results also show the eÔ¨Åectiveness of IDC in text categorization prob- lems. Surprisingly, this unsupervised procedure can be competitive with a (supervised) SVM trained with a small training set. Finally, we propose a simple and natural extension of IDC for semi-supervised and transductive learning where we are given both labeled and unla- beled examples."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/7af6266cc52234b5aa339b16695f7fc4-Abstract.html,Multiagent Planning with Factored MDPs,"Carlos Guestrin, Daphne Koller, Ronald Parr","We present a principled and efÔ¨Åcient planning algorithm for cooperative multia- gent dynamic systems. A striking feature of our method is that the coordination and communication between the agents is not imposed, but derived directly from the system dynamics and function approximation architecture. We view the en- tire multiagent system as a single, large Markov decision process (MDP), which we assume can be represented in a factored way using a dynamic Bayesian net- work (DBN). The action space of the resulting MDP is the joint action space of the entire set of agents. Our approach is based on the use of factored linear value functions as an approximation to the joint value function. This factorization of the value function allows the agents to coordinate their actions at runtime using a natural message passing scheme. We provide a simple and efÔ¨Åcient method for computing such an approximate value function by solving a single linear pro- gram, whose size is determined by the interaction between the value function structure and the DBN. We thereby avoid the exponential blowup in the state and action space. We show that our approach compares favorably with approaches based on reward sharing. We also show that our algorithm is an efÔ¨Åcient alterna- tive to more complicated algorithms even in the single agent case."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/7b1ce3d73b70f1a7246e7b76a35fb552-Abstract.html,"ACh, Uncertainty, and Cortical Inference","Peter Dayan, Angela J. Yu","Acetylcholine (ACh) has been implicated in a wide variety of tasks involving attentional processes and plasticity. Following extensive animal studies, it has previously been suggested that ACh reports on uncertainty and controls hippocampal, cortical and cortico-amygdalar plasticity. We extend this view and consider its effects on cortical representational inference, arguing that ACh controls the balance between bottom-up inference, in(cid:3)uenced by input stimuli, and top-down inference, in(cid:3)uenced by contextual information. We illustrate our proposal using a hierarchical hid- den Markov model."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/7b7a53e239400a13bd6be6c91c4f6c4e-Abstract.html,On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes,"Andrew Y. Ng, Michael I. Jordan","We  compare discriminative  and  generative learning as  typified  by  logistic regression and naive Bayes.  We show,  contrary to a widely(cid:173) held  belief  that  discriminative  classifiers  are  almost  always  to  be  preferred,  that  there  can  often  be  two  distinct  regimes  of  per(cid:173) formance  as  the  training  set  size  is  increased,  one  in  which  each  algorithm  does  better.  This  stems  from  the  observation- which  is  borne  out  in  repeated  experiments- that  while  discriminative  learning has lower asymptotic error, a generative classifier may also  approach its  (higher)  asymptotic error  much faster."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/7ca57a9f85a19a6e4b9a248c1daca185-Abstract.html,Risk Sensitive Particle Filters,"Sebastian Thrun, John Langford, Vandi Verma","We propose a new particle Ô¨Ålter that incorporates a model of costs when generating particles. The approach is motivated by the observation that the costs of accidentally not tracking hypotheses might be signiÔ¨Åcant in some areas of state space, and next to irrelevant in others. By incorporat- ing a cost model into particle Ô¨Åltering, states that are more critical to the system performance are more likely to be tracked. Automatic calculation of the cost model is implemented using an MDP value function calcula- tion that estimates the value of tracking a particular state. Experiments in two mobile robot domains illustrate the appropriateness of the approach."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/7d2b92b6726c241134dae6cd3fb8c182-Abstract.html,Characterizing Neural Gain Control using Spike-triggered Covariance,"Odelia Schwartz, E. J. Chichilnisky, Eero P. Simoncelli","Spike-triggered averaging techniques are effective for linear characterization of neural responses. But neurons exhibit important nonlinear behaviors, such as gain control, that are not captured by such analyses. We describe a spike-triggered covariance method for retrieving suppressive components of the gain control signal in a neuron. We demonstrate the method in simulation and on retinal ganglion cell data. Analysis of physiological data reveals significant suppressive axes and explains neural nonlinearities. This method should be applicable to other sensory areas and modalities."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/7f16109f1619fd7a733daf5a84c708c1-Abstract.html,Speech Recognition with Missing Data using Recurrent Neural Nets,"S. Parveen, P. Green","In the missing data' approach to improving the robustness of automatic speech recognition to added noise, an initial process identifies spectral- temporal regions which are dominated by the speech source. The remaining regions are considered to bemissing'. In this paper we develop a connectionist approach to the problem of adapting speech recognition to the missing data case, using Recurrent Neural Networks. In contrast to methods based on Hidden Markov Models, RNNs allow us to make use of long-term time constraints and to make the problems of classification with incomplete data and imputing missing values interact. We report encouraging results on an isolated digit recognition task. 1. Introduction
Automatic Speech Recognition systems perform reasonably well in controlled and matched training and recognition conditions. However, performance deteriorates when there is a mismatch between training and testing conditions, caused for instance by additive noise (Lippmann, 1997). Conventional techniques for improving recognition robustness (reviewed by Furui 1997) seek to eliminate or reduce the mismatch, for instance by enhancement of the noisy speech, by adapting statistical models for speech units to the noise condition or simply by training in different noise conditions. Missing data techniques provide an alternative solution for speech corrupted by additive noise which make minimal assumptions about the nature of the noise. They are based on identifying uncorrupted, reliable regions in the frequency domain and adapting recognition algorithms so that classification is based on these regions. Present missing data techniques developed at Sheffield (Barker et al. 2000a, Barker et al. 2000b, Cooke et al., 2001) and elsewhere (Drygaglo et al., 1998, Raj et al., 2000) adapt the prevailing technique for ASR based on Continuous Density Hidden Markov Models. CDHMMs are generative models which do not give direct estimates of posterior
probabilities of the classes given the acoustics. Neural Networks, unlike HMMs, are discriminative models which do give direct estimates of posterior probabilities and have been used with success in hybrid ANN/HMM speech recognition systems (Bourlard et al., 1998). In this paper, we adapt a recurrent neural network architecture introduced by (Gingras & Bengio, 1998) for robust ASR with missing data.

Missing data techniques for Robust ASR

2.1 Missing data masks
Speech recognition with missing data is based on the assumption that some regions in time/frequency remain uncorrupted for speech with added noise. See (Cooke et al., 2001) for arguments to support this assumption. Initial processes, based on local signal-to-noise estimates, on auditory grouping cues, or a combination (Barker et al., 2001) define a binary missing data mask': ones in the mask indicate reliable (orpresent') features and zeros indicate unreliable (or `missing') features.
2.2 Classification with missing data
Techniques for classification with incomplete data can be divided into imputation and marginalisation. Imputation is a technique in which missing features are replaced by estimated values to allow the recognition process proceed in normal way. If the missing values are replaced by either zeros, random values or their means based on training data, the approach is called unconditional imputation. On the other hand in conditional imputation conditional statistics are used to estimate the missing values given the present values. In the marginalisation approach missing values are ignored (by integrating over their possible ranges) and recognition is performed with the reduced data vector which is considered reliable. For the multivariate mixture Gaussian distributions used in CDHMMs, marginalisation and conditional imputation can be formulated analytically (Cooke et al., 2001). For missing data ASR further improvements in both techniques follow from using the knowledge that for spectral energy features the unreliable data is bounded between zero and the energy in speech+noise mixture (Vizinho et al., 1999), (Josifovski et al., 1999). These techniques are referred to as bounded marginalisation and
bounded imputation. Coupled with a `softening' of the reliable/unreliable decision, missing data techniques produce good results on a standard connected-digits-in-noise recognition task: performance using models trained on clean data is comparable, and in severe noise superior, to conventional systems trained across different noise conditions (Barker et al., 2001).
2.3 Why recurrent neural nets for missing data robust ASR?
Several neural net architectures have been proposed to deal with the missing data problem in general (Ahmed & Tresp, 1993), (Ghahramani & Jordan, 1994). The problem in using neural networks with missing data is to compute the output of a node/unit when some of its input values are unavailable. For marginalisation, this involves finding a way of integrating over the range of the missing values. A robust ASR system to deal with missing data using neural networks has recently been proposed by (Morris et al., 2000). This is basically a radial basis function neural network with the hidden units associated with a diagonal covariance gaussian. The marginal over the missing values can be computed in this case and hence the resulting system is equivalent to the HMM based missing data speech recognition system using marginalisation. Reported performance is also comparable to that of the HMM based
speech recognition system. In this paper missing data is dealt with by imputation. We use recurrent neural networks to estimate missing values in the input vector. RNNs have the potential to capture long-term contextual effects over time, and hence to use temporal context to compensate for missing data which CDHMM based missing data techniques do not do. The only contextual information available in CDHMM decoding come from the addition of temporal derivatives to the feature vector. RNNs also allow a single net to perform both imputation and classification, with the potential of combining these processes to mutual benefit. The RNN architecture proposed by Gingras et al. (1998) is based on a fully-connected feedforward network with input, hidden and output layers using hyperbolic tangent activation functions. The output layer has one unit for each class and the network is trained with the correct classification as target. Recurrent links are added to the feedforward net with unit delay from output to the hidden units as in Jordan networks (Jordan, 1988). There are also recurrent links with unit delay from hidden units to missing input units to impute missing features. In addition, there are self delayed terms with a fixed weight for each unit which basically serve to stabilise RNN behaviour over time and help in imputation as well. Gingras et al. used this RNN both for a pattern classification task with static data (one input vector for each example) and sequential data (a sequence of input values for each example). Our aim is to adapt this architecture for robust ASR with missing data. Some preliminary static classification experiments were performed on vowel spectra (individual spectral slices excised from the TIMIT database). RNN performance on this task with missing data was better than standard MLP and gaussian classifiers. In the next section we show how the net can be adapted for dynamic classification of the spectral sequences constituting words.

RNN architecture for robust ASR with missing data

Figure 1 illustrates our modified version of the Gingras and Bengio architecture. Instead of taking feedback from the output to the hidden layer we have chosen a fully connected or Elman RNN (Elman, 1990) where there are full recurrent links from the past hidden layer to the present hidden layer (figure 1). We have observed that these links produce faster convergence, in agreement with (Pedersen, 1997). The number of input units depends on the size of feature vector, i.e. the number of spectral channels. The number of hidden units is determined by experimentation. There is one output unit for each pattern class. In our case the classes are taken to be whole words, so in the isolated digit recognition experiments we report, there are eleven output units, for 1' -9', zero' andoh'. In training, missing inputs are initialised with their unconditional means. The RNN is then allowed to impute missing values for the next frame through the recurrent links, after a feedforward pass. Where is the missing feature at time t, is the learning rate, indicates recurrent links from a hidden unit to the missing input and is the activation of hidden unit j at time t-1.
The average of the RNN output over all the frames of an example is taken after these frames have gone through a forward pass. The sum squared error between the correct targets and the RNN output for each frame is back-propagated through time and RNN"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/801272ee79cfde7fa5960571fee36b9b-Abstract.html,On Spectral Clustering: Analysis and an algorithm,"Andrew Y. Ng, Michael I. Jordan, Yair Weiss","Despite many empirical successes of spectral  clustering  methods(cid:173) algorithms  that  cluster  points  using  eigenvectors  of  matrices  de(cid:173) rived  from  the  data- there  are  several  unresolved  issues.  First,  there  are  a  wide  variety  of  algorithms  that  use  the  eigenvectors  in  slightly  different  ways.  Second,  many of these  algorithms  have  no  proof that  they  will  actually  compute  a  reasonable  clustering.  In  this  paper,  we  present  a  simple  spectral  clustering  algorithm  that can be implemented using a  few  lines  of Matlab.  Using  tools  from  matrix  perturbation  theory,  we  analyze  the  algorithm,  and  give  conditions  under  which  it  can  be  expected  to  do  well.  We  also  show  surprisingly  good  experimental  results  on  a  number  of  challenging clustering problems."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/8038da89e49ac5eabb489cfc6cea9fc1-Abstract.html,Grouping with Bias,"Stella X. Yu, Jianbo Shi","With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenprob(cid:173) lema We demonstrate that simple priors can greatly improve image segmentation results."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/814a9c18f5abff398787c9cfcbf3d80c-Abstract.html,Learning Hierarchical Structures with Linear Relational Embedding,"Alberto Paccanaro, Geoffrey E. Hinton","We present Linear Relational Embedding (LRE), a new method of learn- ing a distributed representation of concepts from data consisting of in- stances of relations between given concepts. Its Ô¨Ånal goal is to be able to generalize, i.e. infer new instances of these relations among the con- cepts. On a task involving family relationships we show that LRE can generalize better than any previously published method. We then show how LRE can be used effectively to Ô¨Ånd compact distributed representa- tions for variable-sized recursive data structures, such as trees and lists.
1 Linear Relational Embedding
Our aim is to take a large set of facts about a domain expressed as tuples of arbitrary sym- bols in a simple and rigid syntactic format and to be able to infer other ‚Äúcommon-sense‚Äù facts without having any prior knowledge about the domain. Let us imagine a situation in which we have a set of concepts and a set of relations among these concepts, and that our data consists of few instances of these relations that hold among the concepts. We want to be able to infer other instances of these relations. For example, if the concepts are the people in a certain family, the relations are kinship relations, and we are given the facts ‚ÄùAlberto has-father Pietro‚Äù and ‚ÄùPietro has-brother Giovanni‚Äù, we would like to be able to infer ‚ÄùAlberto has-uncle Giovanni‚Äù. Our approach is to learn appropriate distributed rep- resentations of the entities in the data, and then exploit the generalization properties of the distributed representations [2] to make the inferences. In this paper we present a method, which we have called Linear Relational Embedding (LRE), which learns a distributed rep- resentation for the concepts by embedding them in a space where the relations between concepts are linear transformations of their distributed representations.
involve two concepts. Let us consider the case in which all the relations are binary, i.e. , and the problem In this case our data consists of triplets we are trying to solve is to infer missing triplets when we are given only few of them. Inferring a triplet is equivalent to being able to complete it, that is to come up with one of its elements, given the other two. Here we shall always try to complete the third element of the triplets 1. LRE will then represent each concept in the data as a learned vector in a"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/819e3d6c1381eac87c17617e5165f38c-Abstract.html,Constructing Distributed Representations Using Additive Clustering,Wheeler Ruml,"If the promise of computational modeling is to be fully realized in higher- level cognitive domains such as language processing, principled methods must be developed to construct the semantic representations used in such models. In this paper, we propose the use of an established formalism from mathematical psychology, additive clustering, as a means of auto- matically constructing binary representations for objects using only pair- wise similarity data. However, existing methods for the unsupervised learning of additive clustering models do not scale well to large prob- lems. We present a new algorithm for additive clustering, based on a novel heuristic technique for combinatorial optimization. The algorithm is simpler than previous formulations and makes fewer independence as- sumptions. Extensive empirical tests on both human and synthetic data suggest that it is more effective than previous methods and that it also scales better to larger problems. By making additive clustering practical, we take a significant step toward scaling connectionist models beyond hand-coded examples. 1 Introduction
Many cognitive models posit mental representations based on discrete substructures. Even connectionist models whose processing involves manipulation of real-valued activations typically represent objects as patterns of 0s and 1s across a set of units (Noelle, Cottrell, and Wilms, 1997). Often, individual units are taken to represent specific features of the objects and two representations will share features to the degree to which the two objects are similar. While this arrangement is intuitively appealing, it can be difficult to construct the features to be used in such a model. Using random feature assignments clouds the relationship between the model and the objects it is intended to represent, diminishing the model's value. As Clouse and Cottrell (1996) point out, hand-crafted representations are tedious to construct and it can be difficult to precisely justify (or even articulate) the principles that guided their design. These difficulties effectively limit the number of objects that can be encoded, constraining modeling efforts to small examples. In this paper, we investigate methods for automatically synthesizing feature-based representations directly from the pairwise object similarities that the model is intended to respect. This automatic
Table 1: An 8-feature model derived from consonant confusability data. With c = 0.024,
the model accounts for 91.8% of the variance in the data. Wt. Objects with feature Interpretation .350 f# front unvoiced fricatives .243 dg back voiced stops .197 p k unvoiced stops (without t) .182 b v# front voiced .162 ptk unvoiced stops .127 mn nasals .075 dgv#z z voiced (without b) .049 ptkf#s s unvoiced approach eliminates the manual burden of selecting and assigning features while providing an explicit design criterion that objectively connects the representations to empirical data. After formalizing the problem, we will review existing algorithms that have been proposed for solving it. We will then investigate a new approach, based on combinatorial optimiza- tion. When using a novel heuristic search technique, we find that the new approach, despite its simplicity, performs better than previous algorithms and that, perhaps more important, it maintains its effectiveness on large problems. 1.1 Additive Clustering
We will formalize the problem of constructing discrete features from similarity information using the additive clustering model of Shepard and Arabie (1979). In this framework, abbreviated ADCLUS, clusters represent arbitrarily overlapping discrete features. Each of the k features has a non-negative real-valued weight w k , and the similarity between two objects i and j is just the sum of the weights of the features they share. If f ik is 1 if object
i has feature k and 0 otherwise, and c is a real-valued constant, then the similarity of i and"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/8232e119d8f59aa83050a741631803a6-Abstract.html,Products of Gaussians,"Christopher Williams, Felix V. Agakov, Stephen N. Felderhof","Recently Hinton (1999) has introduced the Products of Experts  (PoE) model in which several individual probabilistic models for  data are combined to provide an overall model of the data. Be(cid:173) low we consider PoE models in which each expert is a Gaussian.  Although the product of Gaussians is also a Gaussian, if each Gaus(cid:173) sian has a simple structure the product can have a richer structure.  We examine (1) Products of Gaussian pancakes which give rise to  probabilistic Minor Components Analysis, (2) products of I-factor  PPCA models and (3) a products of experts construction for an  AR(l) process. 
Recently Hinton (1999) has introduced the Products of Experts (PoE) model in  which several individual probabilistic models for data are combined to provide an  overall model of the data. In this paper we consider PoE models in which each  expert is a Gaussian. It is easy to see that in this case the product model will  also be Gaussian. However, if each Gaussian has a simple structure, the product  can have a richer structure. Using Gaussian experts is attractive as it permits a  thorough analysis of the product architecture, which can be difficult with other  models, e.g. models defined over discrete random variables. 
Below we examine three cases of the products of Gaussians construction: (1) Prod(cid:173) ucts of Gaussian pancakes (PoGP) which give rise to probabilistic Minor Compo(cid:173) nents Analysis (MCA), providing a complementary result to probabilistic Principal  Components Analysis (PPCA) obtained by Tipping and Bishop (1999); (2) Prod(cid:173) ucts of I-factor PPCA models; (3) A products of experts construction for an AR(l)  process. 
Products of Gaussians 
If each expert is a Gaussian pi(xI8i ) '"" N(J1i' ( i), the resulting distribution of the  product of m Gaussians may be expressed as 
By completing the square in the exponent it may be easily shown that p(xI8)  N(/1;E, (2:), where (E l = 2::1 (i l . To simplify the following derivations we will  assume that pi(xI8i ) '"" N(O, (i) and thus that p(xI8) '"" N(O, (2:). J12: i ¬∞ can be 
obtained by translation of the coordinate system. 
1 Products of Gaussian Pancakes 
A Gaussian ""pancake"" (GP) is a d-dimensional Gaussian, contracted in one dimen(cid:173) sion and elongated in the other d - 1 dimensions. In this section we show that the  maximum likelihood solution for a product of Gaussian pancakes (PoGP) yields a  probabilistic formulation of Minor Components Analysis (MCA). 
1.1 Covariance Structure of a GP Expert 
Consider a d-dimensional Gaussian whose probability contours are contracted  in the direction w and equally elongated in mutually orthogonal directions  VI , ... , vd-l.We call this a Gaussian pancake or GP. Its inverse covariance may be  written as"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/84ddfb34126fc3a48ee38d7044e87276-Abstract.html,The Fidelity of Local Ordinal Encoding,"Javid Sadr, Sayan Mukherjee, Keith Thoresz, Pawan Sinha","A key question in neuroscience is how to encode sensory stimuli such as images and sounds. Motivated by studies of response prop- erties of neurons in the early cortical areas, we propose an encoding scheme that dispenses with absolute measures of signal intensity or contrast and uses, instead, only local ordinal measures. In this scheme, the structure of a signal is represented by a set of equalities and inequalities across adjacent regions. In this paper, we focus on characterizing the (cid:12)delity of this representation strategy. We develop a regularization approach for image reconstruction from ordinal measures and thereby demonstrate that the ordinal repre- sentation scheme can faithfully encode signal structure. We also present a neurally plausible implementation of this computation that uses only local update rules. The results highlight the robust- ness and generalization ability of local ordinal encodings for the task of pattern classi(cid:12)cation."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/850af92f8d9903e7a4e0559a98ecc857-Abstract.html,Global Coordination of Local Linear Models,"Sam T. Roweis, Lawrence K. Saul, Geoffrey E. Hinton","High dimensional data that lies on or near a low dimensional manifold can be de- scribed by a collection of local linear models. Such a description, however, does not provide a global parameterization of the manifold‚Äîarguably an important goal of unsupervised learning. In this paper, we show how to learn a collection of local linear models that solves this more difÔ¨Åcult problem. Our local linear models are represented by a mixture of factor analyzers, and the ‚Äúglobal coordi- nation‚Äù of these models is achieved by adding a regularizing term to the standard maximum likelihood objective function. The regularizer breaks a degeneracy in the mixture model‚Äôs parameter space, favoring models whose internal coor- dinate systems are aligned in a consistent way. As a result, the internal coor- dinates change smoothly and continuously as one traverses a connected path on the manifold‚Äîeven when the path crosses the domains of many different local models. The regularizer takes the form of a Kullback-Leibler divergence and illustrates an unexpected application of variational methods: not to perform ap- proximate inference in intractable probabilistic models, but to learn more useful internal representations in tractable ones.
1 Manifold Learning
Consider an ensemble of images, each of which contains a face against a neutral back- ground. Each image can be represented by a point in the high dimensional vector space of pixel intensities. This representation, however, does not exploit the strong correlations between pixels of the same image, nor does it support many useful operations for reasoning about faces. If, for example, we select two images with faces in widely different locations and then average their pixel intensities, we do not obtain an image of a face at their average location. Images of faces lie on or near a low-dimensional, curved manifold, and we can represent them more usefully by the coordinates on this manifold than by pixel intensi- ties. Using these ‚Äúintrinsic coordinates‚Äù, the average of two faces is another face with the average of their locations, poses and expressions.
To analyze and manipulate faces, it is helpful to imagine a ‚Äúmagic black box‚Äù with levers or dials corresponding to the intrinsic coordinates on this manifold. Given a setting of the levers and dials, the box generates an image of a face. Given an image of a face, the box deduces the appropriate setting of the levers and dials. In this paper, we describe a fairly general way to construct such a box automatically from an ensemble of high-dimensional vectors. We assume only that there exists an underlying manifold of low dimensionality and that the relationship between the raw data and the manifold coordinates is locally linear and smoothly varying. Thus our method applies not only to images of faces, but also to many other forms of highly distributed perceptual and scientiÔ¨Åc data (e.g., spectrograms of speech, robotic sensors, gene expression arrays, document collections).
2 Local Linear Models
The global structure of perceptual manifolds (such as images of faces) tends to be highly nonlinear. Fortunately, despite their complicated global structure, we can usually char- acterize these manifolds as locally linear. Thus, to a good approximation, they can be represented by collections of simpler models, each of which describes a locally linear neighborhood[3, 6, 8]. For unsupervised learning tasks, a probabilistic model that nicely captures this intuition is a mixture of factor analyzers (MFA)[5]. The model is used to describe high dimensional data that lies on or near a lower dimensional manifold. MFAs parameterize a joint distribution over observed and hidden variables:"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/88ef51f0bf911e452e8dbb1d807a81ab-Abstract.html,Correlation Codes in Neuronal Populations,"Maoz Shamir, Haim Sompolinsky","Population codes often rely on the tuning of the mean responses to the stimulus parameters. However, this information can be greatly sup- pressed by long range correlations. Here we study the efÔ¨Åciency of cod- ing information in the second order statistics of the population responses. We show that the Fisher Information of this system grows linearly with the size of the system. We propose a bilinear readout model for extract- ing information from correlation codes, and evaluate its performance in discrimination and estimation tasks. It is shown that the main source of information in this system is the stimulus dependence of the variances of the single neuron responses."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/89885ff2c83a10305ee08bd507c1049c-Abstract.html,Grammatical Bigrams,Mark A. Paskin,"Unsupervised learning algorithms have been derived for several sta(cid:173) tistical  models  of English grammar, but their computational com(cid:173) plexity  makes  applying  them  to large  data  sets  intractable.  This  paper  presents  a  probabilistic  model  of English  grammar  that  is  much  simpler than conventional models,  but which  admits an effi(cid:173) cient  EM training algorithm.  The model  is  based upon  grammat(cid:173) ical  bigrams,  i.e. ,  syntactic  relationships  between  pairs  of  words.  We  present the results  of experiments  that  quantify  the represen(cid:173) tational adequacy of the grammatical bigram model, its ability to  generalize  from  labelled  data,  and  its  ability  to  induce  syntactic  structure from  large amounts of raw text."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/8c249675aea6c3cbd91661bbae767ff1-Abstract.html,The Steering Approach for Multi-Criteria Reinforcement Learning,"Shie Mannor, Nahum Shimkin","We consider the problem of learning to attain multiple goals in a dynamic envi- ronment, which is initially unknown. In addition, the environment may contain arbitrarily varying elements related to actions of other agents or to non-stationary moves of Nature. This problem is modelled as a stochastic (Markov) game between the learning agent and an arbitrary player, with a vector-valued reward function. The objective of the learning agent is to have its long-term average reward vector belong to a given target set. We devise an algorithm for achieving this task, which is based on the theory of approachability for stochastic games. This algorithm com- bines, in an appropriate way, a Ô¨Çnite set of standard, scalar-reward learning algo- rithms. Su‚Äìcient conditions are given for the convergence of the learning algorithm to a general target set. The specialization of these results to the single-controller Markov decision problem are discussed as well."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/8d8818c8e140c64c743113f563cf750f-Abstract.html,Stabilizing Value Function Approximation with the BFBP Algorithm,"Xin Wang, Thomas G. Dietterich","We address the problem of non-convergence of online reinforcement learning algorithms (e.g., Q learning and SARSA(A)) by adopt(cid:173) ing an incremental-batch approach that separates the exploration process from the function fitting process. Our BFBP (Batch Fit to Best Paths) algorithm alternates between an exploration phase (during which trajectories are generated to try to find fragments of the optimal policy) and a function fitting phase (during which a function approximator is fit to the best known paths from start states to terminal states). An advantage of this approach is that batch value-function fitting is a global process, which allows it to address the tradeoffs in function approximation that cannot be handled by local, online algorithms. This approach was pioneered by Boyan and Moore with their GROWSUPPORT and ROUT al(cid:173) gorithms. We show how to improve upon their work by applying a better exploration process and by enriching the function fitting procedure to incorporate Bellman error and advantage error mea(cid:173) sures into the objective function. The results show improved per(cid:173) formance on several benchmark problems."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/9185f3ec501c674c7c788464a36e7fb3-Abstract.html,Tree-based reparameterization for approximate inference on loopy graphs,"Martin J. Wainwright, Tommi Jaakkola, Alan S. Willsky","We  develop  a  tree-based  reparameterization framework  that  pro(cid:173) vides a  new conceptual view of a  large class of iterative algorithms  for  computing  approximate  marginals  in  graphs  with  cycles.  It  includes  belief propagation  (BP),  which  can  be  reformulated  as  a  very local form of reparameterization.  More generally, we  consider  algorithms  that  perform  exact  computations  over  spanning  trees  of  the  full  graph.  On  the  practical  side,  we  find  that  such  tree  reparameterization (TRP)  algorithms have convergence properties  superior  to  BP.  The  reparameterization perspective  also  provides  a  number  of  theoretical  insights  into  approximate  inference,  in(cid:173) cluding  a  new  characterization  of fixed  points;  and  an  invariance  intrinsic  to  TRP /BP.  These  two  properties  enable  us  to  analyze  and  bound  the  error  between  the  TRP /BP  approximations  and  the  actual  marginals.  While  our  results  arise  naturally  from  the  TRP perspective, most of them apply in an algorithm-independent  manner  to  any  local  minimum  of the  Bethe  free  energy.  Our  re(cid:173) sults  also  have  natural  extensions  to  more  structured approxima(cid:173) tions  [e.g. ,  1,  2]."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/92bbd31f8e0e43a7da8a6295b251725f-Abstract.html,Cobot: A Social Reinforcement Learning Agent,"Charles Lee Isbell Jr., Christian R. Shelton","We report on the use of reinforcement learning with Cobot, a software agent residing in the well-known online community LambdaMOO. Our initial work on Cobot (Isbell et al.2000) provided him with the ability to collect social statistics and report them to users. Here we describe an application of RL allowing Cobot to take proactive actions in this complex social environment, and adapt behavior from multiple sources of human reward. After 5 months of training, and 3171 reward and punishment events from 254 different LambdaMOO users, Cobot learned nontrivial preferences for a number of users, modiÔ¨Ång his behavior based on his current state. Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/931af583573227f0220bc568c65ce104-Abstract.html,Semi-supervised MarginBoost,"Florence d'Alch√©-Buc, Yves Grandvalet, Christophe Ambroise","In many discrimination problems a large amount of data is available but  only a few of them are labeled. This provides a strong motivation to  improve or develop methods for semi-supervised learning. In this paper,  boosting is generalized to this task within the optimization framework of  MarginBoost . We extend the margin definition to unlabeled data and  develop the gradient descent algorithm that corresponds to the resulting  margin cost function. This meta-learning scheme can be applied to any  base classifier able to benefit from unlabeled data. We propose here to  apply it to mixture models trained with an Expectation-Maximization  algorithm. Promising results are presented on benchmarks with different  rates of labeled data."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/95192c98732387165bf8e396c0f2dad2-Abstract.html,ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition,"Brendan J. Frey, Trausti T. Kristjansson, Li Deng, Alex Acero","A  challenging,  unsolved  problem  in  the  speech  recognition  com(cid:173) munity  is  recognizing  speech  signals  that  are  corrupted  by  loud,  highly  nonstationary noise.  One  approach to  noisy  speech  recog(cid:173) nition is  to automatically  remove the noise from  the  cepstrum se(cid:173) quence before feeding it in to a clean speech recognizer.  In previous  work published in  Eurospeech,  we  showed how  a  probability model  trained on  clean  speech  and  a  separate probability  model  trained  on noise could be combined for the purpose of estimating the noise(cid:173) free speech from the noisy speech.  We showed how an iterative 2nd  order  vector  Taylor  series  approximation  could  be  used  for  prob(cid:173) abilistic  inference  in this  model.  In  many  circumstances,  it is  not  possible to obtain  examples  of noise  without  speech.  Noise  statis(cid:173) tics  may change significantly  during an utterance,  so  that speech(cid:173) free frames are not sufficient for  estimating the noise model.  In this  paper, we  show how the noise model can be learned even when the  data contains speech.  In particular, the noise model can be learned  from the test utterance and then used to de noise the test utterance.  The approximate inference technique is  used as  an approximate E  step  in  a  generalized  EM  algorithm that learns the parameters of  the noise  model from  a  test utterance.  For both Wall  Street  J our(cid:173) nal data with added noise samples and the Aurora benchmark, we  show that the new noise adaptive technique performs as  well  as or  significantly better  than  the  non-adaptive  algorithm,  without  the  need for  a  separate training set of noise examples."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/955cb567b6e38f4c6b3f28cc857fc38c-Abstract.html,Adaptive Nearest Neighbor Classification Using Support Vector Machines,"Carlotta Domeniconi, Dimitrios Gunopulos","The nearest  neighbor technique is  a  simple  and appealing method  to  address  classification  problems.  It relies  on  t he  assumption  of  locally  constant  class  conditional  probabilities.  This  assumption  becomes  invalid  in high  dimensions with a  finite  number of exam(cid:173) ples  due  to  the  curse  of dimensionality.  We  propose  a  technique  that computes a  locally flexible  metric by means of Support Vector  Machines  (SVMs).  The maximum margin boundary found  by the  SVM is used to determine the most discriminant direction over the  query's  neighborhood.  Such  direction  provides  a  local  weighting  scheme  for  input  features.  We  present  experimental  evidence  of  classification  performance  improvement  over  the  SVM  algorithm  alone  and  over  a  variety  of adaptive  learning  schemes,  by  using  both simulated and real data sets."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/95f6870ff3dcd442254e334a9033d349-Abstract.html,"Fast, Large-Scale Transformation-Invariant Clustering","Brendan J. Frey, Nebojsa Jojic","In previous work on transformed mixtures of Gaussians'' andtransformed hidden Markov models'', we showed how the EM al- gorithm in a discrete latent variable model can be used to jointly normalize data (e.g., center images, pitch-normalize spectrograms) and learn a mixture model of the normalized data. The only input to the algorithm is the data, a list of possible transformations, and the number of clusters to find. The main criticism of this work was that the exhaustive computation of the posterior probabili- ties over transformations would make scaling up to large feature vectors and large sets of transformations intractable. Here, we de- scribe how a tremendous speed-up is acheived through the use of a variational technique for decoupling transformations, and a fast Fourier transform method for computing posterior probabilities. For NN images, learning C clusters under N rotations, N scales,
N x-translations and N y-translations takes only (C + 2 log N)N
2
scalar operations per iteration. In contrast, the original algorithm takes CN
6
operations to account for these transformations. We give results on learning a 4-component mixture model from a video sequence with frames of size 320240. The model accounts for 360 rotations and 76,800 translations. Each iteration of EM takes only 10 seconds per frame in MATLAB, which is over 5 million times faster than the original algorithm."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/96055f5b06bf9381ac43879351642cf5-Abstract.html,A Rotation and Translation Invariant Discrete Saliency Network,"Lance R. Williams, John W. Zweck","We describe a neural network which enhances and completes salient closed contours. Our work is different from all previous work in three important ways. First, like the input provided to V1 by LGN, the in- put to our computation is isotropic. That is, the input is composed of spots not edges. Second, our network computes a well deÔ¨Åned function of the input based on a distribution of closed contours characterized by a random process. Third, even though our computation is implemented in a discrete network, its output is invariant to continuous rotations and translations of the input pattern."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/978d76676f5e7918f81d28e7d092ca0d-Abstract.html,A theory of neural integration in the head-direction system,"Richard Hahnloser, Xiaohui Xie, H. S. Seung","Integration in the head-direction system is a computation by which hor- izontal angular head velocity signals from the vestibular nuclei are in- tegrated to yield a neural representation of head direction. In the thala- mus, the postsubiculum and the mammillary nuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their Ô¨Åring is maximal [Blair and Sharp, 1995, Blair et al., 1998, ?]. Integration is a difÔ¨Åcult computation, given that head-velocities can vary over a large range. Previous models of the head-direction system relied on the assumption that the integration is achieved in a Ô¨Åring-rate-based attractor network with a ring structure. In order to correctly integrate head-velocity signals during high-speed head rotations, very fast synaptic dynamics had to be assumed. Here we address the question whether integration in the head-direction system is possible with slow synapses, for example excitatory NMDA and inhibitory GABA(B) type synapses. For neural networks with such slow synapses, rate-based dynamics are a good approximation of spik- ing neurons [Ermentrout, 1994]. We Ô¨Ånd that correct integration during high-speed head rotations imposes strong constraints on possible net- work architectures."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/98c7242894844ecd6ec94af67ac8247d-Abstract.html,(Not) Bounding the True Error,"John Langford, Rich Caruana",We present a new approach to bounding the true error rate of a continuous valued classiÔ¨Åer based upon PAC-Bayes bounds. The method Ô¨Årst con- structs a distribution over classiÔ¨Åers by determining how sensitive each parameter in the model is to noise. The true error rate of the stochastic classiÔ¨Åer found with the sensitivity analysis can then be tightly bounded using a PAC-Bayes bound. In this paper we demonstrate the method on artiÔ¨Åcial neural networks with results of a  order of magnitude im- provement vs. the best deterministic neural net bounds.
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/9a49a25d845a483fae4be7e341368e36-Abstract.html,Novel iteration schemes for the Cluster Variation Method,"Hilbert J. Kappen, Wim Wiegerinck","The  Cluster  Variation  method  is  a  class  of approximation  meth(cid:173) ods  containing  the  Bethe  and  Kikuchi  approximations  as  special  cases.  We  derive  two  novel iteration schemes  for  the  Cluster Vari(cid:173) ation Method.  One is  a fixed  point iteration scheme which gives  a  significant improvement over loopy BP, mean field  and TAP meth(cid:173) ods  on  directed  graphical  models.  The  other  is  a  gradient  based  method, that is  guaranteed to converge and is  shown to give useful  results on random graphs with mild frustration.  We  conclude that  the  methods  are  of  significant  practical  value  for  large  inference  problems."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/9afefc52942cb83c7c1f14b2139b09ba-Abstract.html,Infinite Mixtures of Gaussian Process Experts,"Carl E. Rasmussen, Zoubin Ghahramani","We present an extension to the Mixture of Experts (ME) model, where the individual experts are Gaussian Process (GP) regression models. Us- ing an input-dependent adaptation of the Dirichlet Process, we imple- ment a gating network for an inÔ¨Ånite number of Experts. Inference in this model may be done efÔ¨Åciently using a Markov Chain relying on Gibbs sampling. The model allows the effective covariance function to vary with the inputs, and may handle large datasets ‚Äì thus potentially over- coming two of the biggest hurdles with GP models. Simulations show the viability of this approach."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/9bb6dee73b8b0ca97466ccb24fff3139-Abstract.html,Relative Density Nets: A New Way to Combine Backpropagation with HMM's,"Andrew D. Brown, Geoffrey E. Hinton",Logistic units in the first  hidden layer of a  feedforward neural net(cid:173) work  compute  the  relative  probability  of a  data point  under  two  Gaussians.  This  leads  us  to  consider  substituting  other  density  models.  We  present  an architecture for  performing discriminative  learning of Hidden Markov Models using a  network of many small  HMM's.  Experiments on speech  data show it  to be superior to the  standard method of discriminatively training HMM's.
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/9d7311ba459f9e45ed746755a32dcd11-Abstract.html,Neural Implementation of Bayesian Inference in Population Codes,"Si Wu, Shun-ichi Amari","This study investigates a  population decoding paradigm, in  which  the  estimation  of  stimulus  in  the  previous  step  is  used  as  prior  knowledge for consecutive decoding.  We analyze the decoding accu(cid:173) racy of such a  Bayesian decoder (Maximum a Posteriori Estimate),  and  show  that  it  can  be  implemented  by  a  biologically  plausible  recurrent  network,  where  the  prior  knowledge  of stimulus  is  con(cid:173) veyed by the change in recurrent interactions as a result of Hebbian  learning."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/9f62b8625f914a002496335037e9ad97-Abstract.html,"TAP Gibbs Free Energy, Belief Propagation and Sparsity","Lehel Csat√≥, Manfred Opper, Ole Winther","The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived. We show how a speciÔ¨Åc sequential minimization of the free energy leads to a generalization of Minka‚Äôs expectation propa- gation. Lastly, we derive a sparse representation version of the sequential algorithm. The usefulness of the approach is demonstrated on classiÔ¨Åca- tion and density estimation with Gaussian processes and on an indepen- dent component analysis problem."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a00e5eb0973d24649a4a920fc53d9564-Abstract.html,Quantizing Density Estimators,"Peter Meinicke, Helge Ritter","We suggest a nonparametric framework for unsupervised learning of projection models in terms of density estimation on quantized sample spaces. The objective is not to optimally reconstruct the data but in- stead the quantizer is chosen to optimally reconstruct the density of the data. For the resulting quantizing density estimator (QDE) we present a general method for parameter estimation and model selection. We show how projection sets which correspond to traditional unsupervised meth- ods like vector quantization or PCA appear in the new framework. For a principal component quantizer we present results on synthetic and real- world data, which show that the QDE can improve the generalization of the kernel density estimator although its estimate is based on signiÔ¨Åcantly lower-dimensional projection indices of the data."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a012869311d64a44b5a0d567cd20de04-Abstract.html,The Concave-Convex Procedure (CCCP),"Alan L. Yuille, Anand Rangarajan","We  introduce the  Concave-Convex procedure  (CCCP)  which  con(cid:173) structs discrete  time  iterative  dynamical  systems  which  are  guar(cid:173) anteed to monotonically decrease global optimization/energy func(cid:173) tions.  It can be applied to  (almost)  any optimization problem and  many existing algorithms can be interpreted in terms of CCCP.  In  particular, we  prove relationships to some applications of Legendre  transform techniques.  We  then illustrate CCCP by applications to  Potts models, linear assignment,  EM  algorithms,  and  Generalized  Iterative Scaling  (GIS).  CCCP  can be  used  both as  a  new  way  to  understand existing optimization algorithms and as a procedure for  generating new algorithms."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a03fa30821986dff10fc66647c84c9c3-Abstract.html,Active Learning in the Drug Discovery Process,"Manfred K. Warmuth, Gunnar R√§tsch, Michael Mathieson, Jun Liao, Christian Lemmen","We investigate the following data mining problem from Computational Chemistry: From a large data set of compounds, Ô¨Ånd those that bind to a target molecule in as few iterations of biological testing as possible. In each iteration a comparatively small batch of compounds is screened for binding to the target. We apply active learning techniques for selecting the successive batches. One selection strategy picks unlabeled examples closest to the maximum margin hyperplane. Another produces many weight vectors by running perceptrons over multiple permutations of the data. Each weight vector prediction and we pick the unlabeled examples for which votes with its the prediction is most evenly split between . For a third selec- tion strategy note that each unlabeled example bisects the version space of consistent weight vectors. We estimate the volume on both sides of the split by bouncing a billiard through the version space and select un- labeled examples that cause the most even split of the version space. We demonstrate that on two data sets provided by DuPont Pharmaceu- ticals that all three selection strategies perform comparably well and are much better than selecting random batches for testing.
and"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a088ea2078cd92b0b8a0e78a32c5c082-Abstract.html,Intransitive Likelihood-Ratio Classifiers,"Jeff Bilmes, Gang Ji, Marina Meila","In this work, we introduce an information-theoretic based correction term to the likelihood ratio classiÔ¨Åcation method for multiple classes. Under certain conditions, the term is sufÔ¨Åcient for optimally correcting the dif- ference between the true and estimated likelihood ratio, and we analyze this in the Gaussian case. We Ô¨Ånd that the new correction term signif- icantly improves the classiÔ¨Åcation results when tested on medium vo- cabulary speech recognition tasks. Moreover, the addition of this term makes the class comparisons analogous to an intransitive game and we therefore use several tournament-like strategies to deal with this issue. We Ô¨Ånd that further small improvements are obtained by using an appro- priate tournament. Lastly, we Ô¨Ånd that intransitivity appears to be a good measure of classiÔ¨Åcation conÔ¨Ådence."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a1d7311f2a312426d710e1c617fcbc8c-Abstract.html,Transform-invariant Image Decomposition with Similarity Templates,"Chris Stauffer, Erik Miller, Kinh Tieu","Recent work has shown impressive transform-invariant modeling and clustering for sets of images of objects with similar appearance. We seek to expand these capabilities to sets of images of an object class that show considerable variation across individual instances (e.g. pedestrian images) using a representation based on pixel-wise similarities, similarity templates. Because of its invariance to the colors of particular components of an object, this representation en- ables detection of instances of an object class and enables alignment of those instances. Further, this model implicitly represents the re- gions of color regularity in the class-speci(cid:12)c image set enabling a decomposition of that object class into component regions."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a29d1598024f9e87beab4b98411d48ce-Abstract.html,Activity Driven Adaptive Stochastic Resonance,"Gregor Wenning, Klaus Obermayer","Cortical neurons might be considered as threshold elements inte(cid:173) grating in parallel many excitatory and inhibitory inputs. Due to  the apparent variability of cortical spike trains this yields a strongly  fluctuating membrane potential, such that threshold crossings are  highly irregular. Here we study how a neuron could maximize its  sensitivity w.r.t. a relatively small subset of excitatory input. Weak  signals embedded in fluctuations is the natural realm of stochastic  resonance. The neuron's response is described in a hazard-function  approximation applied to an Ornstein-Uhlenbeck process. We an(cid:173) alytically derive an optimality criterium and give a learning rule  for the adjustment of the membrane fluctuations, such that the  sensitivity is maximal exploiting stochastic resonance. We show  that adaptation depends only on quantities that could easily be  estimated locally (in space and time) by the neuron. The main  results are compared with simulations of a biophysically more re(cid:173) alistic neuron model."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a38b16173474ba8b1a95bcbc30d3b8a5-Abstract.html,Reinforcement Learning with Long Short-Term Memory,Bram Bakker,"This paper presents reinforcement learning with a Long Short(cid:173) Term Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage(,x) learning and directed exploration can solve non-Markovian tasks with long-term dependencies be(cid:173) tween relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a48564053b3c7b54800246348c7fa4a0-Abstract.html,A Dynamic HMM for On-line Segmentation of Sequential Data,"Jens Kohlmorgen, Steven Lemm","We  propose  a  novel  method  for  the  analysis  of  sequential  data  that exhibits  an inherent  mode  switching.  In  particular,  the  data  might  be  a  non-stationary  time  series  from  a  dynamical  system  that switches between multiple operating modes.  Unlike other ap(cid:173) proaches, our method processes the data incrementally and without  any  training of internal  parameters.  We  use  an  HMM  with  a  dy(cid:173) namically changing number of states and an on-line variant of the  Viterbi algorithm that performs an unsupervised segmentation and  classification of the data on-the-fly,  i.e.  the method is  able to pro(cid:173) cess  incoming data in  real-time.  The main idea  of the approach is  to track and segment changes of the probability density of the data  in  a  sliding window  on the incoming  data stream.  The usefulness  of the algorithm is  demonstrated  by an application to a  switching  dynamical system."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html,The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank,"Matthew Richardson, Pedro Domingos","The PageRank algorithm, used in the Google search engine, greatly  improves the results of Web search by taking into account the link  structure  of  the  Web.  PageRank  assigns  to  a  page  a  score  propor- tional to the number of times a random surfer would visit that page,  if  it  surfed  indefinitely  from  page  to  page,  following  all  outlinks  from  a  page  with  equal  probability.  We  propose  to  improve  Page- Rank  by  using  a  more  intelligent  surfer,  one  that  is  guided  by  a  probabilistic model of the relevance of a page to a query. Efficient  execution  of  our  algorithm  at  query  time  is  made  possible  by  pre- computing  at  crawl  time  (and  thus  once  for  all  queries)  the  neces- sary  terms.  Experiments  on  two  large  subsets  of  the  Web  indicate  that  our  algorithm  significantly  outperforms  PageRank  in  the  (hu- man-rated)  quality  of the pages returned, while remaining efficient  enough to be used in today‚Äôs large search engines."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a591024321c5e2bdbd23ed35f0574dde-Abstract.html,Incremental A*,"S. Koenig, M. Likhachev","Incremental search techniques Ô¨Ånd optimal solutions to series of similar search tasks much faster than is possible by solving each search task from scratch. While researchers have developed incremental versions of uninformed search methods, we develop an incremental version of A. The Ô¨Årst search of Lifelong Planning A is the same as that of A* but all subsequent searches are much faster because it reuses those parts of the previous search tree that are identical to the new search tree. We then present experimental results that demonstrate the advantages of Lifelong Planning A* for simple route planning tasks.
1 Overview ArtiÔ¨Åcial intelligence has investigated knowledge-based search techniques that allow one to solve search tasks in large domains. Most of the research on these methods has studied how to solve one-shot search problems. However, search is often a repetitive process, where one needs to solve a series of similar search tasks, for example, because the actual situation turns out to be slightly different from the one initially assumed or because the situation changes over time. An example for route planning tasks are changing trafÔ¨Åc conditions. Thus, one needs to replan for the new situation, for example if one always wants to display the least time-consuming route from the airport to the conference center on a web page. In these situations, most search methods replan from scratch, that is, solve the search problems independently. Incremental search techniques share with case-based planning, plan adaptation, repair-based planning, and learning search-control knowledge the property that they Ô¨Ånd solutions to series of similar search tasks much faster than is possible by solving each search task from scratch. Incremental search techniques, however, differ from the other techniques in that the quality of their solutions is guaranteed to be as good as the quality of the solutions obtained by replanning from scratch.
Although incremental search methods are not widely known in artiÔ¨Åcial intelligence and control, different researchers have developed incremental search versions of uninformed search methods in the algorithms literature. An overview can be found in [FMSN00]. We, on the other hand, develop an incremental version of A, thus combining ideas from the algorithms literature and the artiÔ¨Åcial intelligence literature. We call the algorithm Lifelong Planning A (LPA*), in analogy to ‚Äúlifelong learning‚Äù [Thr98], because it reuses
 We thank Anthony Stentz for his support. The Intelligent Decision-Making Group is partly supported by NSF awards under contracts IIS-
9984827, IIS-0098807, and ITR/AP-0113881. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofÔ¨Åcial policies, either expressed or implied, of the sponsoring organizations and agencies or the U.S. government.

information from previous searches. LPA* uses heuristics to focus the search and always Ô¨Ånds a shortest path for the current edge costs. The Ô¨Årst search of LPA* is the same as that of A* but all subsequent searches are much faster. LPA* produces at least the search tree that A* builds. However, it achieves a substantial speedup over A* because it reuses those parts of the previous search tree that are identical to the new search tree."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a5a61717dddc3501cfdf7a4e22d7dbaa-Abstract.html,Grouping and dimensionality reduction by locally linear embedding,"Marzia Polito, Pietro Perona","(LLE) 
Locally Linear Embedding  is an elegant nonlinear  dimensionality-reduction technique recently introduced by Roweis  and Saul [2]. It fails when the data is divided into separate groups.  We study a variant of LLE that can simultaneously group the data  and calculate local embedding of each group. An estimate for the  upper bound on the intrinsic dimension of the data set is obtained  automatically."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a82d922b133be19c1171534e6594f754-Abstract.html,Partially labeled classification with Markov random walks,"Martin Szummer, Tommi Jaakkola","To classify a large number of unlabeled examples we combine a lim- ited number of labeled examples with a Markov random walk represen- tation over the unlabeled examples. The random walk representation ex- ploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way clas- siÔ¨Åcation with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representa- tion and can be set through a margin-based criterion favoring unambigu- ous classiÔ¨Åcation. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classiÔ¨Åcation problems."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a869ccbcbd9568808b8497e28275c7c8-Abstract.html,Dynamic Time-Alignment Kernel in Support Vector Machine,"Hiroshi Shimodaira, Ken-ichi Noma, Mitsuru Nakai, Shigeki Sagayama","A new class of Support Vector Machine (SVM) that is applica- ble to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, stan- dard SVM training and classification algorithms can be employed without further modifications. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimen- tal results show comparable recognition performance with hidden Markov models (HMMs). 1 Introduction
Support Vector Machine (SVM) [1] is one of the latest and most successful statistical pattern classifier that utilizes a kernel technique [2, 3]. The basic form of SVM classifier which classifies an input vector x"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a96d3afec184766bfeca7a9f989fc7e7-Abstract.html,Associative memory in realistic neuronal networks,Peter E. Latham,"Almost  two  decades  ago,  Hopfield  [1]  showed  that  networks  of  highly reduced model neurons can exhibit multiple attracting fixed  points, thus providing a substrate for  associative memory.  It is still  not clear, however, whether realistic neuronal networks can support  multiple attractors.  The main difficulty  is  that neuronal networks  in  vivo  exhibit  a  stable  background  state  at  low  firing  rate,  typ(cid:173) ically  a  few  Hz.  Embedding  attractor  is  easy;  doing  so  without  destabilizing  the  background  is  not.  Previous  work  [2, 3]  focused  on the sparse coding limit,  in which a  vanishingly small number of  neurons  are involved in  any memory.  Here  we  investigate the case  in which  the number of neurons  involved in  a  memory scales  with  the  number  of neurons  in  the  network.  In  contrast  to  the  sparse  coding limit,  we  find  that multiple attractors can co-exist robustly  with a stable background state.  Mean field theory is  used to under(cid:173) stand how the behavior of the network scales with its  parameters,  and simulations with analog neurons  are presented. 
One of the most important features  of the nervous  system is  its ability to perform  associative memory.  It is generally believed that associative memory is implemented  using  attractor networks  - experimental  studies  point  in that  direction  [4- 7],  and  there are virtually no competing theoretical models.  Perhaps surprisingly, however,  it is  still an open theoretical question whether attractors can exist  in realistic neu(cid:173) ronal  networks.  The  ""realistic""  feature  that  is  probably  hardest  to  capture is  the  steady firing  at low  rates - the background state - that is  observed throughout the  intact nervous system [8- 13].  The reason it is  difficult to build an attractor network  that  is  stable  at  low  firing  rates,  at  least  in  the  sparse  coding  limit,  is  as  follows  [2,3]: 
Attractor networks  are constructed by strengthening recurrent  connections among  sub-populations of neurons.  The strengthening must be large enough that neurons  within a sub-population can sustain a high firing rate state, but not so large that the  sub-population  can  be  spontaneously  active.  This  implies  that  the  neuronal  gain  functions - the firing rate of the post-synaptic neurons as a  function  of the average 
‚Ä¢ http) / culture.neurobio.ucla.edu/ ""'pel 
firing rate of the pre-synaptic neurons - must be sigmoidal:  small at low firing rate  to  provide  stability,  high  at  intermediate firing  rate to  provide  a  threshold  (at  an  unstable  equilibrium),  and  low  again  at high  firing  rate to provide saturation and  a  stable  attractor.  In  other  words,  a  requirement  for  the  co-existence  of a  stable  background state and multiple attractors is that the gain function of the excitatory  neurons be super linear at the observed background rates of a few  Hz  [2,3].  However  - and this is  where  the problem  lies  - above  a  few  Hz  most  realistic gain function  are nearly linear or sublinear  (see, for  example, Fig.  Bl of [14]).  The  superlinearity requirement  rests  on  the  implicit  assumption  that  the  activity  of the  sub-population  involved  in  a  memory  does  not  affect  the  other  neurons  in  the  network.  While  this  assumption  is  valid  in  the  sparse  coding  limit ,  it  breaks  down  in  realistic  networks  containing  both  excitatory  and  inhibitory  neurons.  In  such networks,  activity among excitatory cells  results  in inhibitory feedback.  This  feedback,  if  powerful  enough,  can  stabilize  attractors  even  without  a  saturating  nonlinearity,  essentially  by stabilizing the equilibrium  (above considered unstable)  on  the  steep  part  of  the  gain  function.  The  price  one  pays,  though,  is  that  a  reasonable fraction of the neurons must be involved in each of the memories, which  takes us  away from  the sparse coding limit and thus reduces network capacity [15]."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/a9813e9550fee3110373c21fa012eee7-Abstract.html,A General Greedy Approximation Algorithm with Applications,T. Zhang,"Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/abdbeb4d8dbe30df8430a8394b7218ef-Abstract.html,Reducing multiclass to binary by coupling probability estimates,B. Zadrozny,"This paper presents a method for obtaining class membership probability esti- mates for multiclass classiÔ¨Åcation problems by coupling the probability estimates produced by binary classiÔ¨Åers. This is an extension for arbitrary code matrices of a method due to Hastie and Tibshirani for pairwise coupling of probability estimates. Experimental results with Boosted Naive Bayes show that our method produces calibrated class membership probability estimates, while having similar classiÔ¨Åcation accuracy as loss-based decoding, a method for obtaining the most likely class that does not generate probability estimates."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/aebf7782a3d445f43cf30ee2c0d84dee-Abstract.html,Linear-time inference in Hierarchical HMMs,"Kevin P. Murphy, Mark A. Paskin","The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original infer- is ence algorithm is rather complicated, and takes the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many stan- dard approximation techniques to further speed up inference."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/b2ab001909a8a6f04b51920306046ce5-Abstract.html,Sequential Noise Compensation by Sequential Monte Carlo Method,"K. Yao, S. Nakamura","We present a sequential Monte Carlo method applied to additive noise compensation for robust speech recognition in time-varying noise. The method generates a set of samples according to the prior distribution given by clean speech models and noise prior evolved from previous estimation. An explicit model representing noise ef- fects on speech features is used, so that an extended Kalman filter is constructed for each sample, generating the updated continuous state estimate as the estimation of the noise parameter, and predic- tion likelihood for weighting each sample. Minimum mean square error (MMSE) inference of the time-varying noise parameter is car- ried out over these samples by fusion the estimation of samples ac- cording to their weights. A residual resampling selection step and a Metropolis-Hastings smoothing step are used to improve calcula- tion e#ciency. Experiments were conducted on speech recognition in simulated non-stationary noises, where noise power changed ar- tificially, and highly non-stationary Machinegun noise. In all the experiments carried out, we observed that the method can have sig- nificant recognition performance improvement, over that achieved by noise compensation with stationary noise assumption. 1 Introduction
Speech recognition in noise has been considered to be essential for its real applica- tions. There have been active research e#orts in this area. Among many approaches, model-based approach assumes explicit models representing noise e#ects on speech features. In this approach, most researches are focused on stationary or slow-varying noise conditions. In this situation, environment noise parameters are often esti- mated before speech recognition from a small set of environment adaptation data. The estimated environment noise parameters are then used to compensate noise e#ects in the feature or model space for recognition of noisy speech. However, it is well-known that noise statistics may vary during recognition. In this situation, the noise parameters estimated prior to speech recognition of the utterances is possibly not relevant to the subsequent frames of input speech if en- vironment changes.
A number of techniques have been proposed to compensate time-varying noise ef- fects. They can be categorized into two approaches. In the first approach, time- varying environment sources are modeled by Hidden Markov Models (HMM) or Gaussian mixtures that were trained by prior measurement of environments, so that noise compensation is a task of identification of the underlying state sequences of the noise HMMs, e.g., in [1], by maximum a posterior (MAP) decision. This ap- proach requires making a model representing di#erent conditions of environments (signal-to-noise ratio, types of noise, etc.), so that statistics at some states or mix- tures obtained before speech recognition are close to the real testing environments. In the second approach, environment model parameters are assumed to be time- varying, so it is not only an inference problem but also related to environment statistics estimation during speech recognition. The parameters can be estimated by Maximum Likelihood estimation, e.g., sequential EM algorithm [2][3][4]. They can also be estimated by Bayesian methods. In the Bayesian methods, all relevant information on the set of environment parameters and speech parameters, which are denoted as #(t) at frame t, is included in the posterior distribution given observa- tion sequence Y (0 : t), i.e., p(#(t)|Y (0 : t)). Except for a few cases including linear Gaussian state space model (Kalman filter), it is formidable to evaluate the distri- bution updating analytically. Approximation techniques are required. For example, in [5], a Laplace transform is used to approximate the joint distribution of speech and noise parameters by vector Taylor series. The approximated joint distribution can give analytical formula for posterior distribution updating. We report an alternative approach for Bayesian estimation and compensation of noise e#ects on speech features. The method is based on sequential Monte Carlo method [6]. In the method, a set of samples is generated hierarchically from the prior distribution given by speech models. A state space model representing noise e#ects on speech features is used explicitly, and an extended Kalman filter (EKF) is con- structed in each sample. The prediction likelihood of the EKF in each sample gives its weight for selection, smoothing, and inference of the time-varying noise param- eter, so that noise compensation is carried out afterwards. Since noise parameter estimation, noise compensation and speech recognition are carried out frame-by- frame, we denote this approach as sequential noise compensation. 2 Speech and noise model
Our work is on speech features derived from Mel Frequency Cepstral Coe#cients (MFCC). It is generated by transforming signal power into log-spectral domain, and finally, by discrete Cosine transform (DCT) to the cepstral domain. The following derivation of the algorithm is in log-spectral domain. Let t denote frame (time) index. In our work, speech and noise are respectively modeled by HMMs and a Gaussian mixture. For speech recognition in stationary additive noise, the following for- mula [4] has been shown to be e#ective in compensating noise e#ects. For Gaussian mixture k t at state s t , the Log-Add method transforms the mean vector"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/b3b4d2dbedc99fe843fd3dedb02f086f-Abstract.html,Direct value-approximation for factored MDPs,"Dale Schuurmans, Relu Patrascu","We present a simple approach for computing reasonable policies for factored Markov decision processes (MDPs), when the opti(cid:173) mal value function can be approximated by a compact linear form. Our method is based on solving a single linear program that ap(cid:173) proximates the best linear fit to the optimal value function. By applying an efficient constraint generation procedure we obtain an iterative solution method that tackles concise linear programs. This direct linear programming approach experimentally yields a signif(cid:173) icant reduction in computation time over approximate value- and policy-iteration methods (sometimes reducing several hours to a few seconds). However, the quality of the solutions produced by linear programming is weaker-usually about twice the approxi(cid:173) mation error for the same approximating class. Nevertheless, the speed advantage allows one to use larger approximation classes to achieve similar error in reasonable time."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/b5f1e8fb36cd7fbeb7988e8639ac79e9-Abstract.html,Perceptual Metamers in Stereoscopic Vision,B. T. Backus,Abstract Unavailable
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/b8b4b727d6f5d1b61fff7be687f7970f-Abstract.html,Scaling Laws and Local Minima in Hebbian ICA,"Magnus Rattray, Gleb Basalyga","We study the dynamics of a Hebbian ICA algorithm extracting a sin- gle non-Gaussian component from a high-dimensional Gaussian back- ground. For both on-line and batch learning we Ô¨Ånd that a surprisingly large number of examples are required to avoid trapping in a sub-optimal state close to the initial conditions. To extract a skewed signal at least  examples are required for  -dimensional data and    
 exam- ples are required to extract a symmetrical signal with non-zero kurtosis."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/bd5af7cd922fd2603be4ee3dc43b0b77-Abstract.html,Online Learning with Kernels,"Jyrki Kivinen, Alex J. Smola, Robert C. Williamson","We consider online learning in a Reproducing Kernel Hilbert Space. Our method is computationally efÔ¨Åcient and leads to simple algorithms. In particular we derive update equations for classiÔ¨Åcation, regression, and novelty detection. The inclusion of the  -trick allows us to give a robust parameterization. Moreover, unlike in batch learning where the  -trick only applies to the -insensitive loss function we are able to derive gen- eral trimmed-mean types of estimators such as for Huber‚Äôs robust loss."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/bdc4626aa1d1df8e14d80d345b2a442d-Abstract.html,3 state neurons for contextual processing,"√Åd√°m Kepecs, S. Raghavachari","Neurons  receive  excitatory  inputs  via  both  fast  AMPA  and  slow  NMDA  type  receptors.  We  find  that  neurons  receiving  input  via  NMDA  receptors  can have  two  stable  membrane states  which  are  input  dependent.  Action potentials can only be initiated from  the  higher voltage state.  Similar observations have been  made in sev(cid:173) eral  brain areas  which  might  be  explained  by our model.  The  in(cid:173) teractions  between the two  kinds  of inputs lead us  to suggest that  some  neurons  may  operate in  3  states:  disabled,  enabled  and  fir(cid:173) ing.  Such  enabled,  but non-firing modes  can be used to introduce  context-dependent  processing  in  neural  networks.  We  provide  a  simple example and discuss possible implications for  neuronal pro(cid:173) cessing and response variability."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/bf424cb7b0dea050a42b9739eb261a3a-Abstract.html,Convolution Kernels for Natural Language,"Michael Collins, Nigel Duffy","We describe the application of kernel methods to Natural Language Pro- cessing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional rep- resentations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/c215b446bcdf956d848a8419c1b5a920-Abstract.html,On the Convergence of Leveraging,"Gunnar R√§tsch, Sebastian Mika, Manfred K. Warmuth","We give an unified convergence analysis of ensemble learning meth- ods including e.g. AdaBoost, Logistic Regression and the Least-Square- Boost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes ` 1 -norm regularized cost functions leading to a clean and general way to regularize ensemble learning. 1 Introduction
We show convergence rates of ensemble learning methods such as AdaBoost [10], Logistic Regression (LR) [11, 5] and the Least-Square (LS) regression algorithm called LS-Boost [12]. These algorithms have in common that they iteratively call a base learning algorithm
L (also called weak learner) on a weighted training sample. The base learner is expected to return in each iteration t a hypothesis
^
h t from some hypothesis set of weak hypotheses
H that has small weighted training error. This is the weighted number of false predictions in classification and weighted estimation error in regression. These hypotheses are then linearly combined to form the final hypothesis f ^   (x) ="
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html,A Quantitative Model of Counterfactual Reasoning,"Daniel Yarlett, Michael Ramscar","In this paper we explore two quantitative approaches to the modelling of counterfactual reasoning ‚Äì a linear and a noisy-OR model ‚Äì based on in- formation contained in conceptual dependency networks. Empirical data is acquired in a study and the Ô¨Åt of the models compared to it. We con- clude by considering the appropriateness of non-parametric approaches to counterfactual reasoning, and examining the prospects for other para- metric approaches in the future."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/c3535febaff29fcb7c0d20cbe94391c7-Abstract.html,A Neural Oscillator Model of Auditory Selective Attention,"Stuart N. Wrigley, Guy J. Brown","A model of auditory grouping is described in which auditory attention plays a key role. The model is based upon an oscillatory correlation framework, in which neural oscillators representing a single perceptual stream are synchronised, and are desynchronised from oscillators representing other streams. The model suggests a mechanism by which attention can be directed to the high or low tones in a repeating sequence of tones with alternating frequencies. In addition, it simulates the perceptual segregation of a mistuned harmonic from a complex tone."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/c4de8ced6214345614d33fb0b16a8acd-Abstract.html,Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity,"N. Matsumoto, M. Okada","Recent biological experimental (cid:12)ndings have shown that the synap- tic plasticity depends on the relative timing of the pre- and post- synaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does. The synaptic plasticity has been called \Temporally Asymmetric Hebbian plas- ticity (TAH)"". Many authors have numerically shown that spatio- temporal patterns can be stored in neural networks. However, the mathematical mechanism for storage of the spatio-temporal pat- terns is still unknown, especially the e(cid:11)ects of LTD. In this paper, we employ a simple neural network model and show that inter- ference of LTP and LTD disappears in a sparse coding scheme. On the other hand, it is known that the covariance learning is in- dispensable for storing sparse patterns. We also show that TAH qualitatively has the same e(cid:11)ect as the covariance learning when spatio-temporal patterns are embedded in the network."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/c5866e93cab1776890fe343c9e7063fb-Abstract.html,"An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games","Michael L. Littman, Michael J. Kearns, Satinder P. Singh","We  describe a  new  algorithm for  computing a  Nash equilibrium in  graphical  games,  a compact representation for  multi-agent systems  that  we  introduced  in  previous  work.  The  algorithm  is  the  first  to compute equilibria both efficiently  and exactly for  a  non-trivial  class of graphical games."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/c5a4e7e6882845ea7bb4d9462868219b-Abstract.html,Causal Categorization with Bayes Nets,Bob Rehder,"A  theory  of  categorization  is  presented  in  which  knowledge  of  causal  relationships  between  category  features  is  represented  as  a  Bayesian  network.  Referred  to  as  causal-model  theory,  this  theory  predicts  that  objects  are  classified  as  category  members  to  the  extent they  are  likely  to  have  been  produced  by  a categorys  causal  model.  On  this  view,  people  have  models  of  the  world  that  lead  them  to  expect  a  certain  distribution  of  features  in  category  members  (e.g.,  correlations  between  feature  pairs  that  are  directly  connected  by  causal  relationships),  and  consider  exemplars  good  category  members  when  they  manifest  those  expectations.  These  expectations  include  sensitivity  to  higher-order  feature  interactions  that emerge from the  asymmetries inherent in causal relationships. 
Research  on  the  topic  of categorization  has  traditionally  focused  on  the  problem  of  learning  new  categories  given  observations  of  category  members.  In  contrast,  the  theory-based  view  of  categories  emphasizes  the  influence  of  the  prior  theoretical  knowledge  that  learners  often  contribute  to  their  representations  of  categories  [1].  However, in  contrast to  models  accounting  for  the  effects  of empirical  observations,  there  have  been  few  models  developed  to  account for  the  effects of prior knowledge.  The  purpose  of  this  article  is  to  present  a  model  of  categorization  referred  to  as  causal-model  theory  or  CMT  [2,  3].  According  to  CMT,  people 's  know ledge  of  many  categories  includes  not only  features,  but  also  an  explicit representation  of the  causal mechanisms that people believe link the features  of many categories. 
In  this  article  I  apply  CMT  to  the  problem  of  establishing  objects  category  membership.  In  the  psychological  literature  one  standard  view  of  categorization  is  that  objects  are  placed  in  a  category  to  the  extent they  have  features  that  have  often  been  observed  in  members  of that category.  For example, an  object that has  most of  the  features  of  birds  (e.g.,  wings,  fly,  build  nests  in  trees,  etc.)  and  few  features  of  other  categories  is  thought to  be  a  bird. This  view  of categorization  is  formalized  by  prototype  models  in  which  classification  is  a  function  of the  similarity  (i.e. , number  of  shared  features)  between  a  mental  representation  of  a  category  prototype  and  a  to-be-classified  object.  However ,  a  well-known  difficulty  with  prototype  models  is  that  a  features  contribution  to  category  membership  is  independent  of the  presence  or  absence  of  other  features.  In  contrast ,  consideration  of  a  categorys  theoretical  influence  which  combinations  of  features  make  for  knowledge  acceptable  category  members.  For  example ,  people  believe  that  birds  have  nests  in  trees  because  they  can  fly , and  in  light  of this  knowledge  an  animal  that  doesnt  fly"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/c5b2cebf15b205503560c4e8e6d1ea78-Abstract.html,KLD-Sampling: Adaptive Particle Filters,Dieter Fox,"Over the last years, particle Ô¨Ålters have been applied with great success to a variety of state estimation problems. We present a statistical approach to increasing the efÔ¨Åciency of particle Ô¨Ålters by adapting the size of sample sets on-the-Ô¨Çy. The key idea of the KLD-sampling method is to bound the approximation error introduced by the sample-based representation of the particle Ô¨Ålter. The name KLD-sampling is due to the fact that we measure the approximation error by the Kullback-Leibler distance. Our adaptation approach chooses a small number of samples if the density is focused on a small part of the state space, and it chooses a large number of samples if the state uncertainty is high. Both the implementation and computation overhead of this approach are small. Extensive experiments using mobile robot localization as a test application show that our approach yields drastic improvements over particle Ô¨Ålters with Ô¨Åxed sample set sizes and over a previously introduced adaptation technique."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/c6335734dbc0b1ded766421cfc611750-Abstract.html,Unsupervised Learning of Human Motion Models,"Yang Song, Luis Goncalves, Pietro Perona","This paper presents an unsupervised learning algorithm that can derive the probabilistic dependence structure of parts of an object (a moving hu- man body in our examples) automatically from unlabeled data. The dis- tinguished part of this work is that it is based on unlabeled data, i.e., the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown. We use decomposable triangulated graphs to depict the probabilistic independence of parts, but the unsupervised technique is not limited to this type of graph. In the new approach, labeling of the data (part assignments) is taken as hidden variables and the EM algo- rithm is applied. A greedy algorithm is developed to select parts and to search for the optimal structure based on the differential entropy of these variables. The success of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled real image sequences."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/c8758b517083196f05ac29810b924aca-Abstract.html,Orientational and Geometric Determinants of Place and Head-direction,"Neil Burgess, Tom Hartley","We present a model of the firing of place and head-direction cells in rat hippocampus. The model can predict the response of individual cells and populations to parametric manipulations of both geomet(cid:173) ric (e.g. O'Keefe & Burgess, 1996) and orientational (Fenton et aI., 2000a) cues, extending a previous geometric model (Hartley et al., 2000). It provides a functional description of how these cells' spatial responses are derived from the rat's environment and makes easily testable quantitative predictions. Consideration of the phenomenon of remapping (Muller & Kubie, 1987; Bostock et aI., 1991) indicates that the model may also be consistent with non(cid:173) parametric changes in firing, and provides constraints for its future development."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/c902b497eb972281fb5b4e206db38ee6-Abstract.html,Covariance Kernels from Bayesian Generative Models,Matthias Seeger,"We  propose  the  framework  of  mutual  information  kernels  for  learning  covariance  kernels,  as  used  in  Support  Vector  machines  and  Gaussian  process  classifiers,  from  unlabeled  task  data  using  Bayesian techniques.  We describe an implementation of this frame(cid:173) work which  uses  variational Bayesian mixtures of factor  analyzers  in order to attack classification problems in high-dimensional spaces  where labeled data is  sparse, but unlabeled data is  abundant."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/c92a10324374fac681719d63979d00fe-Abstract.html,Modeling Temporal Structure in Classical Conditioning,"Aaron C. Courville, David S. Touretzky","The Temporal Coding Hypothesis of Miller  and colleagues  [7]  sug(cid:173) gests  that  animals  integrate  related  temporal  patterns  of stimuli  into  single  memory  representations.  We  formalize  this  concept  using  quasi-Bayes  estimation  to  update  the  parameters  of a  con(cid:173) strained hidden Markov model.  This approach allows us to account  for  some surprising temporal effects  in the second order condition(cid:173) ing  experiments  of Miller  et  al.  [1 ,  2,  3],  which  other  models  are  unable to explain."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/cbef46321026d8404bc3216d4774c8a9-Abstract.html,Modeling the Modulatory Effect of Attention on Human Spatial Vision,"Laurent Itti, Jochen Braun, Christof Koch","We present new simulation results, in which a computational model  of interacting visual neurons simultaneously predicts the modula(cid:173) tion of spatial vision thresholds by focal visual attention, for five  dual-task human psychophysics experiments. This new study com(cid:173) plements our previous findings that attention activates a winner(cid:173) take-all competition among early visual neurons within one cortical  hypercolumn. This ""intensified competition"" hypothesis assumed  that attention equally affects all neurons, and yielded two single(cid:173) unit predictions: an increase in gain and a sharpening of tuning  with attention. While both effects have been separately observed  in electrophysiology, no single-unit study has yet shown them si(cid:173) multaneously. Hence, we here explore whether our model could still  predict our data if attention might only modulate neuronal gain,  but do so non-uniformly across neurons and tasks. Specifically, we  investigate whether modulating the gain of only the neurons that  are loudest, best-tuned, or most informative about the stimulus,  or of all neurons equally but in a task-dependent manner, may ac(cid:173) count for the data. We find that none of these hypotheses yields  predictions as plausible as the intensified competition hypothesis,  hence providing additional support for our original findings."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/cd4bb35c75ba84b4f39e547b1416fd35-Abstract.html,Approximate Dynamic Programming via Linear Programming,"Daniela Farias, Benjamin V. Roy","The curse of dimensionality gives rise to prohibitive computational  requirements that render infeasible the exact solution of large- scale  stochastic  control  problems.  We  study  an  efficient  method  based  on  linear  programming for  approximating  solutions  to  such  prob(cid:173) lems.  The  approach  ""fits""  a  linear  combination  of  pre- selected  basis  functions  to the  dynamic  programming cost- to- go  function.  We develop bounds on the approximation error and present experi(cid:173) mental results in the domain of queueing network control, providing  empirical support for  the methodology."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/cee8d6b7ce52554fd70354e37bbf44a2-Abstract.html,A New Discriminative Kernel From Probabilistic Models,"Koji Tsuda, Motoaki Kawanabe, Gunnar R√§tsch, S√∂ren Sonnenburg, Klaus-Robert M√ºller","Recently, Jaakkola and Haussler proposed a  method for  construct(cid:173) ing  kernel  functions  from  probabilistic  models.  Their  so  called  ""Fisher  kernel""  has  been  combined  with  discriminative  classifiers  such  as  SVM  and  applied  successfully  in  e.g.  DNA  and  protein  analysis.  Whereas  the  Fisher  kernel  (FK)  is  calculated  from  the  marginal  log-likelihood,  we  propose  the  TOP  kernel  derived  from  Tangent  vectors  Of  Posterior  log-odds.  Furthermore  we  develop  a  theoretical  framework  on  feature  extractors  from  probabilistic  models  and use  it  for  analyzing  FK  and TOP.  In experiments our  new  discriminative  TOP  kernel  compares  favorably  to  the  Fisher  kernel."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/cf2226ddd41b1a2d0ae51dab54d32c36-Abstract.html,Kernel Feature Spaces and Nonlinear Blind Souce Separation,"Stefan Harmeling, Andreas Ziehe, Motoaki Kawanabe, Klaus-Robert M√ºller","In kernel based learning the data is mapped to a kernel feature space of a dimension that corresponds to the number of training data points. In practice, however, the data forms a smaller submanifold in feature space, a fact that has been used e.g. by reduced set techniques for SVMs. We propose a new mathematical construction that permits to adapt to the in- trinsic dimension and to Ô¨Ånd an orthonormal basis of this submanifold. In doing so, computations get much simpler and more important our theoretical framework allows to derive elegant kernelized blind source separation (BSS) algorithms for arbitrary invertible nonlinear mixings. Experiments demonstrate the good performance and high computational efÔ¨Åciency of our kTDSEP algorithm for the problem of nonlinear BSS."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/d0fb963ff976f9c37fc81fe03c21ea7b-Abstract.html,The Unified Propagation and Scaling Algorithm,"Yee W. Teh, Max Welling","In this paper we will show that a restricted class of constrained mini- mum divergence problems, named generalized inference problems, can be solved by approximating the KL divergence with a Bethe free energy. The algorithm we derive is closely related to both loopy belief propaga- tion and iterative scaling. This uniÔ¨Åed propagation and scaling algorithm reduces to a convergent alternative to loopy belief propagation when no constraints are present. Experiments show the viability of our algorithm."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/d198bd736a97e7cecfdf8f4f2027ef80-Abstract.html,Receptive field structure of flow detectors for heading perception,"J. A. Beintema, M. Lappe, Alexander C. Berg","Observer translation relative to the world  creates image flow  that  expands from the observer's direction of translation (heading)  from  which  the  observer  can  recover  heading  direction.  Yet,  the  image  flow  is often more complex, depending on rotation of the eye,  scene  layout  and  translation  velocity.  A  number  of  models  [1-4]  have  been  proposed  on  how  the human  visual  system extracts  heading  from  flow  in  a  neurophysiologic ally  plausible  way.  These  models  represent heading by a  set of neurons  that respond to large image  flow  patterns and receive input from motion sensed at different im(cid:173) age  locations.  We  analysed  these  models  to  determine  the  exact  receptive  field  of these  heading  detectors.  We  find  most  models  predict that, contrary to widespread believe,  the contribut ing mo(cid:173) tion sensors have a preferred motion directed circularly rather than  radially around the detector's preferred heading.  Moreover, the re(cid:173) sults suggest to look for  more refined structure within the circular  flow,  such as bi-circularity or local motion-opponency."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/d3a7f48c12e697d50c8a7ae7684644ef-Abstract.html,Bayesian morphometry of hippocampal cells suggests same-cell somatodendritic repulsion,"Giorgio A. Ascoli, Alexei V. Samsonovich","Visual inspection of neurons suggests that dendritic orientation may be  determined both by internal constraints (e.g. membrane tension) and by  external vector fields (e.g. neurotrophic gradients). For example, basal  dendrites of pyramidal cells appear nicely fan-out. This regular  orientation is hard to justify completely with a general tendency to  grow straight, given the zigzags observed experimentally. Instead,  dendrites could (A) favor a fixed (‚Äúexternal‚Äù) direction, or (B) repel  from their own soma. To investigate these possibilities quantitatively,  reconstructed hippocampal cells were subjected to Bayesian analysis.  The statistical model combined linearly factors A and B, as well as the  tendency to grow straight. For all morphological classes, B was found  to be significantly positive and consistently greater than A. In addition,  when dendrites were artificially re-oriented according to this model, the  resulting structures closely resembled real morphologies. These results  suggest that somatodendritic repulsion may play a role in determining  dendritic orientation. Since hippocampal cells are very densely packed  and their dendritic trees highly overlap, the repulsion must be cell- specific. We discuss possible mechanisms underlying such specificity. 
1 Int r oduc t i on 
The study of brain dynamics and development at the cellular level would greatly benefit  from a standardized, accurate and yet succinct statistical model characterizing the  morphology of major neuronal classes. Such model could also provide a basis for  simulation of anatomically realistic virtual neurons [1]. The model should accurately  distinguish among different neuronal classes: a morphological difference between classes  would be captured by a difference in model parameters and reproduced in generated  virtual neurons. In addition, the model should be self-consistent: there should be no  statistical difference in model parameters measured from real neurons of a given class  and from virtual neurons of the same class. The assumption that a simple statistical model  of this sort exists relies on the similarity of average environmental and homeostatic  conditions encountered by individual neurons during development and on the limited  amount of genetic information that underlies differentiation of neuronal classes. 
Previous research in computational neuroanatomy has mainly focused on the topology  and internal geometry of dendrites (i.e., the properties described in ‚Äúdendrograms‚Äù) [2,3].  Recently, we attempted to include spatial orientation in the models, thus generating 
2 
virtual neurons in 3D [4]. Dendritic growth was assumed to deviate from the straight  direction both randomly and based on a constant bias in a given direction, or ‚Äútropism‚Äù.  Different models of tropism (e.g. along a fixed axis, towards a plane, or away from the  soma) had dramatic effects on the shape of virtual neurons [5]. Our current strategy is to  split the problem of finding a statistical model describing neuronal morphology in two  parts. First, we maintain that the topology and the internal geometry of a particular  dendritic tree can be described independently of its 3D embedding (i.e., the set of local  dendritic orientations). At the same time, one and the same internal geometry (e.g., the  experimental dendrograms obtained from real neurons) may have many equally plausible  3D embeddings that are statistically consistent with the anatomical characteristics of that  neuronal class. The present work aims at finding a minimal statistical model describing  local dendritic orientation in experimentally reconstructed hippocampal principal cells. 
Hippocampal neurons have a polarized shape: their dendrites tend to grow from the soma  as if enclosed in cones. In pyramidal cells, basal and apical dendrites invade opposite  hemispaces (fig. 1A), while granule cell dendrites all invade the same hemispace. This  behavior could be caused by a tendency to grow towards the layers of incoming fibers to  establish synapses. Such tendency would correspond to a tropism in a direction roughly  parallel to the cell main axis. Alternatively, dendrites could initially stem in the  appropriate (possibly genetically determined) directions, and then continue to grow  approximately in a radial direction from the soma. A close inspection of pyramidal  (basal) trees suggests that dendrites may indeed be repelled from their soma (Fig. 1B). A  typical dendrite may reorient itself (arrow) to grow nearly straight along a radius from the  soma. Remarkably, this happens even after many turns, when the initial direction is lost.  Such behavior may be hard to explain without tropism. If the deviations from straight  growth were random, one should be able to ‚Äúremodel‚Äù th e trees by measuring and  reproducing the statistics of local turn angles, assuming its independence of dendritic  orientation and location. Figure 1C shows the cell from 1A after such remodeling. In this  case basal and apical dendrites retain only their initial (stemming) orientations from the  original data. The resulting ‚Äúcotton ball‚Äù s uggests that dendritic turns are not in dependent  of dendritic orientation. In this paper, we use Bayesian analysis to quantify the dendritic  tropism."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/d46e1fcf4c07ce4a69ee07e4134bcef1-Abstract.html,"Entropy and Inference, Revisited","Ilya Nemenman, F. Shafee, William Bialek","We study properties of popular near‚Äìuniform (Dirichlet) priors for learn- ing undersampled probability distributions on discrete nonmetric spaces and show that they lead to disastrous results. However, an Occam‚Äìstyle phase space argument expands the priors into their inÔ¨Ånite mixture and resolves most of the observed problems. This leads to a surprisingly good estimator of entropies of discrete distributions.
Learning a probability distribution from examples is one of the basic problems in data analysis. Common practical approaches introduce a family of parametric models, leading to questions about model selection. In Bayesian inference, computing the total probability of the data arising from a model involves an integration over parameter space, and the resulting ‚Äúphase space volume‚Äù automatically discriminates against models with larger numbers of parameters‚Äîhence the description of these volume terms as Occam factors [1, 2]. As we move from Ô¨Ånite parameterizations to models that are described by smooth functions, the integrals over parameter space become functional integrals and methods from quantum Ô¨Åeld theory allow us to do these integrals asymptotically; again the volume in model space consistent with the data is larger for models that are smoother and hence less complex [3]. Further, at least under some conditions the relevant degree of smoothness can be determined self‚Äìconsistently from the data, so that we approach something like a model independent method for learning a distribution [4].
describe as the number of times ni each possibility is observed in a set of N =PK
The results emphasizing the importance of phase space factors in learning prompt us to look back at a seemingly much simpler problem, namely learning a distribution on a dis- crete, nonmetric space. Here the probability distribution is just a list of numbers fqig, i = 1; 2;(cid:1)(cid:1)(cid:1) ; K, where K is the number of bins or possibilities. We do not assume any metric on the space, so that a priori there is no reason to believe that any qi and qj should be similar. The task is to learn this distribution from a set of examples, which we can i=1 ni samples. This problem arises in the context of language, where the index i might label words or phrases, so that there is no natural way to place a metric on the space, nor is it even clear that our intuitions about similarity are consistent with the constraints of a met- ric space. Similarly, in bioinformatics the index i might label n‚Äìmers of the the DNA or amino acid sequence, and although most work in the Ô¨Åeld is based on metrics for sequence comparison one might like an alternative approach that does not rest on such assumptions. In the analysis of neural responses, once we Ô¨Åx our time resolution the response becomes a set of discrete ‚Äúwords,‚Äù and estimates of the information content in the response are de-
termined by the probability distribution on this discrete space. What all of these examples have in common is that we often need to draw some conclusions with data sets that are not in the asymptotic limit N (cid:29) K. Thus, while we might use a large corpus to sample the distribution of words in English by brute force (reaching N (cid:29) K with K the size of the vocabulary), we can hardly do the same for three or four word phrases.
In models described by continuous functions, the inÔ¨Ånite number of ‚Äúpossibilities‚Äù can never be overwhelmed by examples; one is saved by the notion of smoothness. Is there some nonmetric analog of this notion that we can apply in the discrete case? Our intuition is that information theoretic quantities may play this role. If we have a joint distribution of two variables, the analog of a smooth distribution would be one which does not have too much mutual information between these variables. Even more simply, we might say that smooth distributions have large entropy. While the idea of ‚Äúmaximum entropy inference‚Äù is common [5], the interplay between constraints on the entropy and the volume in the space of models seems not to have been considered. As we shall explain, phase space factors alone imply that seemingly sensible, more or less uniform priors on the space of discrete probability distributions correspond to disastrously singular prior hypotheses about the entropy of the underlying distribution. We argue that reliable inference outside the asymptotic regime N (cid:29) K requires a more uniform prior on the entropy, and we offer one way of doing this. While many distributions are consistent with the data when N (cid:20) K, we provide empirical evidence that this Ô¨Çattening of the entropic prior allows us to make surprisingly reliable statements about the entropy itself in this regime.
At the risk of being pedantic, we state very explicitly what we mean by uniform or nearly uniform priors on the space of distributions. The natural ‚Äúuniform‚Äù prior is given by"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/d47268e9db2e9aa3827bba3afb7ff94a-Abstract.html,Audio-Visual Sound Separation Via Hidden Markov Models,"John R. Hershey, Michael Casey","It  is  well  known  that  under  noisy  conditions  we  can  hear  speech  much  more  clearly  when  we  read  the  speaker's  lips.  This  sug(cid:173) gests  the utility of audio-visual information for  the task of speech  enhancement.  We  propose  a  method  to exploit  audio-visual  cues  to  enable  speech  separation  under  non-stationary  noise  and  with  a  single  microphone.  We  revise  and  extend  HMM-based  speech  enhancement techniques, in which signal and noise models are fac(cid:173) tori ally  combined,  to  incorporate  visual  lip  information  and  em(cid:173) ploy  novel  signal  HMMs  in  which  the  dynamics  of  narrow-band  and  wide  band  components  are  factorial.  We  avoid  the  combina(cid:173) torial  explosion  in  the  factorial  model  by  using  a  simple  approxi(cid:173) mate  inference  technique  to  quickly  estimate  the  clean  signals  in  a  mixture.  We  present  a  preliminary  evaluation  of this  approach  using a small-vocabulary audio-visual database, showing promising  improvements in  machine  intelligibility for  speech  enhanced using  audio and visual information."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/d5c186983b52c4551ee00f72316c6eaa-Abstract.html,Spectral Relaxation for K-means Clustering,"Hongyuan Zha, Xiaofeng He, Chris Ding, Ming Gu, Horst D. Simon","The popular K-means  clustering partitions a  data set by minimiz(cid:173) ing  a  sum-of-squares cost function.  A  coordinate descend  method  is  then used to find  local minima.  In  this  paper we  show that the  minimization can be reformulated as a trace maximization problem  associated with the Gram matrix of the data vectors.  Furthermore,  we  show that a  relaxed version of the trace maximization problem  possesses  global  optimal solutions  which  can be obtained by  com(cid:173) puting a  partial eigendecomposition  of the  Gram matrix,  and the  cluster assignment for  each data vectors  can be found  by  comput(cid:173) ing  a  pivoted  QR decomposition  of the  eigenvector  matrix.  As  a  by-product  we  also  derive  a  lower bound  for  the  minimum  of the  sum-of-squares cost function."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/d68a18275455ae3eaa2c291eebb46e6d-Abstract.html,Gaussian Process Regression with Mismatched Models,Peter Sollich,"Learning curves for Gaussian process regression are well understood  when the 'student' model happens to match the 'teacher' (true data  generation process).  I derive approximations to the learning curves  for  the more generic case of mismatched models, and find  very rich  behaviour:  For large input space dimensionality,  where the results  become  exact,  there  are  universal  (student-independent)  plateaux  in  the learning curve, with transitions in between  that can exhibit  arbitrarily  many  over-fitting  maxima;  over-fitting  can  occur  even  if the student estimates  the teacher  noise level  correctly.  In lower  dimensions,  plateaux also  appear,  and the learning curve remains  dependent  on  the  mismatch  between  student  and teacher even  in  the asymptotic limit of a large number of training examples.  Learn(cid:173) ing with excessively strong smoothness assumptions can be partic(cid:173) ularly  dangerous:  For  example,  a  student  with  a  standard radial  basis function covariance function will learn a rougher teacher func(cid:173) tion only  logarithmically slowly.  All  predictions  are  confirmed  by  simulations."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/d77f00766fd3be3f2189c843a6af3fb2-Abstract.html,Kernel Machines and Boolean Functions,"Adam Kowalczyk, Alex J. Smola, Robert C. Williamson","We give results about the learnability and required complexity of logical formulae to solve classiÔ¨Åcation problems. These results are obtained by linking propositional logic with kernel machines. In particular we show that decision trees and disjunctive normal forms (DNF) can be repre- sented by the help of a special kernel, linking regularized risk to separa- tion margin. Subsequently we derive a number of lower bounds on the required complexity of logic formulae using properties of algorithms for generation of linear estimators, such as perceptron and maximal percep- tron learning."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/d79c6256b9bdac53a55801a066b70da3-Abstract.html,Efficient Resources Allocation for Markov Decision Processes,R√©mi Munos,"It is desirable that a complex decision-making problem in an uncer(cid:173) tain world be adequately modeled by a Markov Decision Process (MDP) whose structural representation is adaptively designed by a parsimonious resources allocation process. Resources include time and cost of exploration, amount of memory and computational time allowed for the policy or value function representation. Concerned about making the best use of the available resources, we address the problem of efficiently estimating where adding extra resources is highly needed in order to improve the expected performance of the resulting policy. Possible application in reinforcement learning (RL) , when real-world exploration is highly costly, concerns the de(cid:173) tection of those areas of the state-space that need primarily to be explored in order to improve the policy. Another application con(cid:173) cerns approximation of continuous state-space stochastic control problems using adaptive discretization techniques for which highly efficient grid points allocation is mandatory to survive high dimen(cid:173) sionality. Maybe surprisingly these two problems can be formu(cid:173) lated under a common framework: for a given resource allocation, which defines a belief state over possible MDPs, find where adding new resources (thus decreasing the uncertainty of some parame(cid:173) ters -transition probabilities or rewards) will most likely increase the expected performance of the new policy. To do so, we use sam(cid:173) pling techniques for estimating the contribution of each parameter's probability distribution function (Pdf) to the expected loss of us(cid:173) ing an approximate policy (such as the optimal policy of the most probable MDP) instead of the true (but unknown) policy."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/d7a84628c025d30f7b2c52c958767e76-Abstract.html,Information Geometrical Framework for Analyzing Belief Propagation Decoder,"Shiro Ikeda, Toshiyuki Tanaka, Shun-ichi Amari","The mystery of belief propagation (BP) decoder, especially of the turbo decoding, is studied from information geometrical viewpoint. The loopy belief network (BN) of turbo codes makes it difÔ¨Åcult to obtain the true ‚Äúbelief‚Äù by BP, and the characteristics of the algorithm and its equilib- rium are not clearly understood. Our study gives an intuitive understand- ing of the mechanism, and a new framework for the analysis. Based on the framework, we reveal basic properties of the turbo decoding."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html,Speech Recognition using SVMs,"N. Smith, Mark Gales","An important issue in applying SVMs  to speech recognition is  the  ability  to  classify  variable length  sequences.  This  paper  presents  extensions to a  standard scheme  for  handling this  variable length  data, the Fisher score.  A more useful mapping is introduced based  on  the  likelihood-ratio.  The  score-space  defined  by  this  mapping  avoids some limitations of the Fisher score.  Class-conditional gen(cid:173) erative models  are  directly  incorporated into the  definition  of the  score-space.  The mapping, and appropriate normalisation schemes,  are evaluated on a  speaker-independent  isolated letter  task where  the  new  mapping  outperforms  both  the  Fisher  score  and  HMMs  trained to maximise likelihood."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/d860edd1dd83b36f02ce52bde626c653-Abstract.html,Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions,Kari Torkkola,"The marriage of Renyi entropy with Parzen density estimation has been shown to be a viable tool in learning discriminative feature transforms. However, it suffers from computational complexity proportional to the square of the number of samples in the training data. This sets a practical limit to using large databases. We suggest immediate divorce of the two methods and remarriage of Renyi entropy with a semi-parametric density estimation method, such as a Gaussian Mixture Models (GMM). This al- lows all of the computation to take place in the low dimensional target space, and it reduces computational complexity proportional to square of the number of components in the mixtures. Furthermore, a conve- nient extension to Hidden Markov Models as commonly used in speech recognition becomes possible."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/dc513ea4fbdaa7a14786ffdebc4ef64e-Abstract.html,Information-Geometrical Significance of Sparsity in Gallager Codes,"Toshiyuki Tanaka, Shiro Ikeda, Shun-ichi Amari","We report a result of perturbation analysis on decoding error of the belief propagation decoder for Gallager codes. The analysis is based on infor- mation geometry, and it shows that the principal term of decoding error at equilibrium comes from the m-embedding curvature of the log-linear submanifold spanned by the estimated pseudoposteriors, one for the full marginal, and K for partial posteriors, each of which takes a single check into account, where K is the number of checks in the Gallager code. It is then shown that the principal error term vanishes when the parity-check matrix of the code is so sparse that there are no two columns with overlap greater than 1."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/dd055f53a45702fe05e449c30ac80df9-Abstract.html,Adaptive Sparseness Using Jeffreys Prior,M√°rio Figueiredo,"In this paper we introduce a new sparseness inducing prior which does not involve any (hy- per)parameters that need to be adjusted or estimated. Although other applications are possi- ble, we focus here on supervised learning problems: regression and classiÔ¨Åcation. Experi- ments with several publicly available benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms support vector machines and performs competitively with the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparseness- controlling hyper-parameters."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/de03beffeed9da5f3639a621bcab5dd4-Abstract.html,An Efficient Clustering Algorithm Using Stochastic Association Model and Its Implementation Using Nanostructures,"Takashi Morie, Tomohiro Matsuura, Makoto Nagata, Atsushi Iwata","This paper describes a clustering algorithm for vector quantizers using a ‚Äústochastic association model‚Äù. It offers a new simple and powerful soft- max adaptation rule. The adaptation process is the same as the on-line K-means clustering method except for adding random Ô¨Çuctuation in the distortion error evaluation process. Simulation results demonstrate that the new algorithm can achieve efÔ¨Åcient adaptation as high as the ‚Äúneural gas‚Äù algorithm, which is reported as one of the most efÔ¨Åcient clustering methods. It is a key to add uncorrelated random Ô¨Çuctuation in the simi- larity evaluation process for each reference vector. For hardware imple- mentation of this process, we propose a nanostructure, whose operation is described by a single-electron circuit. It positively uses Ô¨Çuctuation in quantum mechanical tunneling processes."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/de73998802680548b916f1947ffbad76-Abstract.html,Grammar Transfer in a Second Order Recurrent Neural Network,"Michiro Negishi, Stephen J. Hanson","It  has  been  known  that  people,  after  being  exposed  to  sentences  generated  by  an  artificial  grammar,  acquire  implicit  grammatical  knowledge and are able to transfer the knowledge to inputs that are  generated  by  a  modified  grammar.  We  show  that  a  second  order  recurrent neural network is able to transfer grammatical knowledge  from one language (generated by a Finite State Machine) to another  language which differ  both in vocabularies and syntax.  Representa(cid:173) tion of the grammatical knowledge in the network is analyzed using  linear  discriminant analysis."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/deb54ffb41e085fd7f69a75b6359c989-Abstract.html,Product Analysis: Learning to Model Observations as Products of Hidden Variables,"Brendan J. Frey, Anitha Kannan, Nebojsa Jojic","Factor analysis  and principal  components  analysis  can be  used  to  model linear relationships between observed variables  and linearly  map  high-dimensional  data to  a  lower-dimensional  hidden  space.  In  factor  analysis,  the  observations  are  modeled  as  a  linear  com(cid:173) bination  of normally  distributed  hidden  variables.  We  describe  a  nonlinear  generalization of factor  analysis,  called  ""product analy(cid:173) sis"",  that  models  the  observed  variables  as  a  linear  combination  of products  of normally  distributed  hidden  variables.  Just  as  fac(cid:173) tor  analysis  can  be  viewed  as  unsupervised  linear  regression  on  unobserved,  normally  distributed  hidden  variables,  product  anal(cid:173) ysis  can  be  viewed  as  unsupervised  linear  regression  on  products  of unobserved,  normally  distributed  hidden  variables.  The  map(cid:173) ping  between  the  data  and  the  hidden  space  is  nonlinear,  so  we  use  an  approximate variational  technique  for  inference  and learn(cid:173) ing.  Since  product  analysis  is  a  generalization  of factor  analysis,  product  analysis  always  finds  a  higher  data likelihood than factor  analysis.  We  give results on pattern recognition  and illumination(cid:173) invariant image clustering."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/df4fe8a8bcd5c95cdb640aa9793bb32b-Abstract.html,Matching Free Trees with Replicator Equations,Marcello Pelillo,"Motivated by our recent work on rooted tree matching, in this paper we provide a solution to the problem of matching two free (i.e., unrooted) trees by constructing an association graph whose maximal cliques are in one-to-one correspondence with maximal common subtrees. We then solve the problem using simple replicator dynamics from evolutionary game theory. Experiments on hundreds of uniformly random trees are presented. The results are impressive: despite the inherent inability of these simple dynamics to escape from local optima, they always returned a globally optimal solution."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/e0a209539d1e74ab9fe46b9e01a19a97-Abstract.html,Multiplicative Updates for Classification by Mixture Models,"Lawrence K. Saul, Daniel D. Lee","We investigate a learning algorithm for the classiÔ¨Åcation of nonnegative data by mixture models. Multiplicative update rules are derived that directly optimize the performance of these models as classiÔ¨Åers. The update rules have a simple closed form and an intuitive appeal. Our algorithm retains the main virtues of the Expectation-Maximization (EM) algorithm‚Äîits guarantee of monotonic im- provement, and its absence of tuning parameters‚Äîwith the added advantage of optimizing a discriminative objective function. The algorithm reduces as a spe- cial case to the method of generalized iterative scaling for log-linear models. The learning rate of the algorithm is controlled by the sparseness of the training data. We use the method of nonnegative matrix factorization (NMF) to discover sparse distributed representations of the data. This form of feature selection greatly accelerates learning and makes the algorithm practical on large problems. Ex- periments show that discriminatively trained mixture models lead to much better classiÔ¨Åcation than comparably sized models trained by EM."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/e3408432c1a48a52fb6c74d926b38886-Abstract.html,The Infinite Hidden Markov Model,"Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen","We show that it is possible to extend hidden Markov models to have a countably inÔ¨Ånite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inÔ¨Ånitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deÔ¨Åne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected num- ber of distinct hidden states in a Ô¨Ånite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inÔ¨Ånite‚Äî consider, for example, symbols being possible words appearing in En- glish text."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/e4dd5528f7596dcdf871aa55cfccc53c-Abstract.html,EM-DD: An Improved Multiple-Instance Learning Technique,"Qi Zhang, Sally A. Goldman","We  present  a  new  multiple-instance  (MI)  learning technique  (EM(cid:173) DD)  that  combines  EM  with  the  diverse  density  (DD)  algorithm.  EM-DD is a general-purpose MI algorithm that can be applied with  boolean  or  real-value  labels  and  makes  real-value  predictions.  On  the boolean Musk benchmarks, the EM-DD algorithm without any  tuning  significantly  outperforms  all  previous  algorithms.  EM-DD  is  relatively  insensitive to the number of relevant  attributes  in  the  data set  and  scales  up  well  to  large  bag  sizes.  Furthermore,  EM(cid:173) DD  provides  a  new  framework  for  MI  learning,  in  which  the  MI  problem  is  converted  to  a  single-instance  setting  by  using  EM  to  estimate the instance  responsible  for  the  label of the bag."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/e7e23670481ac78b3c4122a99ba60573-Abstract.html,Fast Parameter Estimation Using Green's Functions,"K. Wong, F. Li","We  propose  a  method  for  the  fast  estimation  of hyperparameters  in large networks, based on the linear response relation in the cav(cid:173) ity  method,  and  an  empirical  measurement  of  the  Green's  func(cid:173) tion.  Simulation results  show  that it is  efficient  and precise,  when  compared with cross-validation and other techniques which require  matrix inversion."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/ea5a486c712a91e48443cd802642223d-Abstract.html,Information-Geometric Decomposition in Spike Analysis,"Hiroyuki Nakahara, Shun-ichi Amari","We  present  an  information-geometric  measure  to  systematically  investigate  neuronal  firing  patterns,  taking  account  not  only  of  the  second-order  but  also  of higher-order  interactions.  We  begin  with the case of two  neurons for  illustration and show  how to test  whether or not any pairwise correlation in one period is significantly  different from  that in the other period.  In order to test such a  hy(cid:173) pothesis  of different  firing  rates,  the  correlation term needs  to  be  singled out 'orthogonally' to the firing rates, where the null hypoth(cid:173) esis might not be of independent firing.  This method is  also shown  to  directly  associate  neural  firing  with  behavior  via their  mutual  information,  which  is  decomposed  into  two  types  of  information,  conveyed  by  mean  firing  rate  and  coincident  firing,  respectively.  Then, we  show that these results, using the 'orthogonal' decompo(cid:173) sition,  are  naturally  extended  to  the  case  of three  neurons  and  n  neurons in general."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/ea6b2efbdd4255a9f1b3bbc6399b58f4-Abstract.html,Learning Body Pose via Specialized Maps,"R√≥mer Rosales, Stan Sclaroff","A  nonlinear  supervised learning model,  the  Specialized  Mappings  Architecture  (SMA),  is  described and applied to the estimation of  human  body  pose  from  monocular  images.  The  SMA  consists  of  several specialized forward mapping functions and an inverse map(cid:173) ping  function.  Each  specialized  function  maps  certain  domains  of the  input  space  (image  features)  onto  the  output  space  (body  pose parameters).  The key algorithmic problems faced are those of  learning the specialized  domains  and mapping functions  in an op(cid:173) timal way,  as well  as performing inference given inputs and knowl(cid:173) edge  of the  inverse  function.  Solutions  to  these  problems  employ  the EM  algorithm and alternating choices  of conditional  indepen(cid:173) dence assumptions.  Performance of the approach is  evaluated with  synthetic and real video sequences of human motion."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/ef41d488755367316f04fc0e0e9dc9fc-Abstract.html,Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning,"A. Bofill, D. P. Thompson, Alan F. Murray",Experimental data has  shown  that synaptic  strength  modification  in some types of biological neurons depends upon precise spike tim(cid:173) ing  differences  between  presynaptic  and  postsynaptic  spikes.  Sev(cid:173) eral  temporally-asymmetric  Hebbian  learning  rules  motivated  by  this  data have  been  proposed.  We  argue  that  such  learning  rules  are  suitable  to  analog  VLSI  implementation.  We  describe  an  eas(cid:173) ily  tunable circuit  to modify the weight of a  silicon spiking neuron  according to those  learning rules.  Test results from the fabrication  of the circuit using  a  O.6J.lm  CMOS  process  are  given.
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/ef8446f35513a8d6aa2308357a268a7e-Abstract.html,Model Based Population Tracking and Automatic Detection of Distribution Changes,"Igor V. Cadez, P. S. Bradley","Probabilistic mixture models are used for a broad range of data anal- ysis tasks such as clustering, classiÔ¨Åcation, predictive modeling, etc. Due to their inherent probabilistic nature, mixture models can easily be combined with other probabilistic or non-probabilistic techniques thus forming more complex data analysis systems. In the case of online data (where there is a stream of data available) models can be constantly up- dated to reÔ¨Çect the most current distribution of the incoming data. How- ever, in many business applications the models themselves represent a parsimonious summary of the data and therefore it is not desirable to change models frequently, much less with every new data point. In such a framework it becomes crucial to track the applicability of the mixture model and detect the point in time when the model fails to adequately represent the data. In this paper we formulate the problem of change detection and propose a principled solution. Empirical results over both synthetic and real-life data sets are presented.
1 Introduction and Notation
Consider a data set D = fx1; x2; : : : ; xng consisting of n independent, identically dis- tributed (iid) data points. In context of this paper the data points could be vectors, se- quences, etc. Further, consider a probabilistic mixture model that maps each data set to a real number, the probability of observing the data set:"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/f106b7f99d2cb30c3db1c3cc0fde9ccb-Abstract.html,Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering,"Mikhail Belkin, Partha Niyogi","Drawing  on  the  correspondence  between  the graph  Laplacian,  the  Laplace-Beltrami  operator  on  a  manifold , and  the  connections  to  the heat equation , we propose a geometrically motivated algorithm  for  constructing  a  representation  for  data sampled from  a  low  di(cid:173) mensional manifold embedded  in  a  higher  dimensional space.  The  algorithm  provides  a  computationally  efficient  approach  to  non(cid:173) linear  dimensionality  reduction  that  has  locality  preserving  prop(cid:173) erties  and  a  natural  connection  to  clustering.  Several  applications  are  considered. 
In  many  areas  of artificial  intelligence,  information retrieval  and  data mining, one  is  often  confronted  with intrinsically low  dimensional  data lying  in  a  very  high  di(cid:173) mensional space.  For example, gray scale  n  x  n  images of a  fixed  object taken with  a  moving camera yield data points in rn: n2 .  However , the intrinsic dimensionality of  the  space  of all  images  of t he  same object  is  the  number  of degrees  of freedom  of  the camera - in fact  the space  has the natural structure of a  manifold embedded  in  rn: n2 .  While  there  is  a  large  body  of work  on  dimensionality  reduction  in  general,  most  existing  approaches  do  not  explicitly  take  into  account  the  structure  of the  manifold  on  which  the  data  may  possibly  reside.  Recently,  there  has  been  some  interest  (Tenenbaum  et  aI,  2000 ;  Roweis  and  Saul,  2000)  in  the  problem of devel(cid:173) oping  low  dimensional  representations  of data  in  this  particular  context.  In  this  paper , we  present  a  new  algorithm and an accompanying framework of analysis for  geometrically motivated dimensionality reduction. 
The  core  algorithm  is  very  simple,  has  a  few  local  computations  and  one  sparse  eigenvalue  problem.  The  solution  reflects  th e  intrinsic  geom etric structure  of the  manifold.  The justification  comes from  the  role  of the  Laplacian  operator  in  pro(cid:173) viding  an  optimal emb edding.  The  Laplacian  of the  graph obtained  from  the  data  points may be viewed as an approximation to the Laplace-Beltrami operator defined  on  the  manifold.  The  emb edding  maps for  the  data come from  approximations to  a  natural  map  that  is  defined  on  the  entire  manifold.  The  framework  of analysis 
presented  here  makes  this  connection  explicit.  While  this  connection  is  known  to  geometers  and  specialists  in  spectral  graph  theory  (for  example , see  [1,  2])  to  the  best  of our  knowledge  we  do  not  know  of any  application  to  data  representation  yet.  The  connection  of the  Laplacian  to  the  heat  kernel  enables  us  to  choose  the  weights  of the graph in  a  principled  manner. 
The locality preserving character of the Laplacian Eigenmap algorithm makes it rel(cid:173) atively  insensitive  to outliers  and  noise.  A  byproduct  of this  is  that  the  algorithm  implicitly emphasizes the natural clusters in the data.  Connections to spectral clus(cid:173) tering  algorithms  developed  in  learning  and  computer  vision  (see  Shi  and  Malik ,  1997)  become  very  clear.  Following the  discussion  of Roweis  and  Saul  (2000) , and  Tenenbaum et  al  (2000),  we  note  that  the  biological  perceptual  apparatus  is  con(cid:173) fronted  with  high  dimensional stimuli from  which  it must  recover  low  dimensional  structure.  One might argue that if the approach to recovering such low-dimensional  structure  is  inherently  local , then  a  natural  clustering  will  emerge  and  thus  might  serve  as  the  basis for  the  development of categories  in biological perception."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/f15d337c70078947cfe1b5d6f0ed3f13-Abstract.html,A Bayesian Model Predicts Human Parse Preference and Reading Times in Sentence Processing,"S. Narayanan, Daniel Jurafsky","Narayanan and Jurafsky (1998) proposed that human language compre- hension can be modeled by treating human comprehenders as Bayesian reasoners, and modeling the comprehension process with Bayesian de- cision trees. In this paper we extend the Narayanan and Jurafsky model to make further predictions about reading time given the probability of difference parses or interpretations, and test the model against reading time data from a psycholinguistic experiment."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/f1981e4bd8a0d6d8462016d2fc6276b3-Abstract.html,MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation,"Anand Rangarajan, Alan L. Yuille","Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statis- tical physics. After Yedidia et al. demonstrated that belief prop- agation (cid:12)xed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guar- anteed to converge to a local minimum of the Bethe free energy. Yuille‚Äôs algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possi- ble and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpre- tation as pairwise mutual information minimization and marginal entropy maximization (MIME). Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local min- imum. Preliminary computer simulations are in agreement with this theoretical development."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/f3e52c300b822a8123e7ace55fe15c08-Abstract.html,Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning,"Gregory Z. Grudic, Lyle H. Ungar"," 
We address two open theoretical questions in Policy Gradient Reinforce- ment Learning. The Ô¨Årst concerns the efÔ¨Åcacy of using function approx- imation to represent the state action value function,  . Theory is pre- sented showing that linear function approximation representations of  can degrade the rate of convergence of performance gradient estimates by a factor of relative to when no function approximation of  is used, where is the number of basis functions in the function approximation representation. The sec- ond concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by is the number of possible actions. Experimen- 
  tal evidence is presented showing that these theoretical results lead to signiÔ¨Åcant improvement in the convergence properties of Policy Gradi- ent Reinforcement Learning algorithms.
is the number of possible actions and"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/f410588e48dc83f2822a880a68f78923-Abstract.html,A Generalization of Principal Components Analysis to the Exponential Family,"Michael Collins, S. Dasgupta, Robert E. Schapire","Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponen- tial family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss func- tions, and give examples on simulated data."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/f48c04ffab49ff0e5d1176244fdfb65c-Abstract.html,Minimax Probability Machine,"Gert Lanckriet, Laurent E. Ghaoui, Chiranjib Bhattacharyya, Michael I. Jordan","When  constructing  a  classifier,  the  probability  of correct  classifi(cid:173) cation of future  data points  should  be maximized.  In  the  current  paper  this  desideratum is  translated  in  a  very  direct  way  into  an  optimization  problem,  which  is  solved  using  methods  from  con(cid:173) vex  optimization.  We  also  show  how  to exploit  Mercer  kernels  in  this setting to obtain nonlinear decision  boundaries.  A  worst-case  bound  on the  probability of misclassification  of future  data is  ob(cid:173) tained explicitly."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/f6e794a75c5d51de081dbefa224304f9-Abstract.html,"Duality, Geometry, and Support Vector Regression","J. Bi, Kristin P. Bennett","We develop an intuitive geometric framework for support vector regression (SVR). By examining when (cid:15)-tubes exist, we show that SVR can be regarded as a classi(cid:12)cation problem in the dual space. Hard and soft (cid:15)-tubes are constructed by separating the convex or reduced convex hulls respectively of the training data with the response variable shifted up and down by (cid:15). A novel SVR model is proposed based on choosing the max-margin plane between the two shifted datasets. Maximizing the margin corresponds to shrinking the e(cid:11)ective (cid:15)-tube. In the proposed approach the e(cid:11)ects of the choices of all parameters become clear geometrically."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/f80bf05527157a8c2a7bb63b22f49aaa-Abstract.html,Blind Source Separation via Multinode Sparse Representation,"Michael Zibulevsky, Pavel Kisilev, Yehoshua Y. Zeevi, Barak A. Pearlmutter","We  consider a problem of blind  source  separation from  a set  of instan(cid:173) taneous  linear mixtures,  where  the  mixing  matrix  is  unknown.  It was  discovered recently, that  exploiting the  sparsity of sources  in  an  appro(cid:173) priate  representation  according  to  some  signal  dictionary,  dramatically  improves the quality  of separation.  In this work  we  use the property of  multi scale transforms, such as wavelet or wavelet packets, to decompose  signals  into  sets  of local  features  with  various  degrees  of sparsity.  We  use this intrinsic property  for selecting the  best (most sparse) subsets of  features  for further separation.  The performance of the algorithm is ver(cid:173) ified  on noise-free and noisy  data.  Experiments with simulated signals,  musical sounds and images demonstrate significant improvement of sep(cid:173) aration quality over previously reported results."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/f80ff32e08a25270b5f252ce39522f72-Abstract.html,Estimating the Reliability of ICA Projections,"Frank C. Meinecke, Andreas Ziehe, Motoaki Kawanabe, Klaus-Robert M√ºller","When applying unsupervised learning techniques like  ICA or tem(cid:173) poral decorrelation,  a  key  question  is  whether the  discovered  pro(cid:173) jections are reliable.  In other words:  can we  give  error  bars or can  we  assess  the  quality of our separation?  We  use  resampling meth(cid:173) ods  to  tackle  these  questions  and  show  experimentally  that  our  proposed variance estimations are strongly correlated to the sepa(cid:173) ration  error.  We  demonstrate that  this  reliability  estimation  can  be  used  to  choose  the  appropriate ICA-model,  to enhance signifi(cid:173) cantly the  separation performance, and,  most  important,  to mark  the components that have a  actual physical meaning.  Application  to  49-channel-data from  an  magneto encephalography  (MEG)  ex(cid:173) periment underlines the usefulness of our approach."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/f8bf09f5fceaea80e1f864a1b48938bf-Abstract.html,Learning from Infinite Data in Finite Time,"Pedro Domingos, Geoff Hulten","We  propose  the  following  general  method  for  scaling  learning  algorithms  to  arbitrarily  large  data  sets.  Consider  the  model  Mii  learned  by  the  algorithm  using  ni  examples  in  step  i  (ii  =  (nl , ... ,nm)) , and the model  Moo  that would  be learned using in(cid:173) finite  examples.  Upper-bound  the loss  L(Mii' M oo ) between  them  as  a  function  of ii, and  then  minimize  the  algorithm's  time  com(cid:173) plexity f(ii) subject to the constraint  that L(Moo , Mii ) be at most  f  with  probability  at  most  8.  We  apply  this  method  to  the  EM  algorithm for  mixtures  of Gaussians.  Preliminary experiments on  a  series  of large data sets  provide evidence of the potential of this  approach. 
1  An  Approach to Large-Scale  Learning 
Large  data sets  make  it  possible  to  reliably  learn  complex  models.  On  the  other  hand,  they require large  computational  resources  to learn from.  While  in  the past  the factor limiting the quality of learnable models was typically the quantity of data  available,  in many domains today data is  super-abundant, and the bottleneck is t he  time  required  to  process  it.  Many  algorithms for  learning  on  large  data sets  have  been  proposed,  but  in  order  to  achieve  scalability  they  generally  compromise  the  quality of the results to an unspecified degree.  We  believe this  unsatisfactory state  of affairs  is  avoidable,  and  in  this  paper  we  propose  a  general  method  for  scaling  learning algorithms to arbitrarily large databases without compromising the quality  of the  results.  Our  method  makes  it  possible  to  learn  in  finite  time  a  model  that  is  essentially  indistinguishable from  the  one  that  would  be  obtained  using  infinite  data. 
Consider the simplest possible learning problem:  estimating the mean of a  random  variable  x.  If we  have a  very large number of samples,  most  of them  are  probably  superfluous.  If we are willing to accept an error of at most f with probability at most  8,  Hoeffding bounds  [4]  (for example)  tell  us that, irrespective of the distribution of  x,  only n  =  ~(R/f)2 1n (2/8) samples  are needed,  where R is  x's range.  We  propose  to  extend  this  type  of  reasoning  beyond  learning  single  parameters,  to  learning  complex models.  The approach we  propose consists of three steps: 

Derive  an  upper  bound  on  the  relative  loss  between  the  finite-data  and  infinite-data models,  as  a  function  of the  number of samples  used  in  each  step of the finite-data algorithm. 
Derive  an  upper bound  on  the  time  complexity of the learning algorithm, 

as  a function  of the number of samples used in  each  step. 

Minimize  the  time  bound  (via  the  number  of samples  used  in  each  step) 

subject to target limits on the loss. 
In  this  paper we  exemplify  this  approach  using  the  EM algorithm  for  mixtures  of  Gaussians.  In  earlier  papers  we  applied  it  (or  an  earlier  version  of it)  to  decision  tree  induction  [2J  and  k-means  clustering  [3J.  Despite  its  wide  use,  EM  has  long  been criticized for its inefficiency  (see discussion following Dempster et al.  [1]),  and  has been considered unsuitable for large data sets [8J.  Many approaches to speeding  it up have  been proposed  (see Thiesson et al.  [6J  for  a  survey) .  Our method can be  seen as  an extension of progressive  sampling approaches like Meek et al.  [5J:  rather  than  minimize  the total number of samples  needed  by  the algorithm, we  minimize  the  number  needed  by each step,  leading  to  potentially much  greater savings;  and  we  obtain guarantees that do not depend on unverifiable extrapolations of learning  curves. 
2  A  Loss  Bound for  EM 
In  a  mixture of Gaussians  model,  each  D-dimensional data point  Xj  is  assumed  to  have been independently generated by  the following  process:  1)  randomly choose a  mixture component k;  2)  randomly generate a point from it according to a Gaussian  distribution with mean f-Lk  and covariance matrix  ~k. In  this paper we  will  restrict  ourselves to the case where the number K  of mixture components and the probabil(cid:173) ity of selection P(f-Lk)  and covariance matrix for  each  component are known.  Given  a  training  set  S  =  {Xl, ... , X N },  the  learning  goal  is  then  to  find  the  maximum(cid:173) likelihood  estimates  of the  means  f-Lk.  The EM algorithm  [IJ  accomplishes  this  by,  starting from  some set  of initial means, alternating until convergence between esti(cid:173) mating the probability p(f-Lk IXj) that each point was generated by each Gaussian (the  Estep), and computing the ML estimates of the means ilk  =  2::;':1 WjkXj /  2::f=l Wjk  (the  M  step),  where  Wjk  =  p(f-Lklxj)  from  the  previous  E  step.  In  the  basic  EM  algorithm,  all  N  examples  in  the  training set  are  used  in  each  iteration.  The goal  in  this  paper  is  to  speed  up  EM  by  using  only  ni  < N  examples  in  the  ith  itera(cid:173) tion,  while  guaranteeing that  the  means  produced  by  the  algorithm  do  not  differ  significantly from  those that would  be obtained with arbitrarily large N. 
Let  Mii  =  (ill , . . . , ilK) be the vector of mean estimates obtained by the finite-data  EM algorithm (i.e.,  using ni examples in iteration i), and let Moo  =  (f-L1,  ... ,f-LK)  be  the vector obtained  using infinite  examples at each  iteration.  In  order to proceed,  we  need  to  quantify  the  difference  between  Mii  and  Moo .  A  natural  choice  is  the  sum  of the squared errors  between  corresponding  means,  which  is  proportional to  the negative log-likelihood of the finite-data means given  the infinite-data ones: 
L(Mii' Moo ) =  L Ililk  - f-Lkl12  =  L L lilkd  -"
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/f8c0c968632845cd133308b1a494967f-Abstract.html,The g Factor: Relating Distributions on Features to Distributions on Images,"James M. Coughlan, Alan L. Yuille","We  describe  the  g-factor,  which  relates  probability  distributions  on image features  to  distributions on the  images  themselves.  The  g-factor  depends  only  on  our choice  of features  and  lattice  quanti(cid:173) zation and is  independent of the training image data.  We  illustrate  the importance  of the g-factor by analyzing how the parameters of  Markov Random Field (i.e.  Gibbs or log-linear) probability models  of images are learned from data by maximum likelihood estimation.  In particular, we study homogeneous MRF models which learn im(cid:173) age distributions in terms of clique potentials corresponding to fea(cid:173) ture  histogram  statistics  (d.  Minimax  Entropy  Learning  (MEL)  by  Zhu,  Wu  and  Mumford  1997  [11]) .  We  first  use  our  analysis  of the  g-factor  to  determine  when  the  clique  potentials  decouple  for  different  features .  Second,  we  show  that  clique  potentials  can  be  computed  analytically  by  approximating  the  g-factor.  Third,  we  demonstrate a  connection between this  approximation and the  Generalized Iterative Scaling algorithm (GIS),  due to Darroch and  Ratcliff  1972  [2],  for  calculating  potentials.  This  connection  en(cid:173) ables  us  to  use  GIS  to  improve  our  multinomial  approximation,  using  Bethe-Kikuchi[8]  approximations to simplify  the  GIS  proce(cid:173) dure.  We  support our analysis by computer simulations."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/fb2e203234df6dee15934e448ee88971-Abstract.html,Distribution of Mutual Information,Marcus Hutter,"The mutual information of two random variables z and J with joint  probabilities {7rij} is commonly used in learning Bayesian nets as  well as in many other fields. The chances 7rij are usually estimated  by the empirical sampling frequency nij In leading to a point es(cid:173) timate J(nij In) for the mutual information. To answer questions  like ""is J (nij In) consistent with zero?"" or ""what is the probability  that the true mutual information is much larger than the point es(cid:173) timate?"" one has to go beyond the point estimate. In the Bayesian  framework one can answer these questions by utilizing a (second  order) prior distribution p( 7r) comprising prior information about  7r. From the prior p(7r) one can compute the posterior p(7rln), from  which the distribution p(Iln) of the mutual information can be cal(cid:173) culated. We derive reliable and quickly computable approximations  for p(Iln). We concentrate on the mean, variance, skewness, and  kurtosis, and non-informative priors. For the mean we also give an  exact expression. Numerical issues and the range of validity are  discussed."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/fb87582825f9d28a8d42c5e5e5e8b23d-Abstract.html,A Maximum-Likelihood Approach to Modeling Multisensory Enhancement,"H. Colonius, A. Diederich","Multisensory response enhancement (MRE) is  the augmentation of  the  response  of a  neuron  to  sensory  input  of one  modality  by  si(cid:173) multaneous input from another modality.  The maximum likelihood  (ML)  model presented here modifies  the Bayesian model for  MRE  (Anastasio et al.)  by incorporating a decision strategy to maximize  the number of correct decisions.  Thus the ML model can also deal  with  the  important  tasks  of stimulus  discrimination  and  identifi(cid:173) cation in  the presence  of incongruent  visual  and auditory cues.  It  accounts  for  the  inverse  effectiveness  observed  in  neurophysiolog(cid:173) ical  recording  data,  and  it  predicts  a  functional  relation  between  uni- and bimodal levels  of discriminability that is  testable both in  neurophysiological and behavioral experiments."
2001,https://papers.nips.cc/paper_files/paper/2001,https://papers.nips.cc/paper_files/paper/2001/hash/fca0789e7891cbc0583298a238316122-Abstract.html,Model-Free Least-Squares Policy Iteration,"Michail G. Lagoudakis, Ronald Parr","We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration. Our method is model-free and completely off policy. We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efÔ¨Åcient use of sample experiences compared to pure temporal difference algorithms. LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems. Moreover, approximations learned by LSTD are strongly inÔ¨Çuenced by the visitation distribution over states. Our new algorithm, Least Squares Policy Iteration (LSPI) addresses these issues. The result is an off-policy method which can use (or reuse) data collected from any source. We have tested LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efÔ¨Åciently by merely observing a relatively small number of completely random trials."
