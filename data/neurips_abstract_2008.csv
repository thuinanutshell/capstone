year,proceeding_link,paper_link,title,authors,abstract
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/00411460f7c92d2124a67ea0f4cb5f85-Abstract.html,Near-minimax recursive density estimation on the binary hypercube,"Maxim Raginsky, Svetlana Lazebnik, Rebecca Willett, Jorge Silva","This paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions. For $d$ covariates, there are $2^d$ basis coefficients to estimate, which renders conventional approaches computationally prohibitive when $d$ is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error, and (2) the computational complexity is lower for sparser densities. Our method also allows for flexible control of the trade-off between mean-squared error and computational complexity."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html,Translated Learning: Transfer Learning across Different Feature Spaces,"Wenyuan Dai, Yuqiang Chen, Gui-rong Xue, Qiang Yang, Yong Yu","This paper investigates a new machine learning strategy called translated learning. Unlike many previous learning tasks, we focus on how to use labeled data from one feature space to enhance the classification of other entirely different learning spaces. For example, we might wish to use labeled text data to help learn a model for classifying image data, when the labeled images are difficult to obtain. An important aspect of translated learning is to build a ""bridge"" to link one feature space (known as the ""source space"") to another space (known as the ""target space"") through a translator in order to migrate the knowledge from source to target. The translated learning solution uses a language model to link the class labels to the features in the source spaces, which in turn is translated to the features in the target spaces. Finally, this chain of linkages is completed by tracing back to the instances in the target spaces. We show that this path of linkage can be modeled using a Markov chain and risk minimization. Through experiments on the text-aided image classification and cross-language classification tasks, we demonstrate that our translated learning framework can greatly outperform many state-of-the-art baseline methods."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/006f52e9102a8d3be2fe5614f42ba989-Abstract.html,Learning Bounded Treewidth Bayesian Networks,"Gal Elidan, Stephen Gould","With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufficiently expressive for generalization while also allowing for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overfitting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modifications and that is polynomial in the size of the graph and the treewidth bound. At the heart of our method is a triangulated graph that we dynamically update in a way that facilitates the addition of chain structures that increase the bound on the model's treewidth by at most one. We demonstrate the effectiveness of our ``treewidth-friendly'' method on several real-life datasets. Importantly, we also show that by using global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/01161aaa0b6d1345dd8fe4e481144d84-Abstract.html,Local Gaussian Process Regression for Real Time Online Model Learning,"Duy Nguyen-tuong, Jan R. Peters, Matthias Seeger","Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up standard Gaussian Process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Comparisons with other nonparametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and nu-SVR."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html,Grouping Contours Via a Related Image,"Praveen Srinivasan, Liming Wang, Jianbo Shi","Contours have been established in the biological and computer vision literatures as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To find matches for contours, we rely only on shape, which applies directly to all three modalities without modification, in constrant to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identified to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a final grouping on contours in the original image while simultaneously finding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0141a8aedb1b53970fac7c81dac79fbe-Abstract.html,Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds,"Jeremy Lewi, Robert Butera, David M. Schneider, Sarah Woolley, Liam Paninski","Sequential optimal design methods hold great promise for improving the efficiency of neurophysiology experiments. However, previous methods for optimal experimental design have incorporated only weak prior information about the underlying neural system (e.g., the sparseness or smoothness of the receptive field). Here we describe how to use stronger prior information, in the form of parametric models of the receptive field, in order to construct optimal stimuli and further improve the efficiency of our experiments. For example, if we believe that the receptive field is well-approximated by a Gabor function, then our method constructs stimuli that optimally constrain the Gabor parameters (orientation, spatial frequency, etc.) using as few experimental trials as possible. More generally, we may believe a priori that the receptive field lies near a known sub-manifold of the full parameter space; in this case, our method chooses stimuli in order to reduce the uncertainty along the tangent space of this sub-manifold as rapidly as possible. Applications to simulated and real data indicate that these methods may in many cases improve the experimental efficiency."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/02a32ad2669e6fe298e607fe7cc0e1a0-Abstract.html,Analyzing human feature learning as nonparametric Bayesian inference,"Thomas L. Griffiths, Joseph L. Austerweil","Almost all successful machine learning algorithms and cognitive models require powerful representations capturing the features that are relevant to a particular problem. We draw on recent work in nonparametric Bayesian statistics to define a rational model of human feature learning that forms a featural representation from raw sensory data without pre-specifying the number of features. By comparing how the human perceptual system and our rational model use distributional and category information to infer feature representations, we seek to identify some of the forces that govern the process by which people separate and combine sensory primitives to form features."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0336dcbab05b9d5ad24f4333c7658a0e-Abstract.html,From Online to Batch Learning with Cutoff-Averaging,Ofer Dekel,"We present cutoff averaging"", a technique for converting any conservative online learning algorithm into a batch learning algorithm. Most online-to-batch conversion techniques work well with certain types of online learning algorithms and not with others, whereas cutoff averaging explicitly tries to adapt to the characteristics of the online algorithm being converted. An attractive property of our technique is that it preserves the efficiency of the original online algorithm, making it approporiate for large-scale learning problems. We provide a statistical analysis of our technique and back our theoretical claims with experimental results."""
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/03c6b06952c750899bb03d998e631860-Abstract.html,Structured ranking learning using cumulative distribution networks,"Jim C. Huang, Brendan J. Frey","Ranking is at the heart of many information retrieval applications. Unlike standard regression or classification, in which we predict outputs independently, in ranking, we are interested in predicting structured outputs so that misranking one object can significantly affect whether we correctly rank the other objects. In practice, the problem of ranking involves a large number of objects to be ranked and either approximate structured prediction methods are required, or assumptions of independence between object scores must be made in order to make the problem tractable. We present a probabilistic method for learning to rank using the graphical modelling framework of cumulative distribution networks (CDNs), where we can take into account the structure inherent to the problem of ranking by modelling the joint cumulative distribution functions (CDFs) over multiple pairwise preferences. We apply our framework to the problem of document retrieval in the case of the OHSUMED benchmark dataset. We will show that the RankNet, ListNet and ListMLE probabilistic models can be viewed as particular instances of CDNs and that our proposed framework allows for the exploration of a broad class of flexible structured loss functionals for ranking learning."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/06997f04a7db92466a2baa6ebc8b872d-Abstract.html,Asynchronous Distributed Learning of Topic Models,"Padhraic Smyth, Max Welling, Arthur U. Asuncion","Distributed learning is a problem of fundamental interest in machine learning and cognitive science. In this paper, we present asynchronous distributed learning algorithms for two well-known unsupervised learning frameworks: Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Processes (HDP). In the proposed approach, the data are distributed across P processors, and processors independently perform Gibbs sampling on their local data and communicate their information in a local asynchronous manner with other processors. We demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard LDA and HDP samplers, but with significant improvements in computation time and memory. We show speedup results on a 730-million-word text corpus using 32 processors, and we provide perplexity results for up to 1500 virtual processors. As a stepping stone in the development of asynchronous HDP, a parallel HDP sampler is also introduced."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/072b030ba126b2f4b2374f342be9ed44-Abstract.html,Cascaded Classification Models: Combining Models for Holistic Scene Understanding,"Geremy Heitz, Stephen Gould, Ashutosh Saxena, Daphne Koller","One of the original goals of computer vision was to fully understand a natural scene. This requires solving several problems simultaneously, including object detection, labeling of meaningful regions, and 3d reconstruction. While great progress has been made in tackling each of these problems in isolation, only recently have researchers again been considering the difficult task of assembling various methods to the mutual benefit of all. We consider learning a set of such classification models in such a way that they both solve their own problem and help each other. We develop a framework known as Cascaded Classification Models (CCM), where repeated instantiations of these classifiers are coupled by their input/output variables in a cascade that improves performance at each level. Our method requires only a limited âblack boxâ interface with the models, allowing us to use very sophisticated, state-of-the-art classifiers without having to look under the hood. We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d scene reconstruction."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.html,Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes,"Ben Calderhead, Mark Girolami, Neil D. Lawrence","Identification and comparison of nonlinear dynamical systems using noisy and sparse experimental data is a vital task in many fields, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced. We present an accelerated sampling procedure which enables Bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of Gaussian processes (GP). Our method involves GP regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time. We demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0777d5c17d4066b82ab86dff8a46af6f-Abstract.html,Linear Classification and Selective Sampling Under Low Noise Conditions,"Giovanni Cavallanti, Nicolò Cesa-bianchi, Claudio Gentile","We provide a new analysis of an efficient margin-based algorithm for selective sampling in classification problems. Using the so-called Tsybakov low noise condition to parametrize the instance distribution, we show bounds on the convergence rate to the Bayes risk of both the fully supervised and the selective sampling versions of the basic algorithm. Our analysis reveals that, excluding logarithmic factors, the average risk of the selective sampler converges to the Bayes risk at rate $n^{-(1+\alpha)/(3+\alpha)}$, with labels being sampled at the same rate (here $n$ denotes the sample size, and $\alpha > 0$ is the exponent in the low noise condition). We compare this convergence rate to the rate $n^{-(1+\alpha)/(2+\alpha)}$ achieved by the fully supervised algorithm using all labels. Experiments on textual data reveal that simple variants of the proposed selective sampler perform much better than popular and similarly efficient competitors."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/077e29b11be80ab57e1a2ecabb7da330-Abstract.html,On the Efficient Minimization of Classification Calibrated Surrogates,"Richard Nock, Frank Nielsen","Bartlett et al (2006) recently proved that a ground condition for convex surrogates, classification calibration, ties up the minimization of the surrogates and classification risks, and left as an important problem the algorithmic questions about the minimization of these surrogates. In this paper, we propose an algorithm which provably minimizes any classification calibrated surrogate strictly convex and differentiable --- a set whose losses span the exponential, logistic and squared losses ---, with boosting-type guaranteed convergence rates under a weak learning assumption. A particular subclass of these surrogates, that we call balanced convex surrogates, has a key rationale that ties it to maximum likelihood estimation, zero-sum games and the set of losses that satisfy some of the most common requirements for losses in supervised learning. We report experiments on more than 50 readily available domains of 11 flavors of the algorithm, that shed light on new surrogates, and the potential of data dependent strategies to tune surrogates."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/07871915a8107172b3b5dc15a6574ad3-Abstract.html,"Unlabeled data: Now it helps, now it doesn't","Aarti Singh, Robert Nowak, Xiaojin Zhu","Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundancy of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing the situations in which unlabeled data can help have met with little success, and sometimes appear to conflict with each other and intuition. In this paper, we attempt to bridge the gap between practice and theory of semi-supervised learning. We develop a rigorous framework for analyzing the situations in which unlabeled data can help and quantify the improvement possible using finite sample error bounds. We show that there are large classes of problems for which SSL can significantly outperform supervised learning, in finite sample regimes and sometimes also in terms of error convergence rates."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/07a96b1f61097ccb54be14d6a47439b0-Abstract.html,Unifying the Sensory and Motor Components of Sensorimotor Adaptation,"Adrian Haith, Carl P. Jackson, R. C. Miall, Sethu Vijayakumar","Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a unified model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereffects. This unified model also makes the surprising prediction that force field adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We confirm this prediction with an experiment."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/07c5807d0d927dcd0980f86024e5208b-Abstract.html,Modeling human function learning with Gaussian processes,"Thomas L. Griffiths, Chris Lucas, Joseph Williams, Michael L. Kalish","Accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities: that people are estimating explicit functions, or that they are simply performing associative learning supported by similarity. We provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. Using the equivalence of Bayesian linear regression and Gaussian processes, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. We use this insight to define a Gaussian process model of human function learning that combines the strengths of both approaches."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/07cdfd23373b17c6b337251c22b7ea57-Abstract.html,Hebbian Learning of Bayes Optimal Decisions,"Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass","Uncertainty is omnipresent when we perceive or interact with our environment, and the Bayesian framework provides computational methods for dealing with it. Mathematical models for Bayesian decision making typically require datastructures that are hard to implement in neural networks. This article shows that even the simplest and experimentally best supported type of synaptic plasticity, Hebbian learning, in combination with a sparse, redundant neural code, can in principle learn to infer optimal Bayesian decisions. We present a concrete Hebbian learning rule operating on log-probability ratios. Modulated by reward-signals, this Hebbian plasticity rule also provides a new perspective for understanding how Bayesian inference could support fast reinforcement learning in the brain. In particular we show that recent experimental results by Yang and Shadlen [1] on reinforcement learning of probabilistic inference in primates can be modeled in this way."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/087408522c31eeb1f982bc0eaf81d35f-Abstract.html,An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis,"Gabriele Schweikert, Gunnar Rätsch, Christian K. Widmer, Bernhard Schölkopf","We study the problem of domain transfer for a supervised classification task in mRNA splicing. We consider a number of recent domain transfer methods from machine learning, including some that are novel, and evaluate them on genomic sequence data from model organisms of varying evolutionary distance. We find that in cases where the organisms are not closely related, the use of domain adaptation methods can help improve classification performance."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/08b255a5d42b89b0585260b6f2360bdd-Abstract.html,Kernel Change-point Analysis,"Zaïd Harchaoui, Eric Moulines, Francis R. Bach","We introduce a kernel-based method for change-point analysis within a sequence of temporal observations. Change-point analysis of an (unlabelled) sample of observations consists in, first, testing whether a change in the distribution occurs within the sample, and second, if a change occurs, estimating the change-point instant after which the distribution of the observations switches from one distribution to another different distribution. We propose a test statistics based upon the maximum kernel Fisher discriminant ratio as a measure of homogeneity between segments. We derive its limiting distribution under the null hypothesis (no change occurs), and establish the consistency under the alternative hypothesis (a change occurs). This allows to build a statistical hypothesis testing procedure for testing the presence of change-point, with a prescribed false-alarm probability and detection probability tending to one in the large-sample setting. If a change actually occurs, the test statistics also yields an estimator of the change-point location. Promising experimental results in temporal segmentation of mental tasks from BCI data and pop song indexation are presented."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/08c5433a60135c32e34f46a71175850c-Abstract.html,Biasing Approximate Dynamic Programming with a Lower Discount Factor,"Marek Petrik, Bruno Scherrer","Most algorithms for solving Markov decision processes rely on a discount factor, which ensures their convergence. In fact, it is often used in problems with is no intrinsic motivation. In this paper, we show that when used in approximate dynamic programming, an artificially low discount factor may significantly improve the performance on some problems, such as Tetris. We propose two explanations for this phenomenon. Our first justification follows directly from the standard approximation error bounds: using a lower discount factor may decrease the approximation error bounds. However, we also show that these bounds are loose, a thus their decrease does not entirely justify a better practical performance. We thus propose another justification: when the rewards are received only sporadically (as it is the case in Tetris), we can derive tighter bounds, which support a significant performance increase with a decrease in the discount factor."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0a113ef6b61820daa5611c870ed8d5ee-Abstract.html,Performance analysis for L\_2 kernel classification,"Jooseuk Kim, Clayton Scott","We provide statistical performance guarantees for a recently introduced kernel classifier that optimizes the $L_2$ or integrated squared error (ISE) of a difference of densities. The classifier is similar to a support vector machine (SVM) in that it is the solution of a quadratic program and yields a sparse classifier. Unlike SVMs, however, the $L_2$ kernel classifier does not involve a regularization parameter. We prove a distribution free concentration inequality for a cross-validation based estimate of the ISE, and apply this result to deduce an oracle inequality and consistency of the classifier on the sense of both ISE and probability of error. Our results can also be specialized to give performance guarantees for an existing method of $L_2$ kernel density estimation."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0a348ede8ac3768875037baca5de6e26-Abstract.html,Influence of graph construction on graph-based clustering measures,"Markus Maier, Ulrike V. Luxburg, Matthias Hein","Graph clustering methods such as spectral clustering are defined for general weighted graphs. In machine learning, however, data often is not given in form of a graph, but in terms of similarity (or distance) values between points. In this case, first a neighborhood graph is constructed using the similarities between the points and then a graph clustering algorithm is applied to this graph. In this paper we investigate the influence of the construction of the similarity graph on the clustering results. We first study the convergence of graph clustering criteria such as the normalized cut (Ncut) as the sample size tends to infinity. We find that the limit expressions are different for different types of graph, for example the r-neighborhood graph or the k -nearest neighbor graph. In plain words: Ncut on a kNN graph does something systematically different than Ncut on an r-neighborhood graph! This finding shows that graph clustering criteria cannot be studied independently of the kind of graph they are applied to. We also provide examples which show that these differences can be observed for toy and real data already for rather small sample sizes."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html,MDPs with Non-Deterministic Policies,"Mahdi M. Fard, Joelle Pineau","Markov Decision Processes (MDPs) have been extensively studied and used in the context of planning and decision-making, and many methods exist to find the optimal policy for problems modelled as MDPs. Although finding the optimal policy is sufficient in many domains, in certain applications such as decision support systems where the policy is executed by a human (rather than a machine), finding all possible near-optimal policies might be useful as it provides more flexibility to the person executing the policy. In this paper we introduce the new concept of non-deterministic MDP policies, and address the question of finding near-optimal non-deterministic policies. We propose two solutions to this problem, one based on a Mixed Integer Program and the other one based on a search algorithm. We include experimental results obtained from applying this framework to optimize treatment choices in the context of a medical decision support system."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0d0871f0806eae32d30983b62252da50-Abstract.html,Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization,"Liu Yang, Rong Jin, Rahul Sukthankar","The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classification. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classification accuracy becomes a challenge. We introduce Semi-supervised Learning with Weakly-Related Unlabeled Data"" (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classification tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal word-correlation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of state-of-the-art methods for inductive semi-supervised learning and text categorization; and we show that SSLW results in a significant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test beds."""
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0d7de1aca9299fe63f3e0041f02638a3-Abstract.html,Evaluating probabilities under high-dimensional latent variable models,"Iain Murray, Ruslan Salakhutdinov","We present a simple new Monte Carlo algorithm for evaluating probabilities of observations in complex latent variable models, such as Deep Belief Networks. While the method is based on Markov chains, estimates based on short runs are formally unbiased. In expectation, the log probability of a test set will be underestimated, and this could form the basis of a probabilistic bound. The method is much cheaper than gold-standard annealing-based methods and only slightly more expensive than the cheapest Monte Carlo methods. We give examples of the new method substantially improving simple variational bounds at modest extra cost."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0dbb3fb9a5cd1d5f8a9075b5bb8070aa-Abstract.html,Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation,"Lukas Kroc, Ashish Sabharwal, Bart Selman","We show that an important and computationally challenging solution space feature of the graph coloring problem (COL), namely the number of clusters of solutions, can be accurately estimated by a technique very similar to one for counting the number of solutions. This cluster counting approach can be naturally written in terms of a new factor graph derived from the factor graph representing the COL instance. Using a variant of the Belief Propagation inference framework, we can efﬁciently approximate cluster counts in random COL problems over a large range of graph densities. We illustrate the algorithm on instances with up to 100, 000 vertices. Moreover, we supply a methodology for computing the number of clus- ters exactly using advanced techniques from the knowledge compilation literature. This methodology scales up to several hundred variables."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0e01938fc48a2cfb5f2217fbfb00722d-Abstract.html,Supervised Bipartite Graph Inference,Yoshihiro Yamanishi,"We formulate the problem of bipartite graph inference as a supervised learning problem, and propose a new method to solve it from the viewpoint of distance metric learning. The method involves the learning of two mappings of the heterogeneous objects to a unified Euclidean space representing the network topology of the bipartite graph, where the graph is easy to infer. The algorithm can be formulated as an optimization problem in a reproducing kernel Hilbert space. We report encouraging results on the problem of compound-protein interaction network reconstruction from chemical structure data and genomic sequence data."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0e65972dce68dad4d52d063967f0a705-Abstract.html,Domain Adaptation with Multiple Sources,"Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh","This paper presents a theoretical analysis of the problem of adaptation with multiple sources. For each source domain, the distribution over the input points as well as a hypothesis with error at most \epsilon are given. The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions benefit from favorable theoretical guarantees. Our main result shows that, remarkably, for any fixed target function, there exists a distribution weighted combining rule that has a loss of at most \epsilon with respect to any target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3\epsilon. Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0efe32849d230d7f53049ddc4a4b0c60-Abstract.html,Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning,"Ali Rahimi, Benjamin Recht","Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. What are you doing?'' asked Minsky.I am training a randomly wired neural net to play tic-tac-toe,'' Sussman replied. Why is the net wired randomly?'' asked Minsky. Sussman replied,I do not want it to have any preconceptions of how to play.'' Minsky then shut his eyes. Why do you close your eyes?'' Sussman asked his teacher.So that the room will be empty,'' replied Minsky. At that moment, Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Specifically, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classification performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/0f840be9b8db4d3fbd5ba2ce59211f55-Abstract.html,Regularized Learning with Networks of Features,"Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar","For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable.  In predicting the topic of a document, we might know that two words are synonyms, or when performing image recognition, we know which pixels are adjacent.  Such synonymous or neighboring features are near-duplicates and should therefore be expected to have similar weights in a good model.  Here we present a framework for regularized learning in settings where one has prior knowledge about which features are expected to have similar and dissimilar weights.  This prior knowledge is encoded as a graph whose vertices represent features and whose edges represent similarities and dissimilarities between them.  During learning, each feature's weight is penalized by the amount it differs from the average weight of its neighbors.  For text classification, regularization using graphs of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods.  For sentiment analysis, feature graphs constructed from declarative human knowledge, as well as from auxiliary task learning, significantly improve prediction accuracy."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/12092a75caa75e4644fd2869f0b6c45a-Abstract.html,Breaking Audio CAPTCHAs,"Jennifer Tam, Jiri Simsa, Sean Hyde, Luis V. Ahn","CAP T C H A s are computer-generated tests that humans can pass but current computer systems cannot. CAP T C H A s provide a method for automatically distinguishing a human from a computer program, and therefore can protect Web services from abuse by so-called ""bots."" Most CAP T C H A s consist of distorted images, usually text, for which a user must provide some description. Unfortunately, visual CAP T C H A s limit access to the millions of visually impaired people using the Web. Audio CAP T C H A s were created to solve this accessibility issue; however, the security of audio CAP T C H A s was never formally tested. Some visual CAP T C H A s have been broken using machine learning techniques, and we propose using similar ideas to test the security of audio CAP T C H A s . Audio CAP T C H A s are generally composed of a set of words to be identified, layered on top of noise. We analyzed the security of current audio CAP T CH A s from popular Web sites by using AdaBoost, SVM, and k-NN, and achieved correct solutions for test samples with accuracy up to 71%. Such accuracy is enough to consider these CAPTCHAs broken. Training several different machine learning algorithms on different types of audio CAP T C H A s allowed us to analyze the strengths and weaknesses of the algorithms so that we could suggest a design for a more robust audio CAPTCHA.
1
Int r o d u c t i o n
CAP T C H A s [1] are automated tests designed to tell computers and humans apart by presenting users with a problem that humans can solve but current computer programs cannot. Because CAPTCHAs can distinguish between humans and computers with high probability, they are used for many different security applications: they prevent bots from voting continuously in online polls, automatically registering for millions of spam email accounts, automatically purchasing tickets to buy out an event, etc. Once a CAP T C H A is broken (i.e., computer programs can successfully pass the test), bots can impersonate humans and gain access to services that they should not. Therefore, it is important for CAP T C H A s to be secure. To pass the typical visual CAP T C H A , a user must correctly type the characters displayed in an image of distorted text. Many visual CAP T C H A s have been broken with machine
learning techniques [2]-[3], though some remain secure against such attacks. Because visually impaired users who surf the Web using screen-reading programs cannot see this type of CAPTCHA, audio CAP T C H A s were created. Typical audio CAP T C H A s consist of one or several speakers saying letters or digits at randomly spaced intervals. A user must correctly identify the digits or characters spoken in the audio file to pass the CAP T C H A . To make this test difficult for current computer systems, specifically automatic speech recognition (ASR) programs, background noise is injected into the audio files. Since no official evaluation of existing audio CAP T C H A s has been reported, we tested the security of audio CAP T C H A s used by many popular Web sites by running machine learning experiments designed to break them. In the next section, we provide an overview of the literature related to our project. Section 3 describes our methods for creating training data, and section 4 describes how we create classifiers that can recognize letters, digits, and noise. In section 5, we discuss how we evaluated our methods on widely used audio CAP T C H A s and we give our results. In particular, we show that the audio CAP T C H A s used by sites such as Google and Digg are susceptible to machine learning attacks. Section 6 mentions the proposed design of a new more secure audio CAP T C H A based on our findings.
2
Lit e r a t u r e r e v i e w
To break the audio CAP T C H A s , we derive features from the CAP T C H A audio and use several machine learning techniques to perform ASR on segments of the CAPTCHA. There are many popular techniques for extracting features from speech. The three techniques we use are mel-frequency cepstral coefficients (MFCC), perceptual linear prediction (PLP), and relative spectral transform-PLP (RAS TA - P L P) . MFCC is one of the most popular speech feature representations used. Similar to a fast Fourier transform (FF T ) , MFCC transforms an audio file into frequency bands, but (unlike FF T ) MFCC uses mel-frequency bands, which are better for approximating the range of frequencies humans hear. PLP was designed to extract speaker-independent features from speech [4]. Therefore, by using PLP and a variant such as RAS TA - P L P, we were able to train our classifiers to recognize letters and digits independently of who spoke them. Since many different people recorded the digits used in one of the types of audio CAP T C H A s we tested, PLP and RAS TA- PL P were needed to extract the features that were most useful for solving them. In [4]-[5], the authors conducted experiments on recognizing isolated digits in the presence of noise using both PLP and RAS TA - PL P. However, the noise used consisted of telephone or microphone static caused by recording in different locations. The audio CAP T C H A s we use contain this type of noise, as well as added vocal noise and/or music, which is supposed to make the automated recognition process much harder. The authors of [3] emphasize how many visual CAP T C HA s can be broken by successfully splitting the task into two smaller tasks: segmentation and recognition. We follow a similar approach in that we first automatically split the audio into segments, and then we classify these segments as noise or words. In early March 2008, concurrent to our work, the blog of Wintercore Labs [6] claimed to have successfully broken the Google audio CAP T C H A . After reading their Web article and viewing the video of how they solve the CAP T C H A s , we are unconvinced that the process is entirely automatic, and it is unclear what their exact pass rate is. Because we are unable to find any formal technical analysis of this program, we can neither be sure of its accuracy nor the extent of its automation.
3
Cr e a t i o n o f tra i n i n g dat a
Since automated programs can attempt to pass a CAPTCHA repeatedly, a CAPTCHA is essentially broken when a program can pass it more than a non-trivial fraction of the time; e.g., a 5% pass rate is enough. Our approach to breaking the audio CAP T C H A s began by first splitting the audio files into segments of noise or words: for our experiments, the words were spoken letters or digits. We used manual transcriptions of the audio CAP T C H A s to get information regarding the location of each spoken word within the audio file. We were able to label our segments accurately by using this information.
We gathered 1,000 audio CAP T C H A s from each of the following Web sites: google.com, digg.com, and an older version of the audio CAP T C H A in recaptcha.net. Each of the CAP T C H A s was annotated with the information regarding letter/digit locations provided by the manual transcriptions. For each type of CAPTCHA, we randomly selected 900 samples for training and used the remaining 100 for testing. Using the digit/letter location information provided in the manual CAP T C H A transcriptions, each training CAP T C H A is divided into segments of noise, the letters a-z, or the digits 0-9, and labeled as such. We ignore the annotation information of the CAP T C H A s we use for testing, and therefore we cannot identify the size of those segments. Instead, each test CAP T C H A is divided into a number of fixed-size segments. The segments with the highest energy peaks are then classified using machine learning techniques (Figure 1). Since the size of a feature vector extracted from a segment generally depends on the size of the segment, using fixed-size segments allows each segment to be described with a feature vector of the same length. We chose the window size by listening to a few training segments and adjusted accordingly to ensure that the segment contained the entire digit/letter. There is undoubtedly a more optimal way of selecting the window size, however, we were still able to break the three CAP T C H A s we tested with our method.
Figure 1: A test audio CAP T C H A with the fixed-size segments containing the highest energy peaks highlighted. The information provided in the manual transcriptions of the audio CAP T C H A s contains a list of the time intervals within which words are spoken. However, these intervals are of variable size and the word might be spoken anywhere within this interval. To provide fixeds ize segments for training, we developed the following heuristic. First, divide each file into variable-size segments using the time intervals provided and label each segment accordingly. Then, within each segment, detect the highest energy peak and return its fixed-size neighborhood labeled with the current segment's label. This heuristic achieved nearly perfect labeling accuracy for the training set. Rare mistakes occurred when the highest energy peak of a digit or letter segment corresponded to noise rather than to a digit or letter. To summarize this subsection, an audio file is transformed into a set of fixed-size segments labeled as noise, a digit between 0 and 9, or a letter between a and z. These segments are then used for training. Classifiers are trained for one type of CAPTCHA at a time.
4
C l a s s i f i e r co n s t r u c t i o n
From the training data we extracted five sets of features using twelve MFCCs and twelfth-
order spectral (SPEC) and cepstral (CEPS) coefficients from PLP Matlab functions for extracting these features were provided online Voicebox package. We use AdaBoost, SVM, and k-NN algorithms digit and letter recognition. We detail our implementation of following subsections. 4 .1 AdaBoost
and RAS TA- P L P. The at [7] and as part of the to implement automated each algorithm in the
Using decision stumps as weak classifiers for AdaBoost, anywhere from 11 to 37 ensemble classifiers are built. The number of classifiers built depends on which type of CAPTCHA we are solving. Each classifier trains on all the segments associated with that type of CAP T C H A , and for the purpose of building a single classifier, segments are labeled by either -1 (negative example) or +1 (positive example). Using cross-validation, we choose to use 50 iterations for our AdaBoost algorithm. A segment can then be classified as a particular letter, digit, or noise according to the ensemble classifier that outputs the number closest to 1. 4 .2 S u p p o rt v ecto r ma ch i n e
To conduct digit recognition with SVM, we used the C++ implementations of libSVM [8] version 2.85 with C-SMV and RBF kernel. First, all feature values are scaled to the range of -1 to 1 as suggested by [8]. The scale parameters are stored so that test samples can be scaled accordingly. Then, a single multiclass classifier is created for each set of features using all the segments for a particular type of CAPTCHA. We use cross-validation and grid search to discover the optimal slack penalty (C=32) and kernel parameter (=0.011). 4 .3 k - n e a re s t n e i g h b o r ( k - N N )
We use k-NN as our final method for classifying digits. For each type of CAP T C H A , five different classifiers are created by using all of the training data and the five sets of features associated with that particular type of CAP T C H A . Again we use cross-validation to discover the optimal parameter, in this case k=1. We use Euclidian distance as our distance metric.
5
Ass e s s m e n t of cu r r e n t a u d i o CAPTCHAs
Our method for solving CAP T C H A s iteratively extracts an audio segment from a CAP T C H A , inputs the segment to one of our digit or letter recognizers, and outputs the label for that segment. We continue this process until the maximum solution size is reached or there are no unlabeled segments left. Some of the CAPTCHAs we evaluated have solutions that vary in length. Our method ensures that we get solutions of varying length that are never longer than the maximum solution length. A segment to be classified is identified by taking the neighborhood of the highest energy peak of an as yet unlabeled part of the CAP T C H A . Once a prediction of the solution to the CAPTCHA is computed, it is compared to the true solution. Given that at least one of the audio CAP T C H A s allows users to make a mistake in one of the digits (e.g., reCAPTCHA), we compute the pass rate for each of the different types of CAPTCHAs with all of the following conditions:     The prediction matches the true solution exactly. Inserting one digit into the prediction would make it match the solution exactly. Replacing one digit in the prediction would make it match the solution exactly. Removing one digit from the prediction would make it match the solution exactly.
However, since we are only sure that these conditions apply to reCAPTCHA audio CAP T C H A s , we also calculate the percentage of exact solution matches in our results for each type of audio CAP T C H A . These results are described in the following subsections. 5 .1 Goog le
Google audio CAP T C H A s consist of one speaker saying random digits 0-9, the phrase ""once again,"" followed by the exact same recorded sequence of digits originally presented.
The background noise consists of human voices speaking backwards at varying volumes. A solution can range in length from five to eight words. We set our classifier to find the 12 loudest segments and classify these segments as digits or noise. Because the phrase ""once again"" marks the halfway point of the CAPTCHA, we preprocessed the audio to only serve this half of the CAP T C H A to our classifiers. It is important to note, however, that the classifiers were always able to identify the segment containing ""once again,"" and these segments were identified before all other segments. Therefore, if necessary, we could have had our system cut the file in half after first labeling this segment. For AdaBoost, we create 12 classifiers: one classifier for each digit, one for noise, and one for the phrase ""once again."" Our results ( Tab le 1) show that at best we achieved a 90% pass rate using the ""one mistake"" passing conditions and a 66% exact solution match rate. Using SVM and the ""one mistake"" passing conditions, at best we achieve a 92% pass rate and a 67% exact solution match. For k-NN, the ""one mistake"" pass rate is 62% and the exact solution match rate is 26%. Table 1: Google audio CAP T C H A results: Maximum 67% accuracy was achieved by SVM. Classifiers Used AdaBoost One mistake MFCC PLPS PEC PLPCEP S RAS TA P L PS PEC RAS TA P L PCEP S 5 .2 Digg 88% 90% 90% 88% exact match 61% 66% 66% 48% SVM one mistake 92% 90% 92% 90% exact match 67% 67% 67% 61% k-NN one mistake 30% 60% 62% 29% exact match 1% 26% 23% 1%"
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/1385974ed5904a438616ff7bdb3f7439-Abstract.html,Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection,"Takafumi Kanamori, Shohei Hido, Masashi Sugiyama","We address the problem of estimating the ratio of two probability density functions (a.k.a.~the importance). The importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally very efficient and numerically stable. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efficient than competing approaches."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/1458e7509aa5f47ecfb92536e7dd1dc7-Abstract.html,Differentiable Sparse Coding,"J. A. Bagnell, David M. Bradley","Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1) that promotes sparsity. We show how smoother priors can pre- serve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁ- ciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of appli- cations, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/147ebe637038ca50a1265abac8dea181-Abstract.html,Inferring rankings under constrained sensing,"Srikanth Jagabathula, Devavrat Shah","Motivated by applications like elections, web-page ranking, revenue maximization etc., we consider the question of inferring popular rankings using constrained data. More specifically, we consider the problem of inferring a probability distribution over the group of permutations using its first order marginals. We first prove that it is not possible to recover more than O(n) permutations over n elements with the given information. We then provide a simple and novel algorithm that can recover up to O(n) permutations under a natural stochastic model; in this sense, the algorithm is optimal. In certain applications, the interest is in recovering only the most popular (or mode) ranking. As a second result, we provide an algorithm based on the Fourier Transform over the symmetric group to recover the mode under a natural majority condition; the algorithm turns out to be a maximum weight matching on an appropriately defined weighted bipartite graph. The questions considered are also thematically related to Fourier Transforms over the symmetric group and the currently popular topic of compressed sensing."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/149e9677a5989fd342ae44213df68868-Abstract.html,Sparse Convolved Gaussian Processes for Multi-output Regression,"Mauricio Alvarez, Neil D. Lawrence","We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/1595af6435015c77a7149e92a551338e-Abstract.html,A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context,"Abhinav Gupta, Jianbo Shi, Larry S. Davis","Integrating semantic and syntactic analysis is essential for document analysis. Using an analogous reasoning, we present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufficient for representing context. Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or ''informative'' background(context). labeling. We present a ''shape-aware'' model which utilizes contour information for efficient and accurate labeling of features in the image. Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/15d4e891d784977cacbfcbb00c48f133-Abstract.html,Multi-task Gaussian Process Learning of Robot Inverse Dynamics,"Christopher Williams, Stefan Klanke, Sethu Vijayakumar, Kian M. Chai","The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneficial to be able to learn this function for adaptive control. A given robot manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem. We show how the structure of the inverse dynamics problem gives rise to a multi-task Gaussian process prior over functions, where the inter-task similarity depends on the underlying dynamic parameters. Experiments demonstrate that this multi-task formulation generally improves performance over either learning only on single tasks or pooling the data over all tasks."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/1651cf0d2f737d7adeab84d339dbabd3-Abstract.html,Efficient Inference in Phylogenetic InDel Trees,"Alexandre Bouchard-côté, Dan Klein, Michael I. Jordan","Accurate and efficient inference in evolutionary trees is a central problem in computational biology. Realistic models require tracking insertions and deletions along the phylogenetic tree, making inference challenging. We propose new sampling techniques that speed up inference and improve the quality of the samples. We compare our method to previous approaches and show performance improvement on metrics evaluating multiple sequence alignment and reconstruction of ancestral sequences."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/1679091c5a880faf6fb5e6087eb1b2dc-Abstract.html,Estimating vector fields using sparse basis field expansions,"Stefan Haufe, Vadim V. Nikulin, Andreas Ziehe, Klaus-Robert Müller, Guido Nolte","We introduce a novel framework for estimating vector fields using sparse basis field expansions (S-FLEX). The notion of basis fields, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector field, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that significantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/170c944978496731ba71f34c25826a34-Abstract.html,"Probabilistic detection of short events, with application to critical care monitoring","Norm Aleks, Stuart Russell, Michael G. Madden, Diane Morabito, Kristan Staudenmayer, Mitchell Cohen, Geoffrey T. Manley","We describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit (ICU). In particular, we consider the arterial-line blood pressure sensor, which is subject to frequent data artifacts that cause false alarms in the ICU and make the raw data almost useless for automated decision making. The problem is complicated by the fact that the sensor data are acquired at fixed intervals whereas the events causing data artifacts may occur at any time and have durations that may be significantly shorter than the data collection interval. We show that careful modeling of the sensor, combined with a general technique for detecting sub-interval events and estimating their duration, enables effective detection of artifacts and accurate estimation of the underlying blood pressure values."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/185e65bc40581880c4f2c82958de8cfe-Abstract.html,Spectral Clustering with Perturbed Data,"Ling Huang, Donghui Yan, Nina Taft, Michael I. Jordan","Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data sets are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a specification of permitted loss in clustering performance."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/19b650660b253761af189682e03501dd-Abstract.html,"Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG","Julia Owen, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan, David P. Wipf","The synchronous brain activity measured via MEG (or EEG) can be interpreted as arising from a collection (possibly large) of current dipoles or sources located throughout the cortex. Estimating the number, location, and orientation of these sources remains a challenging task, one that is significantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity, sensor noise, and other artifacts. This paper derives an empirical Bayesian method for addressing each of these issues in a principled fashion. The resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations. Robust interference suppression is also easily incorporated. In a restricted setting, the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi-component dipoles even in the presence of correlations, unlike a variety of existing Bayesian localization methods or common signal processing techniques such as beamforming and sLORETA. Empirical results on both simulated and real data sets verify the efficacy of this approach."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/1be3bc32e6564055d5ca3e5a354acbef-Abstract.html,Multi-stage Convex Relaxation for Learning with Sparse Regularization,Tong Zhang,"We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: (1) Heuristic methods such as gradient descent that only find a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. (2) Convex relaxation such as $L_1$-regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-$L_1$ regularization. Our performance bound shows that the procedure is superior to the standard $L_1$ convex relaxation for learning sparse targets. Experiments confirm the effectiveness of this method on some simulation and real data."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/1ce927f875864094e3906a4a0b5ece68-Abstract.html,Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images,"Vincent Q. Vu, Bin Yu, Thomas Naselaris, Kendrick Kay, Jack Gallant, Pradeep K. Ravikumar","We propose a novel hierarchical, nonlinear model that predicts brain activity in area V1 evoked by natural images. In the study reported here brain activity was measured by means of functional magnetic resonance imaging (fMRI), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (~ 2mm cube) of brain tissue. Our model, which we call the SpAM V1 model, is based on the reasonable assumption that fMRI measurements reflect the (possibly nonlinearly) pooled, rectified output of a large population of simple and complex cells in V1. It has a hierarchical filtering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called âpooled-complexâ cells). The pooling stage then obtains the measured fMRI signals as a sparse additive model (SpAM) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed. Our results show that the SpAM V1 model predicts fMRI responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells. Furthermore, the spatial receptive fields, frequency tuning and orientation tuning curves of the SpAM V1 model estimated for each voxel appears to be consistent with the known properties of V1, and with previous analyses of this data set. A visualization procedure applied to the SpAM V1 model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html,A Scalable Hierarchical Distributed Language Model,"Andriy Mnih, Geoffrey E. Hinton","Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the non-hierarchical language model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical models and achieve state-of-the-art performance."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/1fc214004c9481e4c8073e85323bfd4b-Abstract.html,Supervised Exponential Family Principal Component Analysis via Convex Optimization,Yuhong Guo,"Recently, supervised dimensionality reduction has been gaining attention, owing to the realization that data labels are often available and strongly suggest important underlying structures in the data. In this paper, we present a novel convex supervised dimensionality reduction approach based on exponential family PCA and provide a simple but novel form to project new testing data into the embedded space. This convex approach successfully avoids the local optima of the EM learning. Moreover, by introducing a sample-based multinomial approximation to exponential family models, it avoids the limitation of the prevailing Gaussian assumptions of standard PCA, and produces a kernelized formulation for nonlinear supervised dimensionality reduction. A training algorithm is then devised based on a subgradient bundle method, whose scalability can be gained through a coordinate descent procedure. The advantage of our global optimization approach is demonstrated by empirical results over both synthetic and real data."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/2291d2ec3b3048d1a6f86c2c4591b7e0-Abstract.html,Stochastic Relational Models for Large-scale Dyadic Data using MCMC,"Shenghuo Zhu, Kai Yu, Yihong Gong","Stochastic relational models provide a rich family of choices for learning and predicting dyadic data between two sets of entities. It generalizes matrix factorization to a supervised learning problem that utilizes attributes of objects in a hierarchical Bayesian framework. Previously empirical Bayesian inference was applied, which is however not scalable when the size of either object sets becomes tens of thousands. In this paper, we introduce a Markov chain Monte Carlo (MCMC) algorithm to scale the model to very large-scale dyadic data. Both superior scalability and predictive accuracy are demonstrated on a collaborative filtering problem, which involves tens of thousands users and a half million items."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/23ce1851341ec1fa9e0c259de10bf87c-Abstract.html,Online Models for Content Optimization,"Deepak Agarwal, Bee-chung Chen, Pradheep Elango, Nitin Motgi, Seung-taek Park, Raghu Ramakrishnan, Scott Roy, Joe Zachariah","We describe a new content publishing system that selects articles to serve to a user, choosing from an editorially programmed pool that is frequently refreshed. It is now deployed on a major Internet portal, and selects articles to serve to hundreds of millions of user visits per day, significantly increasing the number of user clicks over the original manual approach, in which editors periodically selected articles to display. Some of the challenges we face include a dynamic content pool, short article lifetimes, non-stationary click-through rates, and extremely high traffic volumes. The fundamental problem we must solve is to quickly identify which items are popular(perhaps within different user segments), and to exploit them while they remain current. We must also explore the underlying pool constantly to identify promising alternatives, quickly discarding poor performers. Our approach is based on tracking per article performance in near real time through online models. We describe the characteristics and constraints of our application setting, discuss our design choices, and show the importance and effectiveness of coupling online models with a simple randomization procedure. We discuss the challenges encountered in a production online content-publishing environment and highlight issues that deserve careful attention. Our analysis of this application also suggests a number of future research avenues."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/24681928425f5a9133504de568f5f6df-Abstract.html,Robust Regression and Lasso,"Huan Xu, Constantine Caramanis, Shie Mannor","We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to $\ell_1$ regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a unified robustness perspective."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/24b16fede9a67c9251d3e7c7161c83ac-Abstract.html,Hierarchical Fisher Kernels for Longitudinal Data,"Zhengdong Lu, Jeffrey Kaye, Todd K. Leen","We develop new techniques for time series classification based on hierarchical Bayesian generative models (called mixed-effect models) and the Fisher kernel derived from them. A key advantage of the new formulation is that one can compute the Fisher information matrix despite varying sequence lengths and sampling times. We therefore can avoid the ad hoc replacement of Fisher information matrix with the identity matrix commonly used in literature, which destroys the geometrical grounding of the kernel construction. In contrast, our construction retains the proper geometric structure resulting in a kernel that is properly invariant under change of coordinates in the model parameter space. Experiments on detecting cognitive decline show that classifiers based on the proposed kernel out-perform those based on generative models and other feature extraction routines."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/26e359e83860db1d11b6acca57d8ea88-Abstract.html,Correlated Bigram LSA for Unsupervised Language Model Adaptation,"Yik-cheung Tam, Tanja Schultz","We propose using correlated bigram LSA for unsupervised LM adaptation for automatic speech recognition. The model is trained using efficient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. Our approach can be scalable to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results show that applying unigram and bigram LSA together yields 6%--8% relative perplexity reduction and 0.6% absolute character error rates (CER) reduction compared to applying only unigram LSA on the Mandarin RT04 test set. Comparing with the unadapted baseline, our approach reduces the absolute CER by 1.2%."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/2723d092b63885e0d7c260cc007e8b9d-Abstract.html,The Infinite Factorial Hidden Markov Model,"Jurgen V. Gael, Yee W. Teh, Zoubin Ghahramani",We introduces a new probability distribution over a potentially infinite number of binary Markov chains which we call the Markov Indian buffet process. This process extends the IBP to allow temporal dependencies in the hidden variables. We use this stochastic process to build a nonparametric extension of the factorial hidden Markov model. After working out an inference scheme which combines slice sampling and dynamic programming we demonstrate how the infinite factorial hidden Markov model can be used for blind source separation.
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/287e03db1d99e0ec2edb90d079e142f3-Abstract.html,Sparse Signal Recovery Using Markov Random Fields,"Volkan Cevher, Marco F. Duarte, Chinmay Hegde, Richard Baraniuk","Compressive Sensing (CS) combines sampling and compression into a single sub-Nyquist linear measurement process for sparse and compressible signals. In this paper, we extend the theory of CS to include signals that are concisely represented in terms of a graphical model. In particular, we use Markov Random Fields (MRFs) to represent sparse signals whose nonzero coefficients are clustered. Our new model-based reconstruction algorithm, dubbed Lattice Matching Pursuit (LaMP), stably recovers MRF-modeled signals using many fewer measurements and computations than the current state-of-the-art algorithms."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/288cc0ff022877bd3df94bc9360b9c5d-Abstract.html,Clustering via LP-based Stabilities,"Nikos Komodakis, Nikos Paragios, Georgios Tziritas","A novel center-based clustering algorithm is proposed in this paper. We first formulate clustering as an NP-hard linear integer program and we then use linear programming and the duality theory to derive the solution of this optimization problem. This leads to an efficient and very general algorithm, which works in the dual domain, and can cluster data based on an arbitrary set of distances. Despite its generality, it is independent of initialization (unlike EM-like methods such as K-means), has guaranteed convergence, and can also provide online optimality bounds about the quality of the estimated clustering solutions. To deal with the most critical issue in a center-based clustering algorithm (selection of cluster centers), we also introduce the notion of stability of a cluster center, which is a well defined LP-based quantity that plays a key role to our algorithm's success. Furthermore, we also introduce, what we call, the margins (another key ingredient in our algorithm), which can be roughly thought of as dual counterparts to stabilities and allow us to obtain computationally efficient approximations to the latter. Promising experimental results demonstrate the potentials of our method."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/2a2717956118b4d223ceca17ce3865e2-Abstract.html,Spike Feature Extraction Using Informative Samples,"Zhi Yang, Qi Zhao, Wentai Liu","This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/2a9d121cd9c3a1832bb6d2cc6bd7a8a7-Abstract.html,Fast Computation of Posterior Mode in Multi-Level Hierarchical Models,"Liang Zhang, Deepak Agarwal","Multi-level hierarchical models provide an attractive framework for incorporating correlations induced in a response variable organized in a hierarchy. Model fitting is challenging, especially for hierarchies with large number of nodes. We provide a novel algorithm based on a multi-scale Kalman filter that is both scalable and easy to implement. For non-Gaussian responses, quadratic approximation to the log-likelihood results in biased estimates. We suggest a bootstrap strategy to correct such biases. Our method is illustrated through simulation studies and analyses of real world data sets in health care and online advertising."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/2ab56412b1163ee131e1246da0955bd1-Abstract.html,An improved estimator of Variance Explained in the presence of noise,"Ralf M. Haefner, Bruce G. Cumming","A crucial part of developing mathematical models of how the brain works is the quantification of their success. One of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model. Unfortunately, this metric is biased due to the intrinsic variability in the data. This variability is in principle unexplainable by the model. We derive a simple analytical modification of the traditional formula that significantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying Variance Explained by the model class. Our estimator advances on previous work by a) accounting for the uncertainty in the noise estimate, b) accounting for overfitting due to free model parameters mitigating the need for a separate validation data set and c) adding a conditioning term. We apply our new estimator to binocular disparity tuning curves of a set of macaque V1 neurons and find that on a population level almost all of the variance unexplained by Gabor functions is attributable to noise."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/2bf283c05b601f21364d052ca0ec798d-Abstract.html,The Infinite Hierarchical Factor Regression Model,"Piyush Rai, Hal Daume","We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman's coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/2dace78f80bc92e6d7493423d729448e-Abstract.html,Deep Learning with Kernel Regularization for Visual Recognition,"Kai Yu, Wei Xu, Yihong Gong","In this paper we focus on training deep neural networks for visual recognition tasks. One challenge is the lack of an informative regularization on the network parameters, to imply a meaningful control on the computed function. We propose a training strategy that takes advantage of kernel methods, where an existing kernel function represents useful prior knowledge about the learning task of interest. We derive an efficient algorithm using stochastic gradient descent, and demonstrate very positive results in a wide range of visual recognition tasks."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/2e2c080d5490760af59d0baf5acbb84e-Abstract.html,Learning Transformational Invariants from Natural Movies,"Charles Cadieu, Bruno A. Olshausen","We describe a hierarchical, probabilistic model that learns to extract complex motion from movies of the natural environment. The model consists of two hidden layers: the first layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables. The second layer learns the higher-order structure among the time-varying phase variables. After training on natural movies, the top layer units discover the structure of phase-shifts within the first layer. We show that the top layer units encode transformational invariants: they are selective for the speed and direction of a moving pattern, but are invariant to its spatial structure (orientation/spatial-frequency). The diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in V1 and MT. In addition, the model demonstrates how feedback from higher levels can influence representations at lower levels as a by-product of inference in a graphical model."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/30bb3825e8f631cc6075c0f87bb4978c-Abstract.html,On Bootstrapping the ROC Curve,"Patrice Bertail, Stéphan J. Clémençcon, Nicolas Vayatis","This paper is devoted to thoroughly investigating how to bootstrap the ROC curve, a widely used visual tool for evaluating the accuracy of test/scoring statistics in the bipartite setup. The issue of confidence bands for the ROC curve is considered and a resampling procedure based on a smooth version of the empirical distribution called the smoothed bootstrap"" is introduced. Theoretical arguments and simulation results are presented to show that the ""smoothed bootstrap"" is preferable to a ""naive"" bootstrap in order to construct accurate confidence bands."""
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/31b3b31a1c2f8a370206f111127c0dbd-Abstract.html,QUIC-SVD: Fast SVD Using Cosine Trees,"Michael P. Holmes, Jr. Isbell, Charles Lee, Alexander G. Gray","The Singular Value Decomposition is a key operation in many machine learning methods. Its computational cost, however, makes it unscalable and impractical for the massive-sized datasets becoming common in applications. We present a new method, QUIC-SVD, for fast approximation of the full SVD with automatic sample size minimization and empirical relative error control. Previous Monte Carlo approaches have not addressed the full SVD nor benefited from the efficiency of automatic, empirically-driven sample sizing. Our empirical tests show speedups of several orders of magnitude over exact SVD. Such scalability should enable QUIC-SVD to meet the needs of a wide array of methods and applications."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/31fefc0e570cb3860f2a6d4b38c6490d-Abstract.html,A Massively Parallel Digital Learning Processor,"Hans P. Graf, Srihari Cadambi, Venkata Jakkula, Murugan Sankaradass, Eric Cosatto, Srimat Chakradhar, Igor Dourdanovic","We present a new, massively parallel architecture for accelerating machine learning algorithms, based on arrays of variable-resolution arithmetic vector processing elements (VPE). Groups of VPEs operate in SIMD (single instruction multiple data) mode, and each group is connected to an independent memory bank. In this way memory bandwidth scales with the number of VPE, and the main data flows are local, keeping power dissipation low. With 256 VPEs, implemented on two FPGA (field programmable gate array) chips, we obtain a sustained speed of 19 GMACS (billion multiply-accumulate per sec.) for SVM training, and 86 GMACS for SVM classification. This performance is more than an order of magnitude higher than that of any FPGA implementation reported so far. The speed on one FPGA is similar to the fastest speeds published on a Graphics Processor for the MNIST problem, despite a clock rate of the FPGA that is six times lower. High performance at low clock rates makes this massively parallel architecture particularly attractive for embedded applications, where low power dissipation is critical. Tests with Convolutional Neural Networks and other learning algorithms are under way now."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/322f62469c5e3c7dc3e58f5a4d1ea399-Abstract.html,Characterizing response behavior in multisensory perception with conflicting cues,"Rama Natarajan, Iain Murray, Ladan Shams, Richard S. Zemel","We explore a recently proposed mixture model approach to understand- ing interactions between conﬂicting sensory cues. Alternative model for- mulations, differing in their sensory noise models and inference methods, are compared based on their ﬁt to experimental data. Heavy-tailed sen- sory likelihoods yield a better description of the subjects’ response behavior than standard Gaussian noise models. We study the underlying cause for this result, and then present several testable predictions of these models."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/335f5352088d7d9bf74191e006d8e24c-Abstract.html,The Gaussian Process Density Sampler,"Iain Murray, David MacKay, Ryan P. Adams","We present the Gaussian Process Density Sampler (GPDS), an exchangeable generative model for use in nonparametric Bayesian density estimation. Samples drawn from the GPDS are consistent with exact, independent samples from a fixed density function that is a transformation of a function drawn from a Gaussian process prior. Our formulation allows us to infer an unknown density from data using Markov chain Monte Carlo, which gives samples from the posterior distribution over density functions and from the predictive distribution on data space. We can also infer the hyperparameters of the Gaussian process. We compare this density modeling technique to several existing techniques on a toy problem and a skull-reconstruction task."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html,Cyclizing Clusters via Zeta Function of a Graph,"Deli Zhao, Xiaoou Tang","Detecting underlying clusters from large-scale data plays a central role in machine learning research. In this paper, we attempt to tackle clustering problems for complex data of multiple distributions and large multi-scales. To this end, we develop an algorithm named Zeta $l$-links, or Zell which consists of two parts: Zeta merging with a similarity graph and an initial set of small clusters derived from local $l$-links of the graph. More specifically, we propose to structurize a cluster using cycles in the associated subgraph. A mathematical tool, Zeta function of a graph, is introduced for the integration of all cycles, leading to a structural descriptor of the cluster in determinantal form. The popularity character of the cluster is conceptualized as the global fusion of variations of the structural descriptor by means of the leave-one-out strategy in the cluster. Zeta merging proceeds, in the agglomerative fashion, according to the maximum incremental popularity among all pairwise clusters. Experiments on toy data, real imagery data, and real sensory data show the promising performance of Zell. The $98.1\%$ accuracy, in the sense of the normalized mutual information, is obtained on the FRGC face data of 16028 samples and 466 facial clusters. The MATLAB codes of Zell will be made publicly available for peer evaluation."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/37bc2f75bf1bcfe8450a1a41c200364c-Abstract.html,Integrating Locally Learned Causal Structures with Overlapping Variables,"David Danks, Clark Glymour, Robert E. Tillman","In many domains, data are distributed among datasets that share only some variables; other recorded variables may occur in only one dataset. There are several asymptotically correct, informative algorithms that search for causal information given a single dataset, even with missing values and hidden variables. There are, however, no such reliable procedures for distributed data with overlapping variables, and only a single heuristic procedure (Structural EM). This paper describes an asymptotically correct procedure, ION, that provides all the information about structure obtainable from the marginal independence relations. Using simulated and real data, the accuracy of ION is compared with that of Structural EM, and with inference on complete, unified data."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/38913e1d6a7b94cb0f55994f679f5956-Abstract.html,A mixture model for the evolution of gene expression in non-homogeneous datasets,"Gerald Quon, Yee W. Teh, Esther Chan, Timothy Hughes, Michael Brudno, Quaid D. Morris","We address the challenge of assessing conservation of gene expression in complex, non-homogeneous datasets. Recent studies have demonstrated the success of probabilistic models in studying the evolution of gene expression in simple eukaryotic organisms such as yeast, for which measurements are typically scalar and independent. Models capable of studying expression evolution in much more complex organisms such as vertebrates are particularly important given the medical and scientific interest in species such as human and mouse. We present a statistical model that makes a number of significant extensions to previous models to enable characterization of changes in expression among highly complex organisms. We demonstrate the efficacy of our method on a microarray dataset containing diverse tissues from multiple vertebrate species. We anticipate that the model will be invaluable in the study of gene expression patterns in other diverse organisms as well, such as worms and insects."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/38af86134b65d0f10fe33d30dd76442e-Abstract.html,An Homotopy Algorithm for the Lasso with Online Observations,"Pierre Garrigues, Laurent E. Ghaoui","It has been shown that the problem of $\ell_1$-penalized least-square regression commonly referred to as the Lasso or Basis Pursuit DeNoising leads to solutions that are sparse and therefore achieves model selection. We propose in this paper an algorithm to solve the Lasso with online observations. We introduce an optimization problem that allows us to compute an homotopy from the current solution to the solution after observing a new data point. We compare our method to Lars and present an application to compressed sensing with sequential observations. Our approach can also be easily extended to compute an homotopy from the current solution to the solution after removing a data point, which leads to an efficient algorithm for leave-one-out cross-validation."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/38b3eff8baf56627478ec76a704e9b52-Abstract.html,Adaptive Martingale Boosting,"Phil Long, Rocco Servedio","In recent work Long and Servedio LS05short presented a ``martingale boosting'' algorithm that works by constructing a branching program over weak classifiers and has a simple analysis based on elementary properties of random walks. LS05short showed that this martingale booster can tolerate random classification noise when it is run with a noise-tolerant weak learner; however, a drawback of the algorithm is that it is not adaptive, i.e. it cannot effectively take advantage of variation in the quality of the weak classifiers it receives. In this paper we present a variant of the original martingale boosting algorithm and prove that it is adaptive. This adaptiveness is achieved by modifying the original algorithm so that the random walks that arise in its analysis have different step size depending on the quality of the weak learner at each stage. The new algorithm inherits the desirable properties of the original LS05short algorithm, such as random classification noise tolerance, and has several other advantages besides adaptiveness: it requires polynomially fewer calls to the weak learner than the original algorithm, and it can be used with confidence-rated weak hypotheses that output real values rather than Boolean predictions."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/39059724f73a9969845dfe4146c5660e-Abstract.html,Fast High-dimensional Kernel Summations Using the Monte Carlo Multipole Method,"Dongryeol Lee, Alexander G. Gray","We propose a new fast Gaussian summation algorithm for high-dimensional datasets with high accuracy. First, we extend the original fast multipole-type methods to use approximation schemes with both hard and probabilistic error. Second, we utilize a new data structure called subspace tree which maps each data point in the node to its lower dimensional mapping as determined by any linear dimension reduction method such as PCA. This new data structure is suitable for reducing the cost of each pairwise distance computation, the most dominant cost in many kernel methods. Our algorithm guarantees probabilistic relative error on each kernel sum, and can be applied to high-dimensional Gaussian summations which are ubiquitous inside many kernel methods as the key computational bottleneck. We provide empirical speedup results on low to high-dimensional datasets up to 89 dimensions."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/3dc4876f3f08201c7c76cb71fa1da439-Abstract.html,ICA based on a Smooth Estimation of the Differential Entropy,"Lev Faivishevsky, Jacob Goldberger","In this paper we introduce the MeanNN approach for estimation of main information theoretic measures such as differential entropy, mutual information and divergence. As opposed to other nonparametric approaches the MeanNN results in smooth differentiable functions of the data samples with clear geometrical interpretation. Then we apply the proposed estimators to the ICA problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods. The improved performance on the proposed ICA algorithm is demonstrated on standard tests in comparison with state-of-the-art techniques."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/3df1d4b96d8976ff5986393e8767f5b2-Abstract.html,Support Vector Machines with a Reject Option,"Yves Grandvalet, Alain Rakotomamonjy, Joseph Keshet, Stéphane Canu","We consider the problem of binary classification where the classifier may abstain instead of classifying each observation. The Bayes decision rule for this setup, known as Chow's rule, is defined by two thresholds on posterior probabilities. From simple desiderata, namely the consistency and the sparsity of the classifier, we derive the double hinge loss function that focuses on estimating conditional probabilities only in the vicinity of the threshold points of the optimal decision rule. We show that, for suitable kernel machines, our approach is universally consistent. We cast the problem of minimizing the double hinge loss as a quadratic program akin to the standard SVM optimization problem and propose an active set method to solve it efficiently. We finally provide preliminary experimental results illustrating the interest of our constructive approach to devising loss functions."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/3e77a14629775492504515dc4b23deda-Abstract.html,Generative versus discriminative training of RBMs for classification of fMRI images,"Tanya Schmah, Geoffrey E. Hinton, Steven L. Small, Stephen Strother, Richard S. Zemel","Neuroimaging datasets often have a very large number of voxels and a very small number of training cases, which means that overfitting of models for this data can become a very serious problem. Working with a set of fMRI images from a study on stroke recovery, we consider a classification task for which logistic regression performs poorly, even when L1- or L2- regularized. We show that much better discrimination can be achieved by fitting a generative model to each separate condition and then seeing which model is most likely to have generated the data. We compare discriminative training of exactly the same set of models, and we also consider convex blends of generative and discriminative training."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/42e77b63637ab381e8be5f8318cc28a2-Abstract.html,Particle Filter-based Policy Gradient in POMDPs,"Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos","Our setting is a Partially Observable Markov Decision Process with continuous state, observation and action spaces. Decisions are based on a Particle Filter for estimating the belief state given past observations. We consider a policy gradient approach for parameterized policy optimization. For that purpose, we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy, focusing on Finite Difference (FD) techniques. We show that the naive FD is subject to variance explosion because of the non-smoothness of the resampling procedure. We propose a more sophisticated FD method which overcomes this problem and establish its consistency."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/49ae49a23f67c759bf4fc791ba842aa2-Abstract.html,Algorithms for Infinitely Many-Armed Bandits,"Yizao Wang, Jean-yves Audibert, Rémi Munos",We consider multi-armed bandit problems where the number of arms is larger than the possible number of experiments. We make a stochastic assumption on the mean-reward of a new selected arm which characterizes its probability of being a near-optimal arm. Our assumption is weaker than in previous works. We describe algorithms based on upper-confidence-bounds applied to a restricted set of randomly selected arms and provide upper-bounds on the resulting expected regret. We also derive a lower-bound which matchs (up to logarithmic factors) the upper-bound in some cases.
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/4a533591763dfa743a13affab1a85793-Abstract.html,On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor,"Christoph Kolodziejski, Bernd Porr, Minija Tamosiunaite, Florentin Wörgötter",In this theoretical contribution we provide mathematical proof that two of the most important classes of network learning - correlation-based differential Hebbian learning and reward-based temporal difference learning - are asymptotically equivalent when timing the learning with a local modulatory signal. This opens the opportunity to consistently reformulate most of the abstract reinforcement learning framework from a correlation based perspective that is more closely related to the biophysics of neurons.
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/4b0a59ddf11c58e7446c9df0da541a84-Abstract.html,Gates,"Tom Minka, John Winn","Gates are a new notation for representing mixture models and context-sensitive independence in factor graphs. Factor graphs provide a natural representation for message-passing algorithms, such as expectation propagation. However, message passing in mixture models is not well captured by factor graphs unless the entire mixture is represented by one factor, because the message equations have a containment structure. Gates capture this containment structure graphically, allowing both the independences and the message-passing equations for a model to be readily visualized. Different variational approximations for mixture models can be understood as different ways of drawing the gates in a model. We present general equations for expectation propagation and variational message passing in the presence of gates."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/4e0928de075538c593fbdabb0c5ef2c3-Abstract.html,Multi-Level Active Prediction of Useful Image Annotations for Recognition,"Sudheendra Vijayanarasimhan, Kristen Grauman","We introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples. We propose to allow the category-learner to strategically choose what annotations it receives---based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation. We construct a multiple-instance discriminative classifier based on the initial training data. Then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next. After each request, the current classifier is incrementally updated. Unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e.g., a full segmentation on some images and a present/absent flag on others). As a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/500e75a036dc2d7d2fec5da1b71d36cc-Abstract.html,MAS: a multiplicative approximation scheme for probabilistic inference,"Ydo Wexler, Christopher Meek","We propose a multiplicative approximation scheme (MAS) for inference problems in graphical models, which can be applied to various inference algorithms. The method uses $\epsilon$-decompositions which decompose functions used throughout the inference procedure into functions over smaller sets of variables with a known error $\epsilon$. MAS translates these local approximations into bounds on the accuracy of the results. We show how to optimize $\epsilon$-decompositions and provide a fast closed-form solution for an $L_2$ approximation. Applying MAS to the Variable Elimination inference algorithm, we introduce an algorithm we call DynaDecomp which is extremely fast in practice and provides guaranteed error bounds on the result. The superior accuracy and efficiency of DynaDecomp is demonstrated."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/53e3a7161e428b65688f14b84d61c610-Abstract.html,Learning Hybrid Models for Image Annotation with Partially Labeled Data,"Xuming He, Richard S. Zemel","Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difficult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on two real image datasets demonstrate the effectiveness of incorporating the latent structure."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/5487315b1286f907165907aa8fc96619-Abstract.html,Efficient Sampling for Gaussian Process Inference using Control Variables,"Neil D. Lawrence, Magnus Rattray, Michalis K. Titsias","Sampling functions in Gaussian process (GP) models is challenging because of the highly correlated posterior distribution. We describe an efficient Markov chain Monte Carlo algorithm for sampling from the posterior process of the GP model. This algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function. At each iteration, the algorithm proposes new values for the control variables and generates the function from the conditional GP prior. The control variable input locations are found by continuously minimizing an objective function. We demonstrate the algorithm on regression and classification problems and we use it to estimate the parameters of a differential equation model of gene regulation."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/5751ec3e9a4feab575962e78e006250d-Abstract.html,An Online Algorithm for Maximizing Submodular Functions,"Matthew Streeter, Daniel Golovin","We present an algorithm for solving a broad class of online resource allocation problems. Our online algorithm can be applied in environments where abstract jobs arrive one at a time, and one can complete the jobs by investing time in a number of abstract activities, according to some schedule. We assume that the fraction of jobs completed by a schedule is a monotone, submodular function of a set of pairs (v,t), where t is the time invested in activity v. Under this assumption, our online algorithm performs near-optimally according to two natural metrics: (i) the fraction of jobs completed within time T, for some fixed deadline T > 0, and (ii) the average time required to complete each job. We evaluate our algorithm experimentally by using it to learn, online, a schedule for allocating CPU time among solvers entered in the 2007 SAT solver competition."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/5b69b9cb83065d403869739ae7f0995e-Abstract.html,"On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization","Sham M. Kakade, Karthik Sridharan, Ambuj Tewari","We provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes. These bounds make short work of providing a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either $L_2$ or $L_1$ constraints), margin bounds (including both $L_2$ and $L_1$ margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and $L_2$ covering numbers (with $L_p$ norm constraints and relative entropy constraints). In addition to providing a unified analysis, the results herein provide some of the sharpest risk and margin bounds (improving upon a number of previous results). Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction (up to a constant factor of 2)."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/5f0f5e5f33945135b874349cfbed4fb9-Abstract.html,Bayesian Exponential Family PCA,"Shakir Mohamed, Zoubin Ghahramani, Katherine A. Heller","Principal Components Analysis (PCA) has become established as one of the key tools for dimensionality reduction when dealing with real valued data. Approaches such as exponential family PCA and non-negative matrix factorisation have successfully extended PCA to non-Gaussian data types, but these techniques fail to take advantage of Bayesian inference and can suffer from problems of overfitting and poor generalisation. This paper presents a fully probabilistic approach to PCA, which is generalised to the exponential family, based on Hybrid Monte Carlo sampling. We describe the model which is based on a factorisation of the observed data matrix, and show performance of the model on both synthetic and real data."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/5f2c22cb4a5380af7ca75622a6426917-Abstract.html,Sequential effects: Superstition or rational behavior?,"Angela J. Yu, Jonathan D. Cohen","In a variety of behavioral tasks, subjects exhibit an automatic and apparently sub-optimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reflect the inadvertent engagement of fundamental mechanisms critical for adapting to changing statistics in the natural environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential filtering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential filter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that near-optimal tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/605ff764c617d3cd28dbbdd72be8f9a2-Abstract.html,PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning,"Chunhua Shen, Alan Welsh, Lei Wang","In this work, we consider the problem of learning a positive semidefinite matrix. The critical issue is how to preserve positive semidefiniteness during the course of learning. Our algorithm is mainly inspired by LPBoost [1] and the general greedy convex optimization framework of Zhang [2]. We demonstrate the essence of the algorithm, termed PSDBoost (positive semidefinite Boosting), by focusing on a few different applications in machine learning. The proposed PSDBoost algorithm extends traditional Boosting algorithms in that its parameter is a positive semidefinite matrix with trace being one instead of a classifier. PSDBoost is based on the observation that any trace-one positive semidefinitematrix can be decomposed into linear convex combinations of trace-one rank-one matrices, which serve as base learners of PSDBoost. Numerical experiments are presented."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/61b4a64be663682e8cb037d9719ad8cd-Abstract.html,Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control,"Lavi Shpigelman, Hagai Lalazar, Eilon Vaadia","Using machine learning algorithms to decode intended behavior from neural activity serves a dual purpose. First, these tools can be used to allow patients to interact with their environment through a Brain-Machine Interface (BMI). Second, analysis of the characteristics of such methods can reveal the significance of various features of neural activity, stimuli and responses to the encoding-decoding task. In this study we adapted, implemented and tested a machine learning method, called Kernel Auto-Regressive Moving Average (KARMA), for the task of inferring movements from neural activity in primary motor cortex. Our version of this algorithm is used in an on-line learning setting and is updated when feedback from the last inferred sequence become available. We first used it to track real hand movements executed by a monkey in a standard 3D motor control task. We then applied it in a closed-loop BMI setting to infer intended movement, while arms were restrained, allowing a monkey to perform the task using the BMI alone. KARMA is a recurrent method that learns a nonlinear model of output dynamics. It uses similarity functions (termed kernels) to compare between inputs. These kernels can be structured to incorporate domain knowledge into the method. We compare KARMA to various state-of-the-art methods by evaluating tracking performance and present results from the KARMA based BMI experiments."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/61f2585b0ebcf1f532c4d1ec9a7d51aa-Abstract.html,Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell_1$-regularized MLE,"Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar","We consider the problem of estimating the graph structure associated with a Gaussian Markov random field (GMRF) from i.i.d. samples. We study the performance of study the performance of the 1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufficient conditions on (n, p, d) for the 1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = (d2 log(p)), with the error decaying as O(exp(-c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/62e7f2e090fe150ef8deb4466fdc81b3-Abstract.html,"Stress, noradrenaline, and realistic prediction of mouse behaviour using reinforcement learning","Carmen Sandi, Wulfram Gerstner, Gediminas Lukšys","Suppose we train an animal in a conditioning experiment. Can one predict how a given animal, under given experimental conditions, would perform the task? Since various factors such as stress, motivation, genetic background, and previous errors in task performance can influence animal behaviour, this appears to be a very challenging aim. Reinforcement learning (RL) models have been successful in modeling animal (and human) behaviour, but their success has been limited because of uncertainty as to how to set meta-parameters (such as learning rate, exploitation-exploration balance and future reward discount factor) that strongly influence model performance. We show that a simple RL model whose metaparameters are controlled by an artificial neural network, fed with inputs such as stress, affective phenotype, previous task performance, and even neuromodulatory manipulations, can successfully predict mouse behaviour in the ""hole-box"" - a simple conditioning task. Our results also provide important insights on how stress and anxiety affect animal learning, performance accuracy, and discounting of future rewards, and on how noradrenergic systems can interact with these processes."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/63923f49e5241343aa7acb6a06a751e7-Abstract.html,How memory biases affect information transmission: A rational analysis of serial reproduction,"Jing Xu, Thomas L. Griffiths","Many human interactions involve pieces of information being passed from one person to another, raising the question of how this process of information transmission is affected by the capacities of the agents involved. In the 1930s, Sir Frederic Bartlett explored the influence of memory biases in âserial reproductionâ of information, in which one personâs reconstruction of a stimulus from memory becomes the stimulus seen by the next person. These experiments were done using relatively uncontrolled stimuli such as pictures and stories, but suggested that serial reproduction would transform information in a way that reflected the biases inherent in memory. We formally analyze serial reproduction using a Bayesian model of reconstruction from memory, giving a general result characterizing the effect of memory biases on information transmission. We then test the predictions of this account in two experiments using simple one-dimensional stimuli. Our results provide theoretical and empirical justification for the idea that serial reproduction reflects memory biases."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/647bba344396e7c8170902bcf2e15551-Abstract.html,Diffeomorphic Dimensionality Reduction,"Christian Walder, Bernhard Schölkopf",This paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points. We argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a natural way of ensuring that pairwise distances are approximately preserved. Accordingly we develop an algorithm which diffeomorphically maps the data near to a lower dimensional subspace and then projects onto that subspace. The problem of solving for the mapping is transformed into one of solving for an Eulerian flow field which we compute using ideas from kernel methods. We demonstrate the efficacy of our approach on various real world data sets.
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/65658fde58ab3c2b6e5132a39fae7cb9-Abstract.html,Using Bayesian Dynamical Systems for Motion Template Libraries,"Silvia Chiappa, Jens Kober, Jan R. Peters","Motor primitives or motion templates have become an important concept for both modeling human motor control as well as generating robot behaviors using imitation learning. Recent impressive results range from humanoid robot movement generation to timing models of human motions. The automatic generation of skill libraries containing multiple motion templates is an important step in robot learning. Such a skill learning system needs to cluster similar movements together and represent each resulting motion template as a generative model which is subsequently used for the execution of the behavior by a robot system. In this paper, we show how human trajectories captured as multidimensional time-series can be clustered using Bayesian mixtures of linear Gaussian state-space models based on the similarity of their dynamics. The appropriate number of templates is automatically determined by enforcing a parsimonious parametrization. As the resulting model is intractable, we introduce a novel approximation method based on variational Bayes, which is especially designed to enable the use of efficient inference algorithms. On recorded human Balero movements, this method is not only capable of finding reasonable motion templates but also yields a generative model which works well in the execution of this complex task on a simulated anthropomorphic SARCOS arm."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/65cc2c8205a05d7379fa3a6386f710e1-Abstract.html,An Efficient Sequential Monte Carlo Algorithm for Coalescent Clustering,"Dilan Gorur, Yee W. Teh","We propose an efficient sequential Monte Carlo inference scheme for the recently proposed coalescent clustering model (Teh et al, 2008). Our algorithm has a quadratic runtime while those in (Teh et al, 2008) is cubic. In experiments, we were surprised to find that in addition to being more efficient, it is also a better sequential Monte Carlo sampler than the best in (Teh et al, 2008), when measured in terms of variance of estimated likelihood and effective sample size."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/6602294be910b1e3c4571bd98c4d5484-Abstract.html,Bounding Performance Loss in Approximate MDP Homomorphisms,"Jonathan Taylor, Doina Precup, Prakash Panagaden","We define a metric for measuring behavior similarity between states in a Markov decision process (MDP), in which action similarity is taken into account. We show that the kernel of our metric corresponds exactly to the classes of states defined by MDP homomorphisms (Ravindran \& Barto, 2003). We prove that the difference in the optimal value function of different states can be upper-bounded by the value of this metric, and that the bound is tighter than that provided by bisimulation metrics (Ferns et al. 2004, 2005). Our results hold both for discrete and for continuous actions. We provide an algorithm for constructing approximate homomorphisms, by using this metric to identify states that can be grouped together, as well as actions that can be matched. Previous research on this topic is based mainly on heuristics."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html,Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks,"Alex Graves, Jürgen Schmidhuber","Offline handwriting recognition---the transcription of images of handwritten text---is an interesting task, in that it combines computer vision with sequence learning. In most systems the two elements are handled separately, with sophisticated preprocessing techniques used to extract the image features and sequential models such as HMMs used to provide the transcriptions. By combining two recent innovations in neural networks---multidimensional recurrent neural networks and connectionist temporal classification---this paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input. Unlike competing systems, it does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language. Evidence of its generality and power is provided by data from a recent international Arabic recognition competition, where it outperformed all entries (91.4% accuracy compared to 87.2% for the competition winner) despite the fact that neither author understands a word of Arabic."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/66f041e16a60928b05a7e228a89c3799-Abstract.html,Robust Near-Isometric Matching via Structured Learning of Graphical Models,"Alex J. Smola, Julian J. Mcauley, Tibério S. Caetano","Models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the isometric assumption. However, real shapes from image datasets, even when expected to be related by almost isometric"" transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale. In this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction. The outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations. Our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times."""
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/677e09724f0e2df9b6c000b75b5da10d-Abstract.html,Fast Prediction on a Tree,"Mark Herbster, Massimiliano Pontil, Sergio R. Galeano","Given an $n$-vertex weighted tree with structural diameter $S$ and a subset of $m$ vertices, we present a technique to compute a corresponding $m \times m$ Gram matrix of the pseudoinverse of the graph Laplacian in $O(n+ m^2 + m S)$ time. We discuss the application of this technique to fast label prediction on a generic graph. We approximate the graph with a spanning tree and then we predict with the kernel perceptron. We address the approximation of the graph with either a minimum spanning tree or a shortest path tree. The fast computation of the pseudoinverse enables us to address prediction problems on large graphs. To this end we present experiments on two web-spam classification tasks, one of which includes a graph with 400,000 nodes and more than 10,000,000 edges. The results indicate that the accuracy of our technique is competitive with previous methods using the full graph information."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/68ce199ec2c5517597ce0a4d89620f55-Abstract.html,Exact Convex Confidence-Weighted Learning,"Koby Crammer, Mark Dredze, Fernando Pereira","Confidence-weighted (CW) learning [6], an online learning method for linear classifiers, maintains a Gaussian distributions over weight vectors, with a covariance matrix that represents uncertainty about weights and correlations. Confidence constraints ensure that a weight vector drawn from the hypothesis distribution correctly classifies examples with a specified probability. Within this framework, we derive a new convex form of the constraint and analyze it in the mistake bound model. Empirical evaluation with both synthetic and text data shows our version of CW learning achieves lower cumulative and out-of-sample errors than commonly used first-order and second-order online methods."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/6a10bbd480e4c5573d8f3af73ae0454b-Abstract.html,The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction,"Fabian H. Sinz, Matthias Bethge","Bandpass filtering, orientation selectivity, and contrast gain control are prominent features of sensory coding at the level of V1 simple cells. While the effect of bandpass filtering and orientation selectivity can be assessed within a linear model, contrast gain control is an inherently nonlinear computation. Here we employ the class of $L_p$ elliptically contoured distributions to investigate the extent to which the two features---orientation selectivity and contrast gain control---are suited to model the statistics of natural images. Within this framework we find that contrast gain control can play a significant role for the removal of redundancies in natural images. Orientation selectivity, in contrast, has only a very limited potential for redundancy reduction."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/6aab1270668d8cac7cef2566a1c5f569-Abstract.html,Regularized Co-Clustering with Dual Supervision,"Vikas Sindhwani, Jianying Hu, Aleksandra Mojsilovic","By attempting to simultaneously partition both the rows (examples) and columns (features) of a data matrix, Co-clustering algorithms often demonstrate surpris- ingly impressive performance improvements over traditional one-sided (row) clustering techniques. A good clustering of features may be seen as a combinatorial transformation of the data matrix, effectively enforcing a form of regularization that may lead to a better clustering of examples (and vice-versa). In many applications, partial supervision in the form of a few row labels as well as column labels may be available to potentially assist co-clustering. In this paper, we develop two novel semi-supervised multi-class classification algorithms motivated respectively by spectral bipartite graph partitioning and matrix approximation (e.g., non-negative matrix factorization) formulations for co-clustering. These algorithms (i) support dual supervision in the form of labels for both examples and/or features, (ii) provide principled predictive capability on out-of-sample test data, and (iii) arise naturally from the classical Representer theorem applied to regularization problems posed on a collection of Reproducing Kernel Hilbert Spaces. Empirical results demonstrate the effectiveness and utility of our algorithms."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/6bc24fc1ab650b25b4114e93a98f1eba-Abstract.html,Tighter Bounds for Structured Estimation,"Olivier Chapelle, Chuong B. Do, Choon H. Teo, Quoc V. Le, Alex J. Smola","Large-margin structured estimation methods work by minimizing a convex upper bound of loss functions. While they allow for efficient optimization algorithms, these convex formulations are not tight and sacrifice the ability to accurately model the true loss. We present tighter non-convex bounds based on generalizing the notion of a ramp loss from binary classification to structured estimation. We show that a small modification of existing optimization algorithms suffices to solve this modified problem. On structured prediction tasks such as protein sequence alignment and web page ranking, our algorithm leads to improved accuracy."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/6c3cf77d52820cd0fe646d38bc2145ca-Abstract.html,Multi-Agent Filtering with Infinitely Nested Beliefs,"Luke Zettlemoyer, Brian Milch, Leslie P. Kaelbling","In partially observable worlds with many agents, nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents. The multi-agent filtering problem is to efficiently represent and update these beliefs through time as the agents act in the world. In this paper, we formally define an infinite sequence of nested beliefs about the state of the world at the current time $t$ and present a filtering algorithm that maintains a finite representation which can be used to generate these beliefs. In some cases, this representation can be updated exactly in constant time; we also present a simple approximation scheme to compact beliefs if they become too complex. In experiments, we demonstrate efficient filtering in a range of multi-agent domains."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/6c9882bbac1c7093bd25041881277658-Abstract.html,"Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree","Daphna Weinshall, Hynek Hermansky, Alon Zweig, Jie Luo, Holly Jimison, Frank Ohl, Misha Pavel","Unexpected stimuli are a challenge to any machine learning algorithm. Here we identify distinct types of unexpected events, focusing on 'incongruent events' - when 'general level' and 'specific level' classifiers give conflicting predictions. We define a formal framework for the representation and processing of incongruent events: starting from the notion of label hierarchy, we show how partial order on labels can be deduced from such hierarchies. For each event, we compute its probability in different ways, based on adjacent levels (according to the partial order) in the label hierarchy . An incongruent event is an event where the probability computed based on some more specific level (in accordance with the partial order) is much smaller than the probability computed based on some more general level, leading to conflicting predictions. We derive algorithms to detect incongruent events from different types of hierarchies, corresponding to class membership or part membership. Respectively, we show promising results with real data on two specific problems: Out Of Vocabulary words in speech recognition, and the identification of a new sub-class (e.g., the face of a new individual) in audio-visual facial object recognition."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/6e16656a6ee1de7232164767ccfa7920-Abstract.html,"Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification","Doug Downey, Oren Etzioni","Is accurate classification possible in the absence of hand-labeled data? This paper introduces the Monotonic Feature (MF) abstraction--where the probability of class membership increases monotonically with the MF's value. The paper proves that when an MF is given, PAC learning is possible with no hand-labeled data under certain assumptions. We argue that MFs arise naturally in a broad range of textual classification applications. On the classic ""20 Newsgroups"" data set, a learner given an MF and unlabeled data achieves classification accuracy equal to that of a state-of-the-art semi-supervised learner relying on 160 hand-labeled examples. Even when MFs are not given as input, their presence or absence can be determined from a small amount of hand-labeled data, which yields a new semi-supervised learning method that reduces error by 15% on the 20 Newsgroups data."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/6faa8040da20ef399b63a72d0e4ab575-Abstract.html,Nonparametric regression and classification with joint sparsity constraints,"Han Liu, Larry Wasserman, John D. Lafferty","We propose new families of models and algorithms for high-dimensional nonparametric learning with joint sparsity constraints. Our approach is based on a regularization method that enforces common sparsity patterns across different function components in a nonparametric additive model. The algorithms employ a coordinate descent approach that is based on a functional soft-thresholding operator. The framework yields several new models, including multi-task sparse additive models, multi-response sparse additive models, and sparse additive multi-category logistic regression. The methods are illustrated with experiments on synthetic data and gene microarray data."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/703957b6dd9e3a7980e040bee50ded65-Abstract.html,Recursive Segmentation and Recognition Templates for 2D Parsing,"Leo Zhu, Yuanhao Chen, Yuan Lin, Chenxi Lin, Alan L. Yuille","Language and image understanding are two major goals of artiﬁcial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. Natural language researchers have made great progress by exploiting the 1D structure of language to design efﬁcient polynomial- time parsing algorithms. By contrast, the two-dimensional nature of images makes it much harder to design efﬁcient image parsers and the form of the hierarchical representations is also unclear. Attempts to adapt representations and algorithms from natural language have only been partially successful. In this paper, we propose a Hierarchical Image Model (HIM) for 2D image pars- ing which outputs image segmentation and object recognition. This HIM is rep- resented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. Firstly, the HIM has a coarse-to-ﬁne representation which is capable of capturing long-range de- pendency and exploiting different levels of contextual information. Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dy- namic programming, which enables us to parse the image rapidly in polynomial time. Thirdly, we can learn the HIM efﬁciently in a discriminative manner from a labeled dataset. We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/704afe073992cbe4813cae2f7715336f-Abstract.html,Predicting the Geometry of Metal Binding Sites from Protein Sequence,"Paolo Frasconi, Andrea Passerini","Metal binding is important for the structural and functional characterization of proteins. Previous prediction efforts have only focused on bonding state, i.e. deciding which protein residues act as metal ligands in some binding site. Identifying the geometry of metal-binding sites, i.e. deciding which residues are jointly involved in the coordination of a metal ion is a new prediction problem that has been never attempted before from protein sequence alone. In this paper, we formulate it in the framework of learning with structured outputs. Our solution relies on the fact that, from a graph theoretical perspective, metal binding has the algebraic properties of a matroid, enabling the application of greedy algorithms for learning structured outputs. On a data set of 199 non-redundant metalloproteins, we obtained precision/recall levels of 75\%/46\% correct ligand-ion assignments, which improves to 88\%/88\% in the setting where the metal binding state is known."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/70c639df5e30bdee440e4cdf599fec2b-Abstract.html,Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons,"Adam Ponzi, Jeff Wickens","Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum and hippocampus CA3. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with fixed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and firing state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between firing and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/72007983849f4fcb0ad565439834756b-Abstract.html,High-dimensional support union recovery in multivariate regression,"Guillaume R. Obozinski, Martin J. Wainwright, Michael I. Jordan","We study the behavior of block (cid:96)1/(cid:96)2 regularization for multivariate regression, where a K-dimensional response vector is regressed upon a ﬁxed set of p co- variates. The problem of support union recovery is to recover the subset of covariates that are active in at least one of the regression problems. Study- ing this problem under high-dimensional scaling (where the problem parame- ters as well as sample size n tend to inﬁnity simultaneously), our main result is to show that exact recovery is possible once the order parameter given by θ(cid:96)1/(cid:96)2(n, p, s) : = n/[2ψ(B∗) log(p − s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the size of the union of supports, and ψ(B∗) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefﬁcient vectors that constitute the model. This sparsity-overlap function reveals that block (cid:96)1/(cid:96)2 regularization for multivariate regression never harms performance relative to a naive (cid:96)1-approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal rela- tive to the design. We complement our theoretical results with simulations that demonstrate the sharpness of the result, even for relatively small problems."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/735143e9ff8c47def504f1ba0442df98-Abstract.html,Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm,"Andrew Smith, Hongyuan Zha, Xiao-ming Wu",We study the convergence and the rate of convergence of a local manifold learning algorithm: LTSA [13]. The main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of LTSA. We derive a worst-case upper bound of errors for LTSA which naturally leads to a convergence result. We then derive the rate of convergence for LTSA in a special case.
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/73640de25b7d656733ce2f808a330f18-Abstract.html,A Convex Upper Bound on the Log-Partition Function for Binary Distributions,"Laurent E. Ghaoui, Assane Gueye","We consider the problem of bounding from above the log-partition function corresponding to second-order Ising models for binary distributions. We introduce a new bound, the cardinality bound, which can be computed via convex optimization. The corresponding error on the logpartition function is bounded above by twice the distance, in model parameter space, to a class of ""standard"" Ising models, for which variable inter-dependence is described via a simple mean field term. In the context of maximum-likelihood, using the new bound instead of the exact log-partition function, while constraining the distance to the class of standard Ising models, leads not only to a good approximation to the log-partition function, but also to a model that is parsimonious, and easily interpretable. We compare our bound with the log-determinant bound introduced by Wainwright and Jordan (2006), and show that when the l1 -norm of the model parameter vector is small enough, the latter is outperformed by the new bound."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/7380ad8a673226ae47fce7bff88e9c33-Abstract.html,Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models,Tong Zhang,"Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefficients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneficial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/73c03186765e199c116224b68adc5fa0-Abstract.html,Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking,Nir Ailon,"The problem of ranking arises ubiquitously in almost every aspect of life, and in particular in Machine Learning/Information Retrieval. A statistical model for ranking predicts how humans rank subsets V of some universe U . In this work we define a statistical model for ranking that satisfies certain desirable properties. The model automatically gives rise to a logistic regression based approach to learning how to rank, for which the score and comparison based approaches are dual views. This offers a new generative approach to ranking which can be used for IR. There are two main contexts for this work. The first is the theory of econometrics and study of statistical models explaining human choice of alternatives. In this context, we will compare our model with other well known models. The second context is the problem of ranking in machine learning, usually arising in the context of information retrieval. Here, much work has been done in the discriminative setting, where different heuristics are used to define ranking risk functions. Our model is built rigorously and axiomatically based on very simple desirable properties defined locally for comparisons, and automatically implies the existence of a global score function serving as a natural model parameter which can be efficiently fitted to pairwise comparison judgment data by solving a convex optimization problem."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/758874998f5bd0c393da094e1967a72b-Abstract.html,Theory of matching pursuit,"Zakria Hussain, John S. Shawe-taylor","We analyse matching pursuit for kernel principal components analysis by proving that the sparse subspace it produces is a sample compression scheme. We show that this bound is tighter than the KPCA bound of Shawe-Taylor et al swck-05 and highly predictive of the size of the subspace needed to capture most of the variance in the data. We analyse a second matching pursuit algorithm called kernel matching pursuit (KMP) which does not correspond to a sample compression scheme. However, we give a novel bound that views the choice of subspace of the KMP algorithm as a compression scheme and hence provide a VC bound to upper bound its future loss. Finally we describe how the same bound can be applied to other matching pursuit related algorithms."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/7647966b7343c29048673252e490f736-Abstract.html,Policy Search for Motor Primitives in Robotics,"Jens Kober, Jan R. Peters","Many motor skills in humanoid robotics can be learned using parametrized motor primitives as done in imitation learning. However, most interesting motor learning problems are high-dimensional reinforcement learning problems often beyond the reach of current methods. In this paper, we extend previous work on policy learning from the immediate reward case to episodic reinforcement learning. We show that this results into a general, common framework also connected to policy gradient methods and yielding a novel algorithm for policy learning by assuming a form of exploration that is particularly well-suited for dynamic motor primitives. The resulting algorithm is an EM-inspired algorithm applicable in complex motor learning tasks. We compare this algorithm to alternative parametrized policy search methods and show that it outperforms previous methods. We apply it in the context of motor learning and show that it can learn a complex Ball-in-a-Cup task using a real Barrett WAM robot arm."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/788d986905533aba051261497ecffcbb-Abstract.html,Mortal Multi-Armed Bandits,"Deepayan Chakrabarti, Ravi Kumar, Filip Radlinski, Eli Upfal","We formulate and study a new variant of the $k$-armed bandit problem, motivated by e-commerce applications. In our model, arms have (stochastic) lifetime after which they expire. In this setting an algorithm needs to continuously explore new arms, in contrast to the standard $k$-armed bandit model in which arms are available indefinitely and exploration is reduced once an optimal arm is identified with near-certainty. The main motivation for our setting is online-advertising, where ads have limited lifetime due to, for example, the nature of their content and their campaign budget. An algorithm needs to choose among a large collection of ads, more than can be fully explored within the ads' lifetime. We present an optimal algorithm for the state-aware (deterministic reward function) case, and build on this technique to obtain an algorithm for the state-oblivious (stochastic reward function) case. Empirical studies on various reward distributions, including one derived from a real-world ad serving application, show that the proposed algorithms significantly outperform the standard multi-armed bandit approaches applied to these settings."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/7a614fd06c325499f1680b9896beedeb-Abstract.html,Adapting to a Market Shock: Optimal Sequential Market-Making,"Sanmay Das, Malik Magdon-Ismail","We study the profit-maximization problem of a monopolistic market-maker who sets two-sided prices in an asset market. The sequential decision problem is hard to solve because the state space is a function. We demonstrate that the belief state is well approximated by a Gaussian distribution. We prove a key monotonicity property of the Gaussian state update which makes the problem tractable, yielding the first optimal sequential market-making algorithm in an established model. The algorithm leads to a surprising insight: an optimal monopolist can provide more liquidity than perfectly competitive market-makers in periods of extreme uncertainty, because a monopolist is willing to absorb initial losses in order to learn a new valuation rapidly so she can extract higher profits later."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/7b13b2203029ed80337f27127a9f1d28-Abstract.html,DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification,"Simon Lacoste-Julien, Fei Sha, Michael I. Jordan","Probabilistic topic models (and their extensions) have become popular as models of latent structures in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood estimation, an approach which may be suboptimal in the context of an overall classification problem. In this paper, we describe DiscLDA, a discriminative learning framework for such models as Latent Dirichlet Allocation (LDA) in the setting of dimensionality reduction with supervised side information. In DiscLDA, a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood using Monte Carlo EM. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classification. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroup ocument classification task."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/7d04bbbe5494ae9d2f5a76aa1c00fa2f-Abstract.html,Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing,Moritz Grosse-wentrup,"EEG connectivity measures could provide a new type of feature space for inferring a subject's intention in Brain-Computer Interfaces (BCIs). However, very little is known on EEG connectivity patterns for BCIs. In this study, EEG connectivity during motor imagery (MI) of the left and right is investigated in a broad frequency range across the whole scalp by combining Beamforming with Transfer Entropy and taking into account possible volume conduction effects. Observed connectivity patterns indicate that modulation intentionally induced by MI is strongest in the gamma-band, i.e., above 35 Hz. Furthermore, modulation between MI and rest is found to be more pronounced than between MI of different hands. This is in contrast to results on MI obtained with bandpower features, and might provide an explanation for the so far only moderate success of connectivity features in BCIs. It is concluded that future studies on connectivity based BCIs should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/7eacb532570ff6858afd2723755ff790-Abstract.html,Rademacher Complexity Bounds for Non-I.I.D. Processes,"Mehryar Mohri, Afshin Rostamizadeh","This paper presents the first data-dependent generalization bounds for non-i.i.d. settings based on the notion of Rademacher complexity. Our bounds extend to the non-i.i.d. case existing Rademacher complexity bounds derived for the i.i.d. setting. These bounds provide a strict generalization of the ones found in the i.i.d. case, and can also be used within the standard i.i.d. scenario. They apply to the standard scenario of beta-mixing stationary sequences examined in many previous studies of non-i.i.d. settings and benefit form the crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from finite samples and lead to tighter bounds."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/7f1171a78ce0780a2142a6eb7bc4f3c8-Abstract.html,A rational model of preference learning and choice prediction by children,"Christopher G. Lucas, Thomas L. Griffiths, Fei Xu, Christine Fawcett","Young children demonstrate the ability to make inferences about the preferences of other agents based on their choices. However, there exists no overarching account of what children are doing when they learn about preferences or how they use that knowledge. We use a rational model of preference learning, drawing on ideas from economics and computer science, to explain the behavior of children in several recent experiments. Specifically, we show how a simple econometric model can be extended to capture two- to four-year-oldsâ use of statistical information in inferring preferences, and their generalization of these preferences."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/7f1de29e6da19d22b51c68001e7e0e54-Abstract.html,An ideal observer model of infant object perception,"Charles Kemp, Fei Xu","Before the age of 4 months, infants make inductive inferences about the motions of physical objects. Developmental psychologists have provided verbal accounts of the knowledge that supports these inferences, but often these accounts focus on categorical rather than probabilistic principles. We propose that infant object perception is guided in part by probabilistic principles like persistence: things tend to remain the same, and when they change they do so gradually. To illustrate this idea, we develop an ideal observer model that includes probabilistic formulations of rigidity and inertia. Like previous researchers, we suggest that rigid motions are expected from an early age, but we challenge the previous claim that expectations consistent with inertia are relatively slow to develop (Spelke et al., 1992). We support these arguments by modeling four experiments from the developmental literature."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/7f24d240521d99071c93af3917215ef7-Abstract.html,Empirical performance maximization for linear rank statistics,"Stéphan J. Clémençcon, Nicolas Vayatis","The ROC curve is known to be the golden standard for measuring performance of a test/scoring statistic regarding its capacity of discrimination between two populations in a wide variety of applications, ranging from anomaly detection in signal processing to information retrieval, through medical diagnosis. Most practical performance measures used in scoring applications such as the AUC, the local AUC, the p-norm push, the DCG and others, can be seen as summaries of the ROC curve. This paper highlights the fact that many of these empirical criteria can be expressed as (conditional) linear rank statistics. We investigate the properties of empirical maximizers of such performance criteria and provide preliminary results for the concentration properties of a novel class of random variables that we will call a linear rank process."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/7f975a56c761db6506eca0b37ce6ec87-Abstract.html,Resolution Limits of Sparse Coding in High Dimensions,"Sundeep Rangan, Vivek Goyal, Alyson K. Fletcher","Recent research suggests that neural systems employ sparse coding. However, there is limited theoretical understanding of fundamental resolution limits in such sparse coding. This paper considers a general sparse estimation problem of detecting the sparsity pattern of a $k$-sparse vector in $\R^n$ from $m$ random noisy measurements. Our main results provide necessary and sufficient conditions on the problem dimensions, $m$, $n$ and $k$, and the signal-to-noise ratio (SNR) for asymptotically-reliable detection. We show a necessary condition for perfect recovery at any given SNR for all algorithms, regardless of complexity, is $m = \Omega(k\log(n-k))$ measurements. This is considerably stronger than all previous necessary conditions. We also show that the scaling of $\Omega(k\log(n-k))$ measurements is sufficient for a trivial ``maximum correlation'' estimator to succeed. Hence this scaling is optimal and does not require lasso, matching pursuit, or more sophisticated methods, and the optimal scaling can thus be biologically plausible."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8065d07da4a77621450aa84fee5656d9-Abstract.html,Privacy-preserving logistic regression,"Kamalika Chaudhuri, Claire Monteleoni","This paper addresses the important tradeoff between privacy and learnability, when designing algorithms for learning from private databases. First we apply an idea of Dwork et al. to design a specific privacy-preserving machine learning algorithm, logistic regression. This involves bounding the sensitivity of logistic regression, and perturbing the learned classifier with noise proportional to the sensitivity. Noting that the approach of Dwork et al. has limitations when applied to other machine learning algorithms, we then present another privacy-preserving logistic regression algorithm. The algorithm is based on solving a perturbed objective, and does not depend on the sensitivity. We prove that our algorithm preserves privacy in the model due to Dwork et al., and we provide a learning performance guarantee. Our work also reveals an interesting connection between regularization and privacy."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/816b112c6105b3ebd537828a39af4818-Abstract.html,Efficient Exact Inference in Planar Ising Models,"Nicol N. Schraudolph, Dmitry Kamenetsky","We present polynomial-time algorithms for the exact computation of lowest- energy states, worst margin violators, partition functions, and marginals in binary undirected graphical models. Our approach provides an interesting alternative to the well-known graph cut paradigm in that it does not impose any submodularity constraints; instead we require planarity to establish a correspondence with perfect matchings in an expanded dual graph. Maximum-margin parameter estimation for a boundary detection task shows our approach to be efﬁcient and effective."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html,Deflation Methods for Sparse PCA,Lester W. Mackey,"In analogy to the PCA setting, the sparse PCA problem is often solved by iteratively alternating between two subtasks: cardinality-constrained rank-one variance maximization and matrix deflation. While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justification from the PCA context. In this work, we demonstrate that the standard PCA deflation procedure is seldom appropriate for the sparse PCA setting. To rectify the situation, we first develop several heuristic deflation alternatives with more desirable properties. We then reformulate the sparse PCA optimization problem to explicitly reflect the maximum additional variance objective on each round. The result is a generalized deflation procedure that typically outperforms more standard techniques on real-world datasets."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8613985ec49eb8f757ae6439e879bb2a-Abstract.html,Mixed Membership Stochastic Blockmodels,"Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg, Eric P. Xing","Observations consisting of measurements on relationships for pairs of objects arise in many settings, such as protein interaction and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold. In this paper, we describe a class of latent variable models of such data called Mixed Membership Stochastic Blockmodels. This model extends blockmodels for relational data to ones which capture mixed membership latent relational structure, thus providing an object-specific low-dimensional representation. We develop a general variational inference algorithm for fast approximate posterior inference. We explore applications to social networks and protein interaction networks."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/883e881bb4d22a7add958f2d6b052c9f-Abstract.html,Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes,"Erik B. Sudderth, Michael I. Jordan","We develop a statistical framework for the simultaneous, unsupervised segmentation and discovery of visual object categories from image databases. Examining a large set of manually segmented scenes, we use chi--square tests to show that object frequencies and segment sizes both follow power law distributions, which are well modeled by the Pitman--Yor (PY) process. This nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects, and segmentation methods which automatically adapt their resolution to each image. Generalizing previous applications of PY processes, we use Gaussian processes to discover spatially contiguous segments which respect image boundaries. Using a novel family of variational approximations, our approach produces segmentations which compare favorably to state--of--the--art methods, while simultaneously discovering categories shared among natural scenes."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8844c5f00372df2c3c4ee857c2451b45-Abstract.html,Shape-Based Object Localization for Descriptive Classification,"Geremy Heitz, Gal Elidan, Benjamin Packer, Daphne Koller","Discriminative tasks, including object categorization and detection, are central components of high-level computer vision. Sometimes, however, we are interested in more refined aspects of the object in an image, such as pose or particular regions. In this paper we develop a method (LOOPS) for learning a shape and image feature model that can be trained on a particular object class, and used to outline instances of the class in novel images. Furthermore, while the training data consists of uncorresponded outlines, the resulting LOOPS model contains a set of landmark points that appear consistently across instances, and can be accurately localized in an image. Our model achieves state-of-the-art results in precisely outlining objects that exhibit large deformations and articulations in cluttered natural images. These localizations can then be used to address a range of tasks, including descriptive classification, search, and clustering."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8b16ebc056e613024c057be590b542eb-Abstract.html,Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform,"Guangzhi Cao, Charles Bouman","Covariance estimation for high dimensional vectors is a classically difficult problem in statistical analysis and machine learning due to limited sample size. In this paper, we propose a new approach to covariance estimation, which is based on constrained maximum likelihood (ML) estimation of the covariance. Specifically, the covariance is constrained to have an eigen decomposition which can be represented as a sparse matrix transform (SMT). The SMT is formed by a product of pairwise coordinate rotations known as Givens rotations. Using this framework, the covariance can be efficiently estimated using greedy minimization of the log likelihood function, and the number of Givens rotations can be efficiently computed using a cross-validation procedure. The estimator obtained using this method is always positive definite and well-conditioned even with limited sample size. Experiments on hyperspectral data show that SMT covariance estimation results in consistently better estimates of the covariance for a variety of different classes and sample sizes compared to traditional shrinkage estimators."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8c235f89a8143a28a1d6067e959dd858-Abstract.html,Characterizing neural dependencies with copula models,"Pietro Berkes, Frank Wood, Jonathan W. Pillow","The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. However, there is no simple model that combines the observations that (1) marginal distributions over single-neuron spike counts are often approximately Poisson; and (2) joint distributions over the responses of multiple neurons are often strongly dependent. Here, we show that both marginal and joint properties of neural responses can be captured using Poisson copula models. Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefficients. We explore a variety of Poisson copula models for joint neural response distributions, and derive an efficient maximum likelihood procedure for estimating them. We apply these models to neuronal data collected in and macaque motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8c7bbbba95c1025975e548cee86dfadc-Abstract.html,Learning with Consistency between Inductive Functions and Kernels,"Haixuan Yang, Irwin King, Michael Lyu","Regularized Least Squares (RLS) algorithms have the ability to avoid over-fitting problems and to express solutions as kernel expansions. However, we observe that the current RLS algorithms cannot provide a satisfactory interpretation even on a constant function. On the other hand, while kernel-based algorithms have been developed in such a tendency that almost all learning algorithms are kernelized or being kernelized, a basic fact is often ignored: The learned function from the data and the kernel fits the data well, but may not be consistent with the kernel. Based on these considerations and on the intuition that a good kernel-based inductive function should be consistent with both the data and the kernel, a novel learning scheme is proposed. The advantages of this scheme lie in its corresponding Representer Theorem, its strong interpretation ability about what kind of functions should not be penalized, and its promising accuracy improvements shown in a number of experiments. Furthermore, we provide a detailed technical description about heat kernels, which serves as an example for the readers to apply similar techniques for other kernels. Our work provides a preliminary step in a new direction to explore the varying consistency between inductive functions and kernels under various distributions."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8d3bba7425e7c98c50f52ca1b52d3735-Abstract.html,Syntactic Topic Models,"Jordan L. Boyd-graber, David M. Blei","We develop \name\ (STM), a nonparametric Bayesian model of parsed documents. \Shortname\ generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-specific topic weights and parse-tree specific syntactic transitions. Words are assumed generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8dd48d6a2e2cad213179a3992c0be53c-Abstract.html,A general framework for investigating how far the decoding process in the brain can be simplified,"Masafumi Oizumi, Toshiyuki Ishii, Kazuya Ishibashi, Toshihiko Hosoya, Masato Okada","``How is information decoded in the brain?'' is one of the most difficult and important questions in neuroscience. Whether neural correlation is important or not in decoding neural activities is of special interest. We have developed a general framework for investigating how far the decoding process in the brain can be simplified. First, we hierarchically construct simplified probabilistic models of neural responses that ignore more than $K$th-order correlations by using a maximum entropy principle. Then, we compute how much information is lost when information is decoded using the simplified models, i.e., ``mismatched decoders''. We introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders. We applied our proposed framework to spike data for vertebrate retina. We used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies. We found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding. We also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies, pseudo correlations seem to carry a large portion of the information."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8df707a948fac1b4a0f97aa554886ec8-Abstract.html,Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms,"John W. Roberts, Russ Tedrake","Policy gradient (PG) reinforcement learning algorithms have strong (local) convergence guarantees, but their learning performance is typically limited by a large variance in the estimate of the gradient. In this paper, we formulate the variance reduction problem by describing a signal-to-noise ratio (SNR) for policy gradient algorithms, and evaluate this SNR carefully for the popular Weight Perturbation (WP) algorithm. We confirm that SNR is a good predictor of long-term learning performance, and that in our episodic formulation, the cost-to-go function is indeed the optimal baseline. We then propose two modifications to traditional model-free policy gradient algorithms in order to optimize the SNR. First, we examine WP using anisotropic sampling distributions, which introduces a bias into the update but increases the SNR; this bias can be interpretted as following the natural gradient of the cost function. Second, we show that non-Gaussian distributions can also increase the SNR, and argue that the optimal isotropic distribution is a âshellâ distribution with a constant magnitude and uniform distribution in direction. We demonstrate that both modifications produce substantial improvements in learning performance in challenging policy gradient experiments."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8f468c873a32bb0619eaeb2050ba45d1-Abstract.html,Artificial Olfactory Brain for Mixture Identification,"Mehmet K. Muezzinoglu, Alexander Vergara, Ramon Huerta, Thomas Nowotny, Nikolai Rulkov, Henry Abarbanel, Allen Selverston, Mikhail Rabinovich","The odor transduction process has a large time constant and is susceptible to various types of noise. Therefore, the olfactory code at the sensor/receptor level is in general a slow and highly variable indicator of the input odor in both natural and artificial situations. Insects overcome this problem by using a neuronal device in their Antennal Lobe (AL), which transforms the identity code of olfactory receptors to a spatio-temporal code. This transformation improves the decision of the Mushroom Bodies (MBs), the subsequent classifier, in both speed and accuracy.Here we propose a rate model based on two intrinsic mechanisms in the insect AL, namely integration and inhibition. Then we present a MB classifier model that resembles the sparse and random structure of insect MB. A local Hebbian learning procedure governs the plasticity in the model. These formulations not only help to understand the signal conditioning and classification methods of insect olfactory systems, but also can be leveraged in synthetic problems. Among them, we consider here the discrimination of odor mixtures from pure odors. We show on a set of records from metal-oxide gas sensors that the cascade of these two new models facilitates fast and accurate discrimination of even highly imbalanced mixtures from pure odors."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8f53295a73878494e9bc8dd6c3c7104f-Abstract.html,Robust Kernel Principal Component Analysis,"Minh H. Nguyen, Fernando Torre","Kernel Principal Component Analysis (KPCA) is a popular generalization of linear PCA that allows non-linear feature extraction. In KPCA, data in the input space is mapped to higher (usually) dimensional feature space where the data can be linearly modeled. The feature space is typically induced implicitly by a kernel function, and linear PCA in the feature space is performed via the kernel trick. However, due to the implicitness of the feature space, some extensions of PCA such as robust PCA cannot be directly generalized to KPCA. This paper presents a technique to overcome this problem, and extends it to a unified framework for treating noise, missing data, and outliers in KPCA. Our method is based on a novel cost function to perform inference in KPCA. Extensive experiments, in both synthetic and real data, show that our algorithm outperforms existing methods."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8f7d807e1f53eff5f9efbe5cb81090fb-Abstract.html,Relative Margin Machines,"Tony Jebara, Pannagadatta K. Shivaswamy","In classification problems, Support Vector Machines maximize the margin of separation between two classes. While the paradigm has been successful, the solution obtained by SVMs is dominated by the directions with large data spread and biased to separate the classes by cutting along large spread directions. This article proposes a novel formulation to overcome such sensitivity and maximizes the margin relative to the spread of the data. The proposed formulation can be efficiently solved and experiments on digit datasets show drastic performance improvements over SVMs."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/8f85517967795eeef66c225f7883bdcb-Abstract.html,Bayesian Kernel Shaping for Learning Control,"Jo-anne Ting, Mrinal Kalakrishnan, Sethu Vijayakumar, Stefan Schaal","In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise (i.e., heteroscedasticity) varies spatially. Unfortunately, it presents a complex computational problem as the danger of overfitting is high and the individual optimization of every kernel in a learning system may be overly expensive due to the introduction of too many open learning parameters. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. In this paper, we focus on nonparametric regression and introduce a Bayesian formulation that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efficient (suitable for large data sets), requires no sampling, automatically rejects outliers and has only one prior to be specified. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian Processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/903ce9225fca3e988c2af215d4e544d3-Abstract.html,An Extended Level Method for Efficient Multiple Kernel Learning,"Zenglin Xu, Rong Jin, Irwin King, Michael Lyu","We consider the problem of multiple kernel learning (MKL), which can be formulated as a convex-concave problem. In the past, two efficient methods, i.e., Semi-Infinite Linear Programming (SILP) and Subgradient Descent (SD), have been proposed for large-scale multiple kernel learning. Despite their success, both methods have their own shortcomings: (a) the SD method utilizes the gradient of only the current solution, and (b) the SILP method does not regularize the approximate solution obtained from the cutting plane model. In this work, we extend the level method, which was originally designed for optimizing non-smooth objective functions, to convex-concave optimization, and apply it to multiple kernel learning. The extended level method overcomes the drawbacks of SILP and SD by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set. Empirical study with eight UCI datasets shows that the extended level method can significantly improve efficiency by saving on average 91.9% of computational time over the SILP method and 70.3% over the SD method."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/92c8c96e4c37100777c7190b76d28233-Abstract.html,Sparse Online Learning via Truncated Gradient,"John Langford, Lihong Li, Tong Zhang","We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous---a parameter controls the rate of sparsification from no sparsification to total sparsification. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular $L_1$-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and find that for datasets with large numbers of features, substantial sparsity is discoverable."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/92cc227532d17e56e07902b254dfad10-Abstract.html,Bayesian Model of Behaviour in Economic Games,"Debajyoti Ray, Brooks King-casas, P. R. Montague, Peter Dayan",Classical Game Theoretic approaches that make strong rationality assumptions have difficulty modeling observed behaviour in Economic games of human subjects. We investigate the role of finite levels of iterated reasoning and non-selfish utility functions in a Partially Observable Markov Decision Process model that incorporates Game Theoretic notions of interactivity. Our generative model captures a broad class of characteristic behaviours in a multi-round Investment game. We invert the generative process for a recognition model that is used to classify 200 subjects playing an Investor-Trustee game against randomly matched opponents.
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/934815ad542a4a7c5e8a2dfa04fea9f5-Abstract.html,Skill Characterization Based on Betweenness,"Ozgur Simsek, Andrew G. Barto","We present a characterization of a useful class of skills based on a graphical representation of an agent's interaction with its environment. Our characterization uses betweenness, a measure of centrality on graphs. It may be used directly to form a set of skills suitable for a given environment. More importantly, it serves as a useful guide for developing online, incremental skill discovery algorithms that do not rely on knowing or representing the environment graph in its entirety."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/941e1aaaba585b952b62c14a3a175a61-Abstract.html,Improved Moves for Truncated Convex Models,"Philip Torr, M. P. Kumar","We consider the problem of obtaining the approximate maximum a posteriori estimate of a discrete random field characterized by pairwise potentials that form a truncated convex model. For this problem, we propose an improved st-mincut based move making algorithm. Unlike previous move making approaches, which either provide a loose bound or no bound on the quality of the solution (in terms of the corresponding Gibbs energy), our algorithm achieves the same guarantees as the standard linear programming (LP) relaxation. Compared to previous approaches based on the LP relaxation, e.g. interior-point algorithms or tree-reweighted message passing (TRW), our method is faster as it uses only the efficient st-mincut algorithm in its design. Furthermore, it directly provides us with a primal solution (unlike TRW and other related methods which attempt to solve the dual of the LP). We demonstrate the effectiveness of the proposed approach on both synthetic and standard real data problems. Our analysis also opens up an interesting question regarding the relationship between move making algorithms (such as $\alpha$-expansion and the algorithms presented in this paper) and the randomized rounding schemes used with convex relaxations. We believe that further explorations in this direction would help design efficient algorithms for more complex relaxations."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/94c7bb58efc3b337800875b5d382a072-Abstract.html,Kernelized Sorting,"Novi Quadrianto, Le Song, Alex J. Smola","Object matching is a fundamental operation in data analysis. It typically requires the definition of a similarity measure between the classes of objects to be matched. Instead, we develop an approach which is able to perform matching by requiring a similarity measure only within each of the classes. This is achieved by maximizing the dependency between matched pairs of observations by means of the Hilbert Schmidt Independence Criterion. This problem can be cast as one of maximizing a quadratic assignment problem with special structure and we present a simple algorithm for finding a locally optimal solution."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/94f6d7e04a4d452035300f18b984988c-Abstract.html,Posterior Consistency of the Silverman g-prior in Bayesian Model Choice,"Zhihua Zhang, Michael I. Jordan, Dit-Yan Yeung",Kernel supervised learning methods can be unified by utilizing the tools from regularization theory. The duality between regularization and prior leads to interpreting regularization methods in terms of maximum a posteriori estimation and has motivated Bayesian interpretations of kernel methods. In this paper we pursue a Bayesian interpretation of sparsity in the kernel setting by making use of a mixture of a point-mass distribution and prior that we refer to as ``Silverman's g-prior.'' We provide a theoretical analysis of the posterior consistency of a Bayesian model choice procedure based on this prior. We also establish the asymptotic relationship between this procedure and the Bayesian information criterion.
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/950a4152c2b4aa3ad78bdd6b366cc179-Abstract.html,Nonparametric Bayesian Learning of Switching Linear Dynamical Systems,"Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, Alan S. Willsky","Many nonlinear dynamical phenomena can be effectively modeled by a system that switches among a set of conditionally linear dynamical modes. We consider two such models: the switching linear dynamical system (SLDS) and the switching vector autoregressive (VAR) process. In this paper, we present a nonparametric approach to the learning of an unknown number of persistent, smooth dynamical modes by utilizing a hierarchical Dirichlet process prior. We develop a sampling algorithm that combines a truncated approximation to the Dirichlet process with an efficient joint sampling of the mode and state sequences. The utility and flexibility of our model are demonstrated on synthetic data, sequences of dancing honey bees, and the IBOVESPA stock index."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/98dce83da57b0395e163467c9dae521b-Abstract.html,Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement,"Michael T. Todd, Yael Niv, Jonathan D. Cohen","Working memory is a central topic of cognitive neuroscience because it is critical for solving real world problems in which information from multiple temporally distant sources must be combined to generate appropriate behavior. However, an often neglected fact is that learning to use working memory effectively is itself a difficult problem. The Gating"" framework is a collection of psychological models that show how dopamine can train the basal ganglia and prefrontal cortex to form useful working memory representations in certain types of problems. We bring together gating with ideas from machine learning about using finite memory systems in more general problems. Thus we present a normative Gating model that learns, by online temporal difference methods, to use working memory to maximize discounted future rewards in general partially observable settings. The model successfully solves a benchmark working memory problem, and exhibits limitations similar to those observed in human experiments. Moreover, the model introduces a concise, normative definition of high level cognitive concepts such as working memory and cognitive control in terms of maximizing discounted future rewards."""
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/9a96876e2f8f3dc4f3cf45f02c61c0c1-Abstract.html,Optimization on a Budget: A Reinforcement Learning Approach,"Paul L. Ruvolo, Ian Fasel, Javier R. Movellan","Many popular optimization algorithms, like the Levenberg-Marquardt algorithm (LMA), use heuristic-based controllers'' that modulate the behavior of the optimizer during the optimization process. For example, in the LMA a damping parameter is dynamically modified based on a set rules that were developed using various heuristic arguments. Reinforcement learning (RL) is a machine learning approach to learn optimal controllers by examples and thus is an obvious candidate to improve the heuristic-based controllers implicit in the most popular and heavily used optimization algorithms. Improving the performance of off-the-shelf optimizers is particularly important for time-constrained optimization problems. For example the LMA algorithm has become popular for many real-time computer vision problems, including object tracking from video, where only a small amount of time can be allocated to the optimizer on each incoming video frame. Here we show that a popular modern reinforcement learning technique using a very simply state space can dramatically improve the performance of general purpose optimizers, like the LMA. Most surprisingly the controllers learned for a particular domain appear to work very well also on very different optimization domains. For example we used RL methods to train a new controller for the damping parameter of the LMA. This controller was trained on a collection of classic, relatively small, non-linear regression problems. The modified LMA performed better than the standard LMA on these problems. Most surprisingly, it also dramatically outperformed the standard LMA on a difficult large scale computer vision problem for which it had not been trained before. Thus the controller appeared to have extracted control rules that were not just domain specific but generalized across a wide range of optimization domains."""
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/9ab0d88431732957a618d4a469a0d4c3-Abstract.html,Dependent Dirichlet Process Spike Sorting,"Jan Gasthaus, Frank Wood, Dilan Gorur, Yee W. Teh","In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle appearance"" and ""disappearance"" of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of infinite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which the a partial ground truth labeling is known."""
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html,The Recurrent Temporal Restricted Boltzmann Machine,"Ilya Sutskever, Geoffrey E. Hinton, Graham W. Taylor","The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difficulty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modification of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/9be40cee5b0eee1462c82c6964087ff9-Abstract.html,Short-Term Depression in VLSI Stochastic Synapse,"Peng Xu, Timothy K. Horiuchi, Pamela A. Abshire","We report a compact realization of short-term depression (STD) in a VLSI stochastic synapse. The behavior of the circuit is based on a subtractive single release model of STD. Experimental results agree well with simulation and exhibit expected STD behavior: the transmitted spike train has negative autocorrelation and lower power spectral density at low frequencies which can remove redundancy in the input spike train, and the mean transmission probability is inversely proportional to the input spike rate which has been suggested as an automatic gain control mechanism in neural systems. The dynamic stochastic synapse could potentially be a powerful addition to existing deterministic VLSI spiking neural systems."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/9cf81d8026a9018052c429cc4e56739b-Abstract.html,Generative and Discriminative Learning with Unknown Labeling Bias,"Steven J. Phillips, Miroslav Dudík","We apply robust Bayesian decision theory to improve both generative and discriminative learners under bias in class proportions in labeled training data, when the true class proportions are unknown. For the generative case, we derive an entropy-based weighting that maximizes expected log likelihood under the worst-case true class proportions. For the discriminative case, we derive a multinomial logistic model that minimizes worst-case conditional log loss. We apply our theory to the modeling of species geographic distributions from presence data, an extreme case of label bias since there is no absence data. On a benchmark dataset, we find that entropy-based weighting offers an improvement over constant estimates of class proportions, consistently reducing log loss on unbiased test data."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/9de6d14fff9806d4bcd1ef555be766cd-Abstract.html,Large Margin Taxonomy Embedding for Document Categorization,"Kilian Q. Weinberger, Olivier Chapelle","Applications of multi-class classification, such as document categorization, often appear in cost-sensitive settings. Recent work has significantly improved the state of the art by moving beyond ``flat'' classification through incorporation of class hierarchies [Cai and Hoffman 04]. We present a novel algorithm that goes beyond hierarchical classification and estimates the latent semantic space that underlies the class hierarchy. In this space, each class is represented by a prototype and classification is done with the simple nearest neighbor rule. The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. We show that our optimization is convex and can be solved efficiently for large data sets. Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/a02ffd91ece5e7efeb46db8f10a74059-Abstract.html,Modeling the effects of memory on human online sentence processing with particle filters,"Roger P. Levy, Florencia Reali, Thomas L. Griffiths","Language comprehension in humans is significantly constrained by memory, yet rapid, highly incremental, and capable of utilizing a wide range of contextual information to resolve ambiguity and form expectations about future input. In contrast, most of the leading psycholinguistic models and fielded algorithms for natural language parsing are non-incremental, have run time superlinear in input length, and/or enforce structural locality constraints on probabilistic dependencies between events. We present a new limited-memory model of sentence comprehension which involves an adaptation of the particle filter, a sequential Monte Carlo method, to the problem of incremental parsing. We show that this model can reproduce classic results in online sentence comprehension, and that it naturally provides the first rational account of an outstanding problem in psycholinguistics, in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html,Clusters and Coarse Partitions in LP Relaxations,"David Sontag, Amir Globerson, Tommi S. Jaakkola","We propose a new class of consistency constraints for Linear Programming (LP) relaxations for finding the most probable (MAP) configuration in graphical models. Usual cluster-based LP relaxations enforce joint consistency of the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message-passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/a3d68b461bd9d3533ee1dd3ce4628ed4-Abstract.html,Bounds on marginal probability distributions,"Joris M. Mooij, Hilbert J. Kappen","We propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables. The bound is obtained by propagating bounds (convex sets of probability distributions) over a subtree of the factor graph, rooted in the variable of interest. By construction, the method not only bounds the exact marginal probability distribution of a variable, but also its approximate Belief Propagation marginal (``belief''). Thus, apart from providing a practical means to calculate bounds on marginals, our contribution also lies in providing a better understanding of the error made by Belief Propagation. We show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/a49e9411d64ff53eccfdd09ad10a15b3-Abstract.html,Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation,"Indraneel Mukherjee, David M. Blei","Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean field variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as $O(k-1) + \log m /m$, where $k$ is the number of topics in the model and $m$ is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/a4a042cf4fd6bfb47701cbc8a1653ada-Abstract.html,Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning,Francis R. Bach,"For supervised and unsupervised learning, positive definite kernels allow to use large and potentially infinite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the L1-norm or the block L1-norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efficiently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/a532400ed62e772b9dc0b86f46e583ff-Abstract.html,MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features,"Tae-kyun Kim, Roberto Cipolla","We present a new co-clustering problem of images and visual features. The problem involves a set of non-object images in addition to a set of object images and features to be co-clustered. Co-clustering is performed in a way of maximising discrimination of object images from non-object images, thus emphasizing discriminative features. This provides a way of obtaining perceptual joint-clusters of object images and features. We tackle the problem by simultaneously boosting multiple strong classifiers which compete for images by their expertise. Each boosting classifier is an aggregation of weak-learners, i.e. simple visual features. The obtained classifiers are useful for multi-category and multi-view object detection tasks. Experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classifiers in object detection tasks."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/a5771bce93e200c36f7cd9dfd0e5deaa-Abstract.html,Load and Attentional Bayes,Peter Dayan,"Selective attention is a most intensively studied psychological phenomenon, rife with theoretical suggestions and schisms. A critical idea is that of limited capacity, the allocation of which has produced half a century's worth of conflict about such phenomena as early and late selection. An influential resolution of this debate is based on the notion of perceptual load (Lavie, 2005, TICS, 9: 75), which suggests that low-load, easy tasks, because they underuse the total capacity of attention, mandatorily lead to the processing of stimuli that are irrelevant to the current attentional set; whereas high-load, difficult tasks grab all resources for themselves, leaving distractors high and dry. We argue that this theory presents a challenge to Bayesian theories of attention, and suggest an alternative, statistical, account of key supporting data."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/a5e00132373a7031000fd987a3c9f87b-Abstract.html,Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1,"Klaus Wimmer, Marcel Stimberg, Robert Martin, Lars Schwabe, Jorge Mariño, James Schummers, David C. Lyon, Mriganka Sur, Klaus Obermayer","One major role of primary visual cortex (V1) in vision is the encoding of the orientation of lines and contours. The role of the local recurrent network in these computations is, however, still a matter of debate. To address this issue, we analyze intracellular recording data of cat V1, which combine measuring the tuning of a range of neuronal properties with a precise localization of the recording sites in the orientation preference map. For the analysis, we consider a network model of Hodgkin-Huxley type neurons arranged according to a biologically plausible two-dimensional topographic orientation preference map. We then systematically vary the strength of the recurrent excitation and inhibition relative to the strength of the afferent input. Each parametrization gives rise to a different model instance for which the tuning of model neurons at different locations of the orientation map is compared to the experimentally measured orientation tuning of membrane potential, spike output, excitatory, and inhibitory conductances. A quantitative analysis shows that the data provides strong evidence for a network model in which the afferent input is dominated by strong, balanced contributions of recurrent excitation and inhibition. This recurrent regime is close to a regime of 'instability', where strong, self-sustained activity of the network occurs. The firing rate of neurons in the best-fitting network is particularly sensitive to small modulations of model parameters, which could be one of the functional benefits of a network operating in this particular regime."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/a87ff679a2f3e71d9181a67b7542122c-Abstract.html,An interior-point stochastic approximation method and an L1-regularized delta rule,"Peter Carbonetto, Mark Schmidt, Nando D. Freitas","The stochastic approximation method is behind the solution to many important, actively-studied problems in machine learning. Despite its far-reaching application, there is almost no work on applying stochastic approximation to learning problems with constraints. The reason for this, we hypothesize, is that no robust, widely-applicable stochastic approximation method exists for handling such problems. We propose that interior-point methods are a natural solution. We establish the stability of a stochastic interior-point approximation method both analytically and empirically, and demonstrate its utility by deriving an on-line learning algorithm that also performs feature selection via L1 regularization."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/a8baa56554f96369ab93e4f3bb068c22-Abstract.html,Dynamic visual attention: searching for coding length increments,"Xiaodi Hou, Liqing Zhang","A visual attention system should respond placidly when common stimuli are presented, while at the same time keep alert to anomalous visual inputs. In this paper, a dynamic visual attention model based on the rarity of features is proposed. We introduce the Incremental Coding Length (ICL) to measure the perspective entropy gain of each feature. The objective of our model is to maximize the entropy of the sampled visual features. In order to optimize energy consumption, the limit amount of energy of the system is re-distributed amongst features according to their Incremental Coding Length. By selecting features with large coding length increments, the computational system can achieve attention selectivity in both static and dynamic scenes. We demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation. Moreover, we also show that our model captures several less-reported dynamic visual search behaviors, such as attentional swing and inhibition of return."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/aa169b49b583a2b5af89203c2b78c67c-Abstract.html,Phase transitions for high-dimensional joint support recovery,"Sahand Negahban, Martin J. Wainwright","We consider the following instance of transfer learning: given a pair of regression problems, suppose that the regression coefficients share a partially common support, parameterized by the overlap fraction $\overlap$ between the two supports. This set-up suggests the use of $1, \infty$-regularized linear regression for recovering the support sets of both regression vectors. Our main contribution is to provide a sharp characterization of the sample complexity of this $1,\infty$ relaxation, exactly pinning down the minimal sample size $n$ required for joint support recovery as a function of the model dimension $\pdim$, support size $\spindex$ and overlap $\overlap \in [0,1]$. For measurement matrices drawn from standard Gaussian ensembles, we prove that the joint $1,\infty$-regularized method undergoes a phase transition characterized by order parameter $\orpar(\numobs, \pdim, \spindex, \overlap) = \numobs{(4 - 3 \overlap) s \log(p-(2-\overlap)s)}$. More precisely, the probability of successfully recovering both supports converges to $1$ for scalings such that $\orpar > 1$, and converges to $0$ to scalings for which $\orpar < 1$. An implication of this threshold is that use of $1, \infty$-regularization leads to gains in sample complexity if the overlap parameter is large enough ($\overlap > 2/3$), but performs worse than a naive approach if $\overlap < 2/3$. We illustrate the close agreement between these theoretical predictions, and the actual behavior in simulations. Thus, our results illustrate both the benefits and dangers associated with block-$1,\infty$ regularization in high-dimensional inference."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/aa68c75c4a77c87f97fb686b2f068676-Abstract.html,Online Metric Learning and Fast Similarity Search,"Prateek Jain, Brian Kulis, Inderjit S. Dhillon, Kristen Grauman","Metric learning algorithms can provide useful distance functions for a variety of domains, and recent work has shown good accuracy for problems where the learner can access all distance constraints at once. However, in many real applications, constraints are only available incrementally, thus necessitating methods that can perform online updates to the learned metric. Existing online algorithms offer bounds on worst-case performance, but typically do not perform well in practice as compared to their offline counterparts. We present a new online metric learning algorithm that updates a learned Mahalanobis metric based on LogDet regularization and gradient descent. We prove theoretical worst-case performance bounds, and empirically compare the proposed method against existing online metric learning algorithms. To further boost the practicality of our approach, we develop an online locality-sensitive hashing scheme which leads to efficient updates for approximate similarity search data structures. We demonstrate our algorithm on multiple datasets and show that it outperforms relevant baselines."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/ab88b15733f543179858600245108dd8-Abstract.html,Bayesian Network Score Approximation using a Metagraph Kernel,"Benjamin Yackley, Eduardo Corona, Terran Lane","Many interesting problems, including Bayesian network structure-search, can be cast in terms of finding the optimum value of a function over the space of graphs. However, this function is often expensive to compute exactly. We here present a method derived from the study of reproducing-kernel Hilbert spaces which takes advantage of the regular structure of the space of all graphs on a fixed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy. We then test this method on both a small testing set and a real-world Bayesian network; the results suggest that not only is this method reasonably accurate, but that the BDe score itself varies quadratically over the space of all graphs."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/abd815286ba1007abfbb8415b83ae2cf-Abstract.html,Fast Rates for Regularized Objectives,"Karthik Sridharan, Shai Shalev-shwartz, Nathan Srebro","We show that the empirical minimizer of a stochastic strongly convex objective, where the stochastic component is linear, converges to the population minimizer with rate $O(1/n)$. The result applies, in particular, to the SVM objective. Thus, we get a rate of $O(1/n)$ on the convergence of the SVM objective to its infinite data limit. We demonstrate how this is essential for obtaining tight oracle inequalities for SVMs. The results extend also to strong convexity with respect to other $\ellnorm_p$ norms, and so also to objectives regularized using other norms."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Abstract.html,Bayesian Synchronous Grammar Induction,"Phil Blunsom, Trevor Cohn, Miles Osborne","We present a novel method for inducing synchronous context free grammars (SCFGs) from a corpus of parallel string pairs. SCFGs can model equivalence between strings in terms of substitutions, insertions and deletions, and the reordering of sub-strings. We develop a non-parametric Bayesian model and apply it to a machine translation task, using priors to replace the various heuristics commonly used in this field. Using a variational Bayes training procedure, we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations, showing improvements in translation performance over previously proposed maximum likelihood models."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/ac627ab1ccbdb62ec96e702f07f6425b-Abstract.html,Tracking Changing Stimuli in Continuous Attractor Neural Networks,"K. Wong, Si Wu, Chi Fung","Continuous attractor neural networks (CANNs) are emerging as promising models for describing the encoding of continuous stimuli in neural systems. Due to the translational invariance of their neuronal interactions, CANNs can hold a continuous family of neutrally stable states. In this study, we systematically explore how neutral stability of a CANN facilitates its tracking performance, a capacity believed to have wide applications in brain functions. We develop a perturbative approach that utilizes the dominant movement of the network stationary states in the state space. We quantify the distortions of the bump shape during tracking, and study their effects on the tracking performance. Results are obtained on the maximum speed for a moving stimulus to be trackable, and the reaction time to catch up an abrupt change in stimulus."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/ad972f10e0800b49d76fed33a21f6698-Abstract.html,Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity,"Byron M. Yu, John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, Maneesh Sahani","We consider the problem of extracting smooth low-dimensional neural trajectories'' that summarize the activity recorded simultaneously from tens to hundreds of neurons on individual experimental trials. Beyond the benefit of visualizing the high-dimensional noisy spiking activity in a compact denoised form, such trajectories can offer insight into the dynamics of the neural circuitry underlying the recorded activity. Current methods for extracting neural trajectories involve a two-stage process: the data are firstdenoised'' by smoothing over time, then a static dimensionality reduction technique is applied. We first describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way, and account for spiking variability that may vary both across neurons and across time. We then present a novel method for extracting neural trajectories, Gaussian-process factor analysis (GPFA), which unifies the smoothing and dimensionality reduction operations in a common probabilistic framework. We applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution. By adopting a goodness-of-fit metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that GPFA provided a better characterization of the population activity than the two-stage methods. From the extracted single-trial neural trajectories, we directly observed a convergence in neural state during motor planning, an effect suggestive of attractor dynamics that was shown indirectly by previous studies."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/addfa9b7e234254d26e9c7f2af1005cb-Abstract.html,Kernel Measures of Independence for non-iid Data,"Xinhua Zhang, Le Song, Arthur Gretton, Alex J. Smola","Many machine learning algorithms can be formulated in the framework of statistical independence such as the Hilbert Schmidt Independence Criterion. In this paper, we extend this criterion to deal with with structured and interdependent observations. This is achieved by modeling the structures using undirected graphical models and comparing the Hilbert space embeddings of distributions. We apply this new criterion to independent component analysis and sequence clustering."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/aeb3135b436aa55373822c010763dd54-Abstract.html,Regularized Policy Iteration,"Amir M. Farahmand, Mohammad Ghavamzadeh, Shie Mannor, Csaba Szepesvári","In this paper we consider approximate policy-iteration-based reinforcement learning algorithms. In order to implement a flexible function approximation scheme we propose the use of non-parametric methods with regularization, providing a convenient way to control the complexity of the function approximator. We propose two novel regularized policy iteration algorithms by adding L2-regularization to two widely-used policy evaluation methods: Bellman residual minimization (BRM) and least-squares temporal difference learning (LSTD). We derive efficient implementation for our algorithms when the approximate value-functions belong to a reproducing kernel Hilbert space. We also provide finite-sample performance bounds for our algorithms and show that they are able to achieve optimal rates of convergence under the studied conditions."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/b3e3e393c77e35a4a3f3cbd1e429b5dc-Abstract.html,Dimensionality Reduction for Data in Multiple Feature Representations,"Yen-yu Lin, Tyng-luh Liu, Chiou-shann Fuh","In solving complex visual learning tasks, adopting multiple descriptors to more precisely characterize the data has been a feasible way for improving performance. These representations are typically high dimensional and assume diverse forms. Thus finding a way to transform them into a unified space of lower dimension generally facilitates the underlying tasks, such as object recognition or clustering. We describe an approach that incorporates multiple kernel learning with dimensionality reduction (MKL-DR). While the proposed framework is flexible in simultaneously tackling data in various feature representations, the formulation itself is general in that it is established upon graph embedding. It follows that any dimensionality reduction techniques explainable by graph embedding can be generalized by our method to consider data in multiple feature representations."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/b73ce398c39f506af761d2277d853a92-Abstract.html,Continuously-adaptive discretization for message-passing algorithms,"Michael Isard, John MacCormick, Kannan Achan","Continuously-Adaptive Discretization for Message-Passing (CAD-MP) is a new message-passing algorithm employing adaptive discretization. Most previous message-passing algorithms approximated arbitrary continuous probability distributions using either: a family of continuous distributions such as the exponential family; a particle-set of discrete samples; or a fixed, uniform discretization. In contrast, CAD-MP uses a discretization that is (i) non-uniform, and (ii) adaptive. The non-uniformity allows CAD-MP to localize interesting features (such as sharp peaks) in the marginal belief distributions with time complexity that scales logarithmically with precision, as opposed to uniform discretization which scales at best linearly. We give a principled method for altering the non-uniform discretization according to information-based measures. CAD-MP is shown in experiments on simulated data to estimate marginal beliefs much more precisely than competing approaches for the same computational expense."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/b99d193b66a6542917d2b7bee52c2574-Abstract.html,On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing,"Benjamin Schrauwen, Lars Buesing, Robert A. Legenstein","Randomly connected recurrent neural circuits have proven t o be very powerful models for online computations when a trained memoryless re adout function is appended. Such Reservoir Computing (RC) systems are commonly used in two flavors: with analog or binary (spiking) neurons in the recur rent circuits. Previous work showed a fundamental difference between these two incarnations of the RC idea. The performance of a RC system built from binary neuron s seems to depend strongly on the network connectivity structure. In network s of analog neurons such dependency has not been observed. In this article we investigate this apparent dichotomy in terms of the in-degree of the circuit nodes. Our analyses based amongst others on the Lyapunov exponent reveal that the phase transition between ordered and chaotic network behavior of binary circuits qua litatively differs from the one in analog circuits. This explains the observed decre ased computational performance of binary circuits of high node in-degree. Furt hermore, a novel mean-field predictor for computational performance is intr oduced and shown to accurately predict the numerically obtained results."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/bd686fd640be98efaae0091fa301e613-Abstract.html,Mind the Duality Gap: Logarithmic regret algorithms for online optimization,"Shai Shalev-shwartz, Sham M. Kakade","We describe a primal-dual framework for the design and analysis of online strongly convex optimization algorithms. Our framework yields the tightest known logarithmic regret bounds for Follow-The-Leader and for the gradient descent algorithm proposed in HazanKaKaAg06. We then show that one can interpolate between these two extreme cases. In particular, we derive a new algorithm that shares the computational simplicity of gradient descent but achieves lower regret in many practical situations. Finally, we further extend our framework for generalized strongly convex functions."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/be1bc7997695495f756312886f566110-Abstract.html,Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data,"Tran T. Truyen, Dinh Phung, Hung Bui, Svetha Venkatesh","Inspired by the hierarchical hidden Markov models (HHMM), we present the hierarchical semi-Markov conditional random field (HSCRF), a generalisation of embedded undirected Markov chains to model complex hierarchical, nested Markov processes. It is parameterised in a discriminative framework and has polynomial time algorithms for learning and inference. Importantly, we develop efficient algorithms for learning and constrained inference in a partially-supervised setting, which is important issue in practice where labels can only be obtained sparsely. We demonstrate the HSCRF in two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. We show that the HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/be3159ad04564bfb90db9e32851ebf9c-Abstract.html,"A spatially varying two-sample recombinant coalescent, with applications to HIV escape response","Alexander Braunstein, Zhi Wei, Shane T. Jensen, Jon D. Mcauliffe","Statistical evolutionary models provide an important mechanism for describing and understanding the escape response of a viral population under a particular therapy. We present a new hierarchical model that incorporates spatially varying mutation and recombination rates at the nucleotide level. It also maintains sep- arate parameters for treatment and control groups, which allows us to estimate treatment effects explicitly. We use the model to investigate the sequence evolu- tion of HIV populations exposed to a recently developed antisense gene therapy, as well as a more conventional drug therapy. The detection of biologically rele- vant and plausible signals in both therapy studies demonstrates the effectiveness of the method."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/beed13602b9b0e6ecb5b568ff5058f07-Abstract.html,Measures of Clustering Quality: A Working Set of Axioms for Clustering,"Shai Ben-David, Margareta Ackerman","Aiming towards the development of a general clustering theory, we discuss abstract axiomatization for clustering. In this respect, we follow up on the work of Kelinberg, (Kleinberg) that showed an impossibility result for such axiomatization. We argue that an impossibility result is not an inherent feature of clustering, but rather, to a large extent, it is an artifact of the specific formalism used in Kleinberg. As opposed to previous work focusing on clustering functions, we propose to address clustering quality measures as the primitive object to be axiomatized. We show that principles like those formulated in Kleinberg's axioms can be readily expressed in the latter framework without leading to inconsistency. A clustering-quality measure is a function that, given a data set and its partition into clusters, returns a non-negative real number representing how strong' orconclusive' the clustering is. We analyze what clustering-quality measures should look like and introduce a set of requirements (`axioms') that express these requirement and extend the translation of Kleinberg's axioms to our framework. We propose several natural clustering quality measures, all satisfying the proposed axioms. In addition, we show that the proposed clustering quality can be computed in polynomial time."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/bf62768ca46b6c3b5bea9515d1a1fc45-Abstract.html,Structure Learning in Human Sequential Decision-Making,"Daniel Acuna, Paul R. Schrater","We use graphical models and structure learning to explore how people learn policies in sequential decision making tasks. Studies of sequential decision-making in humans frequently find suboptimal performance relative to an ideal actor that knows the graph model that generates reward in the environment. We argue that the learning problem humans face also involves learning the graph structure for reward generation in the environment. We formulate the structure learning problem using mixtures of reward models, and solve the optimal action selection problem using Bayesian Reinforcement Learning. We show that structure learning in one and two armed bandit problems produces many of the qualitative behaviors deemed suboptimal in previous studies. Our argument is supported by the results of experiments that demonstrate humans rapidly learn and exploit new reward structure."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/c058f544c737782deacefa532d9add4c-Abstract.html,Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance,"Jeremy Hill, Jason Farquhar, Suzanna Martens, Felix Biessmann, Bernhard Schölkopf","From an information-theoretic perspective, a noisy transmission system such as a visual Brain-Computer Interface (BCI) speller could benefit from the use of error-correcting codes. However, optimizing the code solely according to the maximal minimum-Hamming-distance criterion tends to lead to an overall increase in target frequency of target stimuli, and hence a significantly reduced average target-to-target interval (TTI), leading to difficulties in classifying the individual event-related potentials (ERPs) due to overlap and refractory effects. Clearly any change to the stimulus setup must also respect the possible psychophysiological consequences. Here we report new EEG data from experiments in which we explore stimulus types and codebooks in a within-subject design, finding an interaction between the two factors. Our data demonstrate that the traditional, row-column code has particular spatial properties that lead to better performance than one would expect from its TTIs and Hamming-distances alone, but nonetheless error-correcting codes can improve performance provided the right stimulus type is used."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/c0f168ce8900fa56e57789e2a2f2c9d0-Abstract.html,Supervised Dictionary Learning,"Julien Mairal, Jean Ponce, Guillermo Sapiro, Andrew Zisserman, Francis R. Bach","It is now well established that sparse signal models are well suited to restoration tasks and can effectively be learned from audio, image, and video data. Recent research has been aimed at learning discriminative sparse models instead of purely reconstructive ones. This paper proposes a new step in that direction with a novel sparse representation for signals belonging to different classes in terms of a shared dictionary and multiple decision functions. It is shown that the linear variant of the model admits a simple probabilistic interpretation, and that its most general variant also admits a simple interpretation in terms of kernels. An optimization framework for learning all the components of the proposed model is presented, along with experiments on standard handwritten digit and texture classification tasks."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/c16a5320fa475530d9583c34fd356ef5-Abstract.html,Natural Image Denoising with Convolutional Networks,"Viren Jain, Sebastian Seung","We present an approach to low-level vision that combines two main ideas: the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from specific noise models. We demonstrate this approach on the challenging problem of natural image denoising. Using a test set with a hundred natural images, we find that convolutional networks provide comparable and in some cases superior performance to state of the art wavelet and Markov random field (MRF) methods. Moreover, we find that a convolutional network offers similar performance in the blind denoising setting as compared to other techniques in the non-blind setting. We also show how convolutional networks are mathematically related to MRF approaches by presenting a mean field theory for an MRF specially designed for image denoising. Although these approaches are related, convolutional networks avoid computational difficulties in MRF approaches that arise from probabilistic learning and inference. This makes it possible to learn image processing architectures that have a high degree of representational power (we train models with over 15,000 parameters), but whose computational expense is significantly less than that associated with inference in MRF approaches with even hundreds of parameters."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract.html,Optimal Response Initiation: Why Recent Experience Matters,"Matt Jones, Sachiko Kinoshita, Michael Mozer","In most cognitive and motor tasks, speed-accuracy tradeoffs are observed: Individuals can respond slowly and accurately, or quickly yet be prone to errors. Control mechanisms governing the initiation of behavioral responses are sensitive not only to task instructions and the stimulus being processed, but also to the recent stimulus history. When stimuli can be characterized on an easy-hard dimension (e.g., word frequency in a naming task), items preceded by easy trials are responded to more quickly, and with more errors, than items preceded by hard trials. We propose a rationally motivated mathematical model of this sequential adaptation of control, based on a diffusion model of the decision process in which difficulty corresponds to the drift rate for the correct response. The model assumes that responding is based on the posterior distribution over which response is correct, conditioned on the accumulated evidence. We derive this posterior as a function of the drift rate, and show that higher estimates of the drift rate lead to (normatively) faster responding. Trial-by-trial tracking of difficulty thus leads to sequential effects in speed and accuracy. Simulations show the model explains a variety of phenomena in human speeded decision making. We argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theories based on elaborate control structures."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/c361bc7b2c033a83d663b8d9fb4be56e-Abstract.html,Bayesian Experimental Design of Magnetic Resonance Imaging Sequences,"Hannes Nickisch, Rolf Pohmann, Bernhard Schölkopf, Matthias Seeger","We show how improved sequences for magnetic resonance imaging can be found through automated optimization of Bayesian design scores. Combining recent advances in approximate Bayesian inference and natural image statistics with high-performance numerical computation, we propose the first scalable Bayesian experimental design framework for this problem of high relevance to clinical and brain research. Our solution requires approximate inference for dense, non-Gaussian models on a scale seldom addressed before. We propose a novel scalable variational inference algorithm, and show how powerful methods of numerical mathematics can be modified to compute primitives in our framework. Our approach is evaluated on a realistic setup with raw data from a 3T MR scanner."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/c52f1bd66cc19d05628bd8bf27af3ad6-Abstract.html,Temporal Dynamics of Cognitive Control,"Jeremy Reynolds, Michael Mozer","Cognitive control refers to the flexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efficiently refined with subsequent task experience."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/c6e19e830859f2cb9f7c8f8cacb8d2a6-Abstract.html,Overlaying classifiers: a practical approach for optimal ranking,"Stéphan J. Clémençcon, Nicolas Vayatis","ROC curves are one of the most widely used displays to evaluate performance of scoring functions. In the paper, we propose a statistical method for directly optimizing the ROC curve. The target is known to be the regression function up to an increasing transformation and this boils down to recovering the level sets of the latter. We propose to use classifiers obtained by empirical risk minimization of a weighted classification error and then to construct a scoring rule by overlaying these classifiers. We show the consistency and rate of convergence to the optimal ROC curve of this procedure in terms of supremum norm and also, as a byproduct of the analysis, we derive an empirical estimate of the optimal ROC curve."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/cc1aa436277138f61cda703991069eaf-Abstract.html,Model selection and velocity estimation using novel priors for motion patterns,"Shuang Wu, Hongjing Lu, Alan L. Yuille","Psychophysical experiments show that humans are better at perceiving rotation and expansion than translation. These findings are inconsistent with standard models of motion integration which predict best performance for translation [6]. To explain this discrepancy, our theory formulates motion perception at two levels of inference: we first perform model selection between the competing models (e.g. translation, rotation, and expansion) and then estimate the velocity using the selected model. We define novel prior models for smooth rotation and expansion using techniques similar to those in the slow-and-smooth model [17] (e.g. Green functions of differential operators). The theory gives good agreement with the trends observed in human experiments."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/ccb0989662211f61edae2e26d58ea92f-Abstract.html,Estimation of Information Theoretic Measures for Continuous Random Variables,Fernando Pérez-Cruz,"We analyze the estimation of information theoretic measures of continuous random variables such as: differential entropy, mutual information or Kullback-Leibler divergence. The objective of this paper is two-fold. First, we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with fixed k converge almost surely, even though the k-nearest-neighbor density estimation with fixed k does not converge to its true measure. Second, we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples. Nevertheless, these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent. We show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the Hilbert Schmidt independence criterion, respectively."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/cee631121c2ec9232f3a2f028ad5c89b-Abstract.html,On the Reliability of Clustering Stability in the Large Sample Regime,"Ohad Shamir, Naftali Tishby","Clustering stability is an increasingly popular family of methods for performing model selection in data clustering. The basic idea is that the chosen model should be stable under perturbation or resampling of the data. Despite being reasonably effective in practice, these methods are not well understood theoretically, and present some difficulties. In particular, when the data is assumed to be sampled from an underlying distribution, the solutions returned by the clustering algorithm will usually become more and more stable as the sample size increases. This raises a potentially serious practical difficulty with these methods, because it means there might be some hard-to-compute sample size, beyond which clustering stability estimators 'break down' and become unreliable in detecting the most stable model. Namely, all models will be relatively stable, with differences in their stability measures depending mostly on random and meaningless sampling artifacts. In this paper, we provide a set of general sufficient conditions, which ensure the reliability of clustering stability estimators in the large sample regime. In contrast to previous work, which concentrated on specific toy distributions or specific idealized clustering frameworks, here we make no such assumptions. We then exemplify how these conditions apply to several important families of clustering algorithms, such as maximum likelihood clustering, certain types of kernel clustering, and centroid-based clustering with any Bregman divergence. In addition, we explicitly derive the non-trivial asymptotic behavior of these estimators, for any framework satisfying our conditions. This can help us understand what is considered a 'stable' model by these estimators, at least for large enough samples."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/cfa0860e83a4c3a763a7e62d825349f7-Abstract.html,Using matrices to model symbolic relationship,"Ilya Sutskever, Geoffrey E. Hinton","We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn first-order propositions such as $(2, 5) \member +\!3$ or $(Christopher, Penelope)\member has\_wife$, and higher-order propositions such as $(3, +\!3) \member plus$ and $(+\!3, -\!3) \member inverse$ or $(has\_husband, has\_wife)\in higher\_oppsex$. We further demonstrate that the system understands how higher-order propositions are related to first-order ones by showing that it can correctly answer questions about first-order propositions involving the relations $+\!3$ or $has\_wife$ even though it has not been trained on any first-order examples involving these relations."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/d04d42cdf14579cd294e5079e0745411-Abstract.html,Psychiatry: Insights into depression through normative decision-making models,"Quentin J. Huys, Joshua Vogelstein, Peter Dayan","Decision making lies at the very heart of many psychiatric diseases. It is also a central theoretical concern in a wide variety of ﬁelds and has undergone detailed, in-depth, analyses. We take as an example Major Depressive Disorder (MDD), applying insights from a Bayesian reinforcement learning framework. We focus on anhedonia and helplessness. Helplessness—a core element in the conceptual- izations of MDD that has lead to major advances in its treatment, pharmacolog- ical and neurobiological understanding—is formalized as a simple prior over the outcome entropy of actions in uncertain environments. Anhedonia, which is an equally fundamental aspect of the disease, is related to the effective reward size. These formulations allow for the design of speciﬁc tasks to measure anhedonia and helplessness behaviorally. We show that these behavioral measures capture explicit, questionnaire-based cognitions. We also provide evidence that these tasks may allow classiﬁcation of subjects into healthy and MDD groups based purely on a behavioural measure and avoiding any verbal reports.
There are strong ties between decision making and psychiatry, with maladaptive decisions and be- haviors being very prominent in people with psychiatric disorders. Depression is classically seen as following life events such as divorces and job losses. Longitudinal studies, however, have revealed that a signiﬁcant fraction of the stressors associated with depression do in fact follow MDD onset, and that they are likely due to maladaptive behaviors prominent in MDD (Kendler et al., 1999). Clinically effective ’talking’ therapies for MDD such as cognitive and dialectical behavior therapies (DeRubeis et al., 1999; Bortolotti et al., 2008; Gotlib and Hammen, 2002; Power, 2005) explicitly concentrate on altering patients’ maladaptive behaviors and decision making processes.
Decision making is a promising avenue into psychiatry for at least two more reasons. First, it offers powerful analytical tools. Control problems related to decision making are prevalent in a huge diversity of ﬁelds, ranging from ecology to economics, computer science and engineering. These ﬁelds have produced well-founded and thoroughly characterized frameworks within which many issues in decision making can be framed. Here, we will focus on framing issues identiﬁed in psychiatric settings within a normative decision making framework.
Its second major strength comes from its relationship to neurobiology, and particularly those neuro- modulatory systems which are powerfully affected by all major clinically effective pharmacothera- pies in psychiatry. The understanding of these systems has beneﬁted signiﬁcantly from theoretical accounts of optimal control such as reinforcement learning (Montague et al., 1996; Kapur and Rem- ington, 1996; Smith et al., 1999; Yu and Dayan, 2005; Dayan and Yu, 2006). Such accounts may be useful to identify in more speciﬁc terms the roles of the neuromodulators in psychiatry (Smith et al., 2004; Williams and Dayan, 2005; Moutoussis et al., 2008; Dayan and Huys, 2008).
∗qhuys@cantab.net, joshuav@jhu.edu, dayan@gatsby.ucl.ac.uk; www.gatsby.ucl.ac.uk/∼qhuys/pub.html"
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/d07e70efcfab08731a97e7b91be644de-Abstract.html,Characteristic Kernels on Groups and Semigroups,"Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, Bharath K. Sriperumbudur","Embeddings of random variables in reproducing kernel Hilbert spaces (RKHSs) may be used to conduct statistical inference based on higher order moments. For sufficiently rich (characteristic) RKHSs, each probability distribution has a unique embedding, allowing all statistical properties of the distribution to be taken into consideration. Necessary and sufficient conditions for an RKHS to be characteristic exist for $\R^n$. In the present work, conditions are established for an RKHS to be characteristic on groups and semigroups. Illustrative examples are provided, including characteristic kernels on periodic domains, rotation matrices, and $\R^n_+$."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/d1f491a404d6854880943e5c3cd9ca25-Abstract.html,Multi-label Multiple Kernel Learning,"Shuiwang Ji, Liang Sun, Rong Jin, Jieping Ye","We present a multi-label multiple kernel learning (MKL) formulation, in which the data are embedded into a low-dimensional space directed by the instance-label correlations encoded into a hypergraph. We formulate the problem in the kernel-induced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the MKL framework. The proposed learning formulation leads to a non-smooth min-max problem, and it can be cast into a semi-infinite linear program (SILP). We further propose an approximate formulation with a guaranteed error bound which involves an unconstrained and convex optimization problem. In addition, we show that the objective function of the approximate formulation is continuously differentiable with Lipschitz gradient, and hence existing methods can be employed to compute the optimal solution efficiently. We apply the proposed formulation to the automated annotation of Drosophila gene expression pattern images, and promising results have been reported in comparison with representative algorithms."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/d56b9fc4b0f1be8871f5e1c40c0067e7-Abstract.html,Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl,"Juan Huo, Zhijun Yang, Alan F. Murray","The visual and auditory map alignment in the Superior Colliculus (SC) of barn owl is important for its accurate localization for prey behavior. Prism learning or Blindness may interfere this alignment and cause loss of the capability of accurate prey. However, juvenile barn owl could recover its sensory map alignment by shifting its auditory map. The adaptation of this map alignment is believed based on activity dependent axon developing in Inferior Colliculus (IC). A model is built to explore this mechanism. In this model, axon growing process is instructed by an inhibitory network in SC while the strength of the inhibition adjusted by Spike Timing Dependent Plasticity (STDP). We test and analyze this mechanism by application of the neural structures involved in spatial localization in a robotic system."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/d58072be2820e8682c0a27c0518e805e-Abstract.html,Spectral Hashing,"Yair Weiss, Antonio Torralba, Rob Fergus","Semantic hashing seeks compact binary codes of datapoints so that the Hamming distance between codewords correlates with semantic similarity. Hinton et al. used a clever implementation of autoencoders to find such codes. In this paper, we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresh- olded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigen- functions of manifolds, we show how to efficiently calculate the code of a novel datapoint. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes significantly outperform the state-of-the art."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/d5cfead94f5350c12c322b5b664544c1-Abstract.html,Multi-resolution Exploration in Continuous Spaces,"Ali Nouri, Michael L. Littman","The essence of exploration is acting to try to decrease uncertainty. We propose a new methodology for representing uncertainty in continuous-state control problems. Our approach, multi-resolution exploration (MRE), uses a hierarchical mapping to identify regions of the state space that would benefit from additional samples. We demonstrate MRE's broad utility by using it to speed up learning in a prototypical model-based and value-based reinforcement-learning method. Empirical results show that MRE improves upon state-of-the-art exploration approaches."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/d709f38ef758b5066ef31b18039b8ce5-Abstract.html,A computational model of hippocampal function in trace conditioning,"Elliot A. Ludvig, Richard S. Sutton, Eric Verbeek, E. J. Kehoe","We present a new reinforcement-learning model for the role of the hippocampus in classical conditioning, focusing on the differences between trace and delay conditioning. In the model, all stimuli are represented both as unindividuated wholes and as a series of temporal elements with varying delays. These two stimulus representations interact, producing different patterns of learning in trace and delay conditioning. The model proposes that hippocampal lesions eliminate long-latency temporal elements, but preserve short-latency temporal elements. For trace conditioning, with no contiguity between stimulus and reward, these long-latency temporal elements are vital to learning adaptively timed responses. For delay conditioning, in contrast, the continued presence of the stimulus supports conditioned responding, and the short-latency elements suppress responding early in the stimulus. In accord with the empirical data, simulated hippocampal damage impairs trace conditioning, but not delay conditioning, at medium-length intervals. With longer intervals, learning is impaired in both procedures, and, with shorter intervals, in neither. In addition, the model makes novel predictions about the response topography with extended stimuli or post-training lesions. These results demonstrate how temporal contiguity, as in delay conditioning, changes the timing problem faced by animals, rendering it both easier and less susceptible to disruption by hippocampal lesions."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/d736bb10d83a904aefc1d6ce93dc54b8-Abstract.html,Online Prediction on Large Diameter Graphs,"Mark Herbster, Guy Lever, Massimiliano Pontil","Current on-line learning algorithms for predicting the labelling of a graph have an important limitation in the case of large diameter graphs; the number of mistakes made by such algorithms may be proportional to the square root of the number of vertices, even when tackling simple problems. We overcome this problem with an efficient algorithm which achieves a logarithmic mistake bound. Furthermore, current algorithms are optimised for data which exhibits cluster-structure; we give an additional algorithm which performs well locally in the presence of cluster structure and on large diameter graphs."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/d93ed5b6db83be78efb0d05ae420158e-Abstract.html,Sparse probabilistic projections,"Cédric Archambeau, Francis R. Bach","We present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a pre-processing tool for the construction of template attacks."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/d947bf06a885db0d477d707121934ff8-Abstract.html,Sparsity of SVMs that use the epsilon-insensitive loss,"Ingo Steinwart, Andreas Christmann","In this paper lower and upper bounds for the number of support vectors are derived for support vector machines (SVMs) based on the epsilon-insensitive loss function. It turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution. Finally, we briefly discuss a trade-off in epsilon between sparsity and accuracy if the SVM is used to estimate the conditional median."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/d96409bf894217686ba124d7356686c9-Abstract.html,Automatic online tuning for fast Gaussian summation,"Vlad I. Morariu, Balaji V. Srinivasan, Vikas C. Raykar, Ramani Duraiswami, Larry S. Davis","Many machine learning algorithms require the summation of Gaussian kernel functions, an expensive operation if implemented straightforwardly. Several methods have been proposed to reduce the computational complexity of evaluating such sums, including tree and analysis based methods. These achieve varying speedups depending on the bandwidth, dimension, and prescribed error, making the choice between methods difficult for machine learning tasks. We provide an algorithm that combines tree methods with the Improved Fast Gauss Transform (IFGT). As originally proposed the IFGT suffers from two problems: (1) the Taylor series expansion does not perform well for very low bandwidths, and (2) parameter selection is not trivial and can drastically affect performance and ease of use. We address the first problem by employing a tree data structure, resulting in four evaluation methods whose performance varies based on the distribution of sources and targets and input parameters such as desired accuracy and bandwidth. To solve the second problem, we present an online tuning approach that results in a black box method that automatically chooses the evaluation method and its parameters to yield the best performance for the input data, desired accuracy, and bandwidth. In addition, the new IFGT parameter selection approach allows for tighter error bounds. Our approach chooses the fastest method at negligible additional cost, and has superior performance in comparisons with previous approaches."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/da4fb5c6e93e74d3df8527599fa62642-Abstract.html,Reducing statistical dependencies in natural signals using radial Gaussianization,"Siwei Lyu, Eero P. Simoncelli","We consider the problem of efficiently encoding a signal by transforming it to a new representation whose components are statistically independent. A widely studied linear solution, independent components analysis (ICA), exists for the case when the signal is generated as a linear transformation of independent non- Gaussian sources. Here, we examine a complementary case, in which the source is non-Gaussian but elliptically symmetric. In this case, no linear transform suffices to properly decompose the signal into independent components, but we show that a simple nonlinear transformation, which we call radial Gaussianization (RG), is able to remove all dependencies. We then demonstrate this methodology in the context of natural signal statistics. We first show that the joint distributions of bandpass filter responses, for both sound and images, are better described as elliptical than linearly transformed independent sources. Consistent with this, we demonstrate that the reduction in dependency achieved by applying RG to either pairs or blocks of bandpass filter responses is significantly greater than that achieved by PCA or ICA."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/dc6a70712a252123c40d2adba6a11d84-Abstract.html,A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning,"Massih R. Amini, Nicolas Usunier, François Laviolette","In this paper we present two transductive bounds on the risk of the majority vote estimated over partially labeled training sets. Our first bound is tight when the additional unlabeled training data are used in the cases where the voted classifier makes its errors on low margin observations and where the errors of the associated Gibbs classifier can accurately be estimated. In semi-supervised learning, considering the margin as an indicator of confidence constitutes the working hypothesis of algorithms which search the decision boundary on low density regions. In this case, we propose a second bound on the joint probability that the voted classifier makes an error over an example having its margin over a fixed threshold. As an application we are interested on self-learning algorithms which assign iteratively pseudo-labels to unlabeled training examples having margin above a threshold obtained from this bound. Empirical results on different datasets show the effectiveness of our approach compared to the same algorithm and the TSVM in which the threshold is fixed manually."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/dc82d632c9fcecb0778afbc7924494a6-Abstract.html,Nonrigid Structure from Motion in Trajectory Space,"Ijaz Akhter, Yaser Sheikh, Sohaib Khan, Takeo Kanade","Existing approaches to nonrigid structure from motion assume that the instantaneous 3D shape of a deforming object is a linear combination of basis shapes, which have to be estimated anew for each video sequence. In contrast, we propose that the evolving 3D structure be described by a linear combination of basis trajectories. The principal advantage of this lateral approach is that we do not need to estimate any basis vectors during computation. Instead, we show that generic bases over trajectories, such as the Discrete Cosine Transform (DCT) bases, can be used to effectively describe most real motions. This results in a significant reduction in unknowns, and corresponding stability, in estimation. We report empirical performance, quantitatively using motion capture data and qualitatively on several video sequences exhibiting nonrigid motions including piece-wise rigid motion, articulated motion, partially nonrigid motion (such as a facial expression), and highly nonrigid motion (such as a person dancing)."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/df263d996281d984952c07998dc54358-Abstract.html,Transfer Learning by Distribution Matching for Targeted Advertising,"Steffen Bickel, Christoph Sawade, Tobias Scheffer","We address the problem of learning classifiers for several related tasks that may differ in their joint distribution of input and output variables. For each task, small - possibly even empty - labeled samples and large unlabeled samples are available. While the unlabeled samples reflect the target distribution, the labeled samples may be biased. We derive a solution that produces resampling weights which match the pool of all examples to the target distribution of any given task. Our work is motivated by the problem of predicting sociodemographic features for users of web portals, based on the content which they have accessed. Here, questionnaires offered to a small portion of each portal's users produce biased samples. Transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/e0c641195b27425bb056ac56f8953d24-Abstract.html,A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation,"Richard S. Sutton, Hamid R. Maei, Csaba Szepesvári","We introduce the first temporal-difference learning algorithm that is stable with linear function approximation and off-policy training, for any finite Markov decision process, target policy, and exciting behavior policy, and whose complexity scales linearly in the number of parameters. We consider an i.i.d.\ policy-evaluation setting in which the data need not come from on-policy experience. The gradient temporal-difference (GTD) algorithm estimates the expected update vector of the TD(0) algorithm and performs stochastic gradient descent on its L_2 norm. Our analysis proves that its expected update is in the direction of the gradient, assuring convergence under the usual stochastic approximation conditions to the same least-squares solution as found by the LSTD, but without its quadratic computational complexity. GTD is online and incremental, and does not involve multiplying by products of likelihood ratios as in importance-sampling methods."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/e0cf1f47118daebc5b16269099ad7347-Abstract.html,Estimating Robust Query Models with Convex Optimization,Kevyn Collins-thompson,"Query expansion is a long-studied approach for improving retrieval effectiveness by enhancing the userâs original query with additional related terms. Current algorithms for automatic query expansion have been shown to consistently improve retrieval accuracy on average, but are highly unstable and have bad worst-case performance for individual queries. We introduce a novel risk framework that formulates query model estimation as a constrained metric labeling problem on a graph of term relations. Themodel combines assignment costs based on a baseline feedback algorithm, edge weights based on term similarity, and simple constraints to enforce aspect balance, aspect coverage, and term centrality. Results across multiple standard test collections show consistent and dramatic reductions in the number and magnitude of expansion failures, while retaining the strong positive gains of the baseline algorithm."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/e0ec453e28e061cc58ac43f91dc2f3f0-Abstract.html,Learning Taxonomies by Dependence Maximization,"Matthew Blaschko, Arthur Gretton","We introduce a family of unsupervised algorithms, numerical taxonomy clustering, to simultaneously cluster data, and to learn a taxonomy that encodes the relationship between the clusters. The algorithms work by maximizing the dependence between the taxonomy and the original data. The resulting taxonomy is a more informative visualization of complex data than simple clustering; in addition, taking into account the relations between different clusters is shown to substantially improve the quality of the clustering, when compared with state-of-the-art algorithms in the literature (both spectral clustering and a previous dependence maximization approach). We demonstrate our algorithm on image and text data."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/e2230b853516e7b05d79744fbd4c9c13-Abstract.html,Finding Latent Causes in Causal Networks: an Efficient Approach Based on Markov Blankets,"Jean-philippe Pellet, André Elisseeff","Causal structure-discovery techniques usually assume that all causes of more than one variable are observed. This is the so-called causal sufficiency assumption. In practice, it is untestable, and often violated. In this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data. Similar to algorithms such as IC* and FCI, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables. Assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditional-independence tests w.r.t. the number of variables. We show with experiments that our algorithm is comparable to the state-of-the-art FCI algorithm in accuracy, while being several orders of magnitude faster on large problems. We conclude that MBCS* makes a new range of causally insufficient problems computationally tractable."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/e369853df766fa44e1ed0ff613f563bd-Abstract.html,Goal-directed decision making in prefrontal cortex: a computational framework,"Matthew Botvinick, James An","Research in animal learning and behavioral neuroscience has distinguished between two forms of action control: a habit-based form, which relies on stored action values, and a goal-directed form, which forecasts and compares action outcomes based on a model of the environment. While habit-based control has been the subject of extensive computational research, the computational principles underlying goal-directed control in animals have so far received less attention. In the present paper, we advance a computational framework for goal-directed control in animals and humans. We take three empirically motivated points as founding premises: (1) Neurons in dorsolateral prefrontal cortex represent action policies, (2) Neurons in orbitofrontal cortex represent rewards, and (3) Neural computation, across domains, can be appropriately understood as performing structured probabilistic inference. On a purely computational level, the resulting account relates closely to previous work using Bayesian inference to solve Markov decision problems, but extends this work by introducing a new algorithm, which provably converges on optimal plans. On a cognitive and neuroscientific level, the theory provides a unifying framework for several different forms of goal-directed action selection, placing emphasis on a novel form, within which orbitofrontal reward representations directly drive policy selection."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/e3796ae838835da0b6f6ea37bcf8bcb7-Abstract.html,Localized Sliced Inverse Regression,"Qiang Wu, Sayan Mukherjee, Feng Liang","We developed localized sliced inverse regression for supervised dimension reduction. It has the advantages of preventing degeneracy, increasing estimation accuracy, and automatic subclass discovery in classification problems. A semisupervised version is proposed for the use of unlabeled data. The utility is illustrated on simulated as well as real data sets."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/e4a6222cdb5b34375400904f03d8e6a5-Abstract.html,Near-optimal Regret Bounds for Reinforcement Learning,"Peter Auer, Thomas Jaksch, Ronald Ortner","For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s1,s2 there is a policy which moves from s1 to s2 in at most D steps (on average). We present a reinforcement learning algorithm with total regret O(DSAT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. This bound holds with high probability. We also present a corresponding lower bound of Omega(DSAT) on the total regret of any learning algorithm. Both bounds demonstrate the utility of the diameter as structural parameter of the MDP."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/e555ebe0ce426f7f9b2bef0706315e0c-Abstract.html,Non-parametric Regression Between Manifolds,"Florian Steinke, Matthias Hein","This paper discusses non-parametric regression between Riemannian manifolds. This learning problem arises frequently in many application areas ranging from signal processing, computer vision, over robotics to computer graphics. We present a new algorithmic scheme for the solution of this general learning problem based on regularized empirical risk minimization. The regularization functional takes into account the geometry of input and output manifold, and we show that it implements a prior which is particularly natural. Moreover, we demonstrate that our algorithm performs well in a difficult surface registration problem."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/e70611883d2760c8bbafb4acb29e3446-Abstract.html,Unsupervised Learning of Visual Sense Models for Polysemous Words,"Kate Saenko, Trevor Darrell",Polysemy is a problem for methods that exploit image search engines to build object category models. Existing unsupervised approaches do not take word sense into consideration. We propose a new method that uses a dictionary to learn models of visual word sense from a large collection of unlabeled web data. The use of LDA to discover a latent sense space makes the model robust despite the very limited nature of dictionary definitions. The definitions are used to learn a distribution in the latent space that best represents a sense. The algorithm then uses the text surrounding image links to retrieve images with high probability of a particular dictionary sense. An object classifier is trained on the resulting sense-specific images. We evaluate our method on a dataset obtained by searching the web for polysemous words. Category classification experiments show that our dictionary-based approach outperforms baseline methods.
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/e7f8a7fb0b77bcb3b283af5be021448f-Abstract.html,Extended Grassmann Kernels for Subspace-Based Learning,"Jihun Hamm, Daniel D. Lee","Subspace-based learning problems involve data whose elements are linear subspaces of a vector space. To handle such data structures, Grassmann kernels have been proposed and used previously. In this paper, we analyze the relationship between Grassmann kernels and probabilistic similarity measures. Firstly, we show that the KL distance in the limit yields the Projection kernel on the Grassmann manifold, whereas the Bhattacharyya kernel becomes trivial in the limit and is suboptimal for subspace-based problems. Secondly, based on our analysis of the KL distance, we propose extensions of the Projection kernel which can be extended to the set of affine as well as scaled subspaces. We demonstrate the advantages of these extended kernels for classification and recognition tasks with Support Vector Machines and Kernel Discriminant Analysis using synthetic and real image databases."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/e820a45f1dfc7b95282d10b6087e11c0-Abstract.html,Implicit Mixtures of Restricted Boltzmann Machines,"Vinod Nair, Geoffrey E. Hinton","We present a mixture model whose components are Restricted Boltzmann Machines (RBMs). This possibility has not been considered before because computing the partition function of an RBM is intractable, which appears to make learning a mixture of RBMs intractable as well. Surprisingly, when formulated as a third-order Boltzmann machine, such a mixture model can be learned tractably using contrastive divergence. The energy function of the model captures three-way interactions among visible units, hidden units, and a single hidden multinomial unit that represents the cluster labels. The distinguishing feature of this model is that, unlike other mixture models, the mixing proportions are not explicitly parameterized. Instead, they are defined implicitly via the energy function and depend on all the parameters in the model. We present results for the MNIST and NORB datasets showing that the implicit mixture of RBMs learns clusters that reflect the class structure in the data."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/e8a642ed6a9ad20fb159472950db3d65-Abstract.html,Self-organization using synaptic plasticity,"Vicençc Gómez, Andreas Kaltenbrunner, Vicente López, Hilbert J. Kappen","Large networks of spiking neurons show abrupt changes in their collective dynamics resembling phase transitions studied in statistical physics. An example of this phenomenon is the transition from irregular, noise-driven dynamics to regular, self-sustained behavior observed in networks of integrate-and-fire neurons as the interaction strength between the neurons increases. In this work we show how a network of spiking neurons is able to self-organize towards a critical state for which the range of possible inter-spike-intervals (dynamic range) is maximized. Self-organization occurs via synaptic dynamics that we analytically derive. The resulting plasticity rule is defined locally so that global homeostasis near the critical state is achieved by local regulation of individual synapses."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/eb160de1de89d9058fcb0b968dbbbd68-Abstract.html,Interpreting the neural code with Formal Concept Analysis,"Dominik Endres, Peter Foldiak","We propose a novel application of Formal Concept Analysis (FCA) to neural decoding: instead of just trying to figure out which stimulus was presented, we demonstrate how to explore the semantic relationships between the neural representation of large sets of stimuli. FCA provides a way of displaying and interpreting such relationships via concept lattices. We explore the effects of neural code sparsity on the lattice. We then analyze neurophysiological data from high-level visual cortical area STSa, using an exact Bayesian approach to construct the formal context needed by FCA. Prominent features of the resulting concept lattices are discussed, including indications for a product-of-experts code in real neurons."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/ebd9629fc3ae5e9f6611e2ee05a31cef-Abstract.html,Global Ranking Using Continuous Conditional Random Fields,"Tao Qin, Tie-yan Liu, Xu-dong Zhang, De-sheng Wang, Hang Li","This paper studies global ranking problem by learning to rank methods. Conventional learning to rank methods are usually designed for `local ranking', in the sense that the ranking model is defined on a single object, for example, a document in information retrieval. For many applications, this is a very loose approximation. Relations always exist between objects and it is better to define the ranking model as a function on all the objects to be ranked (i.e., the relations are also included). This paper refers to the problem as global ranking and proposes employing a Continuous Conditional Random Fields (CRF) for conducting the learning task. The Continuous CRF model is defined as a conditional probability distribution over ranking scores of objects conditioned on the objects. It can naturally represent the content information of objects as well as the relation information between objects, necessary for global ranking. Taking two specific information retrieval tasks as examples, the paper shows how the Continuous CRF method can perform global ranking better than baselines."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/ec5aa0b7846082a2415f0902f0da88f2-Abstract.html,Improving on Expectation Propagation,"Manfred Opper, Ulrich Paquet, Ole Winther","We develop as series of corrections to Expectation Propagation (EP), which is one of the most popular methods for approximate probabilistic inference. These corrections can lead to improvements of the inference approximation or serve as a sanity check, indicating when EP yields unrealiable results."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/eddea82ad2755b24c4e168c5fc2ebd40-Abstract.html,Learning a discriminative hidden part model for human action recognition,"Yang Wang, Greg Mori","We present a discriminative part-based approach for human action recognition from video sequences using motion features. Our model is based on the recently proposed hidden conditional random field~(hCRF) for object recognition. Similar to hCRF for object recognition, we model a human action by a flexible constellation of parts conditioned on image observations. Different from object recognition, our model combines both large-scale global features and local patch features to distinguish various actions. Our experimental results show that our model is comparable to other state-of-the-art approaches in action recognition. In particular, our experimental results demonstrate that combining large-scale global features and local patch features performs significantly better than directly applying hCRF on local patches alone."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/edfbe1afcf9246bb0d40eb4d8027d90f-Abstract.html,Adaptive Template Matching with Shift-Invariant Semi-NMF,"Jonathan L. Roux, Alain D. Cheveigné, Lucas C. Parra","How does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes? One could think of using template matching or matching pursuit to find the arbitrarily shifted linear components. However, traditional matching approaches require that the templates be known a priori. To overcome this restriction we use instead semi Non-Negative Matrix Factorization (semi-NMF) that we extend to allow for time shifts when matching the templates to the signal. The algorithm estimates templates directly from the data along with their non-negative amplitudes. The resulting method can be thought of as an adaptive template matching procedure. We demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings. On these data the algorithm essentially performs spike detection and unsupervised spike clustering. Results on simulated data and extracellular recordings indicate that the method performs well for signal-to-noise ratios of 6dB or higher and that spike templates are recovered accurately provided they are sufficiently different."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/ee14c41e92ec5c97b54cf9b74e25bd99-Abstract.html,Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM,"Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada","Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the firing rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efficient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/eecca5b6365d9607ee5a9d336962c534-Abstract.html,Partially Observed Maximum Entropy Discrimination Markov Networks,"Jun Zhu, Eric P. Xing, Bo Zhang","Learning graphical models with hidden variables can offer semantic insights to complex data and lead to salient structured predictors without relying on expensive, sometime unattainable fully annotated training data. While likelihood-based methods have been extensively explored, to our knowledge, learning structured prediction models with latent variables based on the max-margin principle remains largely an open problem. In this paper, we present a partially observed Maximum Entropy Discrimination Markov Network (PoMEN) model that attempts to combine the advantages of Bayesian and margin based paradigms for learning Markov networks from partially labeled data. PoMEN leads to an averaging prediction rule that resembles a Bayes predictor that is more robust to overfitting, but is also built on the desirable discriminative laws resemble those of the M$^3$N. We develop an EM-style algorithm utilizing existing convex optimization algorithms for M$^3$N as a subroutine. We demonstrate competent performance of PoMEN over existing methods on a real-world web data extraction task."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f0e52b27a7a5d6a1a87373dffa53dbe5-Abstract.html,Playing Pinball with non-invasive BCI,"Matthias Krauledat, Konrad Grzeska, Max Sagebaum, Benjamin Blankertz, Carmen Vidaurre, Klaus-Robert Müller, Michael Schröder","Compared to invasive Brain-Computer Interfaces (BCI), non-invasive BCI systems based on Electroencephalogram (EEG) signals have not been applied successfully for complex control tasks. In the present study, however, we demonstrate this is possible and report on the interaction of a human subject with a complex real device: a pinball machine. First results in this single subject study clearly show that fast and well-timed control well beyond chance level is possible, even though the environment is extremely rich and requires complex predictive behavior. Using machine learning methods for mental state decoding, BCI-based pinball control is possible within the first session without the necessity to employ lengthy subject training. While the current study is still of anecdotal nature, it clearly shows that very compelling control with excellent timing and dynamics is possible for a non-invasive BCI."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f11bec1411101c743f64df596773d0b2-Abstract.html,Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction,"Shay B. Cohen, Kevin Gimpel, Noah A. Smith","We explore a new Bayesian model for probabilistic grammars, a family of distributions over discrete structures that includes hidden Markov models and probabilistic context-free grammars. Our model extends the correlated topic model framework to probabilistic grammars, exploiting the logistic normal distribution as a prior over the grammar parameters. We derive a variational EM algorithm for that model, and then experiment with the task of unsupervised grammar induction for natural language dependency parsing. We show that our model achieves superior results over previous models that use different priors."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html,Risk Bounds for Randomized Sample Compressed Classifiers,Mohak Shah,"We derive risk bounds for the randomized classifiers in Sample Compressions settings where the classifier-specification utilizes two sources of information viz. the compression set and the message string. By extending the recently proposed Occamâs Hammer principle to the data-dependent settings, we derive point-wise versions of the bounds on the stochastic sample compressed classifiers and also recover the corresponding classical PAC-Bayes bound. We further show how these compare favorably to the existing results."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f2201f5191c4e92cc5af043eebfd0946-Abstract.html,Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text,"Yi Zhang, Artur Dubrawski, Jeff G. Schneider","In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any specific task in the same space through regularization. In an empirical study, we construct 190 different text classification tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classification tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the specific task to be enhanced, and the prediction model used."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f387624df552cea2f369918c5e1e12bc-Abstract.html,Online Optimization in X-Armed Bandits,"Sébastien Bubeck, Gilles Stoltz, Csaba Szepesvári, Rémi Munos","We consider a generalization of stochastic bandit problems where the set of arms, X, is allowed to be a generic topological space. We constraint the mean-payoff function with a dissimilarity function over X in a way that is more general than Lipschitz. We construct an arm selection policy whose regret improves upon previous result for a large class of problems. In particular, our results imply that if X is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally Hölder with a known exponent, then the expected regret is bounded up to a logarithmic factor by $n$, i.e., the rate of the growth of the regret is independent of the dimension of the space. Moreover, we prove the minimax optimality of our algorithm for the class of mean-payoff functions we consider."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html,Variational Mixture of Gaussian Process Experts,"Chao Yuan, Claus Neubauer","Mixture of Gaussian processes models extended a single Gaussian process with ability of modeling multi-modal data and reduction of training complexity. Previous inference algorithms for these models are mostly based on Gibbs sampling, which can be very slow, particularly for large-scale data sets. We present a new generative mixture of experts model. Each expert is still a Gaussian process but is reformulated by a linear model. This breaks the dependency among training outputs and enables us to use a much faster variational Bayesian algorithm for training. Our gating network is more flexible than previous generative approaches as inputs for each expert are modeled by a Gaussian mixture model. The number of experts and number of Gaussian components for an expert are inferred automatically. A variety of tests show the advantages of our method."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f5deaeeae1538fb6c45901d524ee2f98-Abstract.html,"On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost","Hamed Masnadi-shirazi, Nuno Vasconcelos","The machine learning problem of classifier design is studied from the perspective of probability elicitation, in statistics. This shows that the standard approach of proceeding from the specification of a loss, to the minimization of conditional risk is overly restrictive. It is shown that a better alternative is to start from the specification of a functional form for the minimum conditional risk, and derive the loss function. This has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classification problems. These points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classifier design, and is robust to the contamination of data with outliers. A new boosting algorithm, SavageBoost, is derived for the minimization of this loss. Experimental results show that it is indeed less sensitive to outliers than conventional methods, such as Ada, Real, or LogitBoost, and converges in fewer iterations."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f5f8590cd58a54e94377e6ae2eded4d9-Abstract.html,Non-stationary dynamic Bayesian networks,"Joshua W. Robinson, Alexander J. Hartemink","A principled mechanism for identifying conditional dependencies in time-series data is provided through structure learning of dynamic Bayesian networks (DBNs). An important assumption of DBN structure learning is that the data are generated by a stationary processâan assumption that is not true in many important settings. In this paper, we introduce a new class of graphical models called non-stationary dynamic Bayesian networks, in which the conditional dependence structure of the underlying data-generation process is permitted to change over time. Non-stationary dynamic Bayesian networks represent a new framework for studying problems in which the structure of a network is evolving over time. We define the non-stationary DBN model, present an MCMC sampling algorithm for learning the structure of the model from time-series data under different assumptions, and demonstrate the effectiveness of the algorithm on both simulated and biological data."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f7664060cc52bc6f3d620bcedc94a4b6-Abstract.html,Nonlinear causal discovery with additive noise models,"Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jonas Peters, Bernhard Schölkopf","The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models are often used because these models are well understood and there are well-known methods to fit them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that in fact the basic linear framework can be generalized to nonlinear models with additive noise. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identified. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identification power provided by nonlinearities."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f76a89f0cb91bc419542ce9fa43902dc-Abstract.html,Simple Local Models for Complex Dynamical Systems,"Erik Talvitie, Satinder P. Singh","We present a novel mathematical formalism for the idea of a local model,'' a model of a potentially complex dynamical system that makes only certain predictions in only certain situations. As a result of its restricted responsibilities, a local model may be far simpler than a complete model of the system. We then show how one might combine several local models to produce a more detailed model. We demonstrate our ability to learn a collection of local models on a large-scale example and do a preliminary empirical comparison of learning a collection of local models and some other model learning methods."""
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f79921bbae40a577928b76d2fc3edc2a-Abstract.html,Fitted Q-iteration by Advantage Weighted Regression,"Gerhard Neumann, Jan R. Peters","Recently, fitted Q-iteration (FQI) based methods have become more popular due to their increased sample efficiency, a more stable learning process and the higher quality of the resulting policy. However, these methods remain hard to use for continuous action spaces which frequently occur in real-world tasks, e.g., in robotics and other technical applications. The greedy action selection commonly used for the policy improvement step is particularly problematic as it is expensive for continuous actions, can cause an unstable learning process, introduces an optimization bias and results in highly non-smooth policies unsuitable for real-world systems. In this paper, we show that by using a soft-greedy action selection the policy improvement step used in FQI can be simplified to an inexpensive advantage-weighted regression. With this result, we are able to derive a new, computationally efficient FQI algorithm which can even deal with high dimensional action spaces."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f90f2aca5c640289d0a29417bcb63a37-Abstract.html,On the Generalization Ability of Online Strongly Convex Programming Algorithms,"Sham M. Kakade, Ambuj Tewari","This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. The bound also solves an open problem regarding the convergence rate of {\pegasos}, a recently proposed method for solving the SVM optimization problem."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/f9b902fc3289af4dd08de5d1de54f68f-Abstract.html,Multiscale Random Fields with Application to Contour Grouping,"Longin J. Latecki, Chengen Lu, Marc Sobel, Xiang Bai","We introduce a new interpretation of multiscale random fields (MSRFs) that admits efficient optimization in the framework of regular (single level) random fields (RFs). It is based on a new operator, called append, that combines sets of random variables (RVs) to single RVs. We assume that a MSRF can be decomposed into disjoint trees that link RVs at different pyramid levels. The append operator is then applied to map RVs in each tree structure to a single RV. We demonstrate the usefulness of the proposed approach on a challenging task involving grouping contours of target shapes in images. MSRFs provide a natural representation of multiscale contour models, which are needed in order to cope with unstable contour decompositions. The append operator allows us to find optimal image labels using the classical framework of relaxation labeling, Alternative methods like Markov Chain Monte Carlo (MCMC) could also be used."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Abstract.html,Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex,"Arno Onken, Steffen Grünewälder, Matthias Munk, Klaus Obermayer","Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of Shannon entropy are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three significant findings: (1) copula-based distributions provide better fits than discretized multivariate normal distributions; (2) negative binomial margins fit the data better than Poisson margins; and (3) a dependence model that includes only pairwise interactions overestimates the information entropy by at least 19% compared to the model with higher order interactions."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/fa83a11a198d5a7f0bf77a1987bcd006-Abstract.html,Predictive Indexing for Fast Search,"Sharad Goel, John Langford, Alexander L. Strehl","We tackle the computational problem of query-conditioned search. Given a machine-learned scoring rule and a query distribution, we build a predictive index by precomputing lists of potential results sorted based on an expected score of the result over future queries. The predictive index datastructure supports an anytime algorithm for approximate retrieval of the top elements. The general approach is applicable to webpage ranking, internet advertisement, and approximate nearest neighbor search. It is particularly effective in settings where standard techniques (e.g., inverted indices) are intractable. We experimentally find substantial improvement over existing methods for internet advertisement and approximate nearest neighbors."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/fc49306d97602c8ed1be1dfbf0835ead-Abstract.html,Human Active Learning,"Rui M. Castro, Charles Kalish, Robert Nowak, Ruichen Qian, Tim Rogers, Xiaojin Zhu","We investigate a topic at the interface of machine learning and cognitive science. Human active learning, where learners can actively query the world for information, is contrasted with passive learning from random examples. Furthermore, we compare human active learning performance with predictions from statistical learning theory. We conduct a series of human category learning experiments inspired by a machine learning task for which active and passive learning error bounds are well understood, and dramatically distinct. Our results indicate that humans are capable of actively selecting informative queries, and in doing so learn better and faster than if they are given random training data, as predicted by learning theory. However, the improvement over passive learning is not as dramatic as that achieved by machine active learning algorithms. To the best of our knowledge, this is the first quantitative study comparing human category learning in active versus passive settings."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/fccb3cdc9acc14a6e70a12f74560c026-Abstract.html,Clustered Multi-Task Learning: A Convex Formulation,"Laurent Jacob, Jean-philippe Vert, Francis R. Bach","In multi-task learning several related tasks are considered simultaneously, with the hope that by an appropriate sharing of information across tasks, each task may benefit from the others. In the context of learning linear functions for supervised classification or regression, this can be achieved by including a priori information about the weight vectors associated with the tasks, and how they are expected to be related to each other. In this paper, we assume that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors. We design a new spectral norm that encodes this a priori assumption, without the prior knowledge of the partition of tasks into groups, resulting in a new convex optimization formulation for multi-task learning. We show in simulations on synthetic examples and on the iedb MHC-I binding dataset, that our approach outperforms well-known convex methods for multi-task learning, as well as related non convex methods dedicated to the same problem."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/fccb60fb512d13df5083790d64c4d5dd-Abstract.html,Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation,"Dotan D. Castro, Dmitry Volkinshtein, Ron Meir","Actor-critic algorithms for reinforcement learning are achieving renewed popularity due to their good convergence properties in situations where other approaches often fail (e.g., when function approximation is involved). Interestingly, there is growing evidence that actor-critic approaches based on phasic dopamine signals play a key role in biological learning through the cortical and basal ganglia. We derive a temporal difference based actor critic learning algorithm, for which convergence can be proved without assuming separate time scales for the actor and the critic. The approach is demonstrated by applying it to networks of spiking neurons. The established relation between phasic dopamine and the temporal difference signal lends support to the biological relevance of such algorithms."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/fe7ee8fc1959cc7214fa21c4840dff0a-Abstract.html,One sketch for all: Theory and Application of Conditional Random Sampling,"Ping Li, Kenneth W. Church, Trevor J. Hastie","Conditional Random Sampling (CRS) was originally proposed for efficiently computing pairwise ($l_2$, $l_1$) distances, in static, large-scale, and sparse data sets such as text and Web data. It was previously presented using a heuristic argument. This study extends CRS to handle dynamic or streaming data, which much better reflect the real-world situation than assuming static data. Compared with other known sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a significant advantage in that it is ``one-sketch-for-all.'' In particular, we demonstrate that CRS can be applied to efficiently compute the $l_p$ distance and the Hilbertian metrics, both are popular in machine learning. Although a fully rigorous analysis of CRS is difficult, we prove that, with a simple modification, CRS is rigorous at least for an important application of computing Hamming norms. A generic estimator and an approximate variance formula are provided and tested on various applications, for computing Hamming norms, Hamming distances, and $\chi^2$ distances."
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/fe8c15fed5f808006ce95eddb7366e35-Abstract.html,The Mondrian Process,"Daniel M. Roy, Yee W. Teh",We describe a novel stochastic process that can be used to construct a multidimensional generalization of the stick-breaking process and which is related to the classic stick breaking process described by Sethuraman1994 in one dimension. We describe how the process can be applied to relational data modeling using the de Finetti representation for infinitely and partially exchangeable arrays.
2008,https://papers.nips.cc/paper_files/paper/2008,https://papers.nips.cc/paper_files/paper/2008/hash/ffd52f3c7e12435a724a8f30fddadd9c-Abstract.html,Scalable Algorithms for String Kernels with Inexact Matching,"Pavel P. Kuksa, Pai-hsi Huang, Vladimir Pavlovic","We present a new family of linear time algorithms based on sufficient statistics for string comparison with mismatches under the string kernels framework. Our algorithms improve theoretical complexity bounds of existing approaches while scaling well with respect to the sequence alphabet size, the number of allowed mismatches and the size of the dataset. In particular, on large alphabets with loose mismatch constraints our algorithms are several orders of magnitude faster than the existing algorithms for string comparison under the mismatch similarity measure. We evaluate our algorithms on synthetic data and real applications in music genre classification, protein remote homology detection and protein fold prediction. The scalability of the algorithms allows us to consider complex sequence transformations, modeled using longer string features and larger numbers of mismatches, leading to a state-of-the-art performance with significantly reduced running times."
