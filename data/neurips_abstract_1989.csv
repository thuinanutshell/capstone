year,proceeding_link,paper_link,title,authors,abstract
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/01161aaa0b6d1345dd8fe4e481144d84-Abstract.html,Comparing the Performance of Connectionist and Statistical Classifiers on an Image Segmentation Problem,"Sheri L. Gish, W. E. Blanz","In  this  study,  we  test  the  suitability  of a  connection(cid:173)
In the development  of an image segmentation system for  real time  image processing applications,  we  apply the classical decision anal(cid:173) ysis  paradigm by  viewing image segmentation as a  pixel classifica.(cid:173) tion  task.  We  use  supervised  training  to derive  a  classifier for  our  system  from  a  set  of  examples  of a  particular  pixel  classification  problem.  ist  method  against  two  statistical  methods,  Gaussian  maximum  likelihood  classifier  and first,  second,  and third degree  polynomial  classifiers,  for  the  solution  of a  ""real  world""  image  segmentation  problem  taken  from  combustion  research.  Classifiers  are  derived  using  all  three  methods,  and  the  performance  of all  of the  classi(cid:173) fiers  on  the  training  data set  as  well  as  on  3  separate  entire  test  images  is  measured."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/0266e33d3f546cb5436a10798e657d97-Abstract.html,Analog Circuits for Constrained Optimization,John C. Platt,This paper  explores  whether  analog  circuitry  can  adequately  per(cid:173) form  constrained  optimization.  Constrained optimization  circuits  are  designed  using  the  differential  multiplier  method.  These  cir(cid:173) cuits fulfill  time-varying constraints correctly.  Example circuits in(cid:173) clude  a  quadratic programming circuit and a  constrained flip-flop.
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/0336dcbab05b9d5ad24f4333c7658a0e-Abstract.html,Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters,John S. Bridle,"One  of the  attractions  of neural  network  approaches  to  pattern  recognition  is  the  use  of a  discrimination-based  training  method.  We  show  that once  we  have  modified  the  output  layer  of a  multi(cid:173) layer perceptron to provide mathematically correct  probability dis(cid:173) tributions,  and  replaced  the  usual  squared  error  criterion  with  a  probability-based score,  the  result  is equivalent  to  Maximum  Mu(cid:173) tual  Information training,  which  has  been  used  successfully  to im(cid:173) prove  the  performance  of hidden  Markov models for  speech  recog(cid:173) nition.  If the network is specially constructed to perform the recog(cid:173) nition computations of a  given kind of stochastic model based clas(cid:173) sifier  then we  obtain a  method for  discrimination-based training of  the  parameters  of the  models.  Examples  include  an  HMM-based  word discriminator,  which  we  call an 'Alphanet'."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/03c6b06952c750899bb03d998e631860-Abstract.html,Can Simple Cells Learn Curves? A Hebbian Model in a Structured Environment,"William R. Softky, Daniel M. Kammen","In the mammalian visual cortex,  orientation-selective 'simple cells'  which detect  straight lines  may be adapted  to detect  curved  lines  instead.  We  test  a  biologically  plausible,  Hebbian,  single-neuron  model,  which learns oriented receptive fields  upon exposure  to un(cid:173) structured (noise) input and maintains orientation selectivity upon  exposure  to  edges  or  bars  of all  orientations  and  positions.  This  model  can  also  learn  arc-shaped  receptive  fields  upon  exposure  to an environment of only  circular  rings.  Thus,  new  experiments  which try to induce an abnormal (curved) receptive field  may pro(cid:173) vide insight into the plasticity of simple cells.  The model suggests  that exposing  cells  to only  a  single  spatial  frequency  may induce  more  striking spatial frequency  and  orientation  dependent effects  than heretofore observed."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/060ad92489947d410d897474079c1477-Abstract.html,Development and Regeneration of Eye-Brain Maps: A Computational Model,"Jack D. Cowan, A. E. Friedman",We outline a computational model  of the development and regenera(cid:173) tion of specific eye-brain circuits. The model comprises a self-organiz(cid:173) ing map-forming network which uses local Hebb rules. constrained by  molecular markers.  Various  simulations of the development of eye(cid:173) brain maps in fish and frogs are described.
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/06138bc5af6023646ede0e1f7c1eac75-Abstract.html,Predicting Weather Using a Genetic Memory: A Combination of Kanerva's Sparse Distributed Memory with Holland's Genetic Algorithms,David Rogers,"Kanerva's  sparse distributed  memory  (SDM)  is  an  associative-memo(cid:173) ry  model  based  on  the  mathematical  properties  of  high-dimensional  binary address  spaces.  Holland's genetic  algorithms are  a  search  tech(cid:173) nique  for  high-dimensional  spaces  inspired  by  evolutionary  processes  of DNA.  ""Genetic  Memory""  is  a  hybrid  of the  above  two  systems,  in  which  the  memory  uses  a  genetic  algorithm  to  dynamically  recon(cid:173) figure  its  physical  storage  locations  to  reflect  correlations  between  the  stored  addresses  and  data.  For  example,  when  presented  with  raw  weather station  data,  the  Genetic  Memory  discovers  specific  fea(cid:173) tures  in  the  weather  data  which  correlate  well  with  upcoming  rain,  and  reconfigures  the  memory  to  utilize  this  information  effectively.  This  architecture  is  designed  to  maximize  the  ability  of the  system  to scale-up to handle real-world problems."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/077e29b11be80ab57e1a2ecabb7da330-Abstract.html,Neural Network Analysis of Distributed Representations of Dynamical Sensory-Motor Transformations in the Leech,"Shawn R. Lockery, Yan Fang, Terrence J. Sejnowski","Interneurons in leech ganglia receive multiple sensory inputs and make  synaptic contacts with many motor neurons. These ""hidden"" units  coordinate several different behaviors. We used physiological and  anatomical constraints to construct a model of the local bending reflex.  Dynamical networks were trained on experimentally derived input-output  patterns using recurrent back-propagation. Units in the model were  modified to include electrical synapses and multiple synaptic time  constants. The properties of the hidden units that emerged in the  simulations matched those in the leech. The model and data support  distributed rather than localist representations in the local bending reflex.  These results also explain counterintuitive aspects of the local bending  circuitry."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/084b6fbb10729ed4da8c3d3f5a3ae7c9-Abstract.html,A Computer Modeling Approach to Understanding the Inferior Olive and Its Relationships to the Cerebellar Cortex in Rats,"Maurice Lee, James M. Bower","This paper presents the results of a simulation of the spatial relationship  between the inferior olivary nucleus and  folium  crus IIA of the lateral  hemisphere  of  the  rat  cerebellum.  The  principal  objective  of  this  modeling effort was to resolve an apparent conflict between a proposed  zonal organization of olivary projections to cerebellar cortex suggested  by  anatomical  tract-tracing  experiments  (Brodal  &  Kawamura  1980;  Campbell & Armstrong  1983) and a more patchy organization apparent  with physiological mapping (Robertson  1987).  The results suggest that  several unique features  of the olivocerebellar circuit may contribute to  the appearance of zonal organization using anatomical  techniques, but  that  the  detailed  patterns  of  patchy  tactile  projections  seen  with  physiological  techniques  are  a  more  accurate  representation  of  the  afferent organization of this region of cortex."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html,Generalization and Scaling in Reinforcement Learning,"David H. Ackley, Michael L. Littman","In  associative reinforcement learning,  an environment generates input  vectors, a  learning system generates possible output vectors, and a  re(cid:173) inforcement function computes feedback signals from the input-output  pairs.  The  task is  to discover  and  remember  input-output  pairs  that  generate  rewards.  Especially  difficult  cases  occur  when  rewards  are  rare, since the expected time for any algorithm can grow exponentially  with the size  of the problem.  Nonetheless, if a  reinforcement function  possesses regularities, and a learning algorithm exploits them, learning  time  can be reduced  below that  of non-generalizing  algorithms.  This  paper  describes  a  neural  network algorithm called  complementary  re(cid:173) inforcement  back-propagation (CRBP),  and  reports simulation results  on problems designed to offer differing opportunities for generalization. 
1  REINFORCEMENT LEARNING REQUIRES  SEARCH  Reinforcement learning (Sutton, 1984; Barto &  Anandan, 1985; Ackley,  1988; Allen,  1989)  requires more from  a learner than does the more familiar  supervised learning  paradigm.  Supervised learning supplies the correct answers to the learner,  whereas  reinforcement  learning  requires  the  learner  to  discover  the  correct  outputs  before  they  can  be  stored.  The  reinforcement  paradigm  divides  neatly  into  search  and  learning  aspects:  When  rewarded  the  system makes internal adjustments to learn  the discovered input-output pair;  when punished the system makes internal adjust(cid:173) ments to search elsewhere. 
Generalization and Scaling in Reinforcement Learning 
551 
1.1  MAKING REINFORCEMENT INTO  ERROR 
Following work by Anderson (1986)  and Williams  (1988),  we  extend the backprop(cid:173) agation  algorithm  to associative  reinforcement  learning.  Start with  a  ""garden  va(cid:173) riety""  backpropagation  network:  A  vector  i  of n  binary  input  units  propagates  through  zero  or  more  layers  of hidden  units,  ultimately  reaching  a  vector  8  of m  sigmoid  units,  each  taking  continuous  values in  the range  (0,1).  Interpret each  8j  as  the  probability that  an  associated  random  bit  OJ  takes on  value  1.  Let  us  call  the  continuous,  deterministic  vector  8  the  search  vector to distinguish  it from  the  stochastic binary output  vector o. 
Given  an  input  vector,  we  forward  propagate  to  produce  a  search  vector  8,  and  then  perform  m  independent  Bernoulli trials  to  produce  an  output  vector  o.  The  i  - 0  pair  is  evaluated  by  the  reinforcement  function  and  reward  or  punishment  ensues.  Suppose  reward occurs.  We  therefore  want  to make  0  more  likely given  i.  Backpropagation will  do just that if we  take  0  as  the desired  target to produce  an  error vector (0 - 8)  and adjust weights normally.  Now suppose punishment occurs, indicating 0  does not correspond with i.  By choice  of error vector, backpropagation allows us to push the search vector in any direction;  which way should we go?  In absence of problem-specific information, we cannot pick  an appropriate  direction  with certainty.  Any decision  will  involve assumptions.  A  very minimal ""don't be like 0""  assumption-employed in Anderson (1986), Williams  (1988),  and Ackley  (1989)-pushes  s directly  away from 0 by taking (8 - 0)  as the  error vector.  A slightly stronger  ""be like not-o""  assumption-employed in Barto &  Anandan  (1985)  and Ackley  (1987)-pushes s  directly  toward the  complement of 0  by taking  ((1  - 0)  - 8)  as  the  error  vector.  Although  the  two  approaches always  agree  on  the  signs  of the  error  terms,  they  differ  in  magnitudes.  In  this  work,  we  explore  the second  possibility,  embodied in  an algorithm called  complementary  reinforcement  back-propagation ( CRBP). 
Figure 1 summarizes the CRBP algorithm.  The algorithm in the figure reflects three  modifications to the basic approach just sketched.  First, in step 2,  instead of using  the  8j'S  directly as probabilities,  we  found  it advantageous to  ""stretch""  the values  using  a  parameter  v.  When  v  < 1,  it is  not  necessary for  the  8i'S  to reach zero or  one  to  produce  a  deterministic  output.  Second,  in  step  6,  we  found  it  important  to use  a smaller learning rate for  punishment compared to reward.  Third, consider  step 7:  Another  forward  propagation is  performed,  another  stochastic binary out(cid:173) put  vector  0*  is  generated  (using  the procedure  from  step  2),  and  0*  is  compared  to  o.  If they  are  identical  and  punishment  occurred,  or  if they  are  different  and  reward occurred, then another error vector is generated and another weight update  is performed.  This loop continues until a  different  output is generated  (in the case  of failure)  or  until the original  output is  regenerated  (in  the case  of success).  This  modification improved performance significantly, and added only a small percentage  to the total number of weight updates performed. 
552 
Ackley and Littman 
O.  Build  a  back  propagation  network  with  input  dimensionality  n  and  output 
dimensionality m.  Let t  =  0 and te  =  O. 

Pick random i  E 2n  and forward  propagate to produce  a/s.  2.  Generate a binary output vector o.  Given a uniform random variable ~ E  [0,1] 

and parameter 0 < v  < 1, 
OJ  = {1,  if(sj - !)/v+! ~ ~j"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/0aa1883c6411f7873cb83dacb17b0afc-Abstract.html,Neural Network Weight Matrix Synthesis Using Optimal Control Techniques,"O. Farotimi, Amir Dembo, Thomas Kailath","T.  Kailath 
Given a set  of input-output  training samples,  we  describe  a  proce(cid:173) dure  for  determining  the  time sequence  of weights  for  a  dynamic  neural  network  to  model  an  arbitrary  input-output  process.  We  formulate  the  input-output  mapping  problem  as  an  optimal con(cid:173) trol  problem,  defining  a  performance  index  to  be  minimized  as  a  function  of time-varying  weights.  We  solve  the  resulting  nonlin(cid:173) ear two-point-boundary-value problem, and this yields the  training  rule.  For the performance index chosen,  this rule  turns out  to be  a  continuous time generalization of the outer product rule earlier sug(cid:173) gested heuristically by Hopfield for designing associative memories.  Learning curves for  the new  technique are  presented."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/0e01938fc48a2cfb5f2217fbfb00722d-Abstract.html,Collective Oscillations in the Visual Cortex,"Daniel M. Kammen, Christof Koch, Philip J. Holmes","The  firing  patterns  of populations  of cells  in  the  cat  visual  cor(cid:173) tex can  exhibit  oscillatory  responses  in  the  range  of 35  - 85  Hz.  Furthermore,  groups  of neurons  many  mm's  apart  can  be  highly  synchronized  as  long  as  the  cells  have  similar  orientation  tuning.  We investigate two basic network architectures that incorporate ei(cid:173) ther nearest-neighbor or global feedback interactions and conclude  that non-local feedback plays a  fundamental role in the initial syn(cid:173) chronization and dynamic stability of the oscillations."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/0e65972dce68dad4d52d063967f0a705-Abstract.html,Neural Networks: The Early Days,Jack D. Cowan,"A short account is  given of various  investigations of neural  network  properties,  beginning  with  the  classic  work of McCulloch  & Pitts.  Early work on neurodynamics and statistical mechanics, analogies with  magnetic materials, fault tolerance via parallel distributed processing,  memory, learning,  and pattern recognition,  is described."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/0f49c89d1e7298bb9930789c8ed59d48-Abstract.html,Operational Fault Tolerance of CMAC Networks,"Michael J. Carter, Franklin J. Rudolph, Adam J. Nucci","The performance sensitivity of Albus' CMAC network was studied for  the scenario in which faults are introduced into the adjustable weights  after training has been accomplished.  It was found that fault sensitivity  was reduced with increased generalization when ""loss of weight"" faults  were considered,  but sensitivity  was increased for  ""saturated weight""  faults."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/115f89503138416a242f40fb7d7f338e-Abstract.html,Effects of Firing Synchrony on Signal Propagation in Layered Networks,"G. T. Kenyon, Eberhard E. Fetz, R. D. Puff","Spiking  neurons  which  integrate  to  threshold  and  fire  were  used  to  study  the  transmission  of  frequency  modulated  (FM)  signals  through  layered networks.  Firing correlations  between cells  in  the  input  layer  were  found  to  modulate  the  transmission  of  FM  sig(cid:173) nals  under  certain dynamical  conditions.  A  tonic  level  of activity  was  maintained  by  providing  each  cell  with  a  source  of Poisson(cid:173) distributed  synaptic  input.  When  the  average  membrane  depo(cid:173) larization  produced  by  the  synaptic  input  was  sufficiently  below  threshold,  the  firing  correlations  between  cells  in  the  input  layer  could greatly amplify the signal present in subsequent layers.  When  the  depolarization was sufficiently close  to threshold,  however,  the  firing  synchrony  between cells  in the  initial layers could  no  longer  effect  the propagation of FM  signals.  In this latter case,  integrate(cid:173) and-fire  neurons  could  be  effectively  modeled  by  simpler  analog  elements governed by a  linear input-output relation."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/13fe9d84310e77f13a6d184dbf1232f3-Abstract.html,Acoustic-Imaging Computations by Echolocating Bats: Unification of Diversely-Represented Stimulus Features into Whole Images,James A. Simmons,"The  echolocating  bat,  Eptesicus fuscus,  perceives  the  distance  to  sonar  targets  from  the  delay  of echoes  and  the  shape  of targets  from  the  spectrum  of echoes.  However,  shape  is  perceived  in  terms  of the  target's  range  proftle.  The  time  separation  of echo  components from  parts of the target located at  different  distances  is  reconstructed  from  the  echo  spectrum  and  added  to  the  estimate  of absolute  delay  already  derived  from  the  arrival-time  of  echoes.  The  bat  thus  perceives  the  distance  to  targets  and  range  depth  within  dimension,  which  is  computed.  The  image  corresponds  to  the  crosscorrelation  function  of  echoes.  Fusion  of  physiologically  distinct  time- and  frequency-domain  representations  into  a  fmal,  common  time-domain  image  illustrates  the  binding  of  within(cid:173) modality  features  into  a  unified,  whole  image.  To  support  the  structure  of  images  along  the  dimension  of  range,  bats  can  perceive echo  delay  with a hyperacuity of 10  nanoseconds. 
the  same  psychological"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/1534b76d325a8f591b52d302e7181331-Abstract.html,Time Dependent Adaptive Neural Networks,Fernando J. Pineda,"A  comparison  of algorithms  that minimize error functions  to  train  the  trajectories of recurrent networks, reveals how complexity is traded off for  causality.  These  algorithms  are  also  related  to  time-independent  fonnalisms.  It is  suggested  that  causal  and  scalable  algorithms  are  possible  when  the  activation  dynamics  of  adaptive  neurons  is  fast  compared  to  the  behavior  to  be  learned.  Standard  continuous-time  recurrent backpropagation is used in an example."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/16a5cdae362b8d27a1d8f8c7b78b4330-Abstract.html,Neural Network Visualization,"Jakub Wejchert, Gerald Tesauro","We  have developed graphics  to visualize  static and dynamic infor(cid:173) mation in layered neural network learning systems.  Emphasis  was  placed  on  creating new  visuals  that  make  use  of spatial arrange(cid:173) ments,  size  information,  animation  and  color.  We  applied  these  tools  to  the  study  of back-propagation learning of simple  Boolean  predicates,  and  have  obtained  new  insights  into  the  dynamics  of  the learning process."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/1700002963a49da13542e0726b7bb758-Abstract.html,Discovering the Structure of a Reactive Environment by Exploration,"Michael Mozer, Jonathan Bachrach","Consider a robot wandering around an unfamiliar environment. performing ac(cid:173) tions and sensing the resulting environmental states.  The robot's task is to con(cid:173) struct an internal model of its environment. a model that will allow it to predict  the consequences of its actions  and to determine what sequences of actions  to  take  to  reach  particular  goal  states.  Rivest  and  Schapire  (1987&,  1987b;  Schapire.  1988) have studied this problem and have designed a symbolic algo(cid:173) rithm  to  strategically explore  and infer the  structure of ""finite state""  environ(cid:173) ments.  The heart of this algorithm is a clever representation of the environment  called an update graph.  We have developed a connectionist implementation of  the update  graph using  a highly-specialized network  architecture.  With  back  propagation learning and a trivial exploration strategy - tions - gorithm on simple problems.  The network has  the  additional  strength  that it  can accommodate stochastic environments.  Perhaps the  greatest virtue of the  connectionist  approach  is  that  it suggests  generalizations of the update graph  representation that do not arise from a traditional, symbolic perspective. 
choosing random ac(cid:173) the connectionist network can outperfonn the Rivest and Schapire al(cid:173)"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/19f3cd308f1455b3fa09a282e0d496f4-Abstract.html,A Self-organizing Associative Memory System for Control Applications,Michael Hormel,"The  CHAC  storage  scheme  has  been  used  as  a  basis  for  a  software  implementation  of  an  associative  .emory  system  AHS,  which  itself  is  a  major  part  of  the  learning  control  loop  LERNAS.  A  major  this  CHAC-concept  is  that  the  disadvantage  of  degree  of  local  generalization  (area  of  interpo(cid:173) lation)  is  fixed.  This  paper  deals  with  an  algo(cid:173) rithm  for  self-organizing  variable  generaliza(cid:173) tion  for  the  AKS,  based  on  ideas  of  T.  Kohonen."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/20f07591c6fcb220ffe637cda29bb3f6-Abstract.html,Complexity of Finite Precision Neural Network Classifier,"Amir Dembo, Kai-Yeung Siu, Thomas Kailath","A rigorous analysis on the finite  precision computational <)Spects  of  neural  network as  a  pattern  classifier  via a  probabilistic  approach  is  presented.  Even though there exist negative results on  the capa(cid:173) bility of perceptron,  we  show  the following  positive results:  Given  n  pattern vectors each represented by en bits where  e > 1,  that are  uniformly  distributed,  with  high  probability  the  perceptron  can  perform  all  possible  binary  classifications  of the  patterns.  More(cid:173) over, the resulting neural network requires a  vanishingly small pro(cid:173) portion O(log n/n) of the memory that would be required for  com(cid:173) plete  storage  of the  patterns.  Further,  the  perceptron  algorithm  takes  O(n2)  arithmetic operations  with  high  probability,  whereas  other  methods  such  as  linear  programming  takes  O(n3 .5 )  in  the  worst  case.  We also  indicate some mathematical connections  with  VLSI  circuit  testing and the theory  of random matrices."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/274ad4786c3abca69fa097b85867d9a4-Abstract.html,Neuronal Group Selection Theory: A Grounding in Robotics,"Jim Donnett, Tim Smithers","In this paper, we  discuss a current attempt at applying the organi(cid:173) zational principle  Edelman  calls  Neuronal  Group  Selection  to  the  control of a real,  two-link robotic  manipulator.  We begin by moti(cid:173) vating the need for  an alternative to the position-control paradigm  of classical robotics,  and suggest  that a  possible avenue  is  to look  at the primitive animal limb 'neurologically ballistic' control mode.  We  have  been  considering  a  selectionist  approach to coordinating  a  simple perception-action task. 
1  MOTIVATION  The majority of industrial robots in the world are mechanical manipUlators - often  arm-like  devices  consisting of some  number  of rigid  links  with  actuators  mounted  where  the  links  join  that  move  adjacent  links  relative  to  each  other,  rotationally  or  translation ally.  At  the  joints  there  are  typically  also  sensors  measuring  the  relative  position of adjacent links,  and it is  in  terms of position that manipulators  are generally controlled (a desired motion is specified as a desired position of the end  effector, from  which can be derived  the necessary  positions of the links comprising  the  manipulator).  Position control dominates largely for historical reasons,  rooted  in  bang-bang  control:  manipulators bumped between mechanical stops placed so as  to enforce a  desired trajectory for  the end effector. 
Neuronal Group Selection Theory:  A Grounding in Robotics 
309 
1.1  SERVOMECHANISMS 
Mechanical stops  have  been  superceded  by  position-controlling  servomechanisms,  negative feedback systems in which, for a typical manipulator with revolute joints, a  desired joint angle is compared with a feedback signal from the joint sensor signalling  actual measured angle;  the difference controls the motive power output of the joint  actuator proportionally. 
Where  a  manipulator  is  constructed  of  a  number  of  links,  there  might  be  a  ser(cid:173) vomechanism for  each joint.  In  combination,  it  is  well  known  that  joint  motions  can  affect  each other adversely, requiring careful design  and analysis to reduce  the  possibility of unpleasant dynamical instabilities.  This is  especially important when  the  manipulator will  be required  to execute fast  movements involving many or  all  of the joints.  We  are  interested in  such dynamic tasks,  and  acknowledge some suc(cid:173) cessful servomechanistic solutions  (see  [Andersson  19881, who describes a ping pong  playing robot),  but seek an  alternative that is  not as  computationally expensive. 
1.2  ESCAPING POSITION CONTROL 
In  Nature,  fast  reaching and  striking is  a  primitive and fundamental mode of con(cid:173) trol.  In  fast,  time-optimal,  neurologically  ballistic  movements  (such  as  horizontal  rotations  of the  head  where  subjects  are  instructed  to  turn  it  as  fast  as  possible,  [Hannaford  and Stark  1985]),  muscle activity patterns seem  to  show  three phases:  a launching phase  (a burst of agonist), a  braking phase (an antagonist burst), and a  locking phase  (a second agonist  burst).  Experiments have shown  (see  [Wadman  et  al.  1979])  that at least the first  100 mS of activity is  the same even if a movement is  blocked mechanically  (without forewarning the subject), suggesting that the launch  is specified from  predetermined initial conditions  (and is not  immediately modified  from  proprioceptive information).  With  the  braking  and  locking  phases  acting  as  a  damping  device  at  the  end  of  the  motion,  the  complete  motion  of  the  arm  is  essentially specified by the initial conditions - ditional robot positional control.  The overall coordination of movements might even  seem naive and simple when compared with the intricacies of servomechanisms (see  [Braitenberg 1989, N ahvi and Hashemi 19841 who discuss the crane driver's strategy  for  shifting loads quickly  and time-optimally). 
a mode radically differing from tra(cid:173)
The concept of letting insights  (such  as  these)  that can  be gained from  the biolog(cid:173) ical sciences  shape  the  engineering  principles  used  to  create  artificial  autonomous  systems is finding favour with a  growing number of researchers in  robotics.  As  it is  not generally trivial to see how life's devices can be mapped onto machines, there is  a  need for  some fundamental experimental work to develop  and test the basic  the(cid:173) oretical and empirical components of this  approach, and we  have been  considering  various robotics problems from  this perspective.  Here, we discuss  an experimental two-link manipulator that performs a simple ma(cid:173) nipulation  task - hitting  a  simple  object  perceived  to  be  within  its  reach.  The  perception  of the object  specifies  the  initial conditions that determine an  arm  mo-
310 
Donnett and Smithers 
tion that reaches it.  In relating initial conditions with motor currents, we  have been  considering  a  scheme  based  on  Neuronal  Group Selection  Theory  [Edelman  1987,  Reeke  and  Edelman  1988],  a  theory  of brain  organization.  We  believe  this  to  be  the  first  attempt  to  apply  selectionist  ideas  in  a  real  machine,  rather than just  in  simulation. 
2  NEURONAL GROUP SELECTION THEORY  Edelman proposes Neuronal Group Selection  (NGS)  [Edelman  1978]  as  an organiz(cid:173) ing principle for higher brain function - mainly a biological basis for perception - primarily  applicable  to  the  mammalian  (and  specifically,  human)  nervous  system  [Edelman  1981].  The essential idea is  that groups  of cells,  structurally varied  as  a  result  of developmental processes,  comprise  a  population  from  which  are  selected  those  groups  whose  function  leads  to  adaptive  behaviour  of  the  system.  Similar  notions  appear  in  immunology  and,  of  course,  evolutionary  theory,  although  the  effects of neuronal group selection  are manifest  in  the lifetime of the organism. 
There  are  two  premises  on  which  the  principle  rests.  The first  is  that  the  unit  of  selection  is  a  cell  group of perhaps 50  to  10,000 neurons.  Intra-group connections  between cells  are  assumed to vary (greatly)  between groups,  but other connections  in the brain (particularly inter-group) are quite specific.  The second premise is  that  the kinds of nervous systems whose organization the principle addresses are able to  adapt  to circumstances  not  previously  encountered by  the  organism  or  its  species  [Edelman  1978]. 
2.1  THREE CENTRAL TENETS 
There are  three important ideas  in  the NGS  theory  [Edelman  1987]. 
•  A  first  selective  process  (cell  division,  migration,  differentiation,  or  death)  results  in  structural  diversity  providing  a  primary  repertoire  of  variant  cell  groups. 
•  A second selective process occurs as the organism experiences its environment;  group  activity  that  correlates  with  adaptive  behaviour  leads  to  differential  amplification  of intra- and  inter-group  synaptic  strengths  (the  connectivity  pattern remains  unchanged).  From the primary  repertoire  are  thus selected  groups whose  adaptive functioning  means  they are more  likely to find  future  use  -
these groups form  the  ,econdary repertoire. 
•  Secondary repertoires themselves form  populations,  and the NGS  theory ad(cid:173)
ditionally  requires  a  notion  of  reentry,  or  connections  between  repertoires,  usually  arranged  in  maps,  of which  the  well-known  retinotopic  mapping  of  the visual system  is  typical.  These  connections  are critical for  they correlate  motor and sensory repertoires,  and lend the world the kind of spatiotemporal  continuity we  all experience. 
Neuronal Group Selection Theory:  A Grounding in Robotics 
311 
2.2  REQUffiEMENTS  OF  SELECTIVE SYSTEMS 
To be selective, a system must satisfy three requirements IReeke and Edelman 1988].  Given a configuration of input signals  (ultimately from the sensory epithelia, but for  'deeper' repertoires mainly coming from other neuronal groups), if a group responds  in a specific  way it has  matched the input  IEdelman 1978].  The first  requirement of  a selective system is that it have a sufficiently large repertoire of variant elements to  ensure  that an  adequate match can  be found for  a wide range of inputs.  Secondly,  enough of the groups in  a  repertoire must 'see' the diverse input signals effectively  and quickly so that selection can operate on  these  groups.  And finally,  there must  be  a  means  for  'amplifying'  the  contribution,  to  the  repertoire,  of groups  whose  operation when  matching input signals has led  to adaptive behaviour. 
In  determining  the  necessary  number  of groups in  a  repertoire,  one  must consider  the  relationship  between repertoire  size  and  the specificity  of member groups.  On  the  one  hand,  if groups  are  very specific,  repertoires  will  need  to  be  very large  in  order to recognize a  wide range of possible inputs.  On the other hand, if groups are  not  as  discriminating,  it  will  be possible  to have smaller numbers  of them,  but  in  the limit (a single group with virtually no specificity) different signals will no longer  be  distinguishable.  A  simple  way  to  quantify  this  might  be  to  assume  that  each  of N  groups has  a  fixed  probability,  P,  of matching an input configuration;  then  a  typical measure  IEdelman  1978]  relating  the  effectiveness  of recognition,  r,  to  the  number of groups is  r  = 1 -
(1 - p)N  (see  Fig.  1)."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/289dff07669d7a23de0ef88d2f7129e7-Abstract.html,Dimensionality Reduction and Prior Knowledge in E-Set Recognition,"Kevin J. Lang, Geoffrey E. Hinton","It is  well known  that  when an  automatic  learning algorithm  is  applied  to a  fixed  corpus  of data,  the size of the corpus  places  an  upper bound  on  the  number  of degrees  of freedom  that  the  model  can  contain  if  it  is  to generalize  well.  Because  the  amount  of hardware  in  a  neural  network  typically  increases  with  the  dimensionality  of  its  inputs,  it  can be challenging to build a high-performance network for classifying  large input patterns.  In this paper, several techniques for addressing this  problem  are  discussed  in  the context of an  isolated  word  recognition  task."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/335f5352088d7d9bf74191e006d8e24c-Abstract.html,A self-organizing multiple-view representation of 3D objects,"Daphna Weinshall, Shimon Edelman, Heinrich H. Bülthoff","We demonstrate the ability of a two-layer network of thresholded  summation units to support representation of 3D objects in which  several distinct 2D views are stored for ea.ch object. Using unsu(cid:173) pervised Hebbian relaxation, the network learned to recognize ten  objects from different viewpoints. The training process led to the  emergence of compact representations of the specific input views.  When tested on novel views of the same objects, the network ex(cid:173) hibited a substantial generalization capability. In simulated psy(cid:173) chophysical experiments, the network's behavior was qualitatively  similar to that of human subjects. 
1 Background  Model-based object recognition involves, by definition, a compa.rison between the  input image and models of different objects that are internal to the recognition  system. The form in which these models are best stored depends on the kind of  information available in the input, and on the trade-off between the amount of  memory allocated for the storage and the degree of sophistication required of the  recognition process. 
In computer vision, a distinction can be made between representation schemes that  use 3D object-centered coordinate systems and schemes that store viewpoint-specific  information such as 2D views of objects. In principle, storing enough 2D views would 
A Self-Organizing Multiple-View Representation of 3D Objects 
275 
allow the system to use simple recognition techniques such as template matching.  If only a few views of each object are remembered, the system must have the capa(cid:173) bility to normalize the appearance of an input object, by carrying out appropriate  geometrical transformations, before it can be directly compared to the stored rep(cid:173) resen tat ions . 
What representation strategy is employed by the human visual system? The notion  that objects are represented in viewpoint-dependent fashion is supported by the  finding that commonplace objects are more readily recognized from certain so-called  canonical vantage points than from other, random viewpoints (Palmer et al. 1981).  Namely, canonical views are identified more quickly (and more accurately) than  others, with response times decreasing monotonically with increasing subjective  goodness.! 
The monotonic increase in the recognition latency with misorientation of the object  relative to a canonical view prompts the interpretation of the recognition process in  terms of a mechanism related to mental rotation. In the classical mental rotation  task (see Shepard & Cooper 1982), the subject is required to decide whether two  simultaneously presented images are two views of the same 3D object. The average  latency of correct response in this task is linearly dependent on the difference in  the 3D attitude of the object in the two images. This dependence is commonly  accounted for by postulating a process that attempts to rotate the 3D shapes per(cid:173) ceived in the two images into congruence before making the identity decision. The  rotation process is sometimes claimed to be analog, in the sense that the represen(cid:173) tation of the object appears to pass through intermediate orientation stages as the  rotation progresses (Shepard & Cooper 1982).  Psychological findings seem to support the involvement of some kind of mental  rotation in recognition by demonstrating the dependence of recognition latency for  an unfamiliar view of an object on the distance to its closest familiar view. There  is, however, an important qualification. Practice with specific objects appears to  cause this strategy to be abandoned in favor of a more memory-intensive, less time(cid:173) consuming direct comparison strategy. Under direct comparison, many views of the  objects are stored and recognition proceeds in essentially constant time, provided  that the presented views are sufficiently close to one of the stored views (Tarr &  Pinker 1989, Edelman et al. 1989). 
From the preceding outline, it appears that a faithful model of object representa(cid:173) tion in the human visual system should provide both for the ability to ""rotate""  3D objects and for the fast direct-comparison strategy that supersedes mental ro(cid:173) tation for highly familiar objects. Surprisingly, it turns out that mental rotation  in recognition can be replicated by a self-organizing memory-intensive model based  on direct comparison. The rest of the present paper describes such a model, called  CLF (conjunctions of localized features; see Edelman & Weins hall 1989). 
1 Canonical viewl of objects can be reliably identified in lubjective judgement al well as in  recognition talb. For example, when alked to form a mental image of an object, people Ulually  imagine it as leen &om a canonical perspective. 
276 Weinshall, Edelman and Bulthoff 
INPUT (feature) LAYER"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/3644a684f98ea8fe223c713b77189a77-Abstract.html,Associative Memory in a Simple Model of Oscillating Cortex,Bill Baird,"A  generic  model  of oscillating  cortex,  which  assumes  ""minimal""  coupling justified by known anatomy, is shown to function as an as(cid:173) sociative memory, using previously developed  theory.  The network  has  explicit  excitatory  neurons  with  local  inhibitory  interneuron  feedback  that forms  a  set of nonlinear oscillators coupled  only by  long range excitatofy connections.  Using a local Hebb-like learning  rule for  primary and higher order synapses  at the ends of the long  range  connections,  the  system  learns  to store  the  kinds  of oscil(cid:173) lation amplitude patterns observed  in olfactory  and  visual  cortex.  This  rule  is  derived  from  a  more  general  ""projection  algorithm""  for recurrent  analog networks, that analytically guarantees content  addressable  memory  storage  of continuous  periodic  sequences  - capacity:  N /2  Fourier  components for  an  N  node  network  - ""spurious""  attractors. 
no"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/36660e59856b4de58a219bcf4e27eba3-Abstract.html,Adjoint Operator Algorithms for Faster Learning in Dynamical Neural Networks,"Jacob Barhen, Nikzad Benny Toomarian, Sandeep Gulati","A  methodology for  faster supervised learning in  dynamical nonlin(cid:173) ear neural networks is  presented.  It exploits the concept  of adjoint  operntors  to  enable  computation  of changes  in  the  network's  re(cid:173) sponse due to perturbations in all system parameters,  using the so(cid:173) lution of a single set of appropriately constructed  linear equations.  The  lower  bound  on  speedup  per  learning  iteration  over  conven(cid:173) tional methods for  calculating the neuromorphic energy gradient is  O(N2),  where  N  is  the number  of neurons  in  the network."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/38db3aed920cf82ab059bfccbd02be6a-Abstract.html,Computer Simulation of Oscillatory Behavior in Cerebral Cortical Networks,"Matthew A. Wilson, James M. Bower","It has been known for  many years that specific regions of the work(cid:173) ing cerebral cortex display periodic variations in correlated cellular  activity.  While the olfactory system has  been the focus  of much of  this  work,  similar  behavior  has  recently  been observed  in  primary  visual  cortex.  We  have  developed  models  of  both  the  olfactory  and  visual  cortex which  replicate  the observed  oscillatory  proper(cid:173) ties of these  networks.  Using  these  models  we  have  examined  the  dependence of oscillatory behavior on single cell properties and net(cid:173) work  architectures.  We  discuss the idea that  the oscillatory  events  recorded from  cerebral cortex may  be intrinsic  to the architecture  of cerebral  cortex  as  a  whole,  and  that  these  rhythmic  patterns  may be important in coordinating neuronal activity during sensory  processmg."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/39059724f73a9969845dfe4146c5660e-Abstract.html,Coupled Markov Random Fields and Mean Field Theory,"Davi Geiger, Federico Girosi","Federico  Girosi  Artificial Intelligence  Laboratory,  MIT  545  Tech.  Sq.  #  788  Cambridge, MA 02139 
In recent years many researchers have investigated the use of Markov  Random Fields (MRFs)  for  computer vision.  They can be  applied  for  example  to  reconstruct  surfaces  from  sparse  and  noisy  depth  data  coming from  the  output  of a  visual  process,  or  to  integrate  early vision  processes  to label  physical  discontinuities.  In  this  pa(cid:173) per  we  show  that  by  applying  mean field  theory  to  those  MRFs  models a  class of neural  networks is  obtained.  Those networks can  speed  up  the  solution  for  the  MRFs  models.  The  method  is  not  restricted  to computer vision."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/3b8a614226a953a8cd9526fca6fe9ba5-Abstract.html,Pulse-Firing Neural Chips for Hundreds of Neurons,"Michael Brownlow, Lionel Tarassenko, Alan F. Murray, Alister Hamilton, Il Song Han, H. Martin Reekie","We  announce  new  CMOS  synapse  circuits  using  only  three  and four  MOSFETsisynapse.  Neural states are asynchronous  pulse  streams,  upon  which  arithmetic  is  performed  directly.  Chips  implementing  over  100  fully  programmable  synapses  are  described  and  projections  to  networks  of  hundreds  of  neurons are made. 
1 OVERVIEW OF PULSE FIRING NEURAL  VLSI  The  inspiration  for  the  use  of  pulse  firing  in  silicon  neural  networks  is  clearly  the  electrical/chemical  pulse  mechanism  in  ""real""  biological  neurons.  Asynchronous,  digital  voltage  pulses  are  used  to  signal  states  t Si  )  through  synapse  weights  {  Tij  }  to  emulate  neural  dynamics.  Neurons  fire  voltage  pulses  of a frequency  determined  by  their  level  of activity  but of a  constant  magnitude  (usually  5  Volts)  [Murray,1989a].  As  indicated  in  Fig.  1,  to  synapses  perform  arithmetic  directly  on  these  asynchronous  pulses,  increment  or  decrement  the  receiving  neuron's  activity.  The  activity  of  a  receiving  neuron  i,  Xi  is  altered  at  a  frequency  controlled  by  the  sending  neuron  j,  with  state  Sj  by  an  amount  determined  by  the  synapse  weight  (here,  T ij ). 
1 On secondment from  the Korean Telecommunications Authority 
786 
Brownlow, Tarassenko, Murray, Hamilton, Han and Reekie"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/45fbc6d3e05ebd93369ce542e8f2322d-Abstract.html,Rule Representations in a Connectionist Chunker,"David S. Touretzky, Gillette Elvgreen III","We  present  two  connectionist  architectures  for  chunking  of symbolic  rewrite rules.  One uses backpropagation learning, the other competitive  learning.  Although  they  were  developed  for  chunking  the  same  sorts  of rules,  the  two  differ  in  their  representational  abilities  and  learning  behaviors."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/46ba9f2a6976570b0353203ec4474217-Abstract.html,A Neural Network for Real-Time Signal Processing,Donald B. Malkoff,"This paper describes a  neural network algorithm that (1)  performs  temporal pattern matching in real-time, (2)  is trained on-line, with  a single pass,  (3)  requires only a single template for training of each  representative  class,  (4)  is  continuously  adaptable  to  changes  in  background noise, (5) deals with transient signals having low signal(cid:173) to-noise ratios,  (6) works in the presence of non-Gaussian noise,  (7)  makes use of context dependencies and (8) outputs Bayesian proba(cid:173) bility estimates.  The algorithm has been adapted to the problem of  passive sonar signal detection and classification.  It runs on a  Con(cid:173) nection  Machine  and  correctly  classifies,  within  500  ms  of onset,  signals embedded in noise and subject to considerable uncertainty."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/4734ba6f3de83d861c3176a6273cac6d-Abstract.html,Speaker Independent Speech Recognition with Neural Networks and Speech Knowledge,"Yoshua Bengio, Renato de Mori, Régis Cardin","We  attempt to  combine neural  networks with  knowledge  from  speech  science to build  a speaker independent speech recogni(cid:173) tion  system.  This  knowledge  is  utilized  in  designing  the  preprocessing,  input coding, output coding,  output supervision  and  architectural  constraints.  To  handle  the  temporal  aspect  of speech we  combine  delays,  copies  of activations  of hidden  and  output  units  at  the  input  level,  and  Back-Propagation  for  Sequences  (BPS),  a  learning algorithm for networks with  local  self-loops.  This  strategy  is  demonstrated  in  several  experi(cid:173) ments,  in  particular  a  nasal  discrimination  task  for  which  the  application  of  a  speech  theory  hypothesis  dramatically  im(cid:173) proved generalization."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/48aedb8880cab8c45637abc7493ecddd-Abstract.html,Analytic Solutions to the Formation of Feature-Analysing Cells of a Three-Layer Feedforward Visual Information Processing Neural Net,Dun-Sung Tang,Analytic  solutions  to  the  information-theoretic  evolution  equa(cid:173) tion of the connection strength of a three-layer feedforward neural  net  for  visual information processing  are  presented.  The results  are  (1)  the  receptive  fields  of the  feature-analysing  cells  corre(cid:173) spond to the eigenvector of the maximum eigenvalue of the Fred(cid:173) holm integral equation of the first  kind derived from the evolution  equation  of  the  connection  strength;  (2)  a  symmetry-breaking  mechanism  (parity-violation)  has  been  identified  to  be  respon(cid:173) sible  for  the  changes  of  the  morphology  of  the  receptive  field;  (3)  the conditions for  the formation of different  morphologies are  explicitly identified.
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/502e4a16930e414107ee22b6198c578f-Abstract.html,An Analog VLSI Model of Adaptation in the Vestibulo-Ocular Reflex,"Stephen P. DeWeerth, Carver Mead","The vestibulo-ocular reflex (VOR)  is  the primary mechanism that  controls the compensatory eye movements that stabilize retinal im(cid:173) ages during rapid head motion.  The primary pathways of this sys(cid:173) tem are feed-forward,  with inputs from  the semicircular canals and  outputs  to  the  oculomotor system.  Since  visual  feedback  is  not  used  directly  in  the  VOR  computation,  the  system  must  exploit  motor learning to perform correctly.  Lisberger(1988) has proposed  a  model for  adapting the  VOR gain  using  image-slip  information  from  the  retina.  We  have  designed  and  tested  analog  very  large(cid:173) scale integrated (VLSI) circuitry that implements a  simplified ver(cid:173) sion of Lisberger's adaptive VOR model."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/539fd53b59e3bb12d203f45a912eeaf2-Abstract.html,A Reconfigurable Analog VLSI Neural Network Chip,"Srinagesh Satyanarayana, Yannis P. Tsividis, Hans Peter Graf","1024 distributed-neuron synapses have been integrated in an active  area of 6.1mm  x  3.3mm using  a 0.9p.m,  double-metal,  single-poly,  n-well CMOS  technology.  The distributed-neuron synapses are ar(cid:173) ranged in blocks of 16, which we call '4  x  4 tiles'.  Switch matrices  are  interleaved between each of these  tiles  to provide programma(cid:173) bility of interconnections.  With a small area overhead (15  %),  the  1024  units  of the  network can  be rearranged  in various  configura(cid:173) tions.  Some of the possible configurations are, a  12-32-12  network,  a  16-12-12-16  network, two 12-32  networks etc.  (the numbers sep(cid:173) arated  by dashes indicate the number of units per layer, including  the  input  layer).  Weights  are  stored  in  analog  form  on  MaS  ca(cid:173) pacitors.  The synaptic weights  are  usable to a  resolution  of 1 % of  their full  scale  value.  The limitation arises  due  to charge injection  from  the access  switch and  charge leakage.  Other  parameters like  gain and shape of nonlinearity are also programmable."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html,Handwritten Digit Recognition with a Back-Propagation Network,"Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, R. E. Howard, Wayne E. Hubbard, Lawrence D. Jackel","We present an application of back-propagation networks to hand(cid:173) written digit recognition. Minimal preprocessing of the data was  required, but architecture of the network was highly constrained  and specifically designed for the task. The input of the network  consists of normalized images of isolated digits. The method has  1 % error rate and about a 9% reject rate on zipcode digits provided  by the U.S. Postal Service."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/555d6702c950ecb729a966504af0a635-Abstract.html,Digital-Analog Hybrid Synapse Chips for Electronic Neural Networks,"Alexander Moopenn, T. Duong, A. P. Thakoor",Abstract Unavailable
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/577ef1154f3240ad5b9b413aa7346a1e-Abstract.html,Computational Efficiency: A Common Organizing Principle for Parallel Computer Maps and Brain Maps?,"Mark E. Nelson, James M. Bower","It is  well-known  that  neural  responses  in  particular  brain  regions  are  spatially  organized,  but  no  general  principles  have  been  de(cid:173) veloped  that  relate  the structure of a  brain  map  to  the nature of  the associated computation.  On parallel computers, maps of a sort  quite similar to brain maps arise when a computation is distributed  across  multiple  processors.  In  this  paper  we  will  discuss  the rela(cid:173) tionship  between  maps and computations on these  computers and  suggest how similar considerations might also apply to maps in the  brain."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/57aeee35c98205091e18d1140e9f38cf-Abstract.html,A Cost Function for Internal Representations,"Anders Krogh, C. I. Thorbergsson, John A. Hertz",We  introduce  a  cost  function  for  learning  in  feed-forward  neural  networks  which  is  an  explicit  function  of the  internal representa(cid:173) tion  in  addition  to  the  weights.  The  learning  problem  can  then  be formulated  as two simple perceptrons and  a  search for  internal  representations.  Back-propagation  is  recovered  as  a  limit.  The  frequency  of successful  solutions  is  better for  this  algorithm than  for  back-propagation when  weights and hidden  units  are  updated  on the same timescale i.e.  once  every learning step.
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/58a2fc6ed39fd083f55d4182bf88826d-Abstract.html,HMM Speech Recognition with Neural Net Discrimination,"William Y. Huang, Richard P Lippmann","Two approaches were explored which integrate neural net classifiers  with  Hidden  Markov  Model  (HMM)  speech  recognizers.  Both  at(cid:173) tempt to improve speech pattern discrimination while retaining the  temporal processing advantages of HMMs.  One approach used neu(cid:173) ral nets to provide second-stage discrimination following an HMM  recognizer.  On  a  small  vocabulary  task,  Radial  Basis  Function  (RBF)  and  back-propagation  neural  nets  reduced  the  error  rate  substantially (from 7.9% to 4.2% for the RBF classifier).  In a larger  vocabulary task, neural net classifiers did not reduce the error rate.  They, however,  outperformed Gaussian, Gaussian mixture, and k(cid:173) nearest  neighbor  (KNN)  classifiers.  In  another  approach,  neural  nets  functioned  as  low-level  acoustic-phonetic  feature  extractors.  When  classifying  phonemes  based on single  10  msec.  frames,  dis(cid:173) criminant  RBF  neural  net  classifiers outperformed  Gaussian mix(cid:173) ture  classifiers.  Performance,  however,  differed  little when  classi(cid:173) fying  phones  by  accumulating scores  across  all frames  in phonetic  segments using  a single node HMM  recognizer. 
-This  work  was  sponsored  by  the  Department  of  the  Air  Force  and  the  Air  Force  Office  of 
Scientific Research. 
HMM Speech Recognition with Neural Net Discrimination"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/621bf66ddb7c962aa0d22ac97d69b793-Abstract.html,The Effects of Circuit Integration on a Feature Map Vector Quantizer,Jim Mann,"The effects  of parameter  modifications  imposed  by  hardware con(cid:173) straints on  a self-organizing feature  map  algorithm were examined.  Performance was  measured  by  the  error  rate  of a  speech  recogni(cid:173) tion  system which  included  this  algorithm  as  part of the  front-end  processing.  System parameters  which  were  varied  included  weight  (connection  strength)  quantization,  adap tation  quantization,  dis(cid:173) tance  measures  and  circuit  approximations  which  include  device  characteristics  and  process  variability.  Experiments  using  the  TI  isolated word database for  16 speakers demonstrated degradation in  performance when  weight quantization fell  below 8 bits.  The com(cid:173) petitive nature  of the  algorithm  rela..xes  constraints on  uniformity  and  linearity which makes it an  excellent candidate for  a fully  ana(cid:173) log  circuit implementation.  Prototype circuits have been fabricated  and characterized following  the constraints established through the  simulation efforts."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/63923f49e5241343aa7acb6a06a751e7-Abstract.html,Generalization and Parameter Estimation in Feedforward Nets: Some Experiments,"N. Morgan, H. Bourlard","We have done an empirical study of the relation of the number of  parameters (weights) in a feedforward net to generalization perfor(cid:173) mance. Two experiments are reported. In one, we use simulated data  sets with well-controlled parameters, such as the signal-to-noise ratio  of continuous-valued data. In the second, we train the network on  vector-quantized mel cepstra from real speech samples. In each case,  we use back-propagation to train the feedforward net to discriminate in  a multiple class pattern classification problem. We report the results of  these studies, and show the application of cross-validation techniques  to prevent overfitting."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/63dc7ed1010d3c3b8269faf0ba7491d4-Abstract.html,VLSI Implementation of a High-Capacity Neural Network Associative Memory,"Tzi-Dar Chiueh, Rodney M. Goodman","In  this  paper we  describe  the  VLSI  design  and  testing of a  high  capacity  associative  memory  which  we  call  the  exponential  cor(cid:173) relation  associative  memory  (ECAM).  The  prototype  3J.'-CMOS  programmable  chip  is  capable  of storing  32  memory  patterns  of  24 bits each.  The high capacity of the ECAM is  partly due  to the  use of special exponentiation neurons, which  are implemented via  sub-threshold MOS  transistors in this design.  The prototype chip  is  capable of performing one associative recall in  3  J.'S."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/69adc1e107f7f7d035d7baf04342e1ca-Abstract.html,The Cascade-Correlation Learning Architecture,"Scott E. Fahlman, Christian Lebiere","Cascade-Correlation is a new architecture and supervised learning algo(cid:173) rithm for artificial neural networks.  Instead of just adjusting the weights  in a network of fixed topology. Cascade-Correlation begins with a min(cid:173) imal network,  then automatically trains  and adds new hidden  units  one  by  one,  creating a  multi-layer structure.  Once  a  new  hidden  unit  has  been added  to the network, its  input-side weights are frozen.  This  unit  then becomes a permanent feature-detector in the network, available for  producing  outputs  or for  creating other,  more complex  feature  detec(cid:173) tors.  The Cascade-Correlation architecture has  several advantages over  existing algorithms:  it  learns  very quickly,  the network . determines  its  own size and  topology, it retains  the structures  it  has  built even  if the  training set changes, and it requires no back-propagation of error signals  through  the connections of the network. 
1  DESCRIPTION OF CASCADE·CORRELATION  The  most  important  problem  preventing  the  widespread  application  of artificial  neural  networks  to real-world problems  is  the slowness  of existing learning algorithms  such  as  back-propagation  (or ""backprop"").  One factor contributing to  that  slowness  is  what we  call  the moving  target problem:  because all of the  weights  in  the network are changing  at once,  each  hidden  units  sees  a  constantly changing environment.  Instead  of moving  quickly to assume useful roles in the overall problem solution, the hidden units engage in  a complex dance with much  wasted motion.  The Cascade-Correlation learning algorithm  was  developed in an  attempt to solve that problem.  In  the problems  we have examined,  it learns  much faster than back-propagation and  solves  some other problems as  well. 
The Cascade-Correlation Learning Architecture 
525"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Abstract.html,Sigma-Pi Learning: On Radial Basis Functions and Cortical Associative Learning,"Bartlett W. Mel, Christof Koch","The  goal  in  this  work  has  been  to  identify  the  neuronal  elements  of the cortical column that are most likely to support  the learning  of nonlinear associative  maps.  We show that  a  particular style  of  network learning algorithm based on locally-tuned  receptive fields  maps  naturally  onto cortical  hardware,  and  gives  coherence  to  a  variety of features  of cortical anatomy,  physiology,  and  biophysics  whose  relations to learning remain poorly understood."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html,Optimal Brain Damage,"Yann LeCun, John S. Denker, Sara A. Solla","We  have used  information-theoretic ideas  to derive  a class of prac(cid:173) tical  and  nearly  optimal schemes  for  adapting the size  of a  neural  network.  By  removing  unimportant  weights  from  a  network,  sev(cid:173) eral  improvements  can  be  expected:  better  generalization,  fewer  training examples required,  and improved speed  of learning and/or  classification.  The  basic  idea  is  to  use  second-derivative  informa(cid:173) tion to make a  tradeoff between  network  complexity  and  training  set error.  Experiments confirm  the usefulness  of the methods on a  real-world  application."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/6da9003b743b65f4c0ccd295cc484e57-Abstract.html,Learning Aspect Graph Representations from View Sequences,"Michael Seibert, Allen M. Waxman","In our effort to develop a modular neural system for invariant learn(cid:173) ing and  recognition of 3D objects,  we  introduce here a new  module  architecture  called  an  aspect  network constructed  around  adaptive  axo-axo-dendritic synapses.  This  builds  upon  our existing system  (Seibert & Waxman, 1989) which  processes  20 shapes and classifies  t.hem  into  view  categories  (i.e.,  aspects)  invariant  to  illumination,  position,  orientat.ion,  scale,  and  projective  deformations.  From  a  sequence 'of  views,  the  aspect  network  learns  the  transitions  be(cid:173) tween  these  aspects,  crystallizing  a  graph-like  structure  from  an  initially  amorphous  network .  Object  recognition  emerges  by  ac(cid:173) cumulating evidence over  multiple views  which  activate competing  object  hypotheses."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/6f3ef77ac0e3619e98159e9b6febf557-Abstract.html,The Effect of Catecholamines on Performance: From Unit to System Behavior,"David Servan-Schreiber, Harry Printz, Jonathan D. Cohen",At the level of individual neurons. catecholamine release increases  the  responsivity  of cells  to  excitatory and  inhibitory  inputs.  We  present a  model  of catecholamine effects  in  a  network  of neural-like  elements.  We  argue  that  changes  in  the  responsivity  of individual  elements  do  not  affect  their  ability  to  detect  a  signal  and  ignore  noise.  However.  the same changes in cell responsivity in a network of such elements do  improve the signal detection performance of the network as a whole.  We  show how  this result can be used in a computer simulation of behavior  to  account  for  the  effect  of eNS  stimulants  on  the  signal  detection  performance of human subjects.
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/705f2172834666788607efbfca35afb3-Abstract.html,Meiosis Networks,Stephen Jose Hanson,"A  central  problem  in  connectionist  modelling  is  the  control  of  network  and  architectural  resources  during  learning.  In  the  present  approach,  weights  reflect  a  coarse  prediction  history  as  coded  by  a  distribution  of  values  and  parameterized  in  the  mean  and  standard  deviation  of  these  weight  distributions.  Weight  updates  are  a  function  of  both  the  mean  and  standard  deviation  of  each  connection  in  the  network and vary  as  a  function  of  the  error signal  (""stochastic  delta  rule"";  Hanson,  1990).  Consequently,  the  weights  their  maintain  in  ""uncertainty""  establishing  a  policy  concerning  the  size  of  the  nodal  complexity  of  the  network  and  growth  of  new  nodes.  For  example,  during  problem  solving  the  present  network  can  undergo  ""meiosis"",  producing  two  nodes  where  there  was  one  ""overtaxed""  node  as  measured  by  its  coefficient of variation.  It  is  shown  in  a  number of  benchmark  problems  that  meiosis  networks  can  find  minimal  architectures,  reduce  computational complexity,  and overall  increase  the efficiency of the representation learning interaction."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/74db120f0a8e5646ef5a30154e9f6deb-Abstract.html,Synergy of Clustering Multiple Back Propagation Networks,"William P. Lincoln, Josef Skrzypek","The properties of a cluster of multiple back-propagation (BP) networks  are  examined  and  compared  to  the  performance  of a  single  BP  net(cid:173) work.  The underlying idea is that a synergistic effect within the cluster  improves the perfonnance and fault tolerance.  Five networks were ini(cid:173) tially  trained  to  perfonn  the  same  input-output  mapping.  Following  training, a cluster was created by computing an average of the outputs  generated by the individual networks.  The output of the cluster can be  used as the desired output during training by feeding it back to the indi(cid:173) vidual  networks.  In  comparison  to  a  single  BP  network,  a  cluster  of  multiple  BP's generalization  and  significant fault  tolerance.  It appear  that cluster advantage follows from  simple maxim  ""you can fool  some  of the single BP's in a cluster all of the time but you cannot fool all of  them all of the time""  {Lincoln}"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/757b505cfd34c64c85ca5b5690ee5293-Abstract.html,Incremental Parsing by Modular Recurrent Connectionist Networks,"Ajay N. Jain, Alex Waibel","We  present a novel,  modular, recurrent connectionist network architec(cid:173) ture  which  learns  to robustly  perform  incremental  parsing of complex  sentences.  From  sequential  input,  one  word  at  a  time,  our  networks  learn  to  do  semantic  role  assignment,  noun  phrase  attachment,  and  clause structure recognition for sentences with passive constructions and  center embedded  clauses.  The  networks  make  syntactic  and  semantic  predictions  at every point in time, and previous  predictions are revised  as  expectations are affirmed or violated with the arrival of new informa(cid:173) tion.  Our networks  induce  their own ""grammar rules"" for dynamically  transforming  an  input  sequence of words  into a  syntactic/semantic  in(cid:173) terpretation.  These networks  generalize  and  display  tolerance  to  input  which  has  been  corrupted  in ways common in spoken  language."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/7a614fd06c325499f1680b9896beedeb-Abstract.html,Reading a Neural Code,"William Bialek, Fred Rieke, Robert R. de Ruyter van Steveninck, David Warland","Traditional methods of studying neural coding characterize  the en(cid:173) coding  of known  stimuli  in  average  neural  responses.  Organisms  face  nearly the opposite task - decoding short segments of a spike  train to extract information about an unknown, time-varying stim(cid:173) ulus.  Here  we  present strategies for  characterizing the  neural code  from  the  point of view  of the  organism, culminating in  algorithms  for  real-time  stimulus reconstruction  based  on  a  single  sample  of  the spike train.  These methods are applied to the design and anal(cid:173) ysis  of experiments on an  identified movement-sensitive neuron  in  the fly  visual system.  As far  as we  know this is  the first  instance in  which a direct  ""reading""  of the neural code has been  accomplished."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/7eabe3a1649ffa2b3ff8c02ebfd5659f-Abstract.html,A Method for the Associative Storage of Analog Vectors,"Amir F. Atiya, Yaser S. Abu-Mostafa","A method for  storing analog vectors in  Hopfield's continuous feed(cid:173) back model is proposed.  By  analog vectors we mean vectors whose  components  are  real-valued.  The  vectors  to  be  stored  are  set  as  equilibria of the network.  The network model consists of one layer  of visible  neurons  and  one  layer  of hidden  neurons.  We  propose  a  learning  algorithm,  which  results  in  adjusting  the  positions  of  the  equilibria,  as  well  as  guaranteeing  their  stability.  Simulation  results confirm the effectiveness of the method ."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/839ab46820b524afda05122893c2fe8e-Abstract.html,Using Local Models to Control Movement,Christopher G. Atkeson,"This  paper  explores  the  use  of a  model  neural  network  for  motor  learning.  Steinbuch and Taylor presented neural network designs to  do nearest  neighbor lookup in the early 1960s.  In  this  paper their  nearest neighbor network is augmented with a local model network,  which fits  a local model to a set of nearest neighbors.  The network  design  is  equivalent to local  regression.  This  network  architecture  can  represent  smooth  nonlinear functions,  yet  has  simple  training  rules  with  a  single  global  optimum.  The  network  has  been  used  for  motor  learning  of  a  simulated  arm  and  a  simulated  running  machine."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/84d9ee44e457ddef7f2c4f25dc8fa865-Abstract.html,Learning to Control an Unstable System with Forward Modeling,"Michael I. Jordan, Robert A. Jacobs","The forward modeling approach is a  methodology for  learning con(cid:173) trol when data is  available in distal coordinate systems.  We extend  previous work by considering how this methodology can be applied  to  the optimization of quantities that  are  distal not only in space  but also in time. 
In  many  learning control  problems,  the  output  variables  of the  controller  are  not  the natural coordinates in  which to specify tasks and evaluate performance.  Tasks  are generally more naturally specified in  ""distal""  coordinate systems (e.g., endpoint  coordinates for  manipulator  motion)  than in  the  ""proximal""  coordinate  system of  the controller (e.g., joint angles or torques).  Furthermore, the relationship between  proximal  coordinates  and  distal  coordinates  is  often  not  known  a  priori  and,  if  known, not easily inverted. 
The forward  modeling  approach is  a  methodology for  learning  control  when  train(cid:173) ing  data  is  available  in  distal  coordinate  systems.  A  forward  model is  a  network  that  learns  the  transformation  from  proximal  to  distal  coordinates  so  that  distal  specifications  can  be  used  in  training the  controller  (Jordan &  Rumelhart,  1990).  The forward  model  can  often  be learned  separately  from  the  controller  because  it  depends only on  the dynamics of the controlled system and not on  the closed-loop  dynamics. 
In previous work, we studied forward models of kinematic transformations (Jordan,  1988, 1990) and state transitions (Jordan & Rumelhart, 1990).  In the current paper, 
Learning to Control an Unstable System with Forward Modeling 
325 
we  go  beyond the spatial credit  assignment  problems  studied  in  those  papers  and  broaden  the  application  of forward  modeling  to  include  cases  of temporal  credit  assignment  (cf.  Barto,  Sutton,  &  Anderson,  1983;  Werbos,  1987).  As  discussed  below, the function  to be modeled in such  cases  depends on  a  time  integral of the  closed-loop  dynamics.  This  fact  has  two  important  implications.  First,  the  data  needed for learning the forward model can no longer be obtained solely by observing  the  instantaneous state or  output  of the  plant.  Second,  the  forward  model  is  no  longer independent of the controller:  If the parameters of the controller are changed  by  a  learning  algorithm,  then  the  closed-loop  dynamics  change  and  so  does  the  mapping from proximal to distal variables.  Thus the learning of the forward  model  and the learning of the  controller can no longer be separated into different  phases. 
1  FORWARD MODELING  In  this  section  we  briefly  summarize  our  previous  work  on  forward  modeling  (see  also Nguyen & Widrow,  1989 and Werbos,  1987). 
1.1  LEARNING A  FORWARD  MODEL 
Given a fixed  control law , the learning of a forward  model is  a  system identification  problem.  Let z  = g(s, u)  be  a  system to be  modeled,  where  z  is  the output or the  state-derivative, s  is  the state, and u  is  the control.  We  require the forward  model  to minimize  the cost functional 
Jm  = ~ J (z - z)T(z - z)dt."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/854d6fae5ee42911677c739ee1734486-Abstract.html,"Learning in Higher-Order ""Artificial Dendritic Trees",Tony Bell,"If neurons  sum  up  their  inputs  in  a  non-linear  way,  as  some  simula(cid:173) tions  suggest,  how  is  this  distributed  fine-grained  non-linearity  ex(cid:173) ploited  during  learning?  How  are  all  the  small  sigmoids  in  synapse,  spine  and  dendritic  tree  lined  up  in  the  right areas  of their respective  input  spaces?  In  this report,  I show  how  an  abstract atemporal  highly  nested  tree  structure  with  a  quadratic  transfer  function  associated  with  each  branchpoint,  can  self organise  using  only  a  single  global  reinforcement  scalar,  to  perform  binary  classification  tasks.  The  pro(cid:173) cedure  works  well,  solving  the 6-multiplexer and  a difficult phoneme  classification  task  as  well  as  back-propagation  does,  and  faster.  Furthermore, it does not calculate an  error gradient, but uses  a  statist(cid:173) ical  scheme  to  build  moving  models of the  reinforcement signal."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/85d8ce590ad8981ca2c8286f79f59954-Abstract.html,Dynamic Behavior of Constained Back-Propagation Networks,Yves Chauvin,"The learning dynamics of the back-propagation algorithm are in(cid:173) vestigated when complexity constraints are added to the standard  Least Mean Square (LMS) cost function. It is shown that loss of  generalization performance due to overtraining can be avoided  when using such complexity constraints. Furthermore, ""energy,""  hidden representations and weight distributions are observed and  compared during learning. An attempt is made at explaining the  results in terms of linear and non-linear effects in relation to the  gradient descent learning algorithm."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/8c19f571e251e61cb8dd3612f26d5ecf-Abstract.html,Designing Application-Specific Neural Networks Using the Genetic Algorithm,"Steven A. Harp, Tariq Samad, Aloke Guha","We  present  a  general  and  systematic  method  for  neural  network  design  based  on  the  genetic  algorithm.  The  technique  works  in  conjunction  with  network  learning  rules,  addressing  aspects  of  the  network's  gross  architecture,  connectivity,  and  learning  rule  parameters.  Networks  can  be  optimiled  for  various  application(cid:173) specific  criteria, such  as  learning speed, generalilation,  robustness  and  connectivity.  The  approach  is  model-independent.  We  describe  a  prototype  system,  NeuroGENESYS,  that employs  the  backpropagation  learning  rule.  Experiments  on  several  small  problems  have  been  conducted.  In  each  case,  NeuroGENESYS  has  produced  networks  that perform significantly  better than  the  randomly  generated  networks  of its initial population.  The  com(cid:173) putational feasibility  of our approach  is  discussed."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/8f121ce07d74717e0b1f21d122e04521-Abstract.html,A Computational Basis for Phonology,"David S. Touretzky, Deirdre W. Wheeler","The phonological structure of human  languages  is  intricate,  yet highly  constrained.  Through  a  combination  of connectionist  modeling  and  linguistic  analysis,  we are attempting to develop a computational basis  for  the  nature  of phonology.  We  present  a  connectionist  architecture  that  performs  multiple  simultaneous  insertion,  deletion,  and  mutation  operations on sequences  of phonemes, and introduce a novel additional  primitive,  clustering.  Clustering  provides  an  interesting  alternative  to  both iterative and relaxation accounts of assimilation processes  such as  vowel  harmony.  Our resulting  model  is  efficient because it processes  utterances entirely in parallel  using  only  feed-forward circuitry."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/918317b57931b6b7a7d29490fe5ec9f9-Abstract.html,Neural Network Simulation of Somatosensory Representational Plasticity,"Kamil A. Grajski, Michael Merzenich","The  brain  represents  the  skin  surface  as  a  topographic  map  in  the  somatosensory  cortex.  This  map  has  been  shown  experimentally  to  be  modifiable  in  a  use-dependent  fashion  throughout  life.  We  present  a  neural  network  simulation  of  the  competitive  dynamics  underlying  this  cortical  plasticity  by  detailed  analysis  of  receptive  field  properties  of  model  neurons  during  simulations  of  skin  co(cid:173) activation, cortical  lesion,  digit amputation and nerve  section."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/9188905e74c28e489b44e954ec0b9bca-Abstract.html,A Neural Network for Feature Extraction,Nathan Intrator,"The  paper suggests a  statistical framework for  the  parameter esti(cid:173) mation  problem associated  with  unsupervised  learning in  a  neural  network, leading to an exploratory projection pursuit network that  performs  feature  extraction, or  dimensionality  reduction."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/92c8c96e4c37100777c7190b76d28233-Abstract.html,Generalized Hopfield Networks and Nonlinear Optimization,"Gintaras V. Reklaitis, Athanasios G. Tsirukis, Manoel Fernando Tenorio","A  nonlinear  neural  framework,  called  the  Generalized  Hopfield  network,  is  proposed,  which  is  able  to  solve  in  a  parallel  distributed  manner systems  of nonlinear equations.  The  method is  applied  to  the  general  nonlinear  optimization  problem.  We  demonstrate  GHNs  implementing  the  three  most  important  optimization  algorithms,  namely the Augmented Lagrangian, Generalized Reduced Gradient and  Successive  Quadratic  Programming  methods.  The  study  results  in  a  dynamic  view of the optimization problem and offers a straightforward  model  for  the  parallelization  of  the  optimization  computations,  thus  significantly  extending  the  practical  limits  of problems  that  can  be  formulated  as  an  optimization  problem  and  which  can  gain  from  the  introduction of nonlinearities in their structure (eg. pattern recognition,  supervised learning, design of content-addressable memories). 
1 To whom correspondence should be addressed. 
356 
Reklaitis, Tsirukis and Tenorio 
1  RELATED  WORK  The  ability  of networks  of highly  interconnected  simple  nonlinear  analog  processors  (neurons)  to  solve  complicated  optimization  problems  was  demonstrated  in  a  series  of  papers by Hopfield and Tank (Hopfield,  1984), (Tank, 1986).  The  Hopfield  computational  model  is  almost  exclusively  applied  to  the  solution  of  combinatorially  complex  linear  decision  problems  (eg.  Traveling  Salesman  Problem).  Unfortunately  such problems can  not be  solved with  guaranteed  quality,  (Bruck,  1987),  getting trapped in locally optimal solutions.  Jeffrey  and  Rossner,  (Jeffrey,  1986),  extended  Hopfield's  technique  to  the  nonlinear  unconstrained  optimization  problem,  using  Cauchy  dynamics.  Kennedy  and  Chua,  (Kennedy,  1988), presented an  analog  implementation of a  network  solving a  nonlinear  optimization problem.  The underlying optimization algorithm  is a  simple transformation  method, (Reklaitis,  1983), which is known  to be relatively inefficient for large nonlinear  optimization problems. 
2  LINEAR  HOPFIELD  NETWORK  (LHN)  The computation in a Hopfield network is done by  a collection of highly interconnected  simple  neurons.  Each processing element,  i,  is characterized by  the activation  level,  Ui,  which is a function of the input received from  the external environment, Ii, and  the state  of the other neurons. The activation level of i is transmitted to  the other processors, after  passing through a filter that converts Ui to a 0-1 binary value, Vi'  The time behavior of the system is described by the following model: 
~ T·V·  - -' + I·  '  ~ 'J  J  J"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/979d472a84804b9f647bc185a877a8b5-Abstract.html,Connectionist Architectures for Multi-Speaker Phoneme Recognition,"John B. Hampshire II, Alex Waibel","We  present  a  number  of Time-Delay  Neural  Network  (TDNN)  based  architectures  for multi-speaker phoneme recognition (/b,d,g/ task).  We  use  speech  of two females  and four males to  compare the performance  of the various architectures against a baseline recognition rate of 95.9%  for a single IDNN on the six-speaker /b,d,g/ task.  This series of modu(cid:173) lar designs leads to a highly modular multi-network architecture capable  of performing the six-speaker recognition task at the speaker dependent  rate  of 98.4%.  In  addition  to  its  high  recognition  rate,  the  so-called  ""Meta-Pi""  architecture  learns - without direct  supervision - ognize the speech of one particular male speaker using internal models  of other male  speakers exclusively."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/9b04d152845ec0a378394003c96da594-Abstract.html,Bayesian Inference of Regular Grammar and Markov Source Models,"Kurt R. Smith, Michael I. Miller","In this paper we develop a Bayes criterion which includes the Rissanen  complexity, for  inferring regular grammar models.  We develop two  methods for regular grammar Bayesian inference.  The fIrst method is  based  on  treating  the  regular  grammar as  a  I-dimensional  Markov  source, and the second is based on the combinatoric characteristics of  the regular grammar itself.  We apply the resulting Bayes criteria to a  particular example in order to show the efficiency of each method."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/9c838d2e45b2ad1094d42f4ef36764f6-Abstract.html,Dataflow Architectures: Flexible Platforms for Neural Network Simulation,Ira Smotroff,"Dataflow architectures are general computation engines optimized for  the execution of fme-grain parallel algorithms. Neural networks can be  simulated on  these systems with  certain advantages.  In this paper, we  review  dataflow  architectures,  examine  neural  network  simulation  performance  on  a  new  generation  dataflow  machine,  compare  that  performance to other simulation alternatives, and discuss the benefits  and drawbacks of the dataflow approach. 
1  DATAFLOW ARCHITECTURES  Dataflow research has been conducted at MIT (Arvind &  Culler,  1986) and elsewhere  (Hiraki,  et.  aI.,  1987)  for  a  number  of  years.  Dataflow  architectures  are  general  computation engines that treat each instruction of a program as a separate task which is  scheduled in an asynchronous, data-driven fashion.  Dataflow programs are compiled into  graphs which explicitly describe the data dependencies of the computation. These graphs  are directly executed by the machine. Computations which are not linked by a path in the  graphs  can  be executed  in  parallel.  Each  machine  has  a  large  number  of processing  elements  with  hardware  that  is  optimized  to  reduce  task  switching  overhead  to  a  minimum.  As each  computation  executes  and  produces  a  result,  it  causes  all  of the  following computations that require the result to be scheduled. In this manner, fine grain  parallel  computation  is achieved,  with  the limit on  the  amount of possible parallelism  determined by the problem and the number of processing elements in the machine. 
Dataflow Architectures:  Flexible Platforms for Neural Network Simulation"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/9cfdf10e8fc047a44b08ed031e1f0ed1-Abstract.html,The Perceptron Algorithm Is Fast for Non-Malicious Distributions,Eric B. Baum,"Within  the  context  of Valiant's  protocol  for  learning,  the  Perceptron 
algorithm is shown  to learn  an arbitrary half-space in time O(r;;) if D, the proba(cid:173)
bility distribution of examples,  is  taken uniform over the unit sphere sn.  Here  f  is 
the accuracy parameter.  This is surprisingly fast,  as  ""standard""  approaches involve 
solution  of a  linear  programming problem involving  O( 7')  constraints in  n  dimen(cid:173)
sions.  A  modification  of Valiant's  distribution  independent  protocol  for  learning 
is  proposed  in which  the  distribution  and  the function  to be learned  may be  cho(cid:173)
sen  by adversaries,  however  these  adversaries may  not  communicate.  It is  argued 
that  this  definition  is  more  reasonable  and  applicable  to real  world  learning  than 
Valiant's.  Under  this  definition,  the  Perceptron algorithm  is  shown to be  a  distri(cid:173)
bution independent learning algorithm.  In  an  appendix we  show  that, for  uniform 
distributions,  some  classes  of infinite  V-C  dimension  including  convex  sets  and  a 
class of nested  differences of convex sets are learnable."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/a4f23670e1833f3fdb077ca70bbd5d66-Abstract.html,Discovering High Order Features with Mean Field Modules,"Conrad C. Galland, Geoffrey E. Hinton","A new  form of the deterministic Boltzmann machine (DBM) learn(cid:173) ing procedure is presented  which can efficiently train network mod(cid:173) ules  to discriminate  between  input  vectors  according  to some  cri(cid:173) terion.  The new  technique directly utilizes the free  energy  of these  ""mean field  modules""  to represent the probability that the criterion  is  met,  the  free  energy  being readily  manipulated by  the  learning  procedure.  Although  conventional deterministic  Boltzmann learn(cid:173) ing  fails  to  extract  the  higher  order  feature  of shift  at  a  network  bottleneck,  combining  the  new  mean  field  modules  with  the  mu(cid:173) tual information objective function  rapidly produces  modules that  perfectly extract this important higher order feature without direct  external supervision."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/a597e50502f5ff68e3e25b9114205d4a-Abstract.html,Sequential Decision Problems and Neural Networks,"A. G. Barto, R. S. Sutton, C. J. C. H. Watkins","Decision making tasks that involve delayed consequences are very  common yet difficult to address with supervised learning methods.  If there is an accurate model of the underlying dynamical system,  then these tasks can be formulated as sequential decision problems  and solved by Dynamic Programming. This paper discusses rein(cid:173) forcement learning in terms of the sequential decision framework  and shows how a learning algorithm similar to the one implemented  by the Adaptive Critic Element used in the pole-balancer of Barto,  Sutton, and Anderson (1983), and further developed by Sutton  (1984), fits into this framework. Adaptive neural networks can  play significant roles as modules for approximating the functions  required for solving sequential decision problems."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Abstract.html,Asymptotic Convergence of Backpropagation: Numerical Experiments,"Subutai Ahmad, Gerald Tesauro, Yu He","Yu  He  Dept.  of Physics  Ohio  State Univ.  Columbus,  OH 43212 
We  have  calculated, both analytically and in simulations,  the rate  of convergence  at  long  times  in  the  backpropagation learning  al(cid:173) gorithm  for  networks  with  and  without  hidden  units.  Our  basic  finding for units using the standard sigmoid transfer function is  lit  convergence of the  error for  large t,  with  at most logarithmic cor(cid:173) rections  for  networks  with  hidden  units.  Other transfer functions  may lead to a  8lower polynomial rate of convergence.  Our analytic  calculations were presented in (Tesauro, He &  Ahamd, 1989).  Here  we  focus  in more detail on our empirical measurements of the con(cid:173) vergence rate in numerical simulations,  which  confirm our analytic  results."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/b1a59b315fc9a3002ce38bbe070ec3f5-Abstract.html,Training Connectionist Networks with Queries and Selective Sampling,"Les E. Atlas, David A. Cohn, Richard E. Ladner","""Selective  sampling""  is  a  form  of directed  search  that  can greatly  increase  the ability of a  connectionist  network  to  generalize  accu(cid:173) rately.  Based  on  information  from  previous  batches  of samples,  a  network  may  be  trained  on  data selectively  sampled  from  regions  in  the  domain  that  are  unknown.  This is  realizable  in  cases  when  the distribution is  known,  or when  the cost  of drawing points from  the  target  distribution  is  negligible  compared  to the cost  of label(cid:173) ing  them  with  the proper classification.  The  approach  is  justified  by its applicability  to the problem of training a  network for  power  system  security  analysis.  The  benefits  of selective  sampling  are  studied  analytically,  and  the results  are confirmed  experimentally. 
Introduction:  Random  Sampling vs.  Directed  Search 
1  A  great  deal  of attention  has  been  applied  to the  problem of generalization  based  on  random samples  drawn  from  a  distribution,  frequently  referred  to as  ""learning  from  examples.""  Many  natural  learning learning systems  however,  do  not  simply  rely  on this  passive  learning technique,  but instead  make use  of at least some form  of directed  search  to  actively  examine  the  problem  domain.  In  many  problems,  directed  search  is  provably  more  powerful  than  passively  learning  from  randomly  given examples. 
Training Connectionist Networks with Queries and Selective Sampling 
567 
Typically, directed search consists of membership queries,  where the learner asks for  the classification of specific  points in the domain.  Directed  search via membership  queries may proceed simply  by examining the information already given and deter(cid:173) mining  a  region  of uncertainty,  the  area  in  the domain  where  the  learner  believes  mis-classification  is  still  possible.  The  learner  then  asks  for  examples  exclusively  from that region. 
This  paper discusses  one  form  of directed search:  selective  sampling.  In Section 2,  we  describe  theoretical  foundations  of directed  search and  give  a  formal  definition  of selective  sampling.  In  Section  3  we  describe  a  neural  network  implementation  of this technique,  and  we discuss the resulting improvements in  generalization on a  number of tasks in  Section  4. 
2  Learning and Selective  Sampling  For some arbitrary domain learning theory defines a  concept as being some subset of  points in  the domain.  For  example,  if our domain is  ~2, we  might define  a  concept  as  being all  points inside  a  region  bounded  by some  particular  rectangle.  A  concept  class  is  simply  the set of concepts in some description language. 
A concept class of particular interest for  this paper is that defined by neural network  architectures with a single output node.  Architecture refers to the number and types  of units in a network and their connectivity.  The configuration of a network specifies  the  weights on  the connections and  the thresholds of the units  1 . 
A  single-output  architecture  plus  configuration  can  be  seen  as  a  specification  of  a  concept  classifier  in  that  it  classifies  the  set  of all  points  producing  a  network  output  above some  threshold  value.  Similarly,  an  architecture  may  be  seen  as  a  specification of a concept class.  It consists of all concepts classified by configurations  of the network that the learning rule can produce (figure  1)."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/b1d10e7bafa4421218a51b1e1f1b0ba2-Abstract.html,Unsupervised Learning in Neurodynamics Using the Phase Velocity Field Approach,"Michail Zak, Nikzad Benny Toomarian","A  new  concept for  unsupervised  learning based  upon  examples in(cid:173) troduced  to the neural  network  is  proposed.  Each example is  con(cid:173) sidered  as  an  interpolation  node  of the  velocity field  in  the  phase  space.  The velocities  at  these  nodes  are selected such  that all  the  streamlines converge  to an attracting set imbedded in the subspace  occupied by the cluster of examples.  The synaptic interconnections  are  found  from  learning  procedure  providing  selected  field.  The  theory  is  illustrated  by examples. 
This  paper  is  devoted  to  development  of a  new  concept  for  unsupervised  learning  based upon examples introduced to an artificial neural network.  The neural network  is  considered  as  an  adaptive  nonlinear  dissipative  dynamical  system  described  by  the following  coupled  differential equations: 
Ui + K,Ui  = L 11j g( Uj )  + Ii"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/bcbe3365e6ac95ea2c0343a2395834dd-Abstract.html,A Continuous Speech Recognition System Embedding MLP into HMM,"Hervé Bourlard, Nelson Morgan",We  are  developing  a  phoneme  based.  speaker-dependent  continuous  speech  recognition  system  embedding  a  Multilayer Perceptron  (MLP)  (Le .•  a  feedforward  Artificial  Neural  Network).  into  a Hidden  Markov  Model (HMM) approach.  In [Bourlard &  Wellekens]. it was  shown that  MLPs  were approximating Maximum  a Posteriori (MAP) probabilities  and  could  thus  be  embedded  as  an  emission  probability  estimator  in  HMMs.  By using contextual information from  a sliding window on the  input frames.  we  have  been  able  to  improve  frame  or phoneme  clas(cid:173) sification  performance  over the  corresponding  performance  for Simple  Maximum  Likelihood  (ML)  or even  MAP  probabilities  that  are  esti(cid:173) mated without the benefit of context.  However. recognition of words in  continuous speech was  not so simply improved by the use of an  MLP.  and  several  modifications  of the  original  scheme  were  necessary  for  getting acceptable performance.  It is  shown here that word recognition  performance for a  simple discrete density  HMM  system  appears  to  be  somewhat better when MLP methods are used to estimate the emission  probabilities.
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/bd686fd640be98efaae0091fa301e613-Abstract.html,Analysis of Linsker's Simulations of Hebbian Rules,"David J. C. MacKay, Kenneth D. Miller","Linsker has reported the development of centre---surround receptive  fields  and  oriented  receptive  fields  in  simulations  of a  Hebb-type  equation  in  a  linear  network.  The  dynamics  of the  learning  rule  are analysed in  terms of the eigenvectors of the covariance matrix  of cell activities.  Analytic and  computational results  for  Linsker's  covariance matrices,  and some general theorems,  lead  to an expla(cid:173) nation  of the  emergence  of centre---surround  and  certain  oriented  structures. 
Linsker  [Linsker,  1986,  Linsker,  1988]  has  studied  by  simulation  the  evolution  of  weight vectors under a  Hebb-type teacherless learning rule  in a  feed-forward  linear  network.  The equation for  the evolution of the weight vector w  of a  single neuron,  derived  by  ensemble  averaging  the  Hebbian  rule  over  the  statistics  of the  input  patterns,  is:! 
a at Wi = k! + L(Qij + k 2 )wj  subject to -Wmax  ~ Wi < Wmax"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract.html,Analog Neural Networks of Limited Precision I: Computing with Multilinear Threshold Functions,"Zoran Obradovic, Ian Parberry","Experimental  evidence  has  shown  analog  neural  networks  to  be  ex(cid:173) ~mely fault-tolerant;  in  particular.  their  performance  does  not  ap(cid:173) pear  to  be  significantly  impaired  when  precision  is  limited.  Analog  neurons  with  limited  precision  essentially  compute  k-ary  weighted  multilinear threshold  functions.  which  divide  R""  into k  regions  with  k-l hyperplanes.  The behaviour of k-ary  neural networks  is  investi(cid:173) gated.  There  is  no  canonical  set  of  threshold  values  for  k>3.  although  they  exist  for  binary  and  ternary  neural  networks.  The  weights  can be  made  integers of only  0 «z +k ) log  (z +k » bits. where  z  is  the  number  of processors.  without  increasing  hardware  or  run(cid:173) ning  time.  The  weights  can  be  made  ±1  while  increasing  running  time  by a constant multiple and hardware by  a small polynomial  in  z  and  k.  Binary  neurons  can  be  used  if the  running  time  is allowed  to  increase  by  a larger constant  multiple  and  the  hardware  is  allowed  to  increase  by  a  slightly  larger polynomial  in  z  and k.  Any  symmetric  k-ary  function  can  be  computed  in  constant  depth  and  size  o (n k- 1/(k-2)!).  and  any  k-ary  function  can  be  computed  in  constant  depth and  size  0  (nk"").  The alternating neural  networks of Olafsson  and  Abu-Mostafa.  and  the  quantized  neural  networks  of Fleisher  are  closely related  to this model. 
Analog Neural Networks of Limited Precision I 
703"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/c0e190d8267e36708f955d7ab048990d-Abstract.html,A Systematic Study of the Input/Output Properties of a 2 Compartment Model Neuron With Active Membranes,Paul C. Rhodes,"The  input/output  properties  of a  2  compartment  model  neuron  are  systematically  explored.  Taken from  the work of MacGregor (MacGregor,  1987), the model neuron  compartments contain several active conductances, including a potassium conductance in  the  dendritic  compartment driven  by  the  accumulation  of  intradendritic  calcium.  Dynamics of the conductances and potentials are governed by a set of coupled first order  differential equations which are  integrated numerically.  There are a set of 17 internal  parameters  to  this  model,  specificying  conductance rate  constants,  time  constants,  thresholds, etc. 
To study parameter sensitivity,  a set of trials were run in which the input driving the  neuron is kept fixed while each internal parameter is varied with all others left fixed. 
To study the input/output relation, the input to the dendrite (a square wave) was varied  (in frequency and magnitude) while all internal parameters of the system were left flXed,  and the resulting output firing rate and bursting rate was counted. 
The  input/output  relation of the  model  neuron  studied  turns  out  to  be  much  more  sensitive  to  modulation  of certain  dendritic  potassium  current  parameters  than  to  plasticity  of synapse  efficacy per se  (the  amount  of current  influx  due  to  synapse  activation).  This would in turn suggest, as has been recently observed experimentally,  that the potassium current may be as or more important a focus of neural plasticity than  synaptic efficacy."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract.html,Subgrouping Reduces Complexity and Speeds Up Learning in Recurrent Networks,David Zipser,Abstract Unavailable
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/c52f1bd66cc19d05628bd8bf27af3ad6-Abstract.html,Neural Implementation of Motivated Behavior: Feeding in an Artificial Insect,"Randall D. Beer, Hillel J. Chiel","Most  complex  behaviors  appear  to be governed  by  internal  moti(cid:173) vational  states or  drives  that  modify  an  animal's  responses  to  its  environment.  It is  therefore of considerable  interest to understand  the  neural basis of these  motivational states.  Drawing upon work  on  the  neural  basis  of feeding  in  the  marine  mollusc  Aplysia,  we  have  developed  a  heterogeneous  artificial  neural  network  for  con(cid:173) trolling the feeding behavior of a simulated insect.  We demonstrate  that feeding in this artificial insect shares many characteristics with  the motivated behavior of natural animals."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract.html,A Neural Network to Detect Homologies in Proteins,"Yoshua Bengio, Samy Bengio, Yannick Pouliot, Patrick Agin","In order to detect the presence and location of immunoglobu(cid:173) lin (Ig) domains from amino acid sequences we built a system  based on a neural network with one hidden layer trained with  back propagation. The program was designed to efficiently  identify proteins exhibiting such domains, characterized by a  few localized conserved regions and a low overall homology.  When the National Biomedical Research Foundation (NBRF)  NEW protein sequence database was scanned to evaluate the  program's performance, we obtained very low rates of false  negatives coupled with a moderate rate of false positives."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html,Higher Order Recurrent Networks and Grammatical Inference,"C. Lee Giles, Guo-Zheng Sun, Hsing-Hen Chen, Yee-Chun Lee, Dong Chen","A  higher  order  single  layer  recursive  network  easily  learns  to  simulate  a  deterministic  finite  state  machine  and  recognize  regular  grammars.  When an  enhanced version of this  neural net state machine  is connected through a common error term  to an external analog stack  memory, the combination can be interpreted as  a neural net pushdown  automata.  The  neural net finite state machine  is given  the primitives,  push  and  POP.  and  is  able  to  read  the  top  of  the  stack.  Through  a  gradient  descent  learning  rule  derived  from  the  common  error  function,  the  hybrid  network  learns  to  effectively  use  the  stack  actions  to  manipUlate  the  stack  memory  and  to  learn  simple  context(cid:173) free grammars.  INTRODUCTION 
Biological  networks  readily  and  easily  process  temporal  information;  artificial  neural  networks  should  do  the  same.  Recurrent  neural  network  models  permit  the  encoding  and learning of temporal sequences.  There are many recurrent neural net models. for ex(cid:173) ample see  [Jordan  1986. Pineda  1987, Williams & Zipser 1988].  Nearly all encode the  current  state  representation  of the  models  in  the  activity  of the  neuron  and  the  next  state  is  determined  by  the  current  state  and  input.  From  an  automata  perspective,  this  dynamical  structure  is  a state  machine.  One  formal  model  of sequences  and  machines  that  generate  and  recognize  them  are  formal  grammars  and  their  respective  automata.  These models  formalize some of the foundations  of computer science.  In  the Chomsky  hierarchy  of formal  grammars  [Hopcroft  & Ullman  1979]  the  simplest  level  of com(cid:173) plexity  is  defmed  by  the  finite  state  machine  and  its  regular  grammars.  (All  machines 
Higher Order Recurrent Networks and Grammatical Inference 
381 
and  grammars  described  here  are  deterministic.}  The  next  level  of complexity  is  de(cid:173) scribed by  pushdown automata and  their associated context-free grammars.  The push(cid:173) down automaton is a fmite  state machine with  the added power to  use  a stack memory.  Nemal  networks  should  be  able  to  perform  the  same  type  of computation  and  thus  solve such learning problems as grammatical inference [pu 1982] .  Simple grammatical inference is defined as  the problem of finding (learning) a grammar  from  a  fmite  set  of strings,  often  called  the  teaching  sample.  Recall  that  a  grammar  {phrase-structured}  is  defined as a 4-tuple (N, V, P, S) where N and V are a  nonterm i(cid:173) na1 and terminal vocabularies, P is a finite set of production rules and S is the start sym(cid:173) bol.  Here  grammatical  inference  is  also  defined  as  the  learning  of  the  machine  that  recognizes  the  teaching  and  testing  samples.  Potential  applications  of grammatical  in(cid:173) ference  include  such  various  areas  as  pattern  recognition,  information  retrieval,  pro(cid:173) gramming language design, translation and compiling and graphics languages [pu 1982].  There has been a great deal of interest in  teaching nemal nets to recognize grammars and  simulate  automata  [Allen  1989.  Jordan  1986.  Pollack  1989.  Servant-Schreiber  et.  a1.  1989,Williams  & Zipser  1988].  Some  important extensions  of that  work  are  discussed  here.  In  particular we construct recurrent higher order nemal  net state machines which  have  no  hidden  layers and seem  to  be  at least as  powerful  as  any nemal  net multilayer  state machine discussed so  far.  For example,  the learning time and  training  sample size  are  significantly reduced.  In  addition,  we integrate this  neural  net fmite  state machine  with  an  external  stack  memory  and  inform  the  network  through  a  common  objective  function  that  it  has  at its disposal  the  symbol  at  the  top of the  stack and  the  operation  primitives  of push  and  pop.  By  devising  a common error function  which  integrates the  stack and  the  nemal  net state machine,  this  hybrid structure learns to effectively use  the  the  interesting  work  of [Williams  &  stack  to  recognize  context-free  grammars.  Zipser  1988]  a recurrent  net  learns  only  the  state  machine  part of a Turing  Machine.  since the associated move, read, write operations for each input string are known and are  given as part of the  training set.  However,  the  model  we present learns  how  to  manipu(cid:173) late  the push, POP. and read primitives of an external stack memory plus  learns  the ad(cid:173) ditional necessary state operations and  structure.  HIGHER ORDER RECURRENT NETWORK  The  recurrent neural  network  utilized  can  be  considered as  a higher order modification  of the  network  model  developed by  [Williams  & Zipser  1988].  Recall  that  in  a recur(cid:173) rent  net  the  activation  state S of the  neurons  at time  (t+l) is defined as  in  a state  ma(cid:173) chine automata: 
In 
(1)  where  F maps  the state S and  the input I at time  t to  the  next state.  The  weight matrix  W forms  the mapping and is usually  learned.  We  use a higher order form  for this map(cid:173) ping: 
S(t+ 1) = F  ( S(t), I(t); W  }"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/cfa0860e83a4c3a763a7e62d825349f7-Abstract.html,Practical Characteristics of Neural Network and Conventional Pattern Classifiers on Artificial and Speech Problems,"Yuchun Lee, Richard P Lippmann","Eight  neural  net  and  conventional  pattern  classifiers  (Bayesian(cid:173) unimodal Gaussian, k-nearest neighbor, standard back-propagation,  adaptive-stepsize back-propagation, hypersphere, feature-map, learn(cid:173) ing vector  quantizer,  and  binary  decision  tree)  were  implemented  on  a  serial  computer  and  compared  using  two  speech  recognition  and two artificial tasks.  Error rates were statistically equivalent on  almost  all  tasks,  but classifiers  differed  by orders  of magnitude in  memory  requirements,  training time,  classification  time,  and ease  of adaptivity.  Nearest-neighbor  classifiers  trained  rapidly  but  re(cid:173) quired  the most memory.  Tree classifiers  provided  rapid classifica(cid:173) tion but  were  complex to adapt.  Back-propagation classifiers  typ(cid:173) ically  required  long  training  times  and  had intermediate  memory  requirements.  These results suggest that classifier selection should  often  depend  more  heavily  on  practical considerations  concerning  memory  and  computation  resources,  and  restrictions  on  training  and classification times than on error rate. 
-This  work  was  sponsored by  the  Department of the  Air  Force  and  the  Air  Force  Office  of 
Scientific Research. 
Practical Characteristics of Neural Network"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/cfecdb276f634854f3ef915e2e980c31-Abstract.html,Contour-Map Encoding of Shape for Early Vision,Pentti Kanerva,"Contour  maps  provide  a  general  method  for  recognizing  two-dimensional  shapes.  All  but  blank  images  give  rise  to  such  maps,  and  people  are  good  at  recognizing  objects  and  shapes  from  them.  The  maps  are  encoded  easily  in  long  feature  vectors  that  are  suitable  for  recognition  by  an  associative  memory.  These  properties  of  contour  maps  suggest  a  role  for  them  in  early  visual  perception.  The  prevalence  of  direction-sensitive  neurons  in  the  visual  cortex  of  mammals  supports  this  view."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/d1c38a09acc34845c6be3a127a5aacaf-Abstract.html,Maximum Likelihood Competitive Learning,Steven J. Nowlan,"One popular class of unsupervised algorithms are competitive algo(cid:173) rithms.  In the traditional view of competition, only one competitor,  the  winner,  adapts for  any given  case.  I  propose  to view  compet(cid:173) itive adaptation as  attempting to fit  a  blend of simple probability  generators  (such  as  gaussians)  to a  set  of data-points.  The maxi(cid:173) mum likelihood fit  of a model of this type suggests a  ""softer""  form  of competition,  in  which  all  competitors  adapt  in  proportion  to  the relative probability that the input came from each  competitor.  I  investigate one  application of the soft  competitive model,  place(cid:173) ment of radial basis function centers for function interpolation, and  show  that  the  soft  model  can  give  better  performance  with  little  additional computational cost."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/d395771085aab05244a4fb8fd91bf4ee-Abstract.html,Note on Development of Modularity in Simple Cortical Models,"Alex Chernajvsky, John E. Moody","The existence of modularity in the organization of nervous systems  (e.g. cortical columns and olfactory glomeruli) is well known. We  show that localized activity patterns in a layer of cells, collective  excitations, can induce the formation of modular structures in the  anatomical connections via a Hebbian learning mechanism. The  networks are spatially homogeneous before learning, but the spon(cid:173) taneous emergence of localized collective excitations and subse(cid:173) quently modularity in the connection patterns breaks translational  symmetry. This spontaneous symmetry breaking phenomenon is  similar to those which drive pattern formation in reaction-diffusion  systems. We have identified requirements on the patterns of lateral  connections and on the gains of internal units which are essential  for the development of modularity. These essential requirements  will most likely remain operative when more complicated (and bi(cid:173) ologically realistic) models are considered. 
1 Present Address: Molecular and Cellular Physiology, Beckman Center, Stanford University, 
Stanford, CA 94305. 
2 Please address correspondence to John Moody. 
134 
Chernjavsky and Moody 
1 Modularity in Nervous Systems 
Modular organization exists throughout the nervous system on many different spa(cid:173) tial scales. On the very small scale, synapses appear to be clustered on dendrites.  On the very large scale, the brain as a whole is composed of many anatomically  and functionally distinct regions. At intermediate scales, the scales of networks and  maps, the brain exhibits columnar structures. 
The purpose of this work is to suggest possible mechanisms for the development  of modular structures at the intermediate scales of networks and maps. The best  known modular structure at this scale is the column. Many modality- specific  variations of columnar organization are known, for example orientation selective  columns, ocular dominance columns, color sensitive blobs, somatosensory barrels,  and olfactory glomeruli. In addition to these anatomically well-established struc(cid:173) tures, other more speculative modular anatomical structures may exist. These  include the frontal eye fields of association cortex whose modular structure is in(cid:173) ferred only from electrophysiology and the hypothetical existence of minicolumns  and possibly neuronal groups. 
Although a complete biophysical picture of the development of modular structures  is still unavailable, it is well established that electrical activity is crucial for the  development of certain modular structures such as complex synaptic zones and oc(cid:173) ular dominance columns (see Kalil 1989 and references therein). It is also generally  conjectured that a Hebb-like mechanism is operative in this development. These  observations form a basis for our operating hypothesis described below. 
2 Operating Hypothesis and Modeling Approach  Our hypothesis in this work is that localized activity patterns in a layer of cells  induce the development of modular anatomical structure within the layer. We  further hypothesize that the emergence of localized activity patterns in a layer is  due to the properties of the intrinsic network dynamics and does not necessarily  depend upon the system receiving localized patterns of afferent activity. 
Our work therefore has two parts. First, we show that localized patterns of ac(cid:173) tivity on a preferred spatial scale, collective excitations, spontaneously emerge in  homogeneous networks with appropriate lateral connectivity and cellular response  properties when driven with arbitrary stimulus (see Moody 1990). Secondly, we  show that these collective excitations induce the formation of modular structures  in the connectivity patterns when coupled to a Hebbian learning mechanism. 
The emergence of collective excitations at a preferred spatial scale in a homogeneous  network breaks translational symmetry and is an example of spontaneous symmetry  breaking. The Hebbian learning freezes the modular structure into the anatomy.  The time scale of collective excitations is short, while the Hebbian learning process  occurs over a longer time scale. The spontaneous symmetry breaking mechanism is  similar to that which drives pattern formation in reaction-diffusion systems (Turing  1952, Meinhardt 1982). Reaction-diffusion models have been applied to pattern for-
Note on Development or Modularity in Simple Cortical Models 
135"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/d6baf65e0b240ce177cf70da146c8dc8-Abstract.html,Model Based Image Compression and Adaptive Data Representation by Interacting Filter Banks,"Toshiaki Okamoto, Mitsuo Kawato, Toshio Inui, Sei Miyake","introduced. Based on 
To achieve high-rate image data compression  while maintainig a high quality reconstructed  image, a good image model and an efficient  way to represent the specific data of each  image must be  the  physiological knowledge of multi - channel  characteristics and  interactions  between them in the human visual system,  a mathematically coherent parallel architecture  for image data compression which utilizes the  Markov  Image model and  interactions between a vast number of filter  banks, is proposed."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/d947bf06a885db0d477d707121934ff8-Abstract.html,On the Distribution of the Number of Local Minima of a Random Function on a Graph,"Pierre Baldi, Yosef Rinott, Charles Stein",Abstract Unavailable
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/d96409bf894217686ba124d7356686c9-Abstract.html,Using a Translation-Invariant Neural Network to Diagnose Heart Arrhythmia,Susan Ciarrocca Lee,Distinctive electrocardiogram (EeG) patterns are created when the heart  is beating normally and when a dangerous arrhythmia is present. Some  devices which monitor the EeG and react to arrhythmias parameterize  the ECG signal and make a diagnosis based on the parameters. The  author discusses the use of a neural network to classify the EeG signals  directly. without parameterization. The input to such a network must  be translation-invariant. since the distinctive features of the EeG may  appear anywhere in an arbritrarily-chosen EeG segment. The input  must also be insensitive to the episode-to-episode and patient-to-patient  variability in the rhythm pattern.
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/db8e1af0cb3aca1ae2d0018624204529-Abstract.html,Non-Boltzmann Dynamics in Networks of Spiking Neurons,"Michael C. Crair, William Bialek","We study networks  of spiking neurons  in which spikes  are fired  as  a  Poisson process.  The state of a  cell is  determined  by the  instan(cid:173) taneous  firing  rate,  and  in the  limit of high firing  rates our model  reduces  to  that  studied  by  Hopfield.  We  find  that  the  inclusion  of spiking  results  in several new  features,  such  as  a  noise-induced  asymmetry between ""on"" and ""off"" states of the cells and probabil(cid:173) ity currents which destroy the usual description of network dynam(cid:173) ics  in  terms  of energy  surfaces.  Taking account  of spikes  also  al(cid:173) lows us to calibrate network parameters such as  ""synaptic weights""  against  experiments  on  real synapses.  Realistic  forms  of the  post  synaptic  response  alters  the  network dynamics,  which  suggests  a  novel dynamical learning mechanism."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/e165421110ba03099a1c0393373c5b43-Abstract.html,"The ""Moving Targets"" Training Algorithm",Richard Rohwer,"A  simple  method  for  training  the  dynamical  behavior  of  a  neu(cid:173) ral  network  is  derived.  It  is  applicable  to  any  training  problem  in  discrete-time networks with  arbitrary feedback.  The algorithm  resembles back-propagation in  that an error function  is  minimized  using a  gradient-based method,  but the optimization is carried out  in the hidden part of state space either instead of,  or in addition to  weight space.  Computational results are presented for some simple  dynamical  training  problems,  one  of which  requires response  to  a  signal  100 time steps in the past."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/e2c0be24560d78c5e599c2a9c9d0bbd2-Abstract.html,Performance Comparisons Between Backpropagation Networks and Classification Trees on Three Real-World Applications,"Les E. Atlas, Ronald A. Cole, Jerome T. Connor, Mohamed A. El-Sharkawi, Robert J. Marks II, Yeshwant K. Muthusamy, Etienne Barnard","Multi-layer  perceptrons  and  trained  classification  trees  are  two  very  different  techniques  which  have  recently  become  popular.  Given  enough  data  and  time,  both  methods  are  capable  of performing  arbi(cid:173) trary  non-linear  classification.  We  first  consider  the  important  differences  between  multi-layer  perceptrons  and  classification  trees  and  conclude  that  there  is  not enough  theoretical  basis  for  the  clear(cid:173) cut  superiority  of one  technique  over  the  other.  For  this  reason,  we  performed  a  number  of empirical  tests  on  three  real-world  problems  in  power  system  load  forecasting,  power  system  security  prediction,  and  speaker-independent  vowel  identification.  In  all  cases,  even  for  piecewise-linear  trees,  the  multi-layer  perceptron  performed  as  well  as or better than  the trained classification  trees. 
Performance Comparisons 
623"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/e3796ae838835da0b6f6ea37bcf8bcb7-Abstract.html,An Efficient Implementation of the Back-propagation Algorithm on the Connection Machine CM-2,"Xiru Zhang, Michael McKenna, Jill P. Mesirov, David L. Waltz","In this paper, we  present a novel implementation of the widely used  Back-propagation neural net learning algorithm on the Connection  Machine  CM-2  - a  general  purpose,  massively  parallel  computer  with a hypercube topology.  This implementation runs at about 180  million  interconnections per second  (IPS)  on a  64K  processor  CM- 2.  The  main  interprocessor  communication operation  used  is  2D  nearest  neighbor  communication.  The  techniques  developed  here  can  be  easily  extended  to implement  other algorithms for  layered  neural nets on  the CM-2,  or on other massively parallel computers  which have 2D or higher degree connections among their processors."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/e4a6222cdb5b34375400904f03d8e6a5-Abstract.html,Algorithms for Better Representation and Faster Learning in Radial Basis Function Networks,"Avijit Saha, James D. Keeler","in 
learning,  using  radial  basis  functions 
In  this  paper  we  present  upper  bounds  for  the  learning  rates  for  hybrid  models  that  employ  a  combination  of  both  self-organized  and  supervised  to  build  receptive  field  representations  the  hidden  units.  The  learning  performance  in  such  networks  with  nearest  neighbor  heuristic  can  be  improved  upon  by  multiplying  the  individual  receptive  field  widths  by  a  suitable  overlap  factor.  We  present  results  indicat!ng  optimal  values  for  such  overlap  factors.  We  also  present  a  new  algorithm  for  determining  receptive  field  centers.  This  method  negotiates  more  hidden  units  in  the  regions  of the  input  space  as  a  function  of the  output  and  is  conducive  to  better  learning  when  the  number of patterns (hidden units) is small."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/e56954b4f6347e897f954495eab16a88-Abstract.html,The Cocktail Party Problem: Speech/Data Signal Separation Comparison between Backpropagation and SONN,"John Kassebaum, Manoel Fernando Tenorio, Christoph Schaefers","This  work  introduces  a  new  method  called  Self  Organizing  Neural  Network  (SONN)  algorithm  and  compares  its  performance  with Back  Propagation  in  a  signal  separation  application.  The  problem  is  to  separate  two  signals;  a  modem  data signal  and  a  male  speech  signal,  added and transmitted  through  a  4 khz  channel.  The signals  are sam(cid:173) pled  at  8  khz,  and  using  supervised  learning,  an  attempt  is  made  to  reconstruct  them.  The  SONN  is  an  algorithm  that  constructs  its  own  network  topology  during  training,  which  is  shown  to  be  much  smaller  than  the  BP  network,  faster  to  trained,  and  free  from  the  trial-and(cid:173) error network design that characterize BP."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/e96ed478dab8595a7dbda4cbcbee168f-Abstract.html,Real-Time Computer Vision and Robotics Using Analog VLSI Circuits,"Christof Koch, Wyeth Bair, John G. Harris, Timothy K. Horiuchi, Andrew Hsu, Jin Luo","The long-term goal of our laboratory is  the development of analog  resistive  network-based  VLSI implementations of early  and  inter(cid:173) mediate  vision  algorithms.  We  demonstrate  an  experimental cir(cid:173) cuit  for  smoothing  and  segmenting  noisy  and  sparse  depth  data  using  the  resistive  fuse  and  a  1-D  edge-detection  circuit  for  com(cid:173) puting zero-crossings using two  resistive grids with different space(cid:173) constants.  To  demonstrate  the  robustness  of our  algorithms  and  of the fabricated analog CMOS VLSI chips, we  are mounting these  circuits  onto  small  mobile  vehicles  operating in  a  real-time,  labo(cid:173) ratory environment."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/eae27d77ca20db309e056e3d2dcd7d69-Abstract.html,The CHIR Algorithm for Feed Forward Networks with Binary Weights,Tal Grossman,"A new learning algorithm, Learning by Choice of Internal Rep(cid:173) resetations  (CHIR), was  recently introduced.  Whereas many algo(cid:173) rithms  reduce  the  learning  process  to  minimizing a  cost  function  over the  weights, our method treats the  internal representations as  the fundamental entities to  be determined.  The algorithm applies  a  search  procedure  in the  space  of internal representations,  and  a  cooperative adaptation of the weights (e.g.  by using the perceptron  learning rule).  Since the introduction of its basic, single output ver(cid:173) sion, the CHIR algorithm was generalized to train any feed  forward  network of binary neurons.  Here we present the generalised version  of the  CHIR algorithm,  and further  demonstrate  its  versatility by  describing  how it can  be  modified  in order  to  train networks with  binary  (±1)  weights.  Preliminary  tests  of this  binary  version  on  the random teacher  problem are  also  reported."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/eb163727917cbba1eea208541a643e74-Abstract.html,A Large-Scale Neural Network Which Recognizes Handwritten Kanji Characters,"Yoshihiro Mori, Kazuki Joe","We propose a  new  way  to construct a  large-scale neural  network  for  3.000 handwritten  Kanji  characters  recognition.  This  neural  network  consists  of 3  parts:  a  collection  of small-scale  networks  which  are  trained individually on a small number of Kanji characters; a network  which  integrates  the  output  from  the  small-scale  networks,  and  a  process to facilitate  the integration of these neworks. The recognition  rate of the  total  system  is  comparable  with  those  of the  small-scale  networks. Our results indicate that the proposed method is effective for  constructing  a  large-scale  network  without  loss  of  recognition  performance."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/ec8ce6abb3e952a85b8551ba726a1227-Abstract.html,Mechanisms for Neuromodulation of Biological Neural Networks,Ronald M. Harris-Warrick,The pyloric Central Pattern Generator of the crustacean stomatogastric  ganglion is a  well-defined  biological neural  network.  This  14-neuron  network  is  modulated  by  many  inputs.  These inputs reconfigure  the  network  to  produce  multiple  output  patterns  by  three  simple  mechanisms:  1) detennining which  cells are active; 2) modulating the  synaptic  efficacy;  3)  changing  the  intrinsic  response  properties  of  individual  neurons.  The importance of modifiable  intrinsic  response  properties of neurons for network function and modulation is discussed.
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/eda80a3d5b344bc40f3bc04f65b7a357-Abstract.html,Recognizing Hand-Printed Letters and Digits,"Gale Martin, James A. Pittman","We are developing a hand-printed character recognition system using a multi(cid:173) layered neural net trained through backpropagation.  We report on results of  training nets with samples of hand-printed digits scanned off of bank checks  and hand-printed letters interactively entered into a computer through a sty(cid:173) lus digitizer.  Given a large training set,  and a net with sufficient capacity to  achieve  high performance on the training set,  nets  typically achieved error  rates of 4-5% at a 0% reject rate and 1-2% at a 10% reject rate.  The topology  and capacity of the system, as measured by the number of connections in the  net,  have  surprisingly little effect on generalization.  For those  developing  practical pattern recognition systems,  these results suggest that a large and  representative  training  sample  may be  the single,  most important factor in  achieving high recognition accuracy.  From a scientific standpoint, these re(cid:173) sults raise doubts about the relevance to backpropagation of learning models  that estimate the likelihood of high generalization from estimates of capacity.  Reducing capacity does have other benefits however, especially when the re(cid:173) duction is  accomplished by using local receptive fields  with shared weights.  In this latter case, we find the net evolves feature detectors resembling those  in visual cortex and Linsker's orientation-selective nodes. 
Practical interest in hand-printed character recognition is fueled by two current tech(cid:173) nology trends:  one toward systems that interpret hand-printing on hard-copy docu(cid:173) ments and one toward notebook-like computers that replace the keyboard with a stylus  digitizer.  The stylus enables users to write and draw directly on a flat panel display.  In this paper, we report on results applying multi-layered neural nets trained through  backpropagation (Rumelhart, Hinton, & Williams,  1986) to both cases. 
Developing pattern recognition systems is typically a two-stage process.  First, intuition  and experimentation are used to select a set of features to represent the raw input pat(cid:173) tern.  Then a variety of well-developed techniques are used to optimize the classifier  system that assumes this featural representation.  Most applications of backpropaga(cid:173) tion learning to character recognition use the learning capabilities only for this latter 
406  Martin and Pittman 
stage--developing the classifier system  (Burr, 1986; Denker, Gardner, Graf, Hender(cid:173) son, Howard, Hubbard, Jackel, Baird, & Guyon, 1989; Mori & Yokosawa, 1989; Weide(cid:173) man, Manry, &  Yau, 1989). However, backpropagation learning affords the opportuni(cid:173) ty  to optimize feature selection and pattern classification simultaneously. We avoid  using pre-determined features as input to the net in favor of using a pre- segmented,  size-normalized grayscale array for each character. This is a first step toward  the goal  of approximating the raw input projected onto the human retina, in that no pre-proces(cid:173) sing of the input is  required. 
We report on results for both hand-printed digits and letters.  The hand-printed digits  come from a set of 40,000 hand-printed digits scanned from the numeric amount region  of ""real-world"" bank checks.  They were  pre-segmented and  size-normalized  to a  15x24 grayscale array.  The test set consists of 4,000 samples and training sets varied  from 100 to 35,200 samples. Although it is always difficult to compare recognition rates  arising from different pattern sets, some appreciation for the difficulty of categoriza(cid:173) tion can be gained using human performance data as a benchmark.  An independent  person categorizing the test set of pre-segmented, size-normalized digits achieved an  error rate of 3.4%.  This figure is considerably below the near-perfect performance of  operators keying in numbers directly from bank checks, because the segmentation al(cid:173) gorithm is flawed. 
Working with  letters, as well as digits,  enables tests of the generality of results on a  different pattern set having more than double the number of output categories. The  hand-printed letters come from a set of 8,600 upper-case letters collected from over  110 people writing with a stylus input device on a flat panel display.  The stylUS  collects  a sequence of x-y coordinates at 200 points per second at a spatial resolution of 1000  points per inch.  The temporal sequence for each character is first converted to a size(cid:173) normalized bitmap array, keeping aspect ratio constant.  We have found that recogni(cid:173) tion accuracy is significantly improved if these bitmaps are blurred through convolution  with a gaussian distnbution.  Each pattern is represented as a 15x24 grayscale image.  A test set of 2,368 samples was extracted by selecting samples from  18 people, so that  training sets were generated by people different from  those generating the test set.  Training set sizes ranged from 500 to roughly 6,300 samples. 
1  HIGH  RECOGNITION  ACCURACY 
We find  relatively high  recognition accuracy for both pattern sets. Thble  11  reports  the minimal error rates achieved on the test samples for both pattern sets, at various  reject rates.  In the case of the hand-printed digits, the 4% error rate (0% rejects) ap-

Eff~cts of the number.of training samples and network capacity and  topology are reported in  the  next sectIon.  Nets were tramed to error rates of 2-3%.  1i""aining began with a learning rate of .05 and  a mome~tum value of .9.  The learning rate was decreased when  training accuracy began to oscillate or  had stabtlized for a large number of training epochs. We evaluate the output vector on a winner-take(cid:173) all  basis, as  this consistently improves accuracy and results in  network parameters having a smaller  effect on perfonnance. 

Recognizing Hand-Printed Letters and Digits 
407 
proaches the 3.4%  errors made by  the human judge.  This suggests that further im(cid:173) provements to generalization will require improving segmentation accuracy.  The fact  that an error rate of 5% was achieved for  letters is promising.  Accuracy is fairly high, 
Table 1:  Error rates of best nets trained on largest sample sets and tested"
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/f340f1b1f65b6df5b5e3f94d95b11daf-Abstract.html,TRAFFIC: Recognizing Objects Using Hierarchical Reference Frame Transformations,"Richard S. Zemel, Michael Mozer, Geoffrey E. Hinton","We  describe a model that can recognize  two-dimensional shapes in  an  unsegmented  image,  independent  of their orientation,  position,  and scale.  The model,  called TRAFFIC, efficiently  represents  the  structural  relation  between  an  object  and  each  of its  component  features  by  encoding  the fixed  viewpoint-invariant transformation  from the feature's reference frame to the object's in the weights of a  connectionist  network.  Using  a  hierarchy  of such  transformations,  with increasing complexity of features  at each successive  layer,  the  network  can  recognize  multiple objects  in parallel.  An implemen(cid:173) tation  of TRAFFIC  is  described,  along with  experimental  results  demonstrating  the  network's  ability to  recognize  constellations  of  stars in  a viewpoint-invariant manner."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/f718499c1c8cef6730f9fd03c8125cab-Abstract.html,Performance of Connectionist Learning Algorithms on 2-D SIMD Processor Arrays,"Fernando J. Nuñez, José A. B. Fortes",The  mapping  of  the  back-propagation  and  mean  field  theory  learning  algorithms  onto  a  generic  2-D  SIMD  computer  is  described.  This  architecture proves to  be very  adequate for  these  applications  since  efficiencies  close  to  the  optimum  can  be  attained.  Expressions  to  find  the  learning  rates  are  given  and  then particularized to the DAP  array procesor.
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/f7664060cc52bc6f3d620bcedc94a4b6-Abstract.html,Neurally Inspired Plasticity in Oculomotor Processes,Paul A. Viola,"We  have  constructed  a  two axis  camera positioning system which  is roughly analogous to a single human eye.  This Artificial-Eye (A(cid:173) eye)  combines  the  signals  generated  by  two  rate gyroscopes  with  motion  information  extracted from  visual  analysis  to stabilize  its  camera.  This stabilization process is similar to the vestibulo-ocular  response  (VOR);  like the  VOR, A-eye  learns a  system model that  can  be incrementally modified to adapt to changes in its structure,  performance and environment.  A-eye is an example of a robust sen(cid:173) sory system that performs computations that can be of significant  use  to the designers of mobile robots."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/f90f2aca5c640289d0a29417bcb63a37-Abstract.html,Combining Visual and Acoustic Speech Signals with a Neural Network Improves Intelligibility,"Terrence J. Sejnowski, Ben P. Yuhas, Moise H. Goldstein Jr., Robert E. Jenkins","R.E. Jenkins  The Applied  Physics  Laboratory  The Johns Hopkins  University  Laurel,  MD  20707 
Acoustic speech recognition degrades in the presence of noise.  Com(cid:173) pensatory  information  is  available  from  the  visual  speech  signals  around  the  speaker's  mouth.  Previous  attempts  at  using  these  visual speech signals to improve automatic speech  recognition sys(cid:173) tems have combined the acoustic and visual speech information at a  symbolic level  using heuristic rules.  In this paper, we  demonstrate  an  alternative  approach  to fusing  the  visual  and  acoustic  speech  information  by  training  feedforward  neural  networks  to  map  the  visual signal onto the corresponding short-term spectral amplitude  envelope  (STSAE)  of the  acoustic  signal.  This  information  can  be  directly  combined  with  the  degraded  acoustic  STSAE.  Signif(cid:173) icant  improvements  are  demonstrated  in  vowel  recognition  from  noise-degraded  acoustic signals.  These results are compared to the  performance of humans, as well  as other pattern matching and es(cid:173) timation algorithms."
1989,https://papers.nips.cc/paper_files/paper/1989,https://papers.nips.cc/paper_files/paper/1989/hash/fe131d7f5a6b38b23cc967316c13dae2-Abstract.html,The Computation of Sound Source Elevation in the Barn Owl,"Clay D. Spence, John C. Pearson","The midbrain of the barn owl contains a map-like representation of  sound source direction which is used to precisely orient the head to(cid:173) ward targets of interest. Elevation is computed from the interaural  difference in sound level. We present models and computer simula(cid:173) tions of two stages of level difference processing which qualitatively  agree with known anatomy and physiology, and make several strik(cid:173) ing predictions."
