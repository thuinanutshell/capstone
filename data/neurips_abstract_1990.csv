year,proceeding_link,paper_link,title,authors,abstract
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/00411460f7c92d2124a67ea0f4cb5f85-Abstract.html,Modeling Time Varying Systems Using Hidden Control Neural Architecture,Esther Levin,"Multi-layered  neural  networks  have  recently  been  proposed  for  non(cid:173) linear  prediction  and  system  modeling.  Although  proven  successful  for  modeling  time  invariant nonlinear systems,  the  inability  of neural  networks  to  characterize  temporal  variability  has  so  far  been  an  obstacle  in  applying  them  to  complicated  non stationary  signals,  such  as  speech.  In  this  paper  we  present  a  network  architecture,  called  ""Hidden  Control  Neural  Network""  (HCNN),  for  modeling  signals  generated  by  nonlinear  dynamical  systems  with  restricted  time  variability.  The approach  taken  here  is  to  allow  the  mapping  that  is  implemented  by  a  multi  layered  neural  network  to  change  with  time  as  a  function  of an  additional  control  input  signal.  This  network  is  trained  using  an  algorithm  that  is  based  on  ""back-propagation""  and  segmentation  algorithms  for  estimating  the  unknown  control  together  with  the  network's  parameters.  The HCNN  approach  was  applied  to  several  tasks  including  modeling  of  time-varying  nonlinear  systems  and  speaker-independent  recognition  of  connected  digits,  yielding  a  word accuracy of 99.1 %."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/00ec53c4682d36f5c4359f4ae7bd7ba1-Abstract.html,Using Genetic Algorithms to Improve Pattern Classification Performance,"Eric I. Chang, Richard P Lippmann","Genetic  algorithms  were  used  to select  and  create  features  and  to select  reference  exemplar  patterns for  machine vision  and speech  pattern  classi(cid:173) fication  tasks.  For a  complex speech  recognition  task,  genetic  algorithms  required no more computation time than traditional approaches to feature  selection  but reduced  the number of input features  required  by a factor  of  five  (from 153 to 33 features).  On a difficult artificial machine-vision task,  genetic  algorithms were  able  to create  new features  (polynomial functions  of the original features)  which reduced  classification error rates from  19%  to  almost  0%.  Neural  net  and  k  nearest  neighbor  (KNN)  classifiers  were  unable to provide such low error rates using only the original features.  Ge(cid:173) netic algorithms were also used to reduce the number of reference exemplar  patterns for  a  KNN classifier.  On a  338 training pattern vowel-recognition  problem with  10  classes,  genetic  algorithms reduced  the number of stored  exemplars from 338 to 43 without significantly increasing classification er(cid:173) ror  rate.  In  all  applications,  genetic  algorithms  were  easy  to  apply  and  found  good  solutions  in  many fewer  trials  than would  be  required  by ex(cid:173) haustive search.  Run times were long, but not unreasonable.  These results  suggest  that genetic  algorithms  are  becoming practical for  pattern  classi(cid:173) fication  problems  as faster  serial  and  parallel computers  are  developed."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html,Generalization Dynamics in LMS Trained Linear Networks,Yves Chauvin,"For a simple linear case, a mathematical analysis of the training and gener(cid:173) alization (validation)  performance of networks trained by gradient descent  on a Least Mean Square cost function is provided as a function of the learn(cid:173) ing parameters and of the statistics of the training data base.  The analysis  predicts  that generalization error dynamics  are very dependent  on  a  pri(cid:173) ori initial weights.  In particular, the generalization error might sometimes  weave within a computable range during extended training.  In some cases,  the analysis provides bounds on the optimal number of training cycles  for  minimal  validation error.  For a  speech  labeling  task,  predicted  weaving  effects  were qualitatively tested  and  observed  by computer simulations in  networks trained by the linear and non-linear back-propagation algorithm."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/019d385eb67632a7e958e23f24bd07d7-Abstract.html,Optimal Filtering in the Salamander Retina,"Fred Rieke, W. Geoffrey Owen, William Bialek","The dark-adapted  visual  system can  count photons  wit h  a  reliability  lim(cid:173) ited by thermal noise  in  the rod photoreceptors - the processing circuitry  bet.ween  t.he  rod  cells  and the brain is  essentially noiseless  and  in fact  may  be  close  to  optimal.  Here  we  design  an  optimal  signal  processor  which  estimates  the  time-varying  light  intensit.y  at  the  retina  based  on  the  rod  signals.  \Ve show  that.  the first stage of optimal signal processing  involves  passing  the  rod  cell  out.put.  t.hrough  a  linear filter  with  characteristics de(cid:173) termined  entirely  by  the  rod  signal  and  noise  spectra.  This  filter  is  very  general;  in  fact  it.  is  the  first  st.age  in  any  visual  signal  processing  task  at.  10\'  photon  flux.  \Ve  iopntify  the  output  of this  first-st.age  filter  wit.h  the  intracellular  voltage  response of the  bipolar  celL  the  first  anatomical  st.age  in  retinal  signal  processing.  From  recent.  data on  tiger  salamander  phot.oreceptors  we  extract  t.he  relevant.  spect.ra  and  make  parameter-free,  quantit.ative predictions of the bipolar celll'esponse to a dim, diffuse flash.  Agreement  wit.h  experiment  is  essentially  perfect.  As  far  as  we  know  this  is  the  first  successful  predicti ve  t.heory  for  neural  dynamics. 
1"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/04025959b191f8f9de3f924f0940515f-Abstract.html,Oscillation Onset in Neural Delayed Feedback,André Longtin,"This paper studies dynamical aspects of neural systems with delayed  neg(cid:173) ative  feedback  modelled  by  nonlinear  delay-differential  equations.  These  systems  undergo  a  Hopf bifurcation  from  a  stable  fixed  point  to  a  sta(cid:173) ble  limit  cycle  oscillation  as  certain  parameters  are  varied.  It is  shown  that  their  frequency  of oscillation  is  robust  to  parameter  variations  and  noisy  fluctuations,  a  property that  makes  these  systems  good  candidates  for  pacemakers.  The  onset  of oscillation  is  postponed  by  both  additive  and parametric noise in the sense that the state variable spends more time  near the fixed  point than it would  in  the absence of noise.  This is  also the  case  when  noise  affects  the  delayed  variable,  i.e.  when  the  system  has  a  faulty  memory.  Finally,  it  is  shown  that  a  distribution  of delays  (rather  than a  fixed  delay)  also  stabilizes  the fixed  point solution."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/05049e90fa4f5039a8cadc6acbb4b2cc-Abstract.html,Relaxation Networks for Large Supervised Learning Problems,"Joshua Alspector, Robert B. Allen, Anthony Jayakumar, Torsten Zeppenfeld, Ronny Meir",Feedback  connections  are  required  so  that  the  teacher  signal  on  the  output  neurons  can  modify  weights  during  supervised  learning.  Relaxation  methods  are  needed  for  learning  static  patterns  with  full-time  feedback  connections.  Feedback  network  learning  techniques  have  not  achieved  wide  popularity  because  of the  still greater  computational efficiency  of back-propagation.  We  show by simulation that relaxation networks of the kind we  are implementing in  VLSI  are  capable  of  learning  large  problems  just  like  back-propagation  networks.  A microchip incorporates deterministic mean-field theory learning as  well  as  stochastic  Boltzmann  learning.  A  multiple-chip  electronic  system  implementing  these  networks  will  make  high-speed  parallel  learning  in  them  feasible in the future.
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/0584ce565c824b7b7f50282d9a19945b-Abstract.html,Neural Networks Structured for Control Application to Aircraft Landing,"Charles Schley, Yves Chauvin, Van Henkle, Richard Golden","We present a generic neural network architecture capable of con(cid:173) trolling non-linear plants. The network is composed of dynamic.  parallel, linear maps gated by non-linear switches. Using a recur(cid:173) rent form of the back-propagation algorithm, control is achieved  by optimizing the control gains and task-adapted switch parame(cid:173) ters. A mean quadratic cost function computed across a nominal  plant trajectory is minimized along with performance constraint  penalties. The approach is demonstrated for a control task con(cid:173) sisting of landing a commercial aircraft in difficult wind conditions.  We show that the network yields excellent performance while re(cid:173) maining within acceptable damping response constraints."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/06eb61b839a0cefee4967c67ccb099dc-Abstract.html,Closed-Form Inversion of Backpropagation Networks: Theory and Optimization Issues,Michael L. Rossen,"We describe a closed-form technique for mapping the output of a trained  backpropagation network int.o input activity space. The mapping is an in(cid:173) verse mapping in the sense that, when the image of the mapping in input  activity space is propagat.ed forward through the normal network dynam(cid:173) ics, it reproduces the output used to generate that image. When more  than one such inverse mappings exist, our inverse ma.pping is special in  that it has no projection onto the nullspace of the activation flow opera(cid:173) tor for the entire network. An important by-product of our calculation,  when more than one invel'se mappings exist, is an orthogonal basis set of  a significant portion of the activation flow operator nullspace. This basis  set can be used to obtain an alternate inverse mapping that is optimized  for a particular rea.l-world application."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/0bb4aec1710521c12ee76289d9440817-Abstract.html,Dynamics of Generalization in Linear Perceptrons,"Anders Krogh, John A. Hertz","We study the evolution of the generalization ability of a  simple linear per(cid:173) ceptron with N  inputs which learns to imitate a  ""teacher perceptron"".  The  system is  trained  on p  =  aN  binary  example  inputs  and  the  generaliza(cid:173) tion  ability measured by testing for  agreement with  the teacher on all 2N  possible  binary input  patterns.  The dynamics may  be solved  analytically  and  exhibits  a  phase  transition  from  imperfect  to  perfect  generalization  at  a  =  1.  Except  at  this  point  the  generalization  ability  approaches  its  asymptotic  value  exponentially,  with critical slowing  down near  the  tran(cid:173) sition;  the  relaxation  time  is  ex  (1  - y'a)-2.  Right  at  the  critical  point,  1  the  approach  to  perfect  generalization  follows  a  power  law  ex  t - '2.  In  the presence of noise,  the generalization ability is  degraded  by  an amount  ex  (va - 1)-1 just above a  =  1."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html,Translating Locative Prepositions,"Paul W. Munro, Mary Tabasko","A network was trained by back propagation to map locative expressions  of the form ""noun-preposition-noun"" to a semantic representation, as in  Cosic  and  Munro  (1988).  The  network's  performance  was  analyzed  over  several  simulations  with  training  sets  in  both  English  and  German.  Translation  of prepositions  was  attempted  by  presenting  a  locative expression to a  network trained in one language to generate a  semantic representation; the semantic representation was  then presented  to the network trained in the other language to generate the appropriate  preposition."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/0d0fd7c6e093f7b804fa0150b875b868-Abstract.html,Adaptive Spline Networks,Jerome H. Friedman,"A network based on splines is  described.  It automatically adapts the num(cid:173) ber of units, unit parameters, and the architecture of the network for  each  application."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/0deb1c54814305ca9ad266f53bc82511-Abstract.html,An Analog VLSI Chip for Finding Edges from Zero-crossings,"Wyeth Bair, Christof Koch","We  have  designed  and  tested  a  one-dimensional  64  pixel,  analog  CMOS  VLSI chip which localizes intensity edges in real-time.  This device exploits  on-chip photoreceptors and the natural filtering properties of resistive net(cid:173) works to implement a  scheme similar  to  and  motivated  by  the Difference  of Gaussians (DOG) operator proposed by Marr and Hildreth (1980).  Our  chip computes the  zero-crossings associated  with  the difference of two ex(cid:173) ponential weighting functions.  If the  derivative  across  this  zero-crossing  is  above  a  threshold,  an edge  is  reported.  Simulations  indicate  that  this  technique will  extend well to two dimensions."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/1068c6e4c8051cfd4e9ea8072e3189e2-Abstract.html,"Comparison of three classification techniques: CART, C4.5 and Multi-Layer Perceptrons","A. C. Tsoi, R. A. Pearson","In this paper, after some introductory remarks into the classification prob(cid:173) lem as considered in various research communities, and some discussions  concerning some of the reasons for ascertaining the performances of the  three chosen algorithms, viz., CART (Classification and Regression Tree),  C4.5 (one of the more recent versions of a popular induction tree tech(cid:173) nique known as ID3), and a multi-layer perceptron (MLP), it is proposed  to compare the performances of these algorithms under two criteria: classi(cid:173) fication and generalisation. It is found that, in general, the MLP has better  classification and generalisation accuracies compared with the other two  algorithms."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/11b9842e0a271ff252c1903e7132cd68-Abstract.html,ART2/BP architecture for adaptive estimation of dynamic processes,Einar Sørheim,"The goal has been to construct a supervised artificial neural network that  learns incrementally an unknown mapping. As a result a network con(cid:173) sisting of a combination of ART2 and backpropagation is proposed and  is called an ""ART2/BP"" network. The ART2 network is used to build  and focus a supervised backpropagation network. The ART2/BP network  has the advantage of being able to dynamically expand itself in response  to input patterns containing new information. Simulation results show  that the ART2/BP network outperforms a classical maximum likelihood  method for the estimation of a discrete dynamic and nonlinear transfer  function."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/138bb0696595b338afbab333c555292a-Abstract.html,Self-organization of Hebbian Synapses in Hippocampal Neurons,"Thomas H. Brown, Zachary F. Mainen, Anthony M. Zador, Brenda J. Claiborne","We are exploring the significance of biological complexity for neuronal  computation.  Here we demonstrate that Hebbian synapses in realistical(cid:173) ly-modeled  hippocampal  pyramidal  cells  may  give  rise  to  two  novel  forms of self -organization in response to structured synaptic input.  First,  on the basis of the electrotonic relationships between synaptic contacts,  a cell may become tuned to a small subset of its input space.  Second, the  same mechanisms may produce  clusters of potentiated synapses across  the space of the dendrites.  The latter type of self-organization may be  functionally  significant in  the presence of nonlinear dendritic  conduc(cid:173) tances."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/13f9896df61279c928f19721878fac41-Abstract.html,Connection Topology and Dynamics in Lateral Inhibition Networks,"C.M Marcus, F. R. Waugh, R. M. Westervelt","We show analytically how the stability of two-dimensional lateral  inhibition neural networks depends on the local connection topology.  For various network topologies, we calculate the critical time delay for  the onset of oscillation in continuous-time networks and present  analytic phase diagrams characterizing the dynamics of discrete-time  networks."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/142949df56ea8ae0be8b5306971900a4-Abstract.html,A Delay-Line Based Motion Detection Chip,"Tim Horiuchi, John Lazzaro, Andrew Moore, Christof Koch","Inspired by a visual motion detection model for the ra.bbit retina  and by a computational architecture used for early audition in the  barn owl, we have designed a chip that employs a correlation model  to report the one-dimensional field motion of a scene in real time.  Using subthreshold analog VLSI techniques, we have fabricated and  successfully tested a 8000 transistor chip using a standard MOSIS  process."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/1543843a4723ed2ab08e18053ae6dc5b-Abstract.html,Back Propagation is Sensitive to Initial Conditions,"John F. Kolen, Jordan B. Pollack",functions  with
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/158f3069a435b314a80bdcb024f8e422-Abstract.html,Applications of Neural Networks in Video Signal Processing,"John C. Pearson, Clay D. Spence, Ronald Sverdlove","Although color TV is an established technology, there are a number of  longstanding problems for which neural networks may be suited. Impulse  noise is such a problem, and a modular neural network approach is pre(cid:173) sented in this paper. The training and analysis was done on conventional  computers, while real-time simulations were performed on a massively par(cid:173) allel computer called the Princeton Engine. The network approach was  compared to a conventional alternative, a median filter. Real-time simula(cid:173) tions and quantitative analysis demonstrated the technical superiority of  the neural system. Ongoing work is investigating the complexity and cost  of implementing this system in hardware. 
1 THE POTENTIAL FOR NEURAL NETWORKS IN 
CONSUMER ELECTRONICS 
Neural networks are most often considered for application in emerging new tech(cid:173) nologies, such as speech recognition, machine vision, and robotics. The fundamental  ideas behind these technologies are still being developed, and it will be some time  before products containing neural networks are manufactured. As a result, research  in these areas will not drive the development of inexpensive neural network hard(cid:173) ware which could serve as a catalyst for the field of neural networks in general. 
In contrast, neural networks are rarely considered for application in mature tech(cid:173) nologies, such as consumer electronics. These technologies are based on established  principles of information processing and communication, and they are used in mil(cid:173) lions of products per year. The embedding of neural networks within such mass-"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/1651cf0d2f737d7adeab84d339dbabd3-Abstract.html,Grouping Contours by Iterated Pairing Network,"Amnon Shashua, Shimon Ullman","Shimon Ullman 
We describe in this paper a network that performs grouping of image con(cid:173) tours. The input to the net are fragments of image contours, and the  output is the partitioning of the fragments into groups, together with a  saliency measure for each group. The grouping is based on a measure of  overall length and curvature. The network decomposes the overall opti(cid:173) mization problem into independent optimal pairing problems performed  at each node. The resulting computation maps into a uniform locally  connected network of simple computing elements. 
1 The Problenl: Contour Grouping 
A problem that often arises in visual information processing is the linking of con(cid:173) tour fragments into optimal groups. For example, certain subsets of contours spon(cid:173) taneously form perceptual groups, as illustrated in Fig. 1, and are often detected  immediately without scanning the image in a systematic manner. Grouping process  of this type are likely to play an important role in object recognition by segmenting  the image and selecting image structures that are likely to correspond to objects of  interest in the scene. 
'Ve propose that some form of autonomous grouping is performed at an early stage  based on geometrical characteristics, that are independent of the identity of objects  to be selected. The grouping process is governed by the notion of saliency in a way  that priority is given to forming salient groups at the expense of potentially less  salient ones. This general notion can again be illustrated by Fig. 1; it appears that  certain groups spontaneously emerge, while grouping decisions concerning the less  salient parts of the image may remain unresolved. As we shall see, the computation  below exhibits a similar behavior. 
We define a grouping of the image contours as the formation of a set of disjoint"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/17d63b1625c816c22647a73e1482372b-Abstract.html,Simulation of the Neocognitron on a CCD Parallel Processing Architecture,"Michael L. Chuang, Alice M. Chiang","The neocognitron  is  a  neural network for  pattern recognition  and feature  extraction.  An  analog  CCD  parallel  processing  architecture  developed  at Lincoln Laboratory is  particularly  well suited to the computational re(cid:173) quirements of shared-weight networks such as the neocognitron, and imple(cid:173) mentation of the neocognitron using the CCD architecture was simulated.  A  modification  to  the  neocognitron  training  procedure,  which  improves  network performance under the limited arithmetic  precision that would be  imposed  by the CCD  architecture,  is  presented."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/18997733ec258a9fcaf239cc55d53363-Abstract.html,Order Reduction for Dynamical Systems Describing the Behavior of Complex Neurons,"Thomas B. Kepler, L. F. Abbott, Eve Marder",We have devised a scheme to reduce the complexity of dynamical  systems belonging to a class that includes most biophysically realistic  neural models. The reduction is based on transformations of variables  and perturbation expansions and it preserves a high level of fidelity to  the original system. The techniques are illustrated by reductions of the  Hodgkin-Huxley system and an augmented Hodgkin-Huxley system.
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/18d8042386b79e2c279fd162df0205c8-Abstract.html,Note on Learning Rate Schedules for Stochastic Optimization,"Christian Darken, John E. Moody","We  present  and  compare  learning  rate  schedules  for  stochastic  gradient  descent,  a  general  algorithm  which  includes  LMS,  on-line  backpropaga(cid:173) tion  and  k-means  clustering  as  special  cases.  We  introduce  ""search-then(cid:173) converge""  type  schedules  which  outperform  the  classical  constant  and  ""running average""  (1ft) schedules both in speed of convergence and quality  of solution. 
Introduction:  Stochastic  Gradient  Descent 
1  tion G(W).  In  the context  of learning systems  typically G(W) = £x E(W, X),  i.e. 
The  optimization  task  is  to  find  a  parameter  vector  W  which  minimizes  a  func(cid:173)
G  is  the  average  of  an  objective  function  over  the  exemplars,  labeled  E  and  X  respectively.  The stochastic gradient  descent  algorithm is  Ll Wet)  = -1](t)V'w E(W(t), X(t)). 
where  t  is  the  ""time"",  and  X(t)  is  the  most  recent  independently-chosen  random  exemplar.  For comparison, the  deterministic gradient  descent  algorithm is 
Ll Wet)  =  -1](t)V'w£x E(W(t), X)."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/2421fcb1263b9530df88f7f002e78ea5-Abstract.html,Remarks on Interpolation and Recognition Using Neural Nets,Eduardo D. Sontag,"We consider different  types  of single-hidden-Iayer feedforward  nets:  with  or  without  direct  input  to  output  connections,  and  using  either  thresh(cid:173) old  or  sigmoidal activation functions.  The  main results  show  that  direct  connections in  threshold nets  double  the  recognition  but not  the interpo(cid:173) lation power, while using sigmoids  rather than thresholds allows (at least)  doubling  both.  Various results are also given on VC dimension and  other  measures of recognition capabilities."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/248e844336797ec98478f85e7626de4a-Abstract.html,Rapidly Adapting Artificial Neural Networks for Autonomous Navigation,Dean Pomerleau,"The ALVINN (Autonomous Land Vehicle In a Neural Network) project addresses  the problem  of training artificial  neural  networks in  real  time to  perform difficult  perception tasks.  ALVINN ,is  a back-propagation network that uses inputs from  a  video camera and an imaging laser rangefinder to drive the CMU Navlab, a modified  Chevy van.  This paper describes training techniques which allow ALVINN to learn  in under 5 minutes to autonomously control the Navlab by watching a human driver's  response  to  new  situations.  Using  these  techniques,  ALVINN  has  been  trained  to  drive  in  a  variety  of circumstances  including  single-lane  paved  and  unpaved  roads,  multilane  lined  and  unlined  roads,  and  obstacle-ridden  on- and  off-road  environments, at speeds of up to 20 miles per hour."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/24b16fede9a67c9251d3e7c7161c83ac-Abstract.html,A Recurrent Neural Network for Word Identification from Continuous Phoneme Strings,"Robert B. Allen, Candace A. Kamm","A  neural  network  architecture  was  designed  for  locating  word  boundaries  and  identifying  words  from  phoneme  sequences.  This  architecture  was  tested  in  three  sets  of  studies.  First,  a  highly  redundant  corpus  with  a  restricted  vocabulary was  generated and the network was trained with a limited number of  phonemic variations for the words  in the corpus.  Tests of network performance  on a transfer set yielded a very low error rate.  In a second study, a network was  trained  to  identify  words  from  expert  transcriptions  of speech.  On a  transfer  test,  error  rate  for  correct  simultaneous  identification  of  words  and  word  boundaries was  18%.  The third study used the output of a phoneme classifier as  the input to the word and  word boundary identification network.  The error rate  on a transfer test set was 49% for this task.  Overall, these studies provide a first  step at identifying words in connected discourse with a neural network."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/25b2822c2f5a3230abfadd476e8b04c9-Abstract.html,Adjoint-Functions and Temporal Learning Algorithms in Neural Networks,"N. Toomarian, J. Barhen","The development of learning algorithms is generally based upon the min(cid:173) imization of an energy function. It is a fundamental requirement to com(cid:173) pute the gradient of this energy function with respect to the various pa(cid:173) rameters of the neural architecture, e.g., synaptic weights, neural gain,etc.  In principle, this requires solving a system of nonlinear equations for each  parameter of the model, which is computationally very expensive. A new  methodology for neural learning of time-dependent nonlinear mappings is  presented. It exploits the concept of adjoint operators to enable a fast  global computation of the network's response to perturbations in all the  systems parameters. The importance of the time boundary conditions of  the adjoint functions is discussed. An algorithm is presented in which  the adjoint sensitivity equations are solved simultaneously (Le., forward  in time) along with the nonlinear dynamics of the neural networks. This  methodology makes real-time applications and hardware implementation  of temporal learning feasible."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/26e359e83860db1d11b6acca57d8ea88-Abstract.html,Language Induction by Phase Transition in Dynamical Recognizers,Jordan B. Pollack,"A higher order recurrent neural network architecture learns to recognize and  generate languages after being  ""trained""  on categorized exemplars.  Studying  these  networks  from  the  perspective  of  dynamical  systems  yields  two  interesting  discoveries:  First,  a  longitudinal  examination  of  the  learning  process  illustrates  a  new  form  of mechanical  inference:  Induction  by  phase  transition.  A  small  weight  adjustment  causes  a  ""bifurcation""  in  the  limit  behavior of the network. This phase transition corresponds to the onset of the  network's  capacity  for  generalizing  to  arbitrary-length  strings.  Second,  a  study of the  automata resulting  from  the  acquisition  of previously published  languages  indicates  that  while  the architecture  is  NOT  guaranteed  to  find  a  minimal  finite  automata  consistent  with  the  given  exemplars,  which  is  an  NP-Hard  problem,  the  architecture  does  appear capable  of generating  non(cid:173) regular languages by exploiting fractal and chaotic dynamics. I end the paper  with  a  hypothesis  relating  linguistic  generative  capacity  to  the  behavioral  regimes of non-linear dynamical systems."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/28f0b864598a1291557bed248a998d4e-Abstract.html,Chaitin-Kolmogorov Complexity and Generalization in Neural Networks,"Barak A. Pearlmutter, Ronald Rosenfeld","We  present  a  unified  framework  for  a  number  of different  ways  of failing  to  generalize  properly.  During  learning,  sources  of random  information  contaminate  the  network,  effectively  augmenting  the  training  data  with  random information. The complexity of the function computed is therefore  increased,  and generalization is degraded.  We analyze replicated networks,  in  which  a  number of identical networks are independently trained on  the  same data and their results averaged.  We conclude that replication almost  always results in a decrease in the expected complexity of the network, and  that  replication  therefore  increases  expected  generalization.  Simulations  confirming the effect  are  also presented. 
1  BROKEN SYMMETRY CONSIDERED HARMFUL 
Consider  a  one-unit  backpropagation  network  trained  on  exclusive  or.  Without  hidden  units,  the  problem  is  insoluble.  One  point  where  learning  would  stop  is  when  all weights are  zero and the output is  always  ~, resulting in  an mean squared  error  of  ~.  But  this  is  a  saddle  point;  by  placing  the  discrimination  boundary  properly, one point can be gotten correctly, two with errors of ~, and one with error  of i, giving an MSE of i, as shown in figure  1.  Networks are initialized with small random weights, or noise is injected during train(cid:173) ing to break symmetries of this sort.  But in breaking this symmetry, something has  been  lost.  Consider  a  kNN  classifier,  constructed  from  a  kNN  program  and  the  training data.  Anyone  who  has a  copy  of the  kNN  program can construct  an  iden(cid:173) tical classifier  if they  receive  the training data.  Thus,  considering the  classification"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/2f2b265625d76a6704b08093c652fd79-Abstract.html,VLSI Implementations of Learning and Memory Systems: A Review,Mark A. Holler,"A large number of VLSI implementations of neural network models  have been  reported. The diversity of these implementations is  noteworthy. This paper attempts to put a group of representative  VLSI implementations in perspective by comparing and contrast(cid:173) ing them. Design trade-offs are discussed and some suggestions forthe  direction of future implementation efforts are made. 
IMPLEMENTATION  Changing the way information is represented can be beneficial. For example a change  of representation can make information more compact for storage and transmission.  Implementation of neural computational models is just the process of changing the  representation of a neural model from mathmatical symbolism to a physical embodi(cid:173) ement for the purpose of shortening the time it takes to process information according  to the neural model. 
FLEXIBIliTY VS. PERFORMANCE 
Today most neural models are already implemented in silicon VLSI, in the form of pro(cid:173) grams running on general purpose digital von Neumann computers. These machines  are available at low cost and are highly flexible. Their flexibility results from the ease  with which their programs can be changed. Maximizing flexibility, however, usually  results in reduced performance. A program will often have to specify several simple op-"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/310dcbbf4cce62f762a2aaa148d556bd-Abstract.html,Neural Dynamics of Motion Segmentation and Grouping,Ennio Mingolla,"A  neural  network  model  of motion segmentation  by  visual  cortex  is  de(cid:173) scribed.  The  model  clarifies  how  preprocessing  of  motion  signals  by  a  Motion Oriented Contrast Filter (MOC  Filter)  is joined to long-range co(cid:173) operative motion mechanisms in a  motion Cooperative Competitive Loop  (CC Loop)  to control phenomena such as as induced  motion, motion cap(cid:173) ture,  and motion aftereffects.  The total model system is  a  motion Bound(cid:173) ary Contour System (BCS)  that is computed in parallel with a static BCS  before  both systems cooperate  to generate  a  boundary  representation for  three dimensional visual form perception.  The present investigations clari(cid:173) fy how the static BCS can be modified for use in motion segmentation prob(cid:173) lems, notably for analyzing how ambiguous local movements (the aperture  problem) on a  complex moving shape  are suppressed  and actively reorga(cid:173) nized  into a  coherent global motion signal. 
1 
INTRODUCTION: WHY ARE STATIC AND MOTION  BOUNDARY CONTOUR SYSTEMS  NEEDED? 
Some regions,  notably  MT,  of visual  cortex  are  specialized  for  motion processing.  However, even the earliest stages of visual cortex processing,  such as simple cells in  VI, require stimuli that change through  time for  their  maximal activation and  are  direction-sensitive.  Why has  evolution generated  regions  such  as  MT,  when  even  VI  is  change-sensitive and  direction-sensitive?  What computational properties  are  achieved by MT that are not already available in  VI?"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/320722549d1751cf3f247855f937b982-Abstract.html,Exploratory Feature Extraction in Speech Signals,Nathan Intrator,"A  novel  unsupervised  neural  network  for  dimensionality  reduction  which  seeks  directions  emphasizing  multimodality is  presented,  and  its  connec(cid:173) tion  to exploratory projection pursuit  methods is  discussed.  This leads to  a  new  statistical insight  to the  synaptic  modification  equations  governing  learning in  Bienenstock,  Cooper,  and  Munro  (BCM)  neurons  (1982).  The  importance  of a  dimensionality  reduction  principle  based  solely  on  distinguishing  features,  is  demonstrated  using  a  linguistically  motivated  phoneme  recognition  experiment,  and  compared  with  feature  extraction  using  back-propagation network."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/34ed066df378efacc9b924ec161e7639-Abstract.html,Constructing Hidden Units using Examples and Queries,"Eric B. Baum, Kevin J. Lang","While the network loading problem for 2-layer threshold nets is  NP-hard when learning from examples alone (as with backpropaga(cid:173) tion), (Baum, 91) has now proved that a learner can employ queries  to evade the hidden unit credit assignment problem and PAC-load  nets with up to four hidden units in polynomial time. Empirical  tests show that the method can also learn far more complicated  functions such as randomly generated networks with 200 hidden  units. The algorithm easily approximates Wieland's 2-spirals func(cid:173) tion using a single layer of 50 hidden units, and requires only 30  minutes of CPU time to learn 200-bit parity to 99.7% accuracy."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/352fe25daf686bdb4edca223c921acea-Abstract.html,Analog Neural Networks as Decoders,"Ruth Erlanson, Yaser Abu-Mostafa","Analog neural networks with feedback can be used to implement l((cid:173) Winner-Take-All (KWTA) networks. In turn, KWTA networks can be  used as decoders of a class of nonlinear error-correcting codes. By in(cid:173) terconnecting such KWTA networks, we can construct decoders capable  of decoding more powerful codes. We consider several families of inter(cid:173) connected KWTA networks, analyze their performance in terms of coding  theory metrics, and consider the feasibility of embedding such networks in  VLSI technologies. 
1 
INTRODUCTION: THE K-WINNER-TAKE-ALL  NETWORK 
We have previously demonstrated the use of a continuous Hopfield neural network  as a K-Winner-Take-All (KWTA) network [Majani et al., 1989, Erlanson and Abu(cid:173) Mostafa, 1988}. Given an input of N real numbers, such a network will converge  to a vector of K positive one components and (N - K) negative one components,  with the positive positions indicating the K largest input components. In addition,  we have shown that the (~) such vectors are the only stable states of the system.  One application of the KWTA network is the analog decoding of error-correcting  codes [Majani et al., 1989, Platt and Hopfield, 1986]. Here, a known set of vectors  (the codewords) are transmitted over a noisy channel. At the receiver's end of the  channel, the initial vector must be reconstructed from the noisy vector. 
• currently at: Hughes Network Systems, 10790 Roselle St., San Diego, CA 92121"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/357a6fdf7642bf815a88822c447d9dc4-Abstract.html,A Reinforcement Learning Variant for Control Scheduling,Aloke Guha,"We  present  an  algorithm  based  on  reinforcement  and  state  recurrence  learning  techniques  to  solve  control  scheduling  problems.  In  particular,  we  have  devised  a  simple  learning  scheme  called  ""handicapped  learning"",  in  which  the  weights  of the  associative  search  element  are  reinforced,  either  positively  or negatively,  such  that the  system  is forced  to  move  towards the  desired  setpoint  in  the  shortest possible  trajectory.  To  improve  the  learning  rate,  a  variable  reinforcement  scheme  is  employed:  negative  reinforcement  values are  varied depending  on  whether the failure  occurs in  handicapped or  normal  mode  of operation.  Furthermore,  to  realize  a  simulated  annealing  scheme  for  accelerated  learning,  if the  system  visits  the  same  failed  state  successively,  the  negative  reinforcement  value  is  increased.  In  examples  studied,  these  learning  schemes  have  demonstrated  high  learning  rates,  and  therefore may prove useful  for  in-situ learning."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/371bce7dc83817b7893bcdeed13799b5-Abstract.html,Kohonen Networks and Clustering: Comparative Performance in Color Clustering,"Wesley Snyder, Daniel Nissman, David Van den Bout, Griff Bilbro","The problem of color clustering is defined and shown to be a problem of  assigning a large number (hundreds of thousands) of 3-vectors to a  small number (256) of clusters. Finding those clusters in such a way that  they best represent a full color image using only 256 distinct colors is a  burdensome computational problem. In this paper, the problem is solved  using ""classical"" techniques -- k-means clustering, vector quantization  (which turns out to be the same thing in this application), competitive  learning, and Kohonen self-organizing feature maps. Quality of the  result is judged subjectively by how much the pseudo-color result  resembles the true color image, by RMS quantization error, and by run  time. The Kohonen map provides the best solution."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/37bc2f75bf1bcfe8450a1a41c200364c-Abstract.html,Reconfigurable Neural Net Chip with 32K Connections,"H. P. Graf, R. Janow, D. Henderson, R. Lee","We describe a CMOS neural net chip with a reconfigurable network archi(cid:173) tecture. It contains 32,768 binary, programmable connections arranged in  256 'building block' neurons. Several 'building blocks' can be connected to  form long neurons with up to 1024 binary connections or to form neurons  with analog connections. Single- or multi-layer networks can be imple(cid:173) mented with this chip. We have integrated this chip into a board system  together with a digital signal processor and fast memory. This system is  currently in use for image processing applications in which the chip extracts  features such as edges and corners from binary and gray-level images."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/39461a19e9eddfb385ea76b26521ea48-Abstract.html,e-Entropy and the Complexity of Feedforward Neural Networks,Robert C. Williamson,"We develop a. new feedforward neuralnet.work represent.ation of Lipschitz  functions from [0, p]n into [0,1] ba'3ed on the level sets of the function. We  show that 
~~ + ~€r + ( 1 + h) (:~) n 
is an upper bound on the number of nodes needed to represent f to within  uniform error Cr, where L is the Lipschitz constant. \Ve also show that the  number of bits needed to represent the weights in the network in order to  achieve this approximation is given by 
o (~2;~r (:~) n) . 
\Ve compare this bound with the [-entropy of the functional class under  consideration."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/3ad7c2ebb96fcba7cda0cf54a2e802f5-Abstract.html,Extensions of a Theory of Networks for Approximation and Learning: Outliers and Negative Examples,"Federico Girosi, Tomaso Poggio, Bruno Caprile","Learning an input-output mapping from a set of examples can be regarded  as synthesizing an approximation of a multi-dimensional function. From  this point of view, this form of learning is closely related to regularization  theory, and we have previously shown (Poggio and Girosi, 1990a, 1990b)  the equivalence between reglilari~at.ioll and a. class of three-layer networks  that we call regularization networks. In this note, we ext.end the theory  by introducing ways of"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/3c7781a36bcd6cf08c11a970fbe0e2a6-Abstract.html,Stochastic Neurodynamics,J.D. Cowan,"The main point of this paper is that stochastic neural networks have a  mathematical structure that corresponds quite closely with that of  quantum field theory. Neural network Liouvillians and Lagrangians  can be derived, just as can spin Hamiltonians and Lagrangians in QFf.  It remains to show the efficacy of such a description."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/3dd48ab31d016ffcbf3314df2b3cb9ce-Abstract.html,Phonetic Classification and Recognition Using the Multi-Layer Perceptron,"Hong C. Leung, James R. Glass, Michael S. Phillips, Victor W. Zue","In this paper, we will describe several extensions to our earlier work, utiliz(cid:173) ing a segment-based approach. We will formulate our segmental framework  and report our study on the use of multi-layer perceptrons for detection  and classification of phonemes. We will also examine the outputs of the  network, and compare the network performance with other classifiers. Our  investigation is performed within a set of experiments that attempts to  recognize 38 vowels and consonants in American English independent of  speaker. When evaluated on the TIMIT database, our system achieves an  accuracy of 56%."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/3fe94a002317b5f9259f82690aeea4cd-Abstract.html,Learning Trajectory and Force Control of an Artificial Muscle Arm by Parallel-hierarchical Neural Network Model,"Masazumi Katayama, Mitsuo Kawato","We propose a new parallel-hierarchical neural network model to enable motor  learning for simultaneous control of both trajectory and force. by integrating  Hogan's control method and our previous neural network control model using a  feedback-error-learning scheme. Furthermore. two hierarchical control laws  which apply to the model, are derived by using the Moore-Penrose pseudo(cid:173) inverse matrix. One is related to the minimum muscle-tension-change trajectory  and the other is related to the minimum motor-command-change trajectory. The  human arm is redundant at the dynamics level since joint torque is generated by  agonist and antagonist muscles. Therefore, acquisition of the inverse model is  an ill-posed problem. However. the combination of these control laws and  feedback-error-learning resolve the ill-posed problem. Finally. the efficiency of  the parallel-hierarchical neural network model is shown by learning experiments  using an artificial muscle arm and computer simulations."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html,Basis-Function Trees as a Generalization of Local Variable Selection Methods for Function Approximation,Terence D. Sanger,"Local variable selection has proven to be a powerful technique for ap(cid:173) proximating functions in high-dimensional spaces. It is used in several  statistical methods, including CART, ID3, C4, MARS, and others (see the  bibliography for references to these algorithms). In this paper I present  a tree-structured network which is a generalization of these techniques.  The network provides a framework for understanding the behavior of such  algorithms and for modifying them to suit particular applications."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/41ae36ecb9b3eee609d05b90c14222fb-Abstract.html,A Short-Term Memory Architecture for the Learning of Morphophonemic Rules,"Michael Gasser, Chan-Do Lee","Despite its successes,  Rumelhart and McClelland's (1986)  well-known ap(cid:173) proach to the learning of morphophonemic rules  suffers from two deficien(cid:173) cies:  (1)  It performs  the  artificial  task  of associating  forms  with  forms  rather  than  perception  or  production.  (2)  It is  not  constrained  in  ways  that humans learners  are.  This paper describes  a  model which  addresses  both objections.  Using  a  simple recurrent  architecture  which  takes  both  forms  and  ""meanings""  as  inputs,  the  model  learns  to  generate  verbs  in  one  or  another  ""tense"",  given  arbitrary  meanings,  and  to  recognize  the  tenses  of verbs.  Furthermore,  it fails  to learn  reversal  processes  unknown  in human language."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/41f1f19176d383480afa65d325c06ed0-Abstract.html,Associative Memory in a Network of `Biological' Neurons,Wulfram Gerstner,"The Hopfield network (Hopfield,  1982,1984) provides a simple model of an  associative memory in  a neuronal structure.  This model, however, is based  on highly artificial assumptions, especially the use of formal-two state neu(cid:173) rons  (Hopfield,  1982) or graded-response  neurons  (Hopfield,  1984).  \Vhat  happens if we  replace  the formal neurons  by 'real' biological neurons?  \Ve  address  this question  in  two steps.  First, we  show  that a simple model of  a  neuron  can  capture  all  relevant features  of neuron  spiking,  i. e., a  wide  range of spiking frequencies  and a realistic distribution of interspike inter(cid:173) vals.  Second, we construct an associative memory by linking these neurons  together.  The analytical solution for  a  large  and fully  connected  network  shows that the  Hopfield solution  is  valid only for  neurons  with  a short re(cid:173) fractory  period.  If the refractory  period  is  longer  than  a  crit.ical  duration  ie,  the  solutions  are  qualitatively different.  The  associative  character  of  the solutions, however,  is  preserved."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/42e7aaa88b48137a16a1acd04ed91125-Abstract.html,Oriented Non-Radial Basis Functions for Image Coding and Analysis,"Avijit Saha, Jim Christian, Dun-Sung Tang, Wu Chuan-Lin","We introduce oriented non-radial basis function networks (ONRBF)  as a generalization of Radial Basis Function networks (RBF)- wherein  the Euclidean distance metric in the exponent of the Gaussian is re(cid:173) placed by a more general polynomial. This permits the definition of  more general regions and in particular- hyper-ellipses with orienta(cid:173) tions. In the case of hyper-surface estimation this scheme requires a  smaller number of hidden units and alleviates the ""curse of dimen(cid:173) sionality"" associated kernel type approximators.In the case of an im(cid:173) age, the hidden units correspond to features in the image and the  parameters associated with each unit correspond to the rotation, scal(cid:173) ing and translation properties of that particular ""feature"". In the con(cid:173) text of the ONBF scheme, this means that an image can be  represented by a small number of features. Since, transformation of an  image by rotation, scaling and translation correspond to identical  transformations of the individual features, the ONBF scheme can be  used to considerable advantage for the purposes of image recognition  and analysis."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/432aca3a1e345e339f35a30c8f65edce-Abstract.html,Evaluation of Adaptive Mixtures of Competing Experts,"Steven J. Nowlan, Geoffrey E. Hinton","We  compare  the  performance  of the  modular  architecture,  composed  of  competing  expert  networks,  suggested  by  Jacobs,  Jordan,  Nowlan  and  Hinton  (1991)  to  the  performance  of a  single  back-propagation  network  on  a  complex,  but  low-dimensional,  vowel  recognition  task.  Simulations  reveal that this system is capable of uncovering interesting decompositions  in  a  complex  task.  The  type  of decomposition  is  strongly  influenced  by  the  nature  of the  input  to  the  gating  network  that  decides  which  expert  to  use  for  each  case.  The  modular  architecture  also exhibits consistently  better  generalization on many variations of the  task."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/49182f81e6a13cf5eaa496d51fea6406-Abstract.html,Spoken Letter Recognition,"Mark Fanty, Ronald Cole","Through the use of neural network classifiers and careful feature selection,  we have achieved high-accuracy speaker-independent spoken letter recog(cid:173) nition. For isolated letters, a broad-category segmentation is performed  Location of segment boundaries allows us to measure features at specific  locations in the signal such as vowel onset, where important information  resides. Letter classification is performed with a feed-forward neural net(cid:173) work. Recognition accuracy on a test set of 30 speakers was 96%. Neu(cid:173) ral network classifiers are also used for pitch tracking and broad-category  segmentation of letter strings. Our research has been extended to recog(cid:173) nition of names spelled with pauses between the letters. When searching  a database of 50,000 names, we achieved 95% first choice name retrieval.  Work has begun on a continuous letter classifier which does frame-by-frame  phonetic classification of spoken letters."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/4f4adcbf8c6f66dcfc8a3282ac2bf10a-Abstract.html,Design and Implementation of a High Speed CMAC Neural Network Using Programmable CMOS Logic Cell Arrays,"W. Thomas Miller III, Brian A. Box, Erich C. Whitney, James M. Glynn","A high speed implementation of the CMAC neural network was designed  using dedicated CMOS logic. This technology was then used to implement  two general purpose CMAC associative memory boards for the VME bus.  Each board implements up to 8 independent CMAC networks with a total  of one million adjustable weights. Each CMAC network can be configured  to have from 1 to 512 integer inputs and from 1 to 8 integer outputs.  Response times for typical CMAC networks are well below 1 millisecond,  making the networks sufficiently fast for most robot control problems, and  many pattern recognition and signal processing problems."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/4f6ffe13a5d75b2d6a3923922b3922e5-Abstract.html,Shaping the State Space Landscape in Recurrent Networks,"Patrice Simard, Jean Pierre Raysz, Bernard Victorri","Bernard Victorri  ELSAP  Universite  de  Caen  14032 Caen  Cedex  France 
Fully  recurrent  (asymmetrical)  networks  can  be  thought  of as  dynamic  systems.  The  dynamics  can  be  shaped  to  perform  content  addressable  memories,  recognize  sequences,  or  generate  trajectories.  Unfortunately  several  problems  can  arise:  First,  the  convergence  in  the  state  space  is  not  guaranteed.  Second,  the  learned  fixed  points  or  trajectories  are  not  necessarily stable.  Finally,  there  might exist  spurious fixed  points and/or  spurious  ""attracting""  trajectories that do  not correspond  to any patterns.  In this paper,  we  introduce a  new  energy  function  that  presents solutions  to all of these problems.  We present an efficient gradient descent algorithm  which directly acts on the stability of the fixed  points and trajectories and  on the size  and shape of the  corresponding basin and valley of attraction.  The results are illustrated by the simulation of a small content addressable  memory."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/52720e003547c70561bf5e03b95aa99f-Abstract.html,"EMPATH: Face, Emotion, and Gender Recognition Using Holons","Garrison W. Cottrell, Janet Metcalfe","The  dimens~onali~y of a  set Off  160 1:~ :a:~s ~~·.10 .  female  subjects  IS  reduced  ........ .  network  The extracted features do not correspond to  in previ~us face  recognition systems (KaR· na~e, 19~;)y' ......••.•..  f~tures we  call  holons.  The  hol.ons  are fV~~ t~!  ..... .  ..  ......\  d'  tances  between  facial  elements."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Abstract.html,"Interaction Among Ocularity, Retinotopy and On-center/Off-center Pathways During Development",Shigeru Tanaka,"The development of projections from the retinas to the cortex is  mathematically analyzed according to the previously proposed  thermodynamic formulation of the self-organization of neural networks.  Three types of submodality included in the visual afferent pathways are  assumed in two models: model (A), in which the ocularity and retinotopy  are considered separately, and model (B), in which on-center/off-center  pathways are considered in addition to ocularity and retinotopy. Model (A)  shows striped ocular dominance spatial patterns and, in ocular dominance  histograms, reveals a dip in the binocular bin. Model (B) displays  spatially modulated irregular patterns and shows single-peak behavior in  the histograms. When we compare the simulated results with the observed  results, it is evident that the ocular dominance spatial patterns and  histograms for models (A) and (B) agree very closely with those seen in  monkeys and cats."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/577bcc914f9e55d5e4e4f82f9f00e7d4-Abstract.html,Phase-coupling in Two-Dimensional Networks of Interacting Oscillators,"Ernst Niebur, Daniel M. Kammen, Christof Koch, Daniel L. Ruderman, Heinz G. Schuster","Coherent oscillatory activity in large networks of biological or artifi(cid:173) cial neural units may be a useful mechanism for coding information  pertaining to a single perceptual object or for  detailing regularities  within  a  data set.  We  consider  the  dynamics  of a  large  array  of  simple  coupled  oscillators  under  a  variety  of connection  schemes.  Of particular  interest  is  the  rapid  and  robust  phase-locking  that  results  from  a  ""sparse""  scheme  where  each  oscillator  is  strongly  coupled  to a  tiny,  randomly selected, subset of its neighbors."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/58238e9ae2dd305d79c2ebc8c1883422-Abstract.html,An Analog VLSI Splining Network,"Daniel B. Schwartz, Vijay K. Samalam","We  have produced  a  VLSI  circuit capable of learning  to approximate ar(cid:173) bitrary  smooth  of a  single  variable  using  a  technique  closely  related  to  splines.  The circuit effectively  has  512 knots space on  a  uniform grid and  has full support for  learning.  The circuit  also can be used  to approximate  multi-variable functions as sum of splines. 
An  interesting,  and  as  of yet,  nearly  untapped set of applications for  VLSI  imple(cid:173) mentation of neural network learning systems can be found in adaptive control and  non-linear signal processing.  In  most such  applications,  the  learning  task  consists  of approximating a  real  function  of a  small number  of continuous  variables  from  discrete  data points.  Special purpose hardware is especially  interesting for  applica(cid:173) tions of this  type since  they  generally  require  real  time on-line learning  and  there  can  be stiff constraints on  the power  budget and size  of the hardware.  Frequently,  the already difficult  learning problem is made more complex by the non-stationary  nature of the underlying process.  Conventional feed-forward  networks  with sigmoidal units are clearly  inappropriate  for applications of this type.  Although they have exhibited remarkable performance  in some types of time series  prediction  problems (for example, Wiegend,  1990  and  Atlas, 1990), their learning rates in general are too slow for on-line learning.  On-line  performance can be improved most easily by using networks with more constrained  architecture, effectively making the learning problem easier by giving the network a  hint about the learning task.  Networks that build local representations of the data,  such  as  radial  basis  functions,  are excellent  candidates for  these  type of problems.  One  great  advantage  of such  networks  is  that  they  require  only  a  single  layer  of  units.  If the position and width of the units are fixed,  the learning problem is linear"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/5a4b25aaed25c2ee1b74de72dc03c14e-Abstract.html,Designing Linear Threshold Based Neural Network Pattern Classifiers,Terrence L. Fine,"The three problems that concern us are identifying a natural domain of  pattern classification applications of feed forward neural networks, select(cid:173) ing an appropriate feedforward network architecture, and assessing the  tradeoff between network complexity, training set size, and statistical reli(cid:173) ability as measured by the probability of incorrect classification. We close  with some suggestions, for improving the bounds that come from Vapnik(cid:173) Chervonenkis theory, that can narrow, but not close, the chasm between  theory and practice. 
1 Speculations on Neural Network Pattern Classifiers 
The goal is to provide rapid, reliable classification of new inputs from a  (1)  pattern source. Neural networks are appropriate as pattern classifiers when the  pattern sources are ones of which we have little understanding, beyond perhaps a  nonparametric statistical model, but we have been provided with classified samples  of features drawn from each of the pattern categories. Neural networks should be  able to provide rapid and reliable computation of complex decision functions. The  issue in doubt is their statistical response to new inputs. 
(2) The pursuit of optimality is misguided in the context of Point (1). Indeed, it  is unclear what might be meant by 'optimality' in the absence of a more detailed  mathematical framework for the pattern source. 
(3) The well-known, oft-cited 'curse of dimensionality' exposed by Richard Bell(cid:173) man may be a 'blessing' to neural networks. Individual network processing nodes  (e.g., linear threshold units) become more powerful as the number of their inputs  increases. For a large enough number n of points in an input space of d dimensions,  the number of dichotomies that can be generated by such a node grows exponen(cid:173) tially in d. This suggests that, unlike all previous efforts at pattern classification  that required substantial effort directed at the selection of low-dimensional feature  vectors so as to make the decision rule calculable, we may now be approaching a"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/5b8add2a5d98b1a652ea7fd72d942dac-Abstract.html,Qualitative structure from motion,Daphna Weinshall,"Exact structure from motion is an ill-posed computation and therefore  very sensitive to noise. In this work I describe how a qualitative shape  representation, based on the sign of the Gaussian curvature, can be com(cid:173) puted directly from motion disparities, without the computation of an  exact depth map or the directions of surface normals. I show that humans  can judge the curvature sense of three points undergoing 3D motion from  two, three and four views with success rate significantly above chance. A  simple RBF net has been trained to perform the same task."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/5ef698cd9fe650923ea331c15af3b160-Abstract.html,Learning to See Rotation and Dilation with a Hebb Rule,"Martin I. Sereno, Margaret E. Sereno","Previous work (M.I. Sereno, 1989; cf. M.E. Sereno, 1987) showed that a  feedforward network with area VI-like input-layer units and a Hebb rule  can develop area MT-like second layer units that solve the aperture  problem for pattern motion. The present study extends this earlier work  to more complex motions. Saito et al. (1986) showed that neurons with  large receptive fields in macaque visual area MST are sensitive to  different senses of rotation and dilation, irrespective of the receptive field  location of the movement singularity. A network with an MT-like  second layer was trained and tested on combinations of rotating, dilating,  and translating patterns. Third-layer units learn to detect specific senses  of rotation or dilation in a position-independent fashion, despite having  position-dependent direction selectivity within their receptive fields."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html,A Comparative Study of the Practical Characteristics of Neural Network and Conventional Pattern Classifiers,"Kenney Ng, Richard P Lippmann","Seven different pattern classifiers were implemented on a serial computer  and compared using artificial and speech recognition tasks. Two neural  network (radial basis function and high order polynomial GMDH network)  and five conventional classifiers (Gaussian mixture, linear tree, K nearest  neighbor, KD-tree, and condensed K nearest neighbor) were evaluated.  Classifiers were chosen to be representative of different approaches to pat(cid:173) tern classification and to complement and extend those evaluated in a  previous study (Lee and Lippmann, 1989). This and the previous study  both demonstrate that classification error rates can be equivalent across  different classifiers when they are powerful enough to form minimum er(cid:173) ror decision regions, when they are properly tuned, and when sufficient  training data is available. Practical characteristics such as training time,  classification time, and memory requirements, however, can differ by or(cid:173) ders of magnitude. These results suggest that the selection of a classifier  for a particular task should be guided not so much by small differences in  error rate, but by practical considerations concerning memory usage, com(cid:173) putational resources, ease of implementation, and restrictions on training  and classification times."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/66808e327dc79d135ba18e051673d906-Abstract.html,Real-time autonomous robot navigation using VLSI neural networks,"Lionel Tarassenko, Michael Brownlow, Gillian Marshall, Jan Tombs, Alan Murray","We describe a real time robot navigation system based on three VLSI  neural network modules. These are a resistive grid for path planning, a  nearest-neighbour classifier for localization using range data from a time(cid:173) of-flight infra-red sensor and a sensory-motor associative network for dy(cid:173) namic obstacle avoidance ."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/6855456e2fe46a9d49d3d3af4f57443d-Abstract.html,CAM Storage of Analog Patterns and Continuous Sequences with 3N2 Weights,"Bill Baird, Frank Eeckman","A simple architecture and algorithm for analytically guaranteed associa(cid:173) tive memory storage of analog patterns, continuous sequences, and chaotic  attractors in the same network is described. A matrix inversion determines  network weights, given prototype patterns to be stored. There are N units  of capacity in an N node network with 3N 2 weights. It costs one unit per  static attractor, two per Fourier component of each sequence, and four per  chaotic attractor. There are no spurious attractors, and there is a Lia(cid:173) punov function in a special coordinate system which governs the approach  of transient states to stored trajectories. Unsupervised or supervised incre(cid:173) mental learning algorithms for pattern classification, such as competitive  learning or bootstrap Widrow-Hoff can easily be implemented. The archi(cid:173) tecture can be ""folded"" into a recurrent network with higher order weights  that can be used as a model of cortex that stores oscillatory and chaotic  attractors by a Hebb rule. Hierarchical sensory-motor control networks  may be constructed of interconnected ""cortical patches"" of these network  modules. Network performance is being investigated by application to the  problem of real time handwritten digit recognition."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/6883966fd8f918a4aa29be29d2c386fb-Abstract.html,Generalization Properties of Radial Basis Functions,"Sherif M. Botros, Christopher G. Atkeson","We examine the ability of radial basis functions  (RBFs) to generalize.  We  compare the performance of several types of RBFs.  We use the inverse dy(cid:173) namics of an idealized  two-joint  arm as  a  test case.  We find  that without  a  proper  choice of a  norm for  the inputs,  RBFs have  poor  generalization  properties.  A simple global scaling of the input variables greatly improves  performance.  We suggest some efficient methods to approximate this dis(cid:173) tance  metric."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/69cb3ea317a32c4e6143e665fdb20b14-Abstract.html,Learning Time-varying Concepts,"Anthony Kuh, Thomas Petsche, Ronald L. Rivest","This work extends computational learning theory to situations in which concepts  vary over time, e.g., system identification of a time-varying plant. We have  extended formal definitions of concepts and learning to provide a framework  in which an algorithm can track a concept as it evolves over time. Given  this framework and focusing on memory-based algorithms, we have derived  some PAC-style sample complexity results that determine, for example, when  tracking is feasible. We have also used a similar framework and focused on  incremental tracking algorithms for which we have derived some bounds on  the mistake or error rates for some specific concept classes."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/6c524f9d5d7027454a783c841250ba71-Abstract.html,A Theory for Neural Networks with Time Delays,"Bert de Vries, José Carlos Príncipe","We present a new neural network model for processing of temporal  patterns.  This  model,  the  gamma  neural model,  is as  general  as  a  convolution  delay  model  with  arbitrary  weight  kernels  w(t).  We  show  that  the  gamma  model  can  be  formulated  as  a  (partially  prewired)  additive  model.  A  temporal  hebbian  learning  rule  is  derived  and  we  establish  links  to  related  existing  models  for  temporal processing."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/6da37dd3139aa4d9aa55b8d237ec5d4a-Abstract.html,Natural Dolphin Echo Recognition Using an Integrator Gateway Network,"Herbert L. Roitblat, Patrick W. B. Moore, Paul E. Nachtigall, Ralph H. Penner","We have been studying the performance of a bottlenosed dolphin on  a delayed matching-to-sample task to gain insight into the processes and  mechanisms that the animal uses during echolocation. The dolphin  recognizes targets by emitting natural sonar signals and listening to the  echoes that return. This paper describes a novel neural network  architecture, called an integrator gateway network, that we have de(cid:173) veloped to account for this performance. The integrator gateway  network combines information from multiple echoes to classify targets  with about 90% accuracy. In contrast, a standard backpropagation  network performed with only about 63% accuracy."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/6ecbdd6ec859d284dc13885a37ce8d81-Abstract.html,Compact EEPROM-based Weight Functions,"A. Kramer, C. K. Sin, R. Chu, P. K. Ko","We are focusing on the development of a  highly compact neural net weight  function based on the use of EEPROM devices.  These devices have already  proven  useful  for  analog  weight storage,  but  existing  designs  rely  on  the  use  of conventional voltage multiplication as the weight function,  requiring  additional  transistors  per  synapse.  A  parasitic  capacitance  between  the  floating gate and the drain of the EEPROM structure leads to an unusual  J-V characteristic  which can  be used  to advantage in  designing a compact  synapse.  This  novel  behavior  is  well  characterized  by  a  model  we  have  developed.  A single-device circuit results in a  1-quadrant synapse function  which  is  nonlinear,  though  monotonic.  A  simple  extension  employing  2  EEPROMs  results  in  a  2  quadrant  function  which  is  much  more  linear.  This approach offers  the  potential for  more than a  ten-fold  increase  in  the  density of neural net  implementations. 
1 
INTRODUCTION - ANALOG  WEIGHTING 
The recent surge of interest in neural networks and parallel analog computation has  motivated  the  need  for  compact  analog computing blocks.  Analog  weighting is  an  important computational function  of this class.  Analog  weighting is  the combining  of two analog values, one  of which  is  typically varying (the input) and one of which  is  typically fixed  (the  weight)  or  at  least  varying  more  slowly.  The  varying  value  is  ""weighted""  by  the  fixed  value  through  the  ""weighting function"",  typically  mul(cid:173) tiplication.  Analog  weighting is  most  interesting  when  the  overall computational  task  involves  computing  the  ""weighted  sum  of the  inputs.""  That  is,  to  compute 
2:7=1  t(lOj, Vi)  where  to  is  the  weighting function  and  ~v =  {lOb W2,  ... , wn}  and"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/6faa8040da20ef399b63a72d0e4ab575-Abstract.html,Discovering Viewpoint-Invariant Relationships That Characterize Objects,"Richard S. Zemel, Geoffrey E. Hinton","Using  an  unsupervised  learning  procedure,  a  network  is  trained on  an en(cid:173) semble of images of the same two-dimensional object at different positions,  orientations  and  sizes.  Each  half of  the  network  ""sees""  one  fragment  of  the object, and  tries to produce  as  output a set of 4 parameters that have  high mutual information with the 4 parameters output by  the other half of  the network.  Given the ensemble of training patterns, the 4 parameters on  which the two halves of the network can agree are the position, orientation,  and  size  of the  whole  object,  or  some  recoding  of them.  After  training,  the network can reject  instances of other shapes by  using the fact  that the  predictions  made  by  its  two  halves  disagree.  If two  competing  networks  are trained on  an unlabelled mixture of images of two objects, they  cluster  the training cases on the basis of the objects' shapes,  independently of the  position, orientation, and size."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/70c639df5e30bdee440e4cdf599fec2b-Abstract.html,Reinforcement Learning in Markovian and Non-Markovian Environments,Jürgen Schmidhuber,This work addresses three problems with reinforcement learning and adap(cid:173) tive  neuro-control:  1.  Non-Markovian interfaces  between  learner and en(cid:173) vironment.  2.  On-line learning  based  on  system  realization.  3.  Vector(cid:173) valued adaptive critics.  An algorithm is described which is based on system  realization and on two interacting fully recurrent  continually running net(cid:173) works  which  may  learn  in  parallel.  Problems  with  parallel  learning  are  attacked  by  'adaptive randomness'.  It is  also  described  how  interacting  model/controller  systems  can  be  combined  with  vector-valued  'adaptive  critics'  (previous  critics have been  scalar).
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/758874998f5bd0c393da094e1967a72b-Abstract.html,Second Order Properties of Error Surfaces: Learning Time and Generalization,"Yann LeCun, Ido Kanter, Sara A. Solla","The learning time of a simple neural network model is obtained through an  analytic computation of the eigenvalue spectrum for the Hessian matrix,  which describes the second order properties of the cost function in the  space of coupling coefficients. The form of the eigenvalue distribution  suggests new techniques for accelerating the learning process, and provides  a theoretical justification for the choice of centered versus biased state  variables."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/75fc093c0ee742f6dddaa13fff98f104-Abstract.html,Connectionist Music Composition Based on Melodic and Stylistic Constraints,"Michael Mozer, Todd Soukup","We describe  a  recurrent  connectionist  network,  called CONCERT,  that  uses  a  set  of  melodies  written  in  a  given  style  to  compose  new  melodies  in  that  style. CONCERT  is  an extension of a traditional algorithmic composition tech(cid:173) nique  in  which  transition  tables specify  the  probability  of the  next  note  as  a  function of previous context.  A central ingredient of CONCERT  is the use of a  psychologically-grounded representation of pitch."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/7eacb532570ff6858afd2723755ff790-Abstract.html,Transforming Neural-Net Output Levels to Probability Distributions,"John S. Denker, Yann LeCun","(1)  The  outputs  of a  typical  multi-output  classification  network  do  not  satisfy the axioms of probability; probabilities should be positive and sum  to one.  This problem  can  be solved  by  treating  the trained  network  as  a  preprocessor that produces  a  feature  vector that can be further  processed,  for instance by classical statistical estimation techniques.  (2) We present a  method for computing the first two moments ofthe probability distribution  indicating the range of outputs that are  consistent  with the input and the  training  data.  It is  particularly  useful  to  combine  these  two  ideas:  we  implement the  ideas  of section  1 using  Parzen  windows,  where  the  shape  and relative size  of each  window  is  computed  using the ideas of section  2.  This  allows  us  to make  contact  between  important  theoretical ideas  (e.g.  the  ensemble  formalism)  and  practical  techniques  (e.g.  back-prop).  Our  results  also  shed  new  light  on  and  generalize  the  well-known  ""soft max""  scheme. 
1  Distribution of Categories in Output Space 
In  many  neural-net  applications,  it  is  crucial  to  produce  a  set  of C  numbers  that  serve  as estimates of the probability of C  mutually exclusive outcomes.  For exam(cid:173) ple,  in  speech  recognition,  these  numbers  represent  the  probability of C  different  phonemes;  the probabilities of successive segments can be combined using a  Hidden  Markov  Model.  Similarly,  in  an  Optical  Character  Recognition  (""OCR"")  applica(cid:173) tion,  the numbers  represent  C  possible characters.  Probability information for  the  ""best  guess""  category  (and  probable runner-up  categories)  is  combined  with  con(cid:173) text,  cost  information, etcetera,  to produce  recognition of multi-character strings."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/816b112c6105b3ebd537828a39af4818-Abstract.html,Can neural networks do better than the Vapnik-Chervonenkis bounds?,"David Cohn, Gerald Tesauro","\Ve describe a series of careful llumerical experiments which measure the  average generalization capability of neural networks trained on a variety of  simple functions. These experiments are designed to test whether average  generalization performance can surpass the worst-case bounds obtained  from formal learning theory using the Vapnik-Chervonenkis dimension  (Blumer et al., 1989). We indeed find that, in some cases, the average  generalization is significantly better than the VC bound: the approach to  perfect performance is exponential in the number of examples m, rather  than the 11m result of the bound. In other cases, we do find the 11m  behavior of the VC bound, and in these cases, the numerical prefactor is  closely related to prefactor contained in the bound."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/819f46e52c25763a55cc642422644317-Abstract.html,Discovering Discrete Distributed Representations with Iterative Competitive Learning,Michael Mozer,"Competitive learning is an unsupervised algorithm that classifies input pat(cid:173) terns into mutually exclusive clusters. In a neural net framework, each clus(cid:173) ter is represented by a processing unit that competes with others in a winner(cid:173) take-all pool for an input pattern. I present a simple extension to the algo(cid:173) rithm that allows it to construct discrete, distributed representations. Discrete  representations are useful because they are relatively easy to analyze and  their information content can readily be measured. Distributed representa(cid:173) tions are useful because they explicitly encode similarity. The basic idea is  to apply competitive learning iteratively to an input pattern, and after each  stage to subtract from the input pattern the component that was captured in  the representation at that stage. This component is simply the weight vector  of the winning unit of the competitive pool. The subtraction procedure forces  competitive pools at different stages to encode different aspects of the input.  The algorithm is essentially the same as a traditional data compression tech(cid:173) nique known as multistep vector quantization, although the neural net per(cid:173) spective suggests potentially powerful extensions to that approach."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/82cec96096d4281b7c95cd7e74623496-Abstract.html,A Neural Expert System with Automated Extraction of Fuzzy If-Then Rules and Its Application to Medical Diagnosis,Yoichi Hayashi,This paper proposes ajuzzy neural expert system (FNES) with the  following two functions: (1) Generalization of the information derived  from the training data and embodiment of knowledge in the form of the  fuzzy neural network; (2) Extraction of fuzzy If-Then rules with  linguistic relative importance of each proposition in an antecedent  (I f -part) from a trained neural network. This paper also gives a  method to extract automatically fuzzy If-Then rules from the trained  neural network. To prove the effectiveness and validity of the proposed  fuzzy neural expert system. a fuzzy neural expert system for medical  diagnosis has been developed.
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/89f0fd5c927d466d6ec9a21b9ac34ffa-Abstract.html,Leaning by Combining Memorization and Gradient Descent,John C. Platt,"We have created a radial basis function network that allocates a  new computational unit whenever an unusual pattern is presented  to the network. The network learns by allocating new units and  adjusting the parameters of existing units. If the network performs  poorly on a presented pattern, then a new unit is allocated which  memorizes the response to the presented pattern. If the network  performs well on a presented pattern, then the network parameters  are updated using standard LMS gradient descent. For predicting  the Mackey Glass chaotic time series, our network learns much  faster than do those using back-propagation and uses a comparable  number of synapses."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/8bf1211fd4b7b94528899de0a43b9fb3-Abstract.html,A Neural Network Approach for Three-Dimensional Object Recognition,Volker Tresp,Abstract Unavailable
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/8cb22bdd0b7ba1ab13d742e22eed8da2-Abstract.html,Distributed Recursive Structure Processing,"Geraldine Legendre, Yoshiro Miyata, Paul Smolensky","Harmonic grammar (Legendre,  et al., 1990) is a connectionist theory of lin(cid:173) guistic  well-formed ness  based on the assumption  that the well-formedness  of a  sentence  can  be  measured  by  the  harmony  (negative  energy)  of the  corresponding  connectionist  state.  Assuming  a  lower-level  connectionist  network that obeys a  few  general connectionist  principles  but is  otherwise  unspecified,  we  construct  a  higher-level  network  with  an  equivalent  har(cid:173) mony function  that captures the most linguistically relevant global aspects  of the  lower  level  network.  In  this  paper,  we  extend  the  tensor  product  representation  (Smolensky  1990)  to fully  recursive  representations  of re(cid:173) cursively  structured objects like  sentences  in  the lower-level  network.  We  show  theoretically  and  with  an  example  the  power  of the  new  technique  for  parallel distributed  structure  processing."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/8d3bba7425e7c98c50f52ca1b52d3735-Abstract.html,Dynamics of Learning in Recurrent Feature-Discovery Networks,Todd K. Leen,The  self-organization  of  recurrent  feature-discovery  networks  is  studied  from the perspective of dynamical systems.  Bifurcation theory reveals pa(cid:173) rameter regimes in which multiple equilibria or limit cycles coexist with the  equilibrium at which  the networks  perform principal component analysis.
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/8d7d8ee069cb0cbbf816bbb65d56947e-Abstract.html,Navigating through Temporal Difference,Peter Dayan,"Barto, Sutton and Watkins [2] introduced a grid task as a didactic ex(cid:173) ample of temporal difference planning and asynchronous dynamical pre>(cid:173) gramming. This paper considers the effects of changing the coding of the  input stimulus, and demonstrates that the self-supervised learning of a  particular form of hidden unit representation improves performance."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/8dd48d6a2e2cad213179a3992c0be53c-Abstract.html,On the Circuit Complexity of Neural Networks,"V. P. Roychowdhury, K. Y. Siu, A. Orlitsky, T. Kailath","'~le introduce a geometric approach for investigating the power of threshold  circuits. Viewing n-variable boolean functions as vectors in 'R'2"", we invoke  tools from linear algebra and linear programming to derive new results on  the realizability of boolean functions using threshold gat.es.  Using this approach, one can obtain: (1) upper-bounds on the number of  spurious memories in HopfielJ networks, and on the number of functions  implementable by a depth-d threshold circuit; (2) a lower bound on the  number of ort.hogonal input. functions required to implement. a threshold  function; (3) a necessary condit.ion for an arbit.rary set of input. functions to  implement a threshold function; (4) a lower bound on the error introduced  in approximating boolean functions using sparse polynomials; (5) a limit  on the effectiveness of the only known lower-bound technique (based on  computing correlations among boolean functions) for the depth of thresh(cid:173) old circuit.s implement.ing boolean functions, and (6) a constructive proof  that every boolean function f of n input variables is a threshold function  of polynomially many input functions, none of which is significantly cor(cid:173) related with f. Some of these results lead t.o genera.lizations of key results  concerning threshold circuit complexity, particularly t.hose that are based  on the so-called spectral or Ha.rmonic analysis approach. Moreover, our  geometric approach yields simple proofs, based on elementary results from  linear algebra, for many of these earlier results."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/8e98d81f8217304975ccb23337bb5761-Abstract.html,An Attractor Neural Network Model of Recall and Recognition,"Eytan Ruppin, Yehezkel Yeshurun","This  work  presents  an  Attractor  Neural  Network  (ANN)  model  of Re(cid:173) call  and  Recognition.  It is  shown  that  an  ANN  model  can  qualitatively  account  for  a  wide  range  of experimental  psychological  data  pertaining  to  the  these  two  main  aspects  of memory  access.  Certain  psychological  phenomena  are  accounted  for,  including  the  effects  of list-length,  word(cid:173) frequency,  presentation  time,  context  shift,  and  aging.  Thereafter,  the  probabilities of successful  Recall  and  Recognition  are  estimated,  in  order  to  possibly enable further  quantitative examination of the model."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/8efb100a295c0c690931222ff4467bb8-Abstract.html,Training Knowledge-Based Neural Networks to Recognize Genes in DNA Sequences,"Michiel O. Noordewier, Geoffrey G. Towell, Jude W. Shavlik","We describe the application of a hybrid symbolic/connectionist machine  learning algorithm to the task of recognizing important genetic sequences.  The symbolic portion of the KBANN system utilizes inference rules that  provide a roughly-correct method for recognizing a class of DNA sequences  known as eukaryotic splice-junctions. We then map this ""domain theory""  into a neural network and provide training examples. Using the samples,  the neural network's learning algorithm adjusts the domain theory so that  it properly classifies these DNA sequences. Our procedure constitutes  a general method for incorporating preexisting knowledge into artificial  neural networks. We present an experiment in molecular genetics that  demonstrates the value of doing so."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/8fe0093bb30d6f8c31474bd0764e6ac0-Abstract.html,ALCOVE: A Connectionist Model of Human Category Learning,John K. Kruschke,"ALCOVE  is  a  connectionist  model  of human  category  learning  that  fits  a  broad spectrum of human learning data.  Its architecture is  based on well(cid:173) established  psychological  theory,  and  is  related  to  networks  using  radial  basis functions.  From the perspective of cognitive psychology,  ALCOVE can  be construed as a combination of exemplar-based representation and error(cid:173) driven  learning.  From the perspective of connectionism,  it can  be seen  as  incorporating constraints into back-propagation networks  appropriate for  modelling human learning."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/94f6d7e04a4d452035300f18b984988c-Abstract.html,Multi-Layer Perceptrons with B-Spline Receptive Field Functions,"Stephen H. Lane, Marshall Flax, David Handelman, Jack Gelfand","Multi-layer perceptrons are often slow to learn nonlinear functions  with complex local structure due to the global nature of their function  approximations. It is shown that standard multi-layer perceptrons are  actually a special case of a more general network formulation that  incorporates B-splines into the node computations. This allows novel  spline network architectures to be developed that can combine the  generalization capabilities and scaling properties of global multi-layer  feedforward networks with the computational efficiency and learning  speed of local computational paradigms. Simulation results are  presented for the well known spiral problem of Weiland and of Lang  and Witbrock to show the effectiveness of the Spline Net approach. 
1."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/950a4152c2b4aa3ad78bdd6b366cc179-Abstract.html,"Bumptrees for Efficient Function, Constraint and Classification Learning",Stephen M. Omohundro,"A  new  class of data  structures called ""bumptrees"" is described.  These  structures  are  useful  for  efficiently  implementing  a  number  of neural  network related operations.  An empirical comparison with radial  basis  functions  is presented on a  robot ann mapping learning task.  Applica(cid:173) tions to density estimation. classification. and constraint representation  and learning are also outlined. 
1  WHAT IS A BUMPTREE? 
A bumptree is a new geometric data structure which is useful for efficiently learning. rep(cid:173) resenting. and evaluating geometric relationships in a variety of contexts. They are a natural  generalization  of several  hierarchical  geometric  data  structures  including  oct-trees.  k-d  trees. balltrees and boxtrees. They are useful for many geometric learning tasks including  approximating functions. constraint surfaces. classification regions. and probability densi(cid:173) ties from samples. In the function approximation case. the approach is related to radial basis  function neural networks, but supports faster construction, faster access, and more flexible  modification. We provide empirical data comparing bumptrees with radial basis functions  in section 2.  A bumptree is used to provide efficient access to a collection of functions on a Euclidean  space of interest. It is a complete binary tree in which a leaf corresponds to each function  of interest There are also functions  associated with  each internal node and the defining  constraint is that each interior node's function must be everwhere larger than each of the"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/9be40cee5b0eee1462c82c6964087ff9-Abstract.html,Planning with an Adaptive World Model,"Sebastian Thrun, Knut Möller, Alexander Linden","We present a new connectionist planning method [TML90].  By interaction  with  an  unknown  environment,  a  world  model  is  progressively  construc(cid:173) ted  using  gradient  descent.  For  deriving  optimal actions  with  respect  to  future  reinforcement,  planning is  applied in two steps:  an experience  net(cid:173) work proposes a  plan which is subsequently optimized by gradient descent  with  a  chain of world  models,  so  that  an  optimal reinforcement  may be  obtained  when  it  is  actually  run.  The  appropriateness  of this  method  is  demonstrated by a  robotics  application and a  pole balancing task."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/9de6d14fff9806d4bcd1ef555be766cd-Abstract.html,Simple Spin Models for the Development of Ocular Dominance Columns and Iso-Orientation Patches,"J.D. Cowan, A. E. Friedman",Simple classical spin models well-known to physicists as the ANNNI  and Heisenberg XY Models. in which long-range interactions occur in  a pattern given by the Mexican Hat operator. can generate many of the  structural properties characteristic of the ocular dominance columns  and iso-orientation patches seen in cat and primate visual cortex.
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/9dfcd5e558dfa04aaf37f137a1d9d3e5-Abstract.html,A Multiscale Adaptive Network Model of Motion Computation in Primates,"H. Taichi Wang, Bimal Mathur, Christof Koch","We  demonstrate  a  multiscale  adaptive  network  model  of motion  computation in primate area MT. The model consists of two stages:  (l)  local velocities are measured across multiple spatio-temporal channels,  and (2) the optical flow  field  is computed by a  network of direction(cid:173) selective neurons  at multiple  spatial  resolutions.  This model  embeds  the computational efficiency of Multigrid algorithms within a parallel  network as well as adaptively  computes  the most reliable estimate of  the flow  field across different spatial scales. Our model neurons show  the same nonclassical receptive field properties as Allman's type I MT  neurons.  Since local velocities are measured across multiple channels,  various  channels  often  provide  conflicting  measurements  to  the  network. We have incorporated a  veto scheme for conflict resolution.  This mechanism provides a novel explanation for the spatial frequency  dependency of the psychophysical phenomenon called Motion Capture. 
1  MOTIVATION  We previously developed a two-stage model of motion computation in the visual system  of primates  (Le.  magnocellular pathway from  retina to  V1  and  MT;  Wang,  Mathur &  Koch, 1989). This algorithm has these deficiencies:  (1) the issue of optimal spatial scale  for  velocity measurement, and (2)  the issue optimal spatial scale for  the smoothness of  motion field.  To address these deficiencies, we have implemented a multi-scale motion  network based on multigrid algorithms.  All  methods of estimating optical flow  make a basic assumption about the scale of the  velocity relative to the  spatial  neighborhood and to the  temporal  discretization step of  delay. Thus,  if the velocity of the pattern  is much larger than  the ratio of the spatial to  temporal  sampling step, an incorrect velocity  value will be obtained (Battiti, Amaldi  &  Koch,  1991). Battiti et al. proposed a coarse-to-fine strategy for adaptively detennining"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/9fd81843ad7f202f26c1a174c7357585-Abstract.html,"Spherical Units as Dynamic Consequential Regions: Implications for Attention, Competition and Categorization","Stephen Jose Hanson, Mark A. Gluck","to construct dynamic 
Spherical Units can be used  reconfigurable  consequential regions, the geometric bases for Shepard's (1987) theory of  stimulus generalization in animals and humans. We derive from Shepard's  (1987) generalization theory a particular multi-layer network with dynamic  (centers and radii) spherical regions which possesses a specific mass function  (Cauchy). This learning model generalizes the configural-cue network model  (Gluck & Bower 1988): (1) configural cues can be learned and do not require  pre-wiring the power-set of cues, (2) Consequential regions are continuous  rather than discrete and (3) Competition amoungst receptive fields is shown  to be increased by the global extent of a particular mass function (Cauchy).  We compare other common mass functions (Gaussian; used in models of  Moody & Darken; 1989, Krushke, 1990) or just standard backpropogation  networks with hyperplane/logistic hidden units showing that neither fare as  well as models of human generalization and learning. 
1 The Generalization Problem 
Given a favorable or unfavorable consequence, what should an organism assume about  the contingent stimuli? If a moving shadow overhead appears prior to a hawk attack  what should an organism assume about other moving shadows, their shapes and  positions? If a dense food patch is occasioned by a particular density of certain kinds of  shrubbery what should the organism assume about other shurbbery, vegetation or its  spatial density? In an pattern recognition context, given a character of a certain shape,  orientation, noise level etc.. has been recognized correctly what should the system  assume about other shapes, orientations, noise levels it has yet to encounter? 
• Also a member of Cognitive Science Laboratory, Princeton University, Princeton, NJ 08544"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/a01a0380ca3c61428c26a231f0e49a09-Abstract.html,Speech Recognition Using Connectionist Approaches,Khalid Choukri,"This  paper  is  a  summary  of SPRINT  project  aims  and  results.  The  project  focus  on the use  of neuro-computing techniques to tackle various problems that  remain  unsolved  in  speech  recognition.  First  results  concern  the  use  of feed(cid:173) forward  nets  for  phonetic  units  classification,  isolated  word  recognition,  and  speaker  adaptation."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/a02ffd91ece5e7efeb46db8f10a74059-Abstract.html,Feedback Synapse to Cone and Light Adaptation,Josef Skrzypek,"Light  adaptation  (LA)  allows  cone  vIslOn  to  remain  functional  between  twilight  and  the  brightest  time  of day  even  though,  at  anyone  time,  their  intensity-response (I-R)  characteristic  is  limited  to  3  log  units of the  stimu(cid:173) lating  light.  One mechanism  underlying  LA, was localized  in  the outer seg(cid:173) ment of an isolated cone (1,2). We found that by adding annular illhmination,  an  I-R  characteristic  of a  cone  can  be  shifted  along  the  intensity  domain.  Neural network involving feedback  synapse from  horizontal cells to cones is  involved  to  be  in  register  with  ambient  light  level  of  the  periphery.  An  equivalent  electrical  circuit  with  three  different  transmembrane  channels  leakage,  photocurrent  and  feedback  was  used  to  model static  behavior of a  cone.  SPICE simulation showed  that interactions between feedback  synapse  and  the  light  sensitive  conductance  in  the  outer  segment  can  shift  the  I-R  curves along  the  intensity domain, provided  that phototransduction  mechan(cid:173) ism is not saturated during maximally hyperpolarized light response."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/a49e9411d64ff53eccfdd09ad10a15b3-Abstract.html,Direct memory access using two cues: Finding the intersection of sets in a connectionist model,"Janet Wiles, Michael S. Humphreys, John D. Bain, Simon Dennis","For lack of alternative models, search and decision processes have provided the  dominant paradigm for human memory access using two or more cues, despite  evidence against search as an access process (Humphreys, Wiles & Bain, 1990).  We present an alternative process to search, based on calculating the intersection  of sets of targets activated by two or more cues. Two methods of computing  the intersection are presented, one using information about the possible targets,  the other constraining the cue-target strengths in the memory matrix. Analysis  using orthogonal vectors to represent the cues and targets demonstrates the  competence of both processes, and simulations using sparse distributed  representations demonstrate the performance of the latter process for tasks  involving 2 and 3 cues."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/a666587afda6e89aec274a3657558a27-Abstract.html,Flight Control in the Dragonfly: A Neurobiological Simulation,"William E. Faller, Marvin W. Luttges","Neural network simulations of the dragonfly flight neurocontrol system  have  been  developed  to  understand  how  this  insect  uses  complex,  unsteady  aerodynamics.  The  simulation  networks  account  for  the  ganglionic  spatial  distribution  of  cells  as  well  as  the  physiologic  operating range and the stochastic cellular fIring history of each neuron.  In  addition  the  motor  neuron  firing  patterns,  ""flight  command  sequences"", were utilized. Simulation training was targeted against both  the  cellular  and  flight  motor  neuron  firing  patterns.  The  trained  networks  accurately  resynthesized  the  intraganglionic  cellular firing  patterns. These in  tum controlled the  motor neuron fIring patterns that  drive  wing  musculature  during  flight.  Such  networks  provide  both  neurobiological analysis tools and fIrst  generation controls for  the  use  of ""unsteady"" aerodynamics."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/a8c88a0055f636e4a163a5e3d16adab7-Abstract.html,A Framework for the Cooperation of Learning Algorithms,"Léon Bottou, Patrick Gallinari","We introduce a framework  for  training architectures composed of several  modules. This framework,  which  uses a statistical formulation  of learning  systems,  provides  a  unique  formalism  for  describing  many  classical  connectionist  algorithms  as  well  as  complex  systems  where  several  algorithms interact. It allows to design hybrid systems which combine the  advantages of connectionist algorithms as well as other learning algorithms."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/aa942ab2bfa6ebda4840e7360ce6e7ef-Abstract.html,Continuous Speech Recognition by Linked Predictive Neural Networks,"Joe Tebelskis, Alex Waibel, Bojan Petek, Otto Schmidbauer","We present a large vocabulary, continuous speech recognition system based  on  Linked  Predictive  Neural  Networks  (LPNN's).  The system  uses  neu(cid:173) ral  networks  as  predictors  of speech  frames,  yielding  distortion  measures  which  are  used  by  the  One Stage DTW algorithm to perform  continuous  speech  recognition.  The system,  already  deployed  in  a  Speech  to Speech  Translation system, currently achieves 95%,  58%,  and 39% word accuracy  on  tasks  with  perplexity  5,  111,  and  402  respectively,  outperforming sev(cid:173) eral simple HMMs  that  we  tested.  We  also  found  that  the  accuracy  and  speed of the LPNN can be slightly improved by the judicious use of hidden  control  inputs.  We  conclude  by  discussing  the  strengths  and  weaknesses  of the predictive approach."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/ad13a2a07ca4b7642959dc0c4c740ab6-Abstract.html,A B-P ANN Commodity Trader,Joseph E. Collard,"An  Artificial  Neural  Network  recognize  a  buy/sell  particular  commodity  Propagation  of  errors  algorithm  was  used  to  encode  the  the  Long/Short  desired  output  and  18  fundamental  variables  plus  6  (or  18)  Trained  on  one  technical  variables  into  year  of  past  data  to  predict  long/short  market  positions  in  the  future  that  would  have  made  $10,301  profit  on  an  investment  of  less  than  $1000. 

relationship  between"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/ad972f10e0800b49d76fed33a21f6698-Abstract.html,Statistical Mechanics of Temporal Association in Neural Networks,"Andreas V. M. Herz, Zhaoping Li, J. Leo van Hemmen","We study the representation of static patterns and temporal associa(cid:173) tions in  neural networks with  a  broad  distribution of signal  delays.  For a certain class of such systems, a simple intuitive understanding  of the spatia-temporal computation becomes possible with the help  of a  novel  Lyapunov  functional. It allows  a  quantitative  study  of  the  asymptotic  network  behavior  through  a  statistical  mechanical  analysis. We  present  analytic  calculations of both retrieval  quality  and storage capacity and compare  them with  simulation results."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/b2eb7349035754953b57a32e2841bda5-Abstract.html,Cholinergic Modulation May Enhance Cortical Associative Memory Function,"Michael E. Hasselmo, Brooke P. Anderson, James M. Bower","Combining neuropharmacological experiments with computational model(cid:173) ing, we have shown that cholinergic modulation may enhance associative  memory function in piriform (olfactory) cortex. We have shown that the  acetylcholine analogue carbachol selectively suppresses synaptic transmis(cid:173) sion between cells within piriform cortex, while leaving input connections  unaffected. When tested in a computational model of piriform cortex,  this selective suppression, applied during learning, enhances associative  memory performance."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/b3967a0e938dc2a6340e258630febd5a-Abstract.html,Neural Network Application to Diagnostics and Control of Vehicle Control Systems,Kenneth A. Marko,"Diagnosis of faults in complex, real-time control systems is a  complicated task that has resisted solution by traditional methods. We  have shown that neural networks can be successfully employed to  diagnose faults in digitally controlled powertrain systems. This paper  discusses the means we use to develop the appropriate databases for  training and testing in order to select the optimum network architectures  and to provide reasonable estimates of the classification accuracy of  these networks on new samples of data. Recent work applying neural  nets to adaptive control of an active suspension system is presented."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/b6f0479ae87d244975439c6124592772-Abstract.html,Evolution and Learning in Neural Networks: The Number and Distribution of Learning Trials Affect the Rate of Evolution,"Ron Keesing, David G. Stork","and 
*Dept. of Electrical Engineering 
Stanford University  Stanford, CA 94305 
stork@psych.stanford.edu 
Learning can increase the rate of evolution of a population of  biological organisms (the Baldwin effect). Our simulations  show that in a population of artificial neural networks  solving a pattern recognition problem, no learning or too  much learning leads to slow evolution of the genes whereas  an intermediate amount is optimal. Moreover, for a given  total number of training presentations, fastest evoution  occurs if different individuals within each generation receive  different numbers of presentations, rather than equal  numbers. Because genetic algorithms (GAs) help avoid  local minima in energy functions, our hybrid learning-GA  systems can be applied successfully to complex, high(cid:173) dimensional pattern recognition problems."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/b7b16ecf8ca53723593894116071700c-Abstract.html,A Lagrangian Approach to Fixed Points,"Eric Mjolsness, Willard L. Miranker","We  present  a  new  way  to  derive  dissipative,  optimizing  dynamics  from  the  Lagrangian formulation of mechanics.  It can  be  used  to  obtain  both  standard  and  novel  neural  net  dynamics  for  optimization  problems.  To  demonstrate this we  derive standard descent  dynamics as well as nonstan(cid:173) dard variants that introduce  a  computational attention mechanism."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/b83aac23b9528732c23cc7352950e880-Abstract.html,Neural Network Implementation of Admission Control,"Rodolfo A. Milito, Isabelle Guyon, Sara A. Solla",A feedforward layered network implements a mapping required to control an  unknown stochastic nonlinear dynamical system. Training is based on a  novel approach that combines stochastic approximation ideas with back(cid:173) propagation. The method is applied to control admission into a queueing sys(cid:173) tem operating in a time-varying environment.
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/b9228e0962a78b84f3d5d92f4faa000b-Abstract.html,Computing with Arrays of Bell-Shaped and Sigmoid Functions,Pierre Baldi,"We consider feed-forward neural networks with one non-linear hidden layer  and  linear  output units.  The transfer  function  in the hidden  layer  are  ei(cid:173) ther  bell-shaped  or sigmoid.  In the bell-shaped  case,  we  show  how  Bern(cid:173) stein polynomials on one hand and the theory of the heat equation on the  other  are  relevant  for  understanding the  properties  of the  corresponding  networks.  In  particular,  these  techniques  yield  simple proofs of universal  approximation properties, i.e.  of the fact that any reasonable function can  be approximated to any degree of precision by a linear combination of bell(cid:173) shaped functions.  In addition, in  this  framework  the  problem of learning  is equivalent to the problem of reversing the time course of a diffusion pro(cid:173) cess.  The  results  obtained in  the bell-shaped  case  can  then  be  applied  to  the case  of sigmoid  transfer functions  in the hidden layer,  yielding similar  universality  results.  A  conjecture  related  to the problem of generalization  is  briefly  examined."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/bac9162b47c56fc8a4d2a519803d51b3-Abstract.html,VLSI Implementation of TInMANN,"Matt Melton, Tan Phan, Doug Reeves, Dave Van den Bout","A massively parallel, all-digital, stochastic architecture - TlnMAN N - is  described which performs competitive and Kohonen types of learning. A  VLSI design is shown for a TlnMANN neuron which fits within a small,  inexpensive MOSIS TinyChip frame, yet which can be used to build larger  networks of several hundred neurons. The neuron operates at a speed of  15 MHz which allows the network to process 290,000 training examples  per second. Use of level sensitive scan logic provides the chip with 100%  fault coverage, permitting very reliable neural systems to be built."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/bbcbff5c1f1ded46c25d28119a85c6c2-Abstract.html,SEXNET: A Neural Network Identifies Sex From Human Faces,"B.A. Golomb, D.T. Lawrence, T.J. Sejnowski","Sex identification in animals has biological importance. Humans are good  at making this determination visually, but machines have not matched  this ability. A neural network was trained to discriminate sex in human  faces, and performed as well as humans on a set of 90 exemplars. Images  sampled at 30x30 were compressed using a 900x40x900 fully-connected  back-propagation network; activities of hidden units served as input to a  back-propagation ""SexNet"" trained to produce values of 1 for male and  o for female faces. The network's average error rate of 8.1% compared  favorably to humans, who averaged 11.6%. Some SexNet errors mimicked  those of humans."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/bbf94b34eb32268ada57a3be5062fe7d-Abstract.html,From Speech Recognition to Spoken Language Understanding: The Development of the MIT SUMMIT and VOYAGER Systems,"Victor Zue, James Glass, David Goodine, Lynette Hirschman, Hong Leung, Michael Phillips, Joseph Polifroni, Stephanie Seneff","Spoken language is one of the most natural, efficient, flexible, and econom(cid:173) ical means of communication among humans. As computers play an ever  increasing role in our lives, it is important that we address the issue of  providing a graceful human-machine interface through spoken language.  In this paper, we will describe our recent efforts in moving beyond the  scope of speech recognition into the realm of spoken-language understand(cid:173) ing. Specifically, we report on the development of an urban navigation and  exploration system called VOYAGER, an application which we have used as  a basis for performing research in spoken-language understanding."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html,Generalization by Weight-Elimination with Application to Forecasting,"Andreas S. Weigend, David E. Rumelhart, Bernardo A. Huberman","Inspired by the information theoretic idea of minimum description length, we add  a term  to the back propagation cost function that penalizes network complexity.  We  give  the  details  of the  procedure,  called  weight-elimination,  describe  its  dynamics, and clarify the meaning of the parameters involved. From a Bayesian  perspective,  the complexity term  can  be usefully interpreted as  an  assumption  about prior distribution of the weights.  We  use  this  procedure  to  predict  the  sunspot time series and the notoriously noisy series of currency exchange rates."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/bca82e41ee7b0833588399b1fcd177c7-Abstract.html,A Novel Approach to Prediction of the 3-Dimensional Structures of Protein Backbones by Neural Networks,"Henrik Fredholm, Henrik Bohr, Jakob Bohr, Søren Brunak, Rodney M. J. Cotterill, Benny Lautrup, Steffen B. Petersen","Three-dimensional (3D) structures of protein backbones have been pre(cid:173) dicted using neural networks. A feed forward neural network was trained  on a class of functionally, but not structurally, homologous proteins, us(cid:173) ing backpropagation learning. The network generated tertiary structure  information in the form of binary distance constraints for the Co atoms  in the protein backbone. The binary distance between two Co atoms was  o if the distance between them was less than a certain threshold distance,  and 1 otherwise. The distance constraints predicted by the trained neu(cid:173) ral network were utilized to generate a folded conformation of the protein  backbone, using a steepest descent minimization approach."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/beed13602b9b0e6ecb5b568ff5058f07-Abstract.html,Back Propagation Implementation on the Adaptive Solutions CNAPS Neurocomputer Chip,Hal McCartor,"The Adaptive Solutions CN APS architecture chip is a general purpose  neurocomputer chip. It has 64 processors, each with 4 K bytes of local  memory, running at 25 megahertz. It is capable of implementing most  current neural network algorithms with on chip learning. This paper dis(cid:173) cusses the implementation of the Back Propagation algorithm on an array  of these chips and shows performance figures from a clock accurate hard(cid:173) ware simulator. An eight chip configuration on one board can update 2.3  billion connections per second in learning mode and process 9.6 billion  connections per second in feed forward mode."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/c042f4db68f23406c6cecf84a7ebb0fe-Abstract.html,Asymptotic slowing down of the nearest-neighbor classifier,"Robert R. Snapp, Demetri Psaltis, Santosh S. Venkatesh","If patterns are drawn from an n-dimensional feature space according to a  probability distribution that obeys a weak smoothness criterion, we show  that the probability that a random input pattern is misclassified by a  nearest-neighbor classifier using M random reference patterns asymptoti(cid:173) cally satisfies 
PM(error) """" Poo(error) + M2/n' 
a 
for sufficiently large values of M. Here, Poo(error) denotes the probability  of error in the infinite sample limit, and is at most twice the error of a  Bayes classifier. Although the value of the coefficient a depends upon the  underlying probability distributions, the exponent of M is largely distri(cid:173) bution free. We thus obtain a concise relation between a classifier's ability  to generalize from a finite reference sample and the dimensionality of the  feature space, as well as an analytic validation of Bellman's well known  ""curse of dimensionality."""
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/c058f544c737782deacefa532d9add4c-Abstract.html,Connectionist Approaches to the Use of Markov Models for Speech Recognition,"Hervé Bourlard, Nelson Morgan, Chuck Wooters","Previous  work  has  shown  the  ability  of  Multilayer  Perceptrons  (MLPs) to estimate emission probabilities for Hidden Markov Mod(cid:173) els  (HMMs).  The advantages of a  speech recognition system incor(cid:173) porating  both  MLPs  and  HMMs  are  the  best  discrimination  and  the  ability  to  incorporate  multiple  sources  of evidence  (features,  temporal context) without restrictive assumptions of distributions  or  statistical  independence.  This  paper  presents  results  on  the  speaker-dependent portion of DARPA's English language Resource  Management  database.  Results  support  the  previously  reported  utility of MLP probability estimation for  continuous speech recog(cid:173) nition.  An additional approach we  are pursuing is to use  MLPs as  nonlinear predictors for autoregressive HMMs.  While this is shown  to  be  more  compatible  with  the  HMM  formalism,  it  still  suffers  from several limitations.  This approach  is  generalized  to  take ac(cid:173) count of time correlation between successive observations, without  any restrictive assumptions about the driving noise."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/c3e878e27f52e2a57ace4d9a76fd9acf-Abstract.html,"A Second-Order Translation, Rotation and Scale Invariant Neural Network","Shelly D. D. Goggin, Kristina M. Johnson, Karl E. Gustafson","A second-order architecture is presented  here for  translation, rotation and  scale  invariant processing  of 2-D  images  mapped  to  n  input  units.  This  new architecture has a complexity of O( n) weights as opposed to the O( n 3 )  weights usually required  for  a  third-order,  rotation invariant architecture.  The reduction  in complexity is due  to the use  of discrete  frequency  infor(cid:173) mation.  Simulations show favorable  comparisons to other neural  network  architectures."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/c5ff2543b53f4cc0ad3819a36752467b-Abstract.html,Signal Processing by Multiplexing and Demultiplexing in Neurons,David C. Tam,to
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/c86a7ee3d8ef0b551ed58e354a836f2b-Abstract.html,Lg Depth Estimation and Ripple Fire Characterization Using Artificial Neural Networks,"John L. Perry, Douglas R. Baumgardt","This srudy has demonstrated how artificial neural networks (ANNs) can  be used to characterize seismic sources using high-frequency regional  seismic data. We have taken the novel approach of using ANNs as a  research tool for obtaining seismic source information, specifically  depth of focus for earthquakes and ripple-fire characteristics for  economic blasts, rather than as just a feature classifier between  earthquake and explosion populations. Overall, we have found that  ANNs have potential applications to seismic event characterization and  identification, beyond just as a feature classifier. In future studies, these  techniques should be applied to actual data of regional seismic events  recorded at the new regional seismic arrays. The results of this study  indicates that an ANN should be evaluated as part of an operational  seismic event identification system."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/caf1a3dfb505ffed0d024130f58c5cfa-Abstract.html,Adaptive Range Coding,"Bruce E. Rosen, James M. Goodwin, Jacques J. Vidal","these 
to  neuron-like  processing  elements.  ""neurons"" 
This  paper  examines  a  class  of  neuron  based  that  rely  on  learning  systems  for  dynamic  control  adaptive  range  coding  of  sensor  inputs.  Sensors  are  assumed  to  provide  binary  coded  range  vectors  that  coarsely  describe  the  system  state.  These  vectors  are  Output  input  decisions  generated  by  turn  the  system  state,  subsequently  producing  new  affect  inputs.  the  intervals  and  environment  are  evaluated.  The  neural  weights  as  well  as  the  ran g e  b 0  u n dar i e s  determining  the  output  decisions  are  then  altered  with  future  Preliminary  reinforcement  from  the  promise  of  adapting  ""neural  experiments  show  receptive  learning  dynamical  control.  The  observed  performance  with  this  method  exceeds  that  of  earlier  approaches. 
the  goal  of  maximizing  the  environment."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/cd00692c3bfe59267d5ecfac5310286c-Abstract.html,RecNorm: Simultaneous Normalisation and Classification applied to Speech Recognition,"John S. Bridle, Stephen J. Cox","A particular form of neural network is described, which has terminals  for acoustic patterns, class labels and speaker parameters. A method of  training this network to ""tune in"" the speaker parameters to a particular  speaker is outlined, based on a trick for converting a supervised network  to an unsupervised mode. We describe experiments using this approach  in isolated word recognition based on whole-word hidden Markov models.  The results indicate an improvement over speaker-independent perfor(cid:173) mance and, for unlabelled data, a performance close to that achieved on  labelled data."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/cf004fdc76fa1a4f25f62e0eb5261ca3-Abstract.html,A four neuron circuit accounts for change sensitive inhibition in salamander retina,"Jeffrey L. Teeters, Frank H. Eeckman, Frank S. Werblin","In salamander retina, the response of On-Off ganglion cells to a central  flash is reduced by movement in the receptive field surround. Through  computer simulation of a 2-D model which takes into account their  anatomical and physiological properties, we show that interactions  between four neuron types (two bipolar and two amacrine) may be  responsible for the generation and lateral conductance of this change  sensitive inhibition. The model shows that the four neuron circuit can  account for previously observed movement sensitive reductions in  ganglion cell sensitivity and allows visualization and prediction of the  spatio-temporal pattern of activity in change sensitive retinal cells."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/d1f255a373a3cef72e03aa9d980c7eca-Abstract.html,A Method for the Efficient Design of Boltzmann Machines for Classiffication Problems,"Ajay Gupta, Wolfgang Maass",We introduce a method for the efficient design of a Boltzmann machine (or  a Hopfield net) that computes an arbitrary given Boolean function f . This  method is based on an efficient simulation of acyclic circuits with threshold  gates by Boltzmann machines. As a consequence we can show that various  concrete Boolean functions f that are relevant for classification problems  can be computed by scalable Boltzmann machines that are guaranteed  to converge to their global maximum configuration with high probability  after constantly many steps.
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/d296c101daa88a51f6ca8cfc1ac79b50-Abstract.html,How Receptive Field Parameters Affect Neural Learning,"Bartlett W. Mel, Stephen M. Omohundro","We  identify  the three  principle factors  affecting  the  performance of learn(cid:173) ing by  networks  with  localized  units:  unit noise,  sample density,  and  the  structure of the target function.  We then  analyze the effect  of unit recep(cid:173) tive  field  parameters  on  these  factors  and  use  this  analysis  to  propose  a  new  learning algorithm which  dynamically alters receptive field  properties  during learning. 
1  LEARNING WITH LOCALIZED RECEPTIVE FIELDS 
Locally-tuned  representations  are  common in  both  biological  and  artificial  neural  networks.  Several workers have analyzed the effect of receptive field  size, shape, and  overlap  on  representation  accuracy:  (Baldi,  1988),  (Ballard,  1987),  and  (Hinton,  1986).  This paper investigates the additional interactions introduced by  the task of  function learning.  Previous studies which have considered learning have for the most  part restricted attention to the use of the input probability distribution to determine  receptive field  layout (Kohonen,  1984)  and (Moody and Darken,  1989).  We  will see  that the structure of the function  being learned may also  be  advantageously taken  into account. 
Function learning using  radial basis functions  (RBF's)  is  currently  a  popular tech(cid:173) nique  (Broomhead  and  Lowe,  1988)  and  serves  as  an  adequate framework  for  our  discussion.  Because  we  are interested  in constraints on biological systems,  we  must  explictly  consider  the  effects  of unit  noise.  The  goal  is  to  choose  the  layout  of  receptive fields  so as  to minimize average performance error.  Let  y  = f(x)  be  the  function  the  network  is  attempting  to  learn  from  example"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/d34ab169b70c9dcd35e62896010cd9ff-Abstract.html,Proximity Effect Corrections in Electron Beam Lithography Using a Neural Network,"Robert C. Frye, Kevin D. Cummings, Edward A. Rietman",We have used a neural network to compute corrections for images written  by electron beams to eliminate the proximity effects caused by electron  Iterative methods are effective. but require prohibitively  scattering.  computation time. We have instead trained a neural network to perform  equivalent corrections. resulting in a significant speed-up. We have  examined hardware  implementations using both analog and digital  electronic networks. Both had an acceptably small error of 0.5% compared  to the iterative results. Additionally. we verified that the neural network  correctly generalized the solution of the problem to include patterns not  contained in its training set. We have experimentally verified this approach  on a Cambridge Instruments EBMF 10.5 exposure system.
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/d709f38ef758b5066ef31b18039b8ce5-Abstract.html,Exploiting Syllable Structure in a Connectionist Phonology Model,"David S. Touretzky, Deirdre W. Wheeler","In  a  previous  paper  (Touretzky  &  Wheeler,  1990a) we showed  how  adding  a  clustering operation to a connectionist phonology model produced a parallel pro(cid:173) cessing account of certain ""iterative"" phenomena.  In this paper we show how the  addition of a second structuring primitive, syllabification, greatly increases  the  power of the model.  We present examples from  a non-Indo-European language  that appear to require rule ordering to at least a depth of four.  By adding syllab(cid:173) ification circuitry to structure the model's perception of the input string, we are  able to handle these examples with only two derivational steps.  We conclude that  in phonology, derivation can be largely replaced by structuring."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/d9fc5b73a8d78fad3d6dffe419384e70-Abstract.html,Integrated Modeling and Control Based on Reinforcement Learning and Dynamic Programming,Richard S. Sutton,"This is a summary of results with Dyna, a class of architectures for intel(cid:173) ligent systems based on approximating dynamic programming methods.  Dyna architectures integrate trial-and-error (reinforcement) learning and  execution-time planning into a single process operating alternately on the  world and on a learned forward model of the world. We describe and  show results for two Dyna architectures, Dyna-AHC and Dyna-Q. Using a  navigation task, results are shown for a simple Dyna-AHC system which  simultaneously learns by trial and error, learns a world model, and plans  optimal routes using the evolving world model. We show that Dyna-Q  architectures (based on Watkins's Q-Iearning) are easy to adapt for use in  changing environments. 
1 
Introduction to Dyna 
Dyna architectures (Sutton, 1990) use learning algorithms to approximate the con(cid:173) ventional optimal control technique known as dynamic programming (DP) (Bell(cid:173) man, 1957; Bertsekas, 1987). DP itself is not a learning method, but rather a  computational method for determining optimal behavior given a complete model of  the task to be solved. It is very similar to state-space search, but differs in that  it is more incremental and never considers actual action sequences explicitly, only  single actions at a time. This makes DP more amenable to incremental planning  at execution time, and also makes it more suitable for stochastic or incompletely  modeled environments, as it need not consider the extremely large number of se(cid:173) quences possible in an uncertain environment. Learned world models are likely  to be stochastic and uncertain, making DP approaches particularly promising for"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/dc912a253d1e9ba40e2c597ed2376640-Abstract.html,A Connectionist Learning Control Architecture for Navigation,Jonathan R. Bachrach,"A novel learning control architecture is used for navigation. A sophisti(cid:173) cated test-bed is used to simulate a cylindrical robot with a sonar belt  in a planar environment. The task is short-range homing in the pres(cid:173) ence of obstacles. The robot receives no global information and assumes  no comprehensive world model. Instead the robot receives only sensory  information which is inherently limited. A connectionist architecture is  presented which incorporates a large amount of a priori knowledge in the  form of hard-wired networks, architectural constraints, and initial weights.  Instead of hard-wiring static potential fields from object models, myarchi(cid:173) tecture learns sensor-based potential fields, automatically adjusting them  to avoid local minima and to produce efficient homing trajectories. It does  this without object models using only sensory information. This research  demonstrates the use of a large modular architecture on a difficult task."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/ddb30680a691d157187ee1cf9e896d03-Abstract.html,On Stochastic Complexity and Admissible Models for Neural Network Classifiers,Padhraic Smyth,"Given some  training data how  should we  choose a particular network clas(cid:173) sifier  from  a  family  of networks  of different  complexities?  In  this  paper  we  discuss how  the application of stochastic complexity theory to classifier  design problems can provide some insights into this problem.  In particular  we  introduce  the  notion  of admissible  models  whereby  the  complexity  of  models  under consideration is  affected  by  (among other factors)  the class  entropy,  the  amount  of training  data,  and  our  prior  belief.  In  particular  we  discuss the implications of these results with respect to neural architec(cid:173) tures and demonstrate the approach on real data from  a medical diagnosis  task. 
1 
Introduction  and Motivation 
In this paper we  examine in a general sense the application of Minimum Description  Length  (MDL)  techniques to the  problem of selecting a  good  classifier from  a  large  set  of candidate models  or  hypotheses.  Pattern recognition  algorithms  differ  from  more  conventional  statistical  modeling  techniques  in  the  sense  that  they  typically  choose from a very large number of candidate models to describe the available data.  Hence,  the  problem of searching  through  this set of candidate models  is  frequently  a  formidable  one, often  approached in practice  by  the use of greedy  algorithms.  In  this context, techniques which allow us to eliminate portions of the hypothesis space  are of considerable interest.  We  will show  in  this paper that it is  possible to use  the  intrinsic  structure of the  MDL  formalism  to eliminate large  numbers  of candidate  models given  only minimal information about  the  data.  Our results  depend  on the"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/e0c641195b27425bb056ac56f8953d24-Abstract.html,Analog Computation at a Critical Point: A Novel Function for Neuronal Oscillations?,"Leonid Kruglyak, William Bialek","\Ve show that a simple spin system bia.sed at its critical point can en(cid:173) code spatial characteristics of external signals, sHch as the dimensions of  ""objects"" in the visual field. in the temporal correlation functions of indi(cid:173) vidual spins. Qualit.ative arguments suggest that regularly firing neurons  should be described by a planar spin of unit lengt.h. and such XY models  exhibit critical dynamics over a broad range of parameters. \Ve show how  to extract these spins from spike trains and then mea'3ure t.he interaction  Hamilt.onian using simulations of small dusters of cells. Static correla(cid:173) tions among spike trains obtained from simulations of large arrays of cells  are in agreement with the predictions from these Hamiltonians, and dy(cid:173) namic correlat.ions display the predicted encoding of spatial information.  \Ve suggest that this novel representation of object dinwnsions in temporal  correlations may be relevant t.o recent experiment.s on oscillatory neural  firing in the visual cortex."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/e46de7e1bcaaced9a54f1e9d0d2f800d-Abstract.html,Integrated Segmentation and Recognition of Hand-Printed Numerals,"James D. Keeler, David E. Rumelhart, Wee Kheng Leow","Neural  network  algorithms  have  proven  useful  for  recognition  of individ(cid:173) ual,  segmented  characters.  However,  their recognition  accuracy  has  been  limited by  the  accuracy  of the  underlying  segmentation  algorithm.  Con(cid:173) ventional,  rule-based  segmentation  algorithms  encounter  difficulty  if the  characters  are touching, broken,  or noisy.  The problem in these situations  is  that  often  one  cannot  properly  segment  a  character  until  it  is  recog(cid:173) nized yet  one cannot  properly recognize  a  character until it is  segmented.  We present here  a  neural network algorithm that simultaneously segments  and recognizes  in an  integrated  system.  This  algorithm has several  novel  features:  it uses  a supervised learning algorithm (backpropagation), but is  able to take position-independent information as  targets and self-organize  the  activities  of the units  in  a  competitive fashion  to infer the  positional  information.  We  demonstrate  this  ability with  overlapping  hand-printed  numerals."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html,Speech Recognition Using Demi-Syllable Neural Prediction Model,"Ken-ichi Iso, Takao Watanabe","The  Neural  Prediction  Model  is  the  speech  recognition  model  based  on  pattern  prediction  by  multilayer  perceptrons.  Its  effectiveness  was  con(cid:173) firmed  by  the  speaker-independent  digit  recognition  experiments.  This  paper  presents  an  improvement  in  the model  and  its  application  to  large  vocabulary speech recognition,  based on subword units.  The improvement  involves an introduction  of ""backward  prediction,""  which further  improves  the  prediction  accuracy  of the  original  model  with  only  ""forward  predic(cid:173) tion"".  In  application of the model  to  speaker-dependent large vocabulary  speech recognition,  the demi-syllable unit is  used  as  a subword recognition  unit.  Experimental  results  indicated  a  95.2%  recognition  accuracy  for  a  5000  word  test  set  and  the  effectiveness  was  confirmed  for  the  proposed  model  improvement and  the demi-syllable subword units."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/eddea82ad2755b24c4e168c5fc2ebd40-Abstract.html,A Recurrent Neural Network Model of Velocity Storage in the Vestibulo-Ocular Reflex,Thomas J. Anastasio,"A three-layered neural  network model was used to  explore  the  organization of  the  vestibulo-ocular  reflex  (VOR).  The  dynamic  model  was  trained  using  recurrent back-propagation to produce compensatory, long duration eye muscle  motoneuron  outputs  in  response  to  short  duration  vestibular  afferent  head  velocity  inputs.  The  network  learned  to  produce  this  response  prolongation,  known  as  velocity  storage,  by  developing  complex,  lateral  inhibitory  interac(cid:173) tions among the interneurons.  These had the low baseline, long time constant,  rectified  and  skewed  responses  that  are  characteristic  of  real  VOR  inter(cid:173) neurons.  The  model  suggests  that  all  of these  features  are  interrelated  and  result from lateral inhibition. 
1 SIGNAL PROCESSING IN THE VOR  The  VOR  stabilizes  the  visual  image  by  producing  eye  rotations  that  are  nearly  equal  and  opposite  to  head  rotations  (Wilson  and  Melvill  Jones  1979).  The  VOR  utilizes  head  rotational  velocity  signals,  which  originate  in  the  semicircular canal  receptors of  the  inner  ear,  to  control  contractions  of the  extraocular  muscles.  The  reflex  is  coor(cid:173) dinated by brainstem interneurons in the vestibular nuclei (VN),  that relay  signals from  canal afferent sensory neurons to eye muscle motoneurons. 
32 
A Recurrent Neural Network Model of Velocity Storage 
33 
The  VN  intemeurons,  however,  do  more  than just relay  signals.  Among  other func(cid:173) tions,  the  VN neurons process  the  canal afferent signals,  stretching out their time con(cid:173) stants by about four times before transmitting this signal to the motoneurons.  This time  constant  prolongation,  which  is  one  of the  clearest  examples  of signal  processing  in  motor neurophysiology,  has  been  termed velocity  storage  (Raphan  et  al.  1979).  The  neural mechanisms underlying velocity storage, however,  remain unidentified. 
The  VOR is bilaterally  symmetric  (Wilson  and  Melvill Jones  1979).  The semicircular  canals  operate  in  push-pull  pairs,  and  the  extraocular  muscles  are  arranged  in  agonist/antagonist  pairs.  The  VN  are  also  arranged  bilaterally  and  interact  via  in(cid:173) hibitory commissural connections.  The commissures are necessary  for velocity storage,  which is eliminated by cutting the commissures in monkeys (Blair and Gavin  1981). 
When  the  overall  V OR  fails  to  compensate  for  head  rotations,  the visual  image is not  stabilized but moves across the  retina at a velocity  that  is  equal  to  the  amount of VOR  error.  This  'retinal slip'  signal is transmitted back to the  VN,  and is known to  modify  VOR  operation  (Wilson  and  Melvill  Jones  1979).  Thus  the  VOR  can  be  modeled  beautifully as  a  three-layered neural network,  complete with  recurrent connections and  error signal back-propagation at the VN level.  By  modeling  the  VOR  as  a neural  net(cid:173) work,  insight can be gained into the global organization of this reflex. 
Figure  1:  Architecture  of  the  Horizontal  VOR  Neural  Network  Model.  lhc and  rhc,  left and  right  horizontal  canal  afferents;  Ivn  and  rvn,  left  and  right VN  neurons;  lr  and  mr,  lateral  and  medial  rectus  motoneurons  of the left eye.  This  and  all  subsequent  figures  are  redrawn  from  Anastasio  (1991),  with permission."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/eed5af6add95a9a6f1252739b1ad8c24-Abstract.html,The Devil and the Network: What Sparsity Implies to Robustness and Memory,"Sanjay Biswas, Santosh S. Venkatesh","Robustness is a commonly bruited property of neural networks; in particu(cid:173) lar, a folk theorem in neural computation asserts that neural networks-in  contexts with large interconnectivity-continue to function efficiently, al(cid:173) beit with some degradation, in the presence of component damage or loss.  A second folk theorem in such contexts asserts that dense interconnectiv(cid:173) ity between neural elements is a sine qua non for the efficient usage of  resources. These premises are formally examined in this communication  in a setting that invokes the notion of the ""devil"" 1 in the network as an  agent that produces sparsity by snipping connections. 
1 ON REMOVING THE FOLK FROM THE THEOREM 
Robustness in the presence of component damage is a property that is commonly  attributed to neural networks. The content of the following statement embodies  this sentiment. 
Folk Theorem 1: Computation in neural networks is not substantially  affected by damage to network components. 
While such a statement is manifestly not true in general-witness networks with  ""grandmother cells"" where damage to the critical cells fatally impairs the com(cid:173) putational ability of the network-there is anecdotal evidence in support of it in 
1 Well, maybe an imp."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/ef0d3930a7b6c95bd2b32ed45989c61f-Abstract.html,Development and Spatial Structure of Cortical Feature Maps: A Model Study,"Klaus Obermayer, Helge Ritter, Klaus Schulten","K.  Schulten  Beckman -Insti t u te  University  of Illinois  Urbana, IL  61801 
Feature selective cells in  the primary visual cortex of several species are or(cid:173) ganized in hierarchical topographic maps of stimulus features like  ""position  in  visual  space"",  ""orientation""  and"" ocular  dominance"".  In  order  to  un(cid:173) derstand and describe their spatial structure and their development, we  in(cid:173) vestigate a self-organizing neural network model based on the feature map  algorithm.  The  model  explains  map  formation  as  a  dimension-reducing  mapping  from  a  high-dimensional  feature  space  onto  a  two-dimensional  lattice,  such  that ""similarity""  between features  (or feature  combinations)  is  translated  into  ""spatial  proximity""  between  the  corresponding  feature  selective cells.  The model is  able to reproduce several aspects of the spatial  structure of cortical maps  in  the visual  cortex."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/efe937780e95574250dabe07151bdc23-Abstract.html,Connectionist Implementation of a Theory of Generalization,"Roger N. Shepard, Sheila Kannappan","Empirically,  generalization between  a  training  and  a  test  stimulus  falls  off in  close approximation to  an  exponential decay  function  of distance between  the  two stimuli in the ""stimulus space"" obtained by multidimensional scaling.  Math(cid:173) ematically, this result is derivable from  the assumption that an  individual takes  the  training  stimulus  to  belong  to  a  ""consequential""  region  that includes  that  stimulus but is  otherwise of unknown  location, size, and  shape in the stimulus  space (Shepard, 1987).  As the individual gains additional information about the  consequential region-by finding other stimuli to be consequential or nOl-the  theory  predicts  the shape  of the generalization  function  to  change  toward  the  function relating actual probability of the consequence to location in the stimulus  space.  This paper describes a natural connectionist implementation of the theory,  and illustrates how implications of the theory for generalization, discrimination,  and classification learning can be explored by connectionist simulation. 
1  THE THEORY OF GENERALIZATION 
Because we never confront exactly the same situation twice, anything we have learned in  any previous situation can guide us in deciding which action to take in the present situation  only  to  the  extent  that  the  similarity  between  the  two  situations  is  sufficient  to  justify  generalization of our previous learning to the present situation.  Accordingly, principles of  generalization must be foundational for any theory of behavior. 
In  Shepard  (1987)  nonarbitrary principles  of generalization  were  sought that  would  be  optimum in any world in which an object, however distinct from other objects, is generally  a  member of some class or natural kind  sharing some dispositional property of potential  consequence for the individual.  A newly encountered plant or animal  might be edible or"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/f2fc990265c712c49d51a18a32b39f0c-Abstract.html,Discrete Affine Wavelet Transforms For Anaylsis And Synthesis Of Feedfoward Neural Networks,"Y. C. Pati, P. S. Krishnaprasad",In this paper we show that discrete affine wavelet transforms can provide  a tool for the analysis and synthesis of standard feedforward neural net(cid:173) works. It is shown that wavelet frames for L2(IR) can be constructed based  upon sigmoids. The spatia-spectral localization property of wavelets can  be exploited in defining the topology and determining the weights of a  feedforward network. Training a network constructed using the synthe(cid:173) sis procedure described here involves minimization of a convex cost func(cid:173) tional and therefore avoids pitfalls inherent in standard backpropagation  algorithms. Extension of these methods to L2(IRN) is also discussed.
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/f4f6dce2f3a0f9dada0c2b5b66452017-Abstract.html,Convergence of a Neural Network Classifier,"John S. Baras, Anthony LaVigna","In  this  paper,  we  prove  that  the  vectors  in  the  LVQ  learning  algorithm  converge.  We  do  this  by  showing  that  the  learning  algorithm  performs  stochastic  approximation.  Convergence  is  then  obtained  by  identifying  the  appropriate  conditions  on  the  learning  rate  and  on  the  underlying  statistics of the  classification  problem.  We  also  present  a  modification  to  the  learning algorithm which  we  argue  results  in  convergence  of the  LVQ  error to the Bayesian optimal error as  the appropriate parameters become  large."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/f61d6947467ccd3aa5af24db320235dd-Abstract.html,Optimal Sampling of Natural Images: A Design Principle for the Visual System,"William Bialek, Daniel L. Ruderman, A. Zee",We formulate the problem of optimizing the sampling of natural images  using an array of linear filters. Optimization of information capacity is  constrained by the noise levels of the individual channels and by a penalty  for the construction of long-range interconnections in the array. At low  signal-to-noise ratios the optimal filter characteristics correspond to bound  states of a Schrodinger equation in which the signal spectrum plays the  role of the potential. The resulting optimal filters are remarkably similar  to those observed in the mammalian visual cortex and the retinal ganglion  cells of lower vertebrates. The observed scale invariance of natural images  plays an essential role in this construction.
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/f73b76ce8949fe29bf2a537cfa420e8f-Abstract.html,Time Trials on Second-Order and Variable-Learning-Rate Algorithms,Richard Rohwer,"The performance of seven minimization algorithms are compared on five  neural network problems. These include a variable-step-size algorithm,  conjugate gradient, and several methods with explicit analytic or numerical  approximations to the Hessian."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/f74909ace68e51891440e4da0b65a70c-Abstract.html,A competitive modular connectionist architecture,"Robert A. Jacobs, Michael I. Jordan","We describe a multi-network, or modular, connectionist architecture that  captures that fact that many tasks have structure at a level of granularity  intermediate to that assumed by local and global function approximation  schemes. The main innovation of the architecture is that it combines  associative and competitive learning in order to learn task decompositions.  A task decomposition is discovered by forcing the networks comprising the  architecture to compete to learn the training patterns. As a result of the  competition, different networks learn different training patterns and, thus,  learn to partition the input space. The performance of the architecture on  a ""what"" and ""where"" vision task and on a multi-payload robotics task  are presented."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/f85454e8279be180185cac7d243c5eb3-Abstract.html,A Model of Distributed Sensorimotor Control in the Cockroach Escape Turn,"R.D. Beer, G. J. Kacmarcik, R.E. Ritzmann, H.J. Chiel","In response to a puff of wind, the American cockroach turns away and runs.  The circuit underlying the initial turn of this escape response consists of  three populations of individually identifiable nerve cells and appears to em(cid:173) ploy distributed representations in its operation. We have reconstructed  several neuronal and behavioral properties of this system using simplified  neural network models and the backpropagation learning algorithm con(cid:173) strained by known structural characteristics of the circuitry. In order to  test and refine the model, we have also compared the model's responses to  various lesions with the insect's responses to similar lesions."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/f8c1f23d6a8d8d7904fc0ea8e066b3bb-Abstract.html,A VLSI Neural Network for Color Constancy,"Andrew W. Moore, John Allman, Geoffrey Fox, Rodney Goodman","A system for color correction has been designed, built, and tested suc(cid:173) cessfully; the essential components are three custom chips built using sub(cid:173) threshold analog CMOS VLSI. The system, based on Land's Retinex the(cid:173) ory of color constancy, produces colors similar in many respects to those  produced by the visual system. Resistive grids implemented in analog  VLSI perform the smoothing operation central to the algorithm at video  rates. With the electronic system, the strengths and weaknesses of the  algorithm are explored. 
1 A MODEL FOR COLOR CONSTANCY 
Humans have the remarkable ability to perceive object colors as roughly constant  even if the color of the illumination is varied widely. Edwin Land, founder of the  Polaroid Corporation, models the computation that results in this ability as three  identical center-surround operations performed independently in three color planes,  such as red, green, and blue (Land, 1986). The basis for this model is as follows. 
Consider first an array of grey papers with different reflectances. (Land designated  these arrays Mondrians, since they resemble the works of the Dutch painter Piet 
·Present address: Dept. of Physics, Syracuse University, Syracuse, NY 13244"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/f9b902fc3289af4dd08de5d1de54f68f-Abstract.html,Stereopsis by a Neural Network Which Learns the Constraints,"Alireza Khotanzad, Ying-Wung Lee",This paper presents a neural network (NN) approach to the problem of  stereopsis. The correspondence problem (finding the correct matches  between the pixels of the epipolar lines of the stereo pair from amongst all  the possible matches) is posed as a non-iterative many-to-one mapping . A  two-layer feed forward NN architecture is developed to learn and code this  nonlinear and complex mapping using the back-propagation learning rule  and a training set. The important aspect of this technique is that none of  the typical constraints such as uniqueness and continuity are explicitly  imposed. All the applicable constraints are learned and internally coded  by the NN enabling it to be more flexible and more accurate than the  existing methods. The approach is successfully tested on several random(cid:173) dot stereograms. It is shown that the net can generalize its learned map(cid:173) ping to cases outside its training set. Advantages over the Marr-Poggio  Algorithm are discussed and it is shown that the NN performance is supe(cid:173) rIOr.
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/faa9afea49ef2ff029a833cccc778fd0-Abstract.html,The Tempo 2 Algorithm: Adjusting Time-Delays By Supervised Learning,"Ulrich Bodenhausen, Alex Waibel","In this work we describe a new method that adjusts time-delays and the widths of  time-windows in artificial neural networks automatically.  The input of the units  are weighted by a gaussian input-window over time which allows the learning  rules for the delays and widths to be derived in the same way as it is used for the  weights.  Our results on a phoneme classification task compare well with results  obtained with the TDNN by Waibel et al., which was manually optimized for the  same task."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/fb7b9ffa5462084c5f4e7e85a093e6d7-Abstract.html,Learning Theory and Experiments with Competitive Networks,"Griff L. Bilbro, David E. van den Bout","We  apply  the  theory  of Tishby,  Levin,  and Sol1a  (TLS)  to two problems.  First  we analyze  an elementary problem for  which we find  the predictions  consistent  with  conventional  statistical  results.  Second  we  numerically  examine  the  more realistic  problem of training a  competitive net  to learn  a  probability  density  from  samples.  We  find  TLS  useful  for  predicting  average  training  behavior. 
. 
1  TLS  APPLIED  TO  LEARNING DENSITIES 
Recently  a  theory  of learning  has  been  constructed  which  describes  the  learning  of a  relation  from  examples  (Tishby, Levin,  and Sol1a,  1989),  (Schwarb, Samalan,  Sol1a,  and Denker,  1990).  The original derivation  relies  on  a  statistical  mechanics  treatment  of the  probability  of independent  events  in  a  system  with  a  specified  average value of an additive error function. 
The resulting  theory is  not restricted  to learning  relations  and it is  not  essentially  statistical  mechanical.  The TLS  theory  can be derived  from  the  principle  of maz(cid:173) imum  entropy,  a  general  inference  tool which  produces  probabilities  characterized  by certain values of the averages of specified functions(Jaynes,  1979).  A TLS theory  can be  constructed  whenever  the specified  function  is  additive and associated  with  independent examples.  In this paper we treat the problem of learning a  probability  density from samples.  Consider the model as some function p( z Iw) of fixed form and adjustable parameters  w  which are  to be chosen  to approximate 1'(z)  where  the overline denotes  the  true  density.  All we  know about l' are the elements of a  training set T  which are drawn"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/fccb60fb512d13df5083790d64c4d5dd-Abstract.html,Further Studies of a Model for the Development and Regeneration of Eye-Brain Maps,"Jack D. Cowan, A. E. Friedman","We describe a computational model of the development and regenera(cid:173) tion of specific eye-brain circuits. The model comprises a self-organiz(cid:173) ing map-forming network which uses local Hebb rules, constrained by  (genetically determined) molecular markers. Various simulations of  the development and regeneration of eye-brain maps in fish and frogs  are described, in particular successful simulations of experiments by  Schmidt-Cicerone-Easter; Meyer; and Y oon."
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/fe73f687e5bc5280214e0486b273a5f9-Abstract.html,The Recurrent Cascade-Correlation Architecture,Scott E. Fahlman,"Recurrent  Cascade-Correlation  CRCC)  is  a recurrent  version  of the  Cascade(cid:173) Correlation learning architecture of Fah I man and Lebiere [Fahlman, 1990].  RCC  can learn from examples to map a sequence of inputs into a desired sequence of  outputs.  New hidden units with recurrent connections are added to the network  as needed during training.  In effect, the network builds up a finite-state machine  tailored  specifically  for  the current problem.  RCC  retains  the  advantages  of  Cascade-Correlation:  fast learning, good generalization, automatic construction  of a near-minimal multi-layered network, and incremental training. 
1  THE ARCHITECTURE 
Cascade-Correlation  [Fahlman, 1990]  is  a supervised  learning  architecture  that builds  a  near-minimal multi-layer network topology in the course of training.  Initially the network  contains only inputs, output units, and the connections between them.  This single layer of  connections is trained (using the Quickprop algorithm [Fahlman, 1988]) to minimize the  error.  When no further improvement is seen in the level of error, the network's performance  is evaluated.  If the error is small enough, we stop.  Otherwise we add a new hidden unit to  the network in an attempt to reduce the residual error.  To create a new hidden unit, we begin with a pool of candidate units, each of which receives  weighted connections from the network's inputs and from any hidden units already present  in the net.  The outputs of these candidate units are not yet connected into the active network.  Multiple passes through the training set are run, and each candidate unit adjusts its incoming  weights to maximize the correlation between its output and the residual error in the active  net.  When the correlation scores stop improving, we choose the best candidate, freeze  its  incoming weights, and add it to the network.  This process is called ""tenure.""  After tenure,"
1990,https://papers.nips.cc/paper_files/paper/1990,https://papers.nips.cc/paper_files/paper/1990/hash/ffd52f3c7e12435a724a8f30fddadd9c-Abstract.html,Sequential Adaptation of Radial Basis Function Neural Networks and its Application to Time-series Prediction,"V. Kadirkamanathan, M. Niranjan, F. Fallside","We develop a sequential adaptation algorithm for radial basis function  (RBF) neural networks of Gaussian nodes, based on the method of succes(cid:173) sive F-Projections. This method makes use of each observation efficiently  in that the network mapping function so obtained is consistent with that  information and is also optimal in the least L 2-norm sense. The RBF  network with the F-Projections adaptation algorithm was used for pre(cid:173) dicting a chaotic time-series. We compare its performance to an adapta(cid:173) tion scheme based on the method of stochastic approximation, and show  that the F-Projections algorithm converges to the underlying model much  faster."
