year,proceeding_link,paper_link,title,authors,abstract
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/0172d289da48c48de8c5ebf3de9f7ee1-Abstract.html,Bayesian Surprise Attracts Human Attention,"Laurent Itti, Pierre F. Baldi","The concept of surprise is central to sensory processing, adaptation, learning, and attention. Yet, no widely-accepted mathematical theory currently exists to quantitatively characterize surprise elicited by a stimulus or event, for observers that range from single neurons to complex natural or engineered systems. We describe a formal Bayesian definition of surprise that is the only consistent formulation under minimal axiomatic assumptions. Surprise quantifies how data affects a natural or artificial observer, by measuring the difference between posterior and prior beliefs of the observer. Using this framework we measure the extent to which humans direct their gaze towards surprising items while watching television and video games. We find that subjects are strongly attracted towards surprising locations, with 72% of all human gaze shifts directed towards locations more surprising than the average, a figure which rises to 84% when considering only gaze targets simultaneously selected by all subjects. The resulting theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction. Life is full of surprises, ranging from a great christmas gift or a new magic trick, to wardrobe malfunctions, reckless drivers, terrorist attacks, and tsunami waves. Key to survival is our ability to rapidly attend to, identify, and learn from surprising events, to decide on present and future courses of action [1]. Yet, little theoretical and computational understanding exists of the very essence of surprise, as evidenced by the absence from our everyday vocabulary of a quantitative unit of surprise: Qualities such as the ""wow factor"" have remained vague and elusive to mathematical analysis. Informal correlates of surprise exist at nearly all stages of neural processing. In sensory neuroscience, it has been suggested that only the unexpected at one stage is transmitted to the next stage [2]. Hence, sensory cortex may have evolved to adapt to, to predict, and to quiet down the expected statistical regularities of the world [3, 4, 5, 6], focusing instead on events that are unpredictable or surprising. Electrophysiological evidence for this early sensory emphasis onto surprising stimuli exists from studies of adaptation in visual [7, 8, 4, 9], olfactory [10, 11], and auditory cortices [12], subcortical structures like the LGN [13], and even retinal ganglion cells [14, 15] and cochlear hair cells [16]: neural response greatly attenuates with repeated or prolonged exposure to an initially novel stimulus. Surprise and novelty are also central to learning and memory formation [1], to the point that surprise is believed to be a necessary trigger for associative learning [17, 18], 
as supported by mounting evidence for a role of the hippocampus as a novelty detector [19, 20, 21]. Finally, seeking novelty is a well-identified human character trait, with possible association with the dopamine D4 receptor gene [22, 23, 24]. In the Bayesian framework, we develop the only consistent theory of surprise, in terms of the difference between the posterior and prior distributions of beliefs of an observer over the available class of models or hypotheses about the world. We show that this definition derived from first principles presents key advantages over more ad-hoc formulations, typically relying on detecting outlier stimuli. Armed with this new framework, we provide direct experimental evidence that surprise best characterizes what attracts human gaze in large amounts of natural video stimuli. We here extend a recent pilot study [25], adding more comprehensive theory, large-scale human data collection, and additional analysis."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/02180771a9b609a26dcea07f272e141f-Abstract.html,On Local Rewards and Scaling Distributed Reinforcement Learning,"Drew Bagnell, Andrew Y. Ng","We consider the scaling of the number of examples necessary to achieve good performance in distributed, cooperative, multi-agent reinforcement learning, as a function of the the number of agents n. We prove a worstcase lower bound showing that algorithms that rely solely on a global reward signal to learn policies confront a fundamental limit: They require a number of real-world examples that scales roughly linearly in the number of agents. For settings of interest with a very large number of agents, this is impractical. We demonstrate, however, that there is a class of algorithms that, by taking advantage of local reward signals in large distributed Markov Decision Processes, are able to ensure good performance with a number of samples that scales as O(log n). This makes them applicable even in settings with a very large number of agents n."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/030e65da2b1c944090548d36b244b28d-Abstract.html,Learning Shared Latent Structure for Image Synthesis and Robotic Imitation,"Aaron Shon, Keith Grochow, Aaron Hertzmann, Rajesh P. Rao","We propose an algorithm that uses Gaussian process regression to learn common hidden structure shared between corresponding sets of heterogenous observations. The observation spaces are linked via a single, reduced-dimensionality latent variable space. We present results from two datasets demonstrating the algorithms's ability to synthesize novel data from learned correspondences. We first show that the method can learn the nonlinear mapping between corresponding views of objects, filling in missing data as needed to synthesize novel views. We then show that the method can learn a mapping between human degrees of freedom and robotic degrees of freedom for a humanoid robot, allowing robotic imitation of human poses from motion capture data."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/044a23cadb567653eb51d4eb40acaa88-Abstract.html,Recovery of Jointly Sparse Signals from Few Random Projections,"Michael B. Wakin, Marco F. Duarte, Shriram Sarvotham, Dror Baron, Richard G. Baraniuk","Compressed sensing is an emerging Ô¨Åeld based on the revelation that a small group of linear projections of a sparse signal contains enough information for reconstruc- tion. In this paper we introduce a new theory for distributed compressed sensing (DCS) that enables new distributed coding algorithms for multi-signal ensembles that exploit both intra- and inter-signal correlation structures. The DCS theory rests on a new concept that we term the joint sparsity of a signal ensemble. We study three simple models for jointly sparse signals, propose algorithms for joint recov- ery of multiple signals from incoherent projections, and characterize theoretically and empirically the number of measurements per sensor required for accurate re- construction. In some sense DCS is a framework for distributed compression of sources with memory, which has remained a challenging problem in information theory for some time. DCS is immediately applicable to a range of problems in sensor networks and arrays."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/045cf83ab0722e782cf72d14e44adf98-Abstract.html,Fixing two weaknesses of the Spectral Method,Kevin Lang,"We discuss two intrinsic weaknesses of the spectral graph partitioning method, both of which have practical consequences. The first is that spectral embeddings tend to hide the best cuts from the commonly used hyperplane rounding method. Rather than cleaning up the resulting suboptimal cuts with local search, we recommend the adoption of flow-based rounding. The second weakness is that for many ""power law"" graphs, the spectral method produces cuts that are highly unbalanced, thus decreasing the usefulness of the method for visualization (see figure 4(b)) or as a basis for divide-and-conquer algorithms. These balance problems, which occur even though the spectral method's quotient-style objective function does encourage balance, can be fixed with a stricter balance constraint that turns the spectral mathematical program into an SDP that can be solved for million-node graphs by a method of Burer and Monteiro."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/0738069b244a1c43c83112b735140a16-Abstract.html,Saliency Based on Information Maximization,"Neil Bruce, John Tsotsos","A model of bottom-up overt attention is proposed based on the principle  of maximizing information sampled from  a scene.  The proposed opera(cid:173) tion  is  based on  Shannon's self-information  measure and is  achieved in  a neural circuit, which is demonstrated as having close ties with  the cir(cid:173) cuitry existent in  the primate visual cortex.  It is  further shown  that the  proposed saliency measure may  be extended  to  address  issues that cur(cid:173) rently elude explanation in the domain of saliency based models.  Results  on  natural  images are compared with experimental eye tracking data re(cid:173) vealing  the  efficacy of the  model  in  predicting  the  deployment of overt  attention as compared with existing efforts."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/09b69adcd7cbae914c6204984097d2da-Abstract.html,"Learning vehicular dynamics, with application to modeling helicopters","Pieter Abbeel, Varun Ganapathi, Andrew Y. Ng","We consider the problem of modeling a helicopter's dynamics based on state-action trajectories collected from it. The contribution of this paper is two-fold. First, we consider the linear models such as learned by C I F E R (the industry standard in helicopter identification), and show that the linear parameterization makes certain properties of dynamical systems, such as inertia, fundamentally difficult to capture. We propose an alternative, acceleration based, parameterization that does not suffer from this deficiency, and that can be learned as efficiently from data. Second, a Markov decision process model of a helicopter's dynamics would explicitly model only the one-step transitions, but we are often interested in a model's predictive performance over longer timescales. In this paper, we present an efficient algorithm for (approximately) minimizing the prediction error over long time scales. We present empirical results on two different helicopters. Although this work was motivated by the problem of modeling helicopters, the ideas presented here are general, and can be applied to modeling large classes of vehicular dynamics."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/0c1c995b77ea7312f887ddd9f9d35de5-Abstract.html,Active Learning for Misspecified Models,Masashi Sugiyama,"expressed as
Active learning is the problem in supervised learning to design the loca- tions of training input points so that the generalization error is minimized. Existing active learning methods often assume that the model used for learning is correctly speciÔ¨Åed, i.e., the learning target function can be ex- pressed by the model at hand. In many practical situations, however, this assumption may not be fulÔ¨Ålled. In this paper, we Ô¨Årst show that the ex- isting active learning method can be theoretically justiÔ¨Åed under slightly weaker condition: the model does not have to be correctly speciÔ¨Åed, but slightly misspeciÔ¨Åed models are also allowed. However, it turns out that the weakened condition is still restrictive in practice. To cope with this problem, we propose an alternative active learning method which can be theoretically justiÔ¨Åed for a wider class of misspeciÔ¨Åed models. Thus, the proposed method has a broader range of applications than the exist- ing method. Numerical studies show that the proposed active learning method is robust against the misspeciÔ¨Åcation of models and is thus reli- able.
Let us discuss the regression problem of learning a real-valued functionfx deÔ¨Åned on Rd from training examplesfxi;yijyi=fxi(cid:15)igi=1; wheref(cid:15)igi=1 are i.i.d. noise with mean zero and unknown variance(cid:27)2 bfx=Xi=1(cid:11)iix; wherefixgi=1 are Ô¨Åxed linearly independent functions and(cid:11)=(cid:11)1;(cid:11)2;:::;(cid:11)> We evaluate the goodness of the learned functionbfx by the expected squared test error are drawn independently from a distribution with densityx, the generalization error is G=E(cid:15)Z(cid:16)bfxfx(cid:17)2xdx;
over test input points and noise (i.e., the generalization error). When the test input points
1
Introduction and Problem Formulation
are parameters to be learned.
lowing linear regression model for learning."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/0d9095b0d6bbe98ea0c9c02b11b59ee3-Abstract.html,Identifying Distributed Object Representations in Human Extrastriate Visual Cortex,"Rory Sayres, David Ress, Kalanit Grill-spector","The category of visual stimuli has been reliably decoded from patterns of neural activity in extrastriate visual cortex [1]. It has yet to be seen whether object identity can be inferred from this activity. We present fMRI data measuring responses in human extrastriate cortex to a set of 12 distinct object images. We use a simple winner-take-all classifier, using half the data from each recording session as a training set, to evaluate encoding of object identity across fMRI voxels. Since this approach is sensitive to the inclusion of noisy voxels, we describe two methods for identifying subsets of voxels in the data which optimally distinguish object identity. One method characterizes the reliability of each voxel within subsets of the data, while another estimates the mutual information of each voxel with the stimulus set. We find that both metrics can identify subsets of the data which reliably encode object identity, even when noisy measurements are artificially added to the data. The mutual information metric is less efficient at this task, likely due to constraints in fMRI data."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/0fc170ecbb8ff1afb2c6de48ea5343e7-Abstract.html,Convex Neural Networks,"Yoshua Bengio, Nicolas L. Roux, Pascal Vincent, Olivier Delalleau, Patrice Marcotte","Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artificial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an infinite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time finding a linear classifier that minimizes a weighted sum of errors."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/12311d05c9aa67765703984239511212-Abstract.html,Temporal Abstraction in Temporal-difference Networks,"Eddie Rafols, Anna Koop, Richard S. Sutton","We present a generalization of temporal-difference networks to include temporally abstract options on the links of the question network. Temporal-difference (TD) networks have been proposed as a way of representing and learning a wide variety of predictions about the interaction between an agent and its environment. These predictions are compositional in that their targets are defined in terms of other predictions, and subjunctive in that that they are about what would happen if an action or sequence of actions were taken. In conventional TD networks, the inter-related predictions are at successive time steps and contingent on a single action; here we generalize them to accommodate extended time intervals and contingency on whole ways of behaving. Our generalization is based on the options framework for temporal abstraction. The primary contribution of this paper is to introduce a new algorithm for intra-option learning in TD networks with function approximation and eligibility traces. We present empirical examples of our algorithm's effectiveness and of the greater representational expressiveness of temporallyabstract TD networks. The primary distinguishing feature of temporal-difference (TD) networks (Sutton & Tanner, 2005) is that they permit a general compositional specification of the goals of learning. The goals of learning are thought of as predictive questions being asked by the agent in the learning problem, such as ""What will I see if I step forward and look right?"" or ""If I open the fridge, will I see a bottle of beer?"" Seeing a bottle of beer is of course a complicated perceptual act. It might be thought of as obtaining a set of predictions about what would happen if certain reaching and grasping actions were taken, about what would happen if the bottle were opened and turned upside down, and of what the bottle would look like if viewed from various angles. To predict seeing a bottle of beer is thus to make a prediction about a set of other predictions. The target for the overall prediction is a composition in the mathematical sense of the first prediction with each of the other predictions. TD networks are the first framework for representing the goals of predictive learning in a compositional, machine-accessible form. Each node of a TD network represents an individual question--something to be predicted--and has associated with it a value representing an answer to the question--a prediction of that something. The questions are represented by a set of directed links between nodes. If node 1 is linked to node 2, then node 1 rep-
resents a question incorporating node 2's question; its value is a prediction about node 2's prediction. Higher-level predictions can be composed in several ways from lower ones, producing a powerful, structured representation language for the targets of learning. The compositional structure is not just in a human designer's head; it is expressed in the links and thus is accessible to the agent and its learning algorithm. The network of these links is referred to as the question network. An entirely separate set of directed links between the nodes is used to compute the values (predictions, answers) associated with each node. These links collectively are referred to as the answer network. The computation in the answer network is compositional in a conventional way--node values are computed from other node values. The essential insight of TD networks is that the notion of compositionality should apply to questions as well as to answers. A secondary distinguishing feature of TD networks is that the predictions (node values) at each moment in time can be used as a representation of the state of the world at that time. In this way they are an instance of the idea of predictive state representations (PSRs) introduced by Littman, Sutton and Singh (2002), Jaeger (2000), and Rivest and Schapire (1987). Representing a state by its predictions is a potentially powerful strategy for state abstraction (Rafols et al., 2005). We note that the questions used in all previous work with PSRs are defined in terms of concrete actions and observations, not other predictions. They are not compositional in the sense that TD-network questions are. The questions we have discussed so far are subjunctive, meaning that they are conditional on a certain way of behaving. We predict what we would see if we were to step forward and look right, or if we were to open the fridge. The questions in conventional TD networks are subjunctive, but they are conditional only on primitive actions or open-loop sequences of primitive actions (as are conventional PSRs). It is natural to generalize this, as we have in the informal examples above, to questions that are conditional on closed-loop temporally extended ways of behaving. For example, opening the fridge is a complex, high-level action. The arm must be lifted to the door, the hand shaped for grasping the handle, etc. To ask questions like ""if I were to go to the coffee room, would I see John?"" would require substantial temporal abstraction in addition to state abstraction. The options framework (Sutton, Precup & Singh, 1999) is a straightforward way of talking about temporally extended ways of behaving and about predictions of their outcomes. In this paper we extend the options framework so that it can be applied to TD networks. Significant extensions of the original options framework are needed. Novel features of our option-extended TD networks are that they 1) predict components of option outcomes rather than full outcome probability distributions, 2) learn according to the first intra-option method to use eligibility traces (see Sutton & Barto, 1998), and 3) include the possibility of options whose `policies' are indifferent to which of several actions are selected.
1
The options framework
In this section we present the essential elements of the options framework (Sutton, Precup & Singh, 1999) that we will need for our extension of TD networks. In this framework, an agent and an environment interact at discrete time steps t = 1, 2, 3.... In each state st  S , the agent selects an action at  A, determining the next state st+1 .1 An action is a way of behaving for one time step; the options framework lets us talk about temporally extended ways of behaving. An individual option consists of three parts. The first is the initiation set, I  S , the subset of states in which the option can be started. The second component of an option is its policy,  : S  A  [0, 1], specifying how the agent behaves when 1 Although the options framework includes rewards, we omit them here because we are concerned only with prediction, not control.
following the option. Finally, a termination function,  : S  A  [0, 1], specifies how the option ends:  (s) denotes the probability of terminating when in state s. The option is thus completely and formally defined by the 3-tuple (I ,  ,  ).
2
Conventional TD networks
In this section we briefly present the details of the structure and the learning algorithm comprising TD networks as introduced by Sutton and Tanner (2005). TD networks address a prediction problem in which the agent may not have direct access to the state of the environment. Instead, at each time step the agent receives an observation ot  O dependent on the state. The experience stream thus consists of a sequence of alternating actions and observations, o1 , a1 , o2 , a2 , o3   . The TD network consists of a set of nodes, each representing a single scalar prediction, interlinked by the question and answer networks as suggested previously. For a network 1 n of n nodes, the vector of all predictions at time step t is denoted yt = (yt , . . . , yt )T . The predictions are estimates of the expected value of some scalar quantity, typically of a bit, in which case they can be interpreted as estimates of probabilities. The predictions are updated at each time step according to a vector-valued function u with modifiable parameter W, which is often taken to be of a linear form: yt = u(yt-1 , at-1 , ot , Wt ) =  (Wt xt ), (1) where xt  m is an m-vector of features created from (yt-1 , at-1 , ot ), Wt is an n  m matrix (whose elements are sometimes referred to as weights), and  is the n-vector form of either the identity function or the S-shaped logistic function  (s) = 1+1 -s . The e feature vector is an arbitrary vector-valued function of yt-1 , at-1 , and ot . For example, in the simplest case the feature vector is a unit basis vector with the location of the one communicating the current state. In a partially observable environment, the feature vector may be a combination of the agent's action, observations, and predictions from the previous time step. The overall update u defines the answer network. The question network consists of a set of target functions, z i : O  n  , and condition i y functions, ci : A  n  [0, 1]n . We define zt = z i (ot+1 , ~t+1 ) as the target for prediction i2 i i yt . Similarly, we define ct = c (at , yt ) as the condition at time t. The learning algorithm i for each component wtj of Wt can then be written i zi c  yt ij i i wt+1 = wtj +  t - yt i , (2) t i  wt j where  is a positive step-size parameter. Note that the targets here are functions of the observation and predictions exactly one time step later, and that the conditions are functions of a single primitive action. This is what makes this algorithm suitable only for learning about one-step TD relationships. By chaining together multiple nodes, Sutton and Tanner (2005) used it to predict k steps ahead, for various particular values of k , and to predict the outcome of specific action sequences (as in PSRs, e.g., Littman et al., 2002; Singh et al., 2004). Now we consider the extension to temporally abstract actions.
3
Option-extended TD networks
In this section we present our intra-option learning algorithm for TD networks with options and eligibility traces. As suggested earlier, each node's outgoing link in the question The quantity ~ is almost the same as y, and we encourage the reader to think of them as identical y here. The difference is that ~ is calculated by weights that are one step out of date as compared to y, y i.e., ~t = u(yt-1 , at-1 , ot , Wt-1 ) (cf. equation 1). y 2
network will now correspond to an option applying over possibly many steps. The policy of the ith node's option corresponds to the condition function ci , which we think of as a recognizer for the option. It inspects each action taken to assess whether the option is being followed: ci = 1 if the agent is acting consistently with the option policy and ci = 0 othert t wise (intermediate values are also possible). When an agent ceases to act consistently with the option policy, we say that the option has diverged. The possibility of recognizing more than one action as consistent with the option is a significant generalization of the original idea of options. If no actions are recognized as acceptable in a state, then the option cannot be followed and thus cannot be initiated. Here we take the set of states with at least one recognized action to be the initiation set of the option. The option-termination function  generalizes naturally to TD networks. Each node i is i given a corresponding termination function,  i : O  n  [0, 1], where t =  i (ot+1 , yt ) i is the probability of terminating at time t.3 t = 1 indicates that the option has terminated i at time t; t = 0 indicates that it has not, and intermediate values of  correspond to soft i or stochastic termination conditions. If an option terminates, then zt acts as the target, but if the option is ongoing without termination, then the node's own next value, yt+1 , should ~i be the target. The termination function specifies which of the two targets (or mixture of the two targets) is used to produce a form of TD error for each node i: i ii ii i t = t zt + (1 - t )yt+1 - yt . ~
(3)
Our option-extended algorithm incorporates eligibility traces (see Sutton & Barto, 1998) as short-term memory variables organized in an n  m matrix E, paralleling the weight matrix. The traces are a record of the effect that each weight could have had on each node's prediction during the time the agent has been acting consistently with the node's option. The components eij of the eligibility matrix are updated by  , i  yt ij ij i i (4) et = ct et-1 (1 - t ) + i  wt j where 0    1 is the trace-decay parameter familiar from the TD() learning algorithm. Because of the ci factor, all of a node's traces will be immediately reset to zero whenever t the agent deviates from the node's option's policy. If the agent follows the policy and the option does not terminate, then the trace decays by  and increments by the gradient in the way typical of eligibility traces. If the policy is followed and the option does terminate, then the trace will be reset to zero on the immediately following time step, and a new trace will start building. Finally, our algorithm updates the weights on each time step by ij i i wt+1 = wtj +  t eij . t"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/1264a061d82a2edae1574b07249800d6-Abstract.html,Robust Fisher Discriminant Analysis,"Seung-jean Kim, Alessandro Magnani, Stephen Boyd","Fisher linear discriminant analysis (LDA) can be sensitive to the problem data. Robust Fisher LDA can systematically alleviate the sensitivity problem by explicitly incorporating a model of data uncertainty in a classification problem and optimizing for the worst-case scenario under this model. The main contribution of this paper is show that with general convex uncertainty models on the problem data, robust Fisher LDA can be carried out using convex optimization. For a certain type of product form uncertainty model, robust Fisher LDA can be carried out at a cost comparable to standard Fisher LDA. The method is demonstrated with some numerical examples. Finally, we show how to extend these results to robust kernel Fisher discriminant analysis, i.e., robust Fisher LDA in a high dimensional feature space."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/12a1d073d5ed3fa12169c67c4e2ce415-Abstract.html,Measuring Shared Information and Coordinated Activity in Neuronal Networks,"Kristina Klinkner, Cosma Shalizi, Marcelo Camperi","Most nervous systems encode information about stimuli in the responding activity of large neuronal networks. This activity often manifests itself as dynamically coordinated sequences of action potentials. Since multiple electrode recordings are now a standard tool in neuroscience research, it is important to have a measure of such network-wide behavioral coordination and information sharing, applicable to multiple neural spike train data. We propose a new statistic, informational coherence, which measures how much better one unit can be predicted by knowing the dynamical state of another. We argue informational coherence is a measure of association and shared information which is superior to traditional pairwise measures of synchronization and correlation. To find the dynamical states, we use a recently-introduced algorithm which reconstructs effective state spaces from stochastic time series. We then extend the pairwise measure to a multivariate analysis of the network by estimating the network multi-information. We illustrate our method by testing it on a detailed model of the transition from gamma to beta rhythms. Much of the most important information in neural systems is shared over multiple neurons or cortical areas, in such forms as population codes and distributed representations [1]. On behavioral time scales, neural information is stored in temporal patterns of activity as opposed to static markers; therefore, as information is shared between neurons or brain regions, it is physically instantiated as coordination between entire sequences of neural spikes. Furthermore, neural systems and regions of the brain often require coordinated neural activity to perform important functions; acting in concert requires multiple neurons or cortical areas to share information [2]. Thus, if we want to measure the dynamic network-wide behavior of neurons and test hypotheses about them, we need reliable, practical methods to detect and quantify behavioral coordination and the associated information sharing across multiple neural units. These would be especially useful in testing ideas about how particular forms of coordination relate to distributed coding (e.g., that of [3]). Current techniques to analyze relations among spike trains handle only pairs of neurons, so we further need a method which is extendible to analyze the coordination in the network, system, or region as a whole. Here we propose a new measure of behavioral coordination and information sharing, informational coherence, based on the notion of dynamical state. Section 1 argues that coordinated behavior in neural systems is often not captured by exist-
ing measures of synchronization or correlation, and that something sensitive to nonlinear, stochastic, predictive relationships is needed. Section 2 defines informational coherence as the (normalized) mutual information between the dynamical states of two systems and explains how looking at the states, rather than just observables, fulfills the needs laid out in Section 1. Since we rarely know the right states a prori, Section 2.1 briefly describes how we reconstruct effective state spaces from data. Section 2.2 gives some details about how we calculate the informational coherence and approximate the global information stored in the network. Section 3 applies our method to a model system (a biophysically detailed conductance-based model) comparing our results to those of more familiar second-order statistics. In the interest of space, we omit proofs and a full discussion of the existing literature, giving only minimal references here; proofs and references will appear in a longer paper now in preparation.
1
Synchrony or Coherence?
Most hypotheses which involve the idea that information sharing is reflected in coordinated activity across neural units invoke a very specific notion of coordinated activity, namely strict synchrony: the units should be doing exactly the same thing (e.g., spiking) at exactly the same time. Investigators then measure coordination by measuring how close the units come to being strictly synchronized (e.g., variance in spike times). From an informational point of view, there is no reason to favor strict synchrony over other kinds of coordination. One neuron consistently spiking 50 ms after another is just as informative a relationship as two simultaneously spiking, but such stable phase relations are missed by strict-synchrony approaches. Indeed, whatever the exact nature of the neural code, it uses temporally extended patterns of activity, and so information sharing should be reflected in coordination of those patterns, rather than just the instantaneous activity. There are three common ways of going beyond strict synchrony: cross-correlation and related second-order statistics, mutual information, and topological generalized synchrony. The cross-correlation function (the normalized covariance function; this includes, for present purposes, the joint peristimulus time histogram [2]), is one of the most widespread measures of synchronization. It can be efficiently calculated from observable series; it handles statistical as well as deterministic relationships between processes; by incorporating variable lags, it reduces the problem of phase locking. Fourier transformation of the covariance function X Y (h) yields the cross-spectrum FX Y ( ), which in turn gives the 2 spectral coherence cX Y ( ) = FX Y ( )/FX ( )FY ( ), a normalized correlation between the Fourier components of X and Y . Integrated over frequencies, the spectral coherence measures, essentially, the degree of linear cross-predictability of the two series. ([4] applies spectral coherence to coordinated neural activity.) However, such second-order statistics only handle linear relationships. Since neural processes are known to be strongly nonlinear, there is little reason to think these statistics adequately measure coordination and synchrony in neural systems. Mutual information is attractive because it handles both nonlinear and stochastic relationships and has a very natural and appealing interpretation. Unfortunately, it often seems to fail in practice, being disappointingly small even between signals which are known to be tightly coupled [5]. The major reason is that the neural codes use distinct patterns of activity over time, rather than many different instantaneous actions, and the usual approach misses these extended patterns. Consider two neurons, one of which drives the other to spike 50 ms after it does, the driving neuron spiking once every 500 ms. These are very tightly coordinated, but whether the first neuron spiked at time t conveys little information about what the second neuron is doing at t -- it's not spiking, but it's not spiking most of the time anyway. Mutual information calculated from the direct observations conflates the
""no spike"" of the second neuron preparing to fire with its just-sitting-around ""no spike"". Here, mutual information could find the coordination if we used a 50 ms lag, but that won't work in general. Take two rate-coding neurons with base-line firing rates of 1 Hz, and suppose that a stimulus excites one to 10 Hz and suppresses the other to 0.1 Hz. The spiking rates thus share a lot of information, but whether the one neuron spiked at t is uninformative about what the other neuron did then, and lagging won't help. Generalized synchrony is based on the idea of establishing relationships between the states of the various units. ""State"" here is taken in the sense of physics, dynamics and control theory: the state at time t is a variable which fixes the distribution of observables at all times  t, rendering the past of the system irrelevant [6]. Knowing the state allows us to predict, as well as possible, how the system will evolve, and how it will respond to external forces [7]. Two coupled systems are said to exhibit generalized synchrony if the state of one system is given by a mapping from the state of the other. Applications to data employ statespace reconstruction [8]: if the state x  X evolves according to smooth, d-dimensional deterministic dynamics, and we observe a generic function y = f (x), then the space Y of time-delay vectors [y (t), y (t -  ), ...y (t - (k - 1) )] is diffeomorphic to X if k > 2d, for generic choices of lag  . The various versions of generalized synchrony differ on how, precisely, to quantify the mappings between reconstructed state spaces, but they all appear to be empirically equivalent to one another and to notions of phase synchronization based on Hilbert transforms [5]. Thus all of these measures accommodate nonlinear relationships, and are potentially very flexible. Unfortunately, there is essentially no reason to believe that neural systems have deterministic dynamics at experimentally-accessible levels of detail, much less that there are deterministic relationships among such states for different units. What we want, then, but none of these alternatives provides, is a quantity which measures predictive relationships among states, but allows those relationships to be nonlinear and stochastic. The next section introduces just such a measure, which we call ""informational coherence"".
2
States and Informational Coherence
There are alternatives to calculating the ""surface"" mutual information between the sequences of observations themselves (which, as described, fails to capture coordination). If we know that the units are phase oscillators, or rate coders, we can estimate their instantaneous phase or rate and, by calculating the mutual information between those variables, see how coordinated the units' patterns of activity are. However, phases and rates do not exhaust the repertoire of neural patterns and a more general, common scheme is desirable. The most general notion of ""pattern of activity"" is simply that of the dynamical state of the system, in the sense mentioned above. We now formalize this. Assuming the usual notation for Shannon information [9], the information content of a state variable X is H [X ] and the mutual information between X and Y is I [X ; Y ]. As is well-known, I [X ; Y ]  min H [X ], H [Y ]. We use this to normalize the mutual state information to a 0 - 1 scale, and this is the informational coherence (IC).  (X, Y ) = I [X ; Y ] , with 0/0 = 0 . min H [X ], H [Y ] (1)
can be interpreted as follows. I [X ; Y ] is the Kullback-Leibler divergence between the joint distribution of X and Y , and the product of their marginal distributions [9], indicating the error involved in ignoring the dependence between X and Y . The mutual information between predictive, dynamical states thus gauges the error involved in assuming the two systems are independent, i.e., how much predictions could improve by taking into account the dependence. Hence it measures the amount of dynamically-relevant information shared
between the two systems.  simply normalizes this value, and indicates the degree to which two systems have coordinated patterns of behavior (cf. [10], although this only uses directly observable quantities). 2.1 Reconstruction and Estimation of Effective State Spaces
As mentioned, the state space of a deterministic dynamical system can be reconstructed from a sequence of observations. This is the main tool of experimental nonlinear dynamics [8]; but the assumption of determinism is crucial and false, for almost any interesting neural system. While classical state-space reconstruction won't work on stochastic processes, such processes do have state-space representations [11], and, in the special case of discretevalued, discrete-time series, there are ways to reconstruct the state space. Here we use the CSSR algorithm, introduced in [12] (code available at http://bactra.org/CSSR). This produces causal state models, which are stochastic automata capable of statistically-optimal nonlinear prediction; the state of the machine is a minimal sufficient statistic for the future of the observable process[13].1 The basic idea is to form a set of states which should be (1) Markovian, (2) sufficient statistics for the next observable, and (3) have deterministic transitions (in the automata-theory sense). The algorithm begins with a minimal, one-state, IID model, and checks whether these properties hold, by means of hypothesis tests. If they fail, the model is modified, generally but not always by adding more states, and the new model is checked again. Each state of the model corresponds to a distinct distribution over future events, i.e., to a statistical pattern of behavior. Under mild conditions, which do not involve prior knowledge of the state space, CSSR converges in probability to the unique causal state model of the data-generating process [12]. In practice, CSSR is quite fast (linear in the data size), and generalizes at least as well as training hidden Markov models with the EM algorithm and using cross-validation for selection, the standard heuristic [12]. One advantage of the causal state approach (which it shares with classical state-space reconstruction) is that state estimation is greatly simplified. In the general case of nonlinear state estimation, it is necessary to know not just the form of the stochastic dynamics in the state space and the observation function, but also their precise parametric values and the distribution of observation and driving noises. Estimating the state from the observable time series then becomes a computationally-intensive application of Bayes's Rule [17]. Due to the way causal states are built as statistics of the data, with probability 1 there is a finite time, t, at which the causal state at time t is certain. This is not just with some degree of belief or confidence: because of the way the states are constructed, it is impossible for the process to be in any other state at that time. Once the causal state has been established, it can be updated recursively, i.e., the causal state at time t + 1 is an explicit function of the causal state at time t and the observation at t + 1. The causal state model can be automatically converted, therefore, into a finite-state transducer which reads in an observation time series and outputs the corresponding series of states [18, 13]. (Our implementation of CSSR filters its training data automatically.) The result is a new time series of states, from which all non-predictive components have been filtered out. 2.2 Estimating the Coherence
Our algorithm for estimating the matrix of informational coherences is as follows. For each unit, we reconstruct the causal state model, and filter the observable time series to produce a series of causal states. Then, for each pair of neurons, we construct a joint histogram of 1 Causal state models have the same expressive power as observable operator models [14] or predictive state representations [7], and greater power than variable-length Markov models [15, 16].
a
b
Figure 1: Rastergrams of neuronal spike-times in the network. Excitatory, pyramidal neurons (numbers 1 to 1000) are shown in green, inhibitory interneurons (numbers 1001 to 1300) in red. During the first 10 seconds (a), the current connections among the pyramidal cells are suppressed and a gamma rhythm emerges (left). At t = 10s, those connections become active, leading to a beta rhythm (b, right).
the state distribution, estimate the mutual information between the states, and normalize by the single-unit state informations. This gives a symmetric matrix of  values. Even if two systems are independent, their estimated IC will, on average, be positive, because, while they should have zero mutual information, the empirical estimate of mutual information is non-negative. Thus, the significance of IC values must be assessed against the null hypothesis of system independence. The easiest way to do so is to take the reconstructed state models for the two systems and run them forward, independently of one another, to generate a large number of simulated state sequences; from these calculate values of the IC. This procedure will approximate the sampling distribution of the IC under a null model which preserves the dynamics of each system, but not their interaction. We can then find p-values as usual. We omit them here to save space. 2.3 Approximating the Network Multi-Information
There is broad agreement [2] that analyses of networks should not just be an analysis of pairs of neurons, averaged over pairs. Ideally, an analysis of information sharing in a network would look at the over-all structure of statistical dependence between the various units, reflected in the complete joint probability distribution P of the states. This would then allow us, for instance, to calculate the n-fold multi-information, I [X1 , X2 , . . . Xn ]  D(P ||Q), the Kullback-Leibler divergence between the joint distribution P and the product of marginal distributions Q, analogous to the pairwise mutual information [19]. Calculated over the predictive states, the multi-information would give the total amount of shared dynamical information in the system. Just as we normalized the mutual information I [X1 , X2 ] by its maximum possible value, min H [X1 ], H [X2 ], we normalize the multiinformation by its maximum, which is the smallest sum of n - 1 marginal entropies: i I [X1 ; X2 ; . . . Xn ]  min H [Xn ] k =k
Unfortunately, P is a distribution over a very high dimensional space and so, hard to estimate well without strong parametric constraints. We thus consider approximations. The lowest-order approximation treats all the units as independent; this is the distribution Q. One step up are tree distributions, where the global distribution is a function of the joint distributions of pairs of units. Not every pair of units needs to enter into such a distribution,
though every unit must be part of some pair. Graphically, a tree distribution corresponds to a spanning tree, with edges linking units whose interactions enter into the global probability, and conversely spanning trees determine tree distributions. Writing ET for the set of pairs (i, j ) and abbreviating X1 = x1 , X2 = x2 , . . . Xn = xn by X = x, one has n ( T (Xi = xi , Xj = xj ) i T (X = x) = T (Xi = xi ) (2) T (Xi = xi )T (Xj = xj ) =1 i,j )ET
where the marginal distributions T (Xi ) and the pair distributions T (Xi , Xj ) are estimated by the empirical marginal and pair distributions. We must now pick edges ET so that T best approximates the true global distribution P . A natural approach is to minimize D(P ||T ), the divergence between P and its tree approximation. Chow and Liu [20] showed that the maximum-weight spanning tree gives the divergence-minimizing distribution, taking an edge's weight to be the mutual information between the variables it links. There are three advantages to using the Chow-Liu approximation. (1) Estimating T from empirical probabilities gives a consistent maximum likelihood estimator of the ideal ChowLiu tree [20], with reasonable rates of convergence, so T can be reliably known even if P cannot. (2) There are efficient algorithms for constructing maximum-weight spanning trees, such as Prim's algorithm [21, sec. 23.2], which runs in time O(n2 + n log n). Thus, the approximation is computationally tractable. (3) The KL divergence of the Chow-Liu distribution from Q gives a lower bound on the network multi-information; that bound is just the sum of the mutual informations along the edges in the tree: ( I [Xi ; Xj ] (3) I [X1 ; X2 ; . . . Xn ]  D(T ||Q) = i,j )ET
Even if we knew P exactly, Eq. 3 would be useful as an alternative to calculating D(P ||Q) directly, evaluating log P (x)/Q(x) for all the exponentially-many configurations x. It is natural to seek higher-order approximations to P , e.g., using three-way interactions not decomposable into pairwise interactions [22, 19]. But it is hard to do so effectively, because finding the optimal approximation to P when such interactions are allowed is NP [23], and analytical formulas like Eq. 3 generally do not exist [19]. We therefore confine ourselves to the Chow-Liu approximation here.
3
Example: A Model of Gamma and Beta Rhythms
We use simulated data as a test case, instead of empirical multiple electrode recordings, which allows us to try the method on a system of over 1000 neurons and compare the measure against expected results. The model, taken from [24], was originally designed to study episodes of gamma (3080Hz) and beta (1230Hz) oscillations in the mammalian nervous system, which often occur successively with a spontaneous transition between them. More concretely, the rhythms studied were those displayed by in vitro hippocampal (CA1) slice preparations and by in vivo neocortical EEGs. The model contains two neuron populations: excitatory (AMPA) pyramidal neurons and inhibitory (GABAA ) interneurons, defined by conductance-based Hodgkin-Huxley-style equations. Simulations were carried out in a network of 1000 pyramidal cells and 300 interneurons. Each cell was modeled as a one-compartment neuron with all-to-all coupling, endowed with the basic sodium and potassium spiking currents, an external applied current, and some Gaussian input noise. The first 10 seconds of the simulation correspond to the gamma rhythm, in which only a group of neurons is made to spike via a linearly increasing applied current. The beta rhythm"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/1397386b7a1507535c59764a15ee0c98-Abstract.html,Computing the Solution Path for the Regularized Support Vector Regression,"Lacey Gunter, Ji Zhu","In this paper we derive an algorithm that computes the entire solu- tion path of the support vector regression, with essentially the same computational cost as Ô¨Åtting one SVR model. We also propose an unbiased estimate for the degrees of freedom of the SVR model, which allows convenient selection of the regularization parameter."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/15c00b5250ddedaabc203b67f8b034fd-Abstract.html,Soft Clustering on Graphs,"Kai Yu, Shipeng Yu, Volker Tresp","We propose a simple clustering framework on graphs encoding pairwise data similarities. Unlike usual similarity-based methods, the approach softly assigns data to clusters in a probabilistic way. More importantly, a hierarchical clustering is naturally derived in this framework to gradually merge lower-level clusters into higher-level ones. A random walk analysis indicates that the algorithm exposes clustering structures in various resolutions, i.e., a higher level statistically models a longer-term diffusion on graphs and thus discovers a more global clustering structure. Finally we provide very encouraging experimental results."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/15cf76466b97264765356fcc56d801d1-Abstract.html,Efficient estimation of hidden state dynamics from spike trains,"Marton G. Danoczy, Richard H. R. Hahnloser","Neurons can have rapidly changing spike train statistics dictated by the underlying network excitability or behavioural state of an animal. To estimate the time course of such state dynamics from single- or multi- ple neuron recordings, we have developed an algorithm that maximizes the likelihood of observed spike trains by optimizing the state lifetimes and the state-conditional interspike-interval (ISI) distributions. Our non- parametric algorithm is free of time-binning and spike-counting prob- lems and has the computational complexity of a Mixed-state Markov Model operating on a state sequence of length equal to the total num- ber of recorded spikes. As an example, we Ô¨Åt a two-state model to paired recordings of premotor neurons in the sleeping songbird. We Ô¨Ånd that the two state-conditional ISI functions are highly similar to the ones mea- sured during waking and singing, respectively."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/17693c91d9204b7a7646284bb3adb603-Abstract.html,From Batch to Transductive Online Learning,"Sham Kakade, Adam Tauman Kalai","It is well-known that everything that is learnable in the difÔ¨Åcult online setting, where an arbitrary sequences of examples must be labeled one at a time, is also learnable in the batch setting, where examples are drawn independently from a distribution. We show a result in the opposite di- rection. We give an efÔ¨Åcient conversion algorithm from batch to online that is transductive: it uses future unlabeled data. This demonstrates the equivalence between what is properly and efÔ¨Åciently learnable in a batch model and a transductive online model."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/17d8da815fa21c57af9829fb0a869602-Abstract.html,Learning Depth from Single Monocular Images,"Ashutosh Saxena, Sung H. Chung, Andrew Y. Ng","We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/17eb7ecc4c38e4705361cccd903ad8c6-Abstract.html,Non-Local Manifold Parzen Windows,"Yoshua Bengio, Hugo Larochelle, Pascal Vincent","To escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x. With this objective, we present a non-local non-parametric density esti- mator. It builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold. It also builds upon recent work on non-local estimators of the tangent plane of a manifold, which are able to generalize in places with little training data, unlike traditional, local, non-parametric models."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/182e6c2d3d78eef40e5dac7da77a748f-Abstract.html,Bayesian Sets,"Zoubin Ghahramani, Katherine A. Heller","Inspired by ‚ÄúGoogle‚Ñ¢ Sets‚Äù, we consider the problem of retrieving items from a concept or cluster, given a query consisting of a few items from that cluster. We formulate this as a Bayesian inference problem and de- scribe a very simple algorithm for solving it. Our algorithm uses a model- based concept of a cluster and ranks items using a score which evaluates the marginal probability that each item belongs to a cluster containing the query items. For exponential family models with conjugate priors this marginal probability is a simple function of sufÔ¨Åcient statistics. We focus on sparse binary data and show that our score can be evaluated ex- actly using a single sparse matrix multiplication, making it possible to apply our algorithm to very large datasets. We evaluate our algorithm on three datasets: retrieving movies from EachMovie, Ô¨Ånding completions of author sets from the NIPS dataset, and Ô¨Ånding completions of sets of words appearing in the Grolier encyclopedia. We compare to Google‚Ñ¢ Sets and show that Bayesian Sets gives very reasonable set completions."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/1a6727711b84fd1efbb87fc565199d13-Abstract.html,Modeling Neuronal Interactivity using Dynamic Bayesian Networks,"Lei Zhang, Dimitris Samaras, Nelly Alia-klein, Nora Volkow, Rita Goldstein","Functional Magnetic Resonance Imaging (fMRI) has enabled scientists to look into the active brain. However, interactivity between functional brain regions, is still little studied. In this paper, we contribute a novel framework for modeling the interactions between multiple active brain regions, using Dynamic Bayesian Networks (DBNs) as generative mod- els for brain activation patterns. This framework is applied to modeling of neuronal circuits associated with reward. The novelty of our frame- work from a Machine Learning perspective lies in the use of DBNs to reveal the brain connectivity and interactivity. Such interactivity mod- els which are derived from fMRI data are then validated through a group classiÔ¨Åcation task. We employ and compare four different types of DBNs: Parallel Hidden Markov Models, Coupled Hidden Markov Models, Fully-linked Hidden Markov Models and Dynamically Multi- Linked HMMs (DML-HMM). Moreover, we propose and compare two schemes of learning DML-HMMs. Experimental results show that by using DBNs, group classiÔ¨Åcation can be performed even if the DBNs are constructed from as few as 5 brain regions. We also demonstrate that, by using the proposed learning algorithms, different DBN structures charac- terize drug addicted subjects vs. control subjects. This Ô¨Ånding provides an independent test for the effect of psychopathology on brain function. In general, we demonstrate that incorporation of computer science prin- ciples into functional neuroimaging clinical studies provides a novel ap- proach for probing human brain function."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/1a99f6821980ac99136dcd2f1e9c8740-Abstract.html,A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels,"Eizaburo Doi, Doru C. Balcan, Michael S. Lewicki","Biological sensory systems are faced with the problem of encoding a high-fidelity sensory signal with a population of noisy, low-fidelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/1b79b52d1bf6f71b2b1eb7ca08ed0776-Abstract.html,A Probabilistic Approach for Optimizing Spectral Clustering,"Rong Jin, Feng Kang, Chris H. Ding","Spectral clustering enjoys its success in both data clustering and semisupervised learning. But, most spectral clustering algorithms cannot handle multi-class clustering problems directly. Additional strategies are needed to extend spectral clustering algorithms to multi-class clustering problems. Furthermore, most spectral clustering algorithms employ hard cluster membership, which is likely to be trapped by the local optimum. In this paper, we present a new spectral clustering algorithm, named ""Soft Cut"". It improves the normalized cut algorithm by introducing soft membership, and can be efficiently computed using a bound optimization algorithm. Our experiments with a variety of datasets have shown the promising performance of the proposed clustering algorithm."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html,Learning Multiple Related Tasks using Latent Independent Component Analysis,"Jian Zhang, Zoubin Ghahramani, Yiming Yang","We propose a probabilistic model based on Independent Component Analysis for learning multiple related tasks. In our model the task parameters are assumed to be generated from independent sources which account for the relatedness of the tasks. We use Laplace distributions to model hidden sources which makes it possible to identify the hidden, independent components instead of just modeling correlations. Furthermore, our model enjoys a sparsity property which makes it both parsimonious and robust. We also propose efficient algorithms for both empirical Bayes method and point estimation. Our experimental results on two multi-label text classification data sets show that the proposed approach is promising."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/1dba5eed8838571e1c80af145184e515-Abstract.html,Context as Filtering,"Daichi Mochihashi, Yuji Matsumoto","Long-distance language modeling is important not only in speech recognition and machine translation, but also in high-dimensional discrete sequence modeling in general. However, the problem of context length has almost been neglected so far and a nave bag-of-words history has been i employed in natural language processing. In contrast, in this paper we view topic shifts within a text as a latent stochastic process to give an explicit probabilistic generative model that has partial exchangeability. We propose an online inference algorithm using particle filters to recognize topic shifts to employ the most appropriate length of context automatically. Experiments on the BNC corpus showed consistent improvement over previous methods involving no chronological order."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/1ec3e7af38e33222bde173fecaef6bfa-Abstract.html,A matching pursuit approach to sparse Gaussian process regression,"Sathiya Keerthi, Wei Chu","In this paper we propose a new basis selection criterion for building sparse GP regression models that provides promising gains in accuracy as well as efficiency over previous methods. Our algorithm is much faster than that of Smola and Bartlett, while, in generalization it greatly outperforms the information gain approach proposed by Seeger et al, especially on the quality of predictive distributions."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/2217ad1dd50c1017d3df6b44b7c45508-Abstract.html,Phase Synchrony Rate for the Recognition of Motor Imagery in Brain-Computer Interface,"Le Song, Evian Gordon, Elly Gysels","Motor imagery attenuates EEG  and  rhythms over sensorimotor cortices. These amplitude changes are most successfully captured by the method of Common Spatial Patterns (CSP) and widely used in braincomputer interfaces (BCI). BCI methods based on amplitude information, however, have not incoporated the rich phase dynamics in the EEG rhythm. This study reports on a BCI method based on phase synchrony rate (SR). SR, computed from binarized phase locking value, describes the number of discrete synchronization events within a window. Statistical nonparametric tests show that SRs contain significant differences between 2 types of motor imageries. Classifiers trained on SRs consistently demonstrate satisfactory results for all 5 subjects. It is further observed that, for 3 subjects, phase is more discriminative than amplitude in the first 1.5-2.0 s, which suggests that phase has the potential to boost the information transfer rate in BCIs."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/227f6afd3b7f89b96c4bb91f95d50f6d-Abstract.html,Inference with Minimal Communication: a Decision-Theoretic Variational Approach,"O. P. Kreidl, Alan S. Willsky","Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed offline, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efficiency and (iii) connections to active research areas."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/25766f01628f3d34b93a36a2301dffc9-Abstract.html,Variational EM Algorithms for Non-Gaussian Latent Variable Models,"Jason Palmer, Kenneth Kreutz-Delgado, Bhaskar D. Rao, David P. Wipf","We consider criteria for variational representations of non-Gaussian latent variables, and derive variational EM algorithms in general form. We establish a general equivalence among convex bounding methods, evidence based methods, and ensemble learning/Variational Bayes methods, which has previously been demonstrated only for particular cases."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/25ef0d887bc7a2b30089a025618e1c62-Abstract.html,Fast Information Value for Graphical Models,"Brigham S. Anderson, Andrew W. Moore","Calculations that quantify the dependencies between variables are vital to many operations with graphical models, e.g., active learning and sen- sitivity analysis. Previously, pairwise information gain calculation has involved a cost quadratic in network size. In this work, we show how to perform a similar computation with cost linear in network size. The loss function that allows this is of a form amenable to computation by dynamic programming. The message-passing algorithm that results is described and empirical results demonstrate large speedups without de- crease in accuracy. In the cost-sensitive domains examined, superior ac- curacy is achieved."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html,Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions,"Sridhar Mahadevan, Mauro Maggioni","We investigate the problem of automatically constructing efÔ¨Åcient rep- resentations or basis functions for approximating value functions based on analyzing the structure and topology of the state space. In particu- lar, two novel approaches to value function approximation are explored based on automatically constructing basis functions on state spaces that can be represented as graphs or manifolds: one approach uses the eigen- functions of the Laplacian, in effect performing a global Fourier analysis on the graph; the second approach is based on diffusion wavelets, which generalize classical wavelets to graphs using multiscale dilations induced by powers of a diffusion operator or random walk on the graph. Together, these approaches form the foundation of a new generation of methods for solving large Markov decision processes, in which the underlying repre- sentation and policies are simultaneously learned."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/28acfe2da49d2b9a7f177458256f2540-Abstract.html,A Bayesian Spatial Scan Statistic,"Daniel B. Neill, Andrew W. Moore, Gregory F. Cooper","We propose a new Bayesian method for spatial cluster detection, the ‚ÄúBayesian spatial scan statistic,‚Äù and compare this method to the standard (frequentist) scan statistic approach. We demonstrate that the Bayesian statistic has several advantages over the frequentist approach, including increased power to detect clusters and (since randomization testing is unnecessary) much faster runtime. We evaluate the Bayesian and fre- quentist methods on the task of prospective disease surveillance: detect- ing spatial clusters of disease cases resulting from emerging disease out- breaks. We demonstrate that our Bayesian methods are successful in rapidly detecting outbreaks while keeping number of false positives low."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/2a0f97f81755e2878b264adf39cba68e-Abstract.html,"Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators","Boaz Nadler, Stephane Lafon, Ioannis Kevrekidis, Ronald R. Coifman","This paper presents a diffusion based probabilistic interpretation of spectral clustering and dimensionality reduction algorithms that use the eigenvectors of the normalized graph Laplacian. Given the pairwise adjacency matrix of all points, we define a diffusion distance between any two data points and show that the low dimensional representation of the data by the first few eigenvectors of the corresponding Markov matrix is optimal under a certain mean squared error criterion. Furthermore, assuming that data points are random samples from a density p(x) = e-U (x) we identify these eigenvectors as discrete approximations of eigenfunctions of a Fokker-Planck operator in a potential 2U (x) with reflecting boundary conditions. Finally, applying known results regarding the eigenvalues and eigenfunctions of the continuous Fokker-Planck operator, we provide a mathematical justification for the success of spectral clustering and dimensional reduction algorithms based on these first few eigenvectors. This analysis elucidates, in terms of the characteristics of diffusion processes, many empirical findings regarding spectral clustering algorithms. Keywords: Algorithms and architectures, learning theory."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/2b64c2f19d868305aa8bbc2d72902cc5-Abstract.html,Scaling Laws in Natural Scenes and the Inference of 3D Shape,"Tai-sing Lee, Brian R. Potetz","This paper explores the statistical relationship between natural images and their underlying range (depth) images. We look at how this relationship changes over scale, and how this information can be used to enhance low resolution range data using a full resolution intensity image. Based on our findings, we propose an extension to an existing technique known as shape recipes [3], and the success of the two methods are compared using images and laser scans of real scenes. Our extension is shown to provide a two-fold improvement over the current method. Furthermore, we demonstrate that ideal linear shape-from-shading filters, when learned from natural scenes, may derive even more strength from shadow cues than from the traditional linear-Lambertian shading cues."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/2b6921f2c64dee16ba21ebf17f3c2c92-Abstract.html,On the Convergence of Eigenspaces in Kernel Principal Component Analysis,"Laurent Zwald, Gilles Blanchard",This paper presents a non-asymptotic statistical analysis of Kernel-PCA with a focus different from the one proposed in previous work on this topic. Here instead of considering the reconstruction error of KPCA we are interested in approximation error bounds for the eigenspaces themselves. We prove an upper bound depending on the spacing between eigenvalues but not on the dimensionality of the eigenspace. As a consequence this allows to infer stability results for these estimated spaces.
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/2bc8ae25856bc2a6a1333d1331a3b7a6-Abstract.html,Layered Dynamic Textures,"Antoni B. Chan, Nuno Vasconcelos","A dynamic texture is a video model that treats a video as a sample from a spatio-temporal stochastic process, speciÔ¨Åcally a linear dynamical sys- tem. One problem associated with the dynamic texture is that it cannot model video where there are multiple regions of distinct motion. In this work, we introduce the layered dynamic texture model, which addresses this problem. We also introduce a variant of the model, and present the EM algorithm for learning each of the models. Finally, we demonstrate the efÔ¨Åcacy of the proposed model for the tasks of segmentation and syn- thesis of video."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/2e0bff759d057e28460eaa5b2cb118e5-Abstract.html,Subsequence Kernels for Relation Extraction,"Raymond J. Mooney, Razvan C. Bunescu","We present a new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels. This kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities. Experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/2eb5657d37f474e4c4cf01e4882b8962-Abstract.html,Optimal cue selection strategy,"Vidhya Navalpakkam, Laurent Itti","Survival in the natural world demands the selection of relevant visual cues to rapidly and reliably guide attention towards prey an d predators in cluttered environments. We investigate whether our visu al system selects cues that guide search in an optimal manner. We formall y obtain the optimal cue selection strategy by maximizing the signal to noise ratio (S N R) between a search target and surrounding distractors. This optimal strategy successfully accounts for several phenom ena in visual search behavior, including the effect of target-distracto r discriminability, uncertainty in target's features, distractor heterogenei ty, and linear separability. Furthermore, the theory generates a new predict ion, which we verify through psychophysical experiments with human subj ects. Our results provide direct experimental evidence that humans sel ect visual cues so as to maximize S N R between the targets and surrounding clutter."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/2ef35a8b78b572a47f56846acbeef5d3-Abstract.html,Infinite latent feature models and the Indian buffet process,"Zoubin Ghahramani, Thomas L. Griffiths","We define a probability distribution over equivalence classes of binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features. We identify a simple generative process that results in the same distribution over equivalence classes, which we call the Indian buffet process. We illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a Markov chain Monte Carlo algorithm for inference in this model and applying the algorithm to an image dataset."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/3070e6addcd702cb58de5d7897bfdae1-Abstract.html,Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction,"Gilles Blanchard, Masashi Sugiyama, Motoaki Kawanabe, Vladimir Spokoiny, Klaus-Robert M√ºller","We propose a new linear method for dimension reduction to identify nonGaussian components in high dimensional data. Our method, NGCA (non-Gaussian component analysis), uses a very general semi-parametric framework. In contrast to existing projection methods we define what is uninteresting (Gaussian): by projecting out uninterestingness, we can estimate the relevant non-Gaussian subspace. We show that the estimation error of finding the non-Gaussian components tends to zero at a parametric rate. Once NGCA components are identified and extracted, various tasks can be applied in the data analysis process, like data visualization, clustering, denoising or classification. A numerical study demonstrates the usefulness of our method."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/314450613369e0ee72d0da7f6fee773c-Abstract.html,"Divergences, surrogate loss functions and experimental design","Xuanlong Nguyen, Martin J. Wainwright, Michael I. Jordan","In this paper, we provide a general theorem that establishes a correspon- dence between surrogate loss functions in classiÔ¨Åcation and the family of f-divergences. Moreover, we provide constructive procedures for determining the f-divergence induced by a given surrogate loss, and conversely for Ô¨Ånding all surrogate loss functions that realize a given f-divergence. Next we introduce the notion of universal equivalence among loss functions and corresponding f-divergences, and provide nec- essary and sufÔ¨Åcient conditions for universal equivalence to hold. These ideas have applications to classiÔ¨Åcation problems that also involve a com- ponent of experiment design; in particular, we leverage our results to prove consistency of a procedure for learning a classiÔ¨Åer under decen- tralization requirements."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/327708dd10d68b1361ad3addbaca01f2-Abstract.html,Mixture Modeling by Affinity Propagation,"Brendan J. Frey, Delbert Dueck","Clustering is a fundamental problem in machine learning and has been approached in many ways. Two general and quite different approaches include iteratively Ô¨Åtting a mixture model (e.g., using EM) and linking to- gether pairs of training cases that have high afÔ¨Ånity (e.g., using spectral methods). Pair-wise clustering algorithms need not compute sufÔ¨Åcient statistics and avoid poor solutions by directly placing similar examples in the same cluster. However, many applications require that each cluster of data be accurately described by a prototype or model, so afÔ¨Ånity-based clustering ‚Äì and its beneÔ¨Åts ‚Äì cannot be directly realized. We describe a technique called ‚ÄúafÔ¨Ånity propagation‚Äù, which combines the advantages of both approaches. The method learns a mixture model of the data by recursively propagating afÔ¨Ånity messages. We demonstrate afÔ¨Ånity prop- agation on the problems of clustering image patches for image segmen- tation and learning mixtures of gene expression models from microar- ray data. We Ô¨Ånd that afÔ¨Ånity propagation obtains better solutions than mixtures of Gaussians, the K-medoids algorithm, spectral clustering and hierarchical clustering, and is both able to Ô¨Ånd a pre-speciÔ¨Åed number of clusters and is able to automatically determine the number of clusters. Interestingly, afÔ¨Ånity propagation can be viewed as belief propagation in a graphical model that accounts for pairwise training case likelihood functions and the identiÔ¨Åcation of cluster centers."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/332647f433a1c10fa2e2ae04abfdf83e-Abstract.html,Tensor Subspace Analysis,"Xiaofei He, Deng Cai, Partha Niyogi","Previous work has demonstrated that the image variations of many ob- jects (human faces in particular) under variable lighting can be effec- tively modeled by low dimensional linear spaces. The typical linear sub- space learning algorithms include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Locality Preserving Projec- tion (LPP). All of these methods consider an n1 √ó n2 image as a high dimensional vector in Rn1√ón2, while an image represented in the plane is intrinsically a matrix. In this paper, we propose a new algorithm called Tensor Subspace Analysis (TSA). TSA considers an image as the sec- ond order tensor in Rn1 ‚äó Rn2, where Rn1 and Rn2 are two vector spaces. The relationship between the column vectors of the image ma- trix and that between the row vectors can be naturally characterized by TSA. TSA detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace. We compare our proposed approach with PCA, LDA and LPP methods on two standard databases. Experimental results demonstrate that TSA achieves better recognition rate, while being much more efÔ¨Åcient."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/340a39045c40d50dda207bcfdece883a-Abstract.html,Query by Committee Made Real,"Ran Gilad-bachrach, Amir Navot, Naftali Tishby","Training a learning algorithm is a costly task. A major goal of active learning is to reduce this cost. In this paper we introduce a new algorithm, KQBC, which is capable of actively learning large scale problems by using selective sampling. The algorithm overcomes the costly sampling step of the well known Query By Committee (QBC) algorithm by projecting onto a low dimensional space. KQBC also enables the use of kernels, providing a simple way of extending QBC to the non-linear scenario. Sampling the low dimension space is done using the hit and run random walk. We demonstrate the success of this novel algorithm by applying it to both artificial and a real world problems."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/353de26971b93af88da102641069b440-Abstract.html,An Application of Markov Random Fields to Range Sensing,"James Diebel, Sebastian Thrun","This paper describes a highly successful application of MRFs to the problem of generating high-resolution range images. A new generation of range sensors combines the capture of low-resolution range images with the acquisition of registered high-resolution camera images. The MRF in this paper exploits the fact that discontinuities in range and coloring tend to co-align. This enables it to generate high-resolution, low-noise range images by integrating regular camera images into the range data. We show that by using such an MRF, we can substantially improve over existing range imaging technology."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/35c5a2cb362c4d214156f930e7d13252-Abstract.html,Analysis of Spectral Kernel Design based Semi-supervised Learning,"Tong Zhang, Rie Kubota Ando","We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach sub- sumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such meth- ods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Ex- periments are used to illustrate the main consequences of our analysis."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/370bfb31abd222b582245b977ea5f25a-Abstract.html,Representing Part-Whole Relationships in Recurrent Neural Networks,"Viren Jain, Valentin Zhigulin, H. S. Seung","There is little consensus about the computational function of top-down synaptic connections in the visual system. Here we explore the hypothesis that top-down connections, like bottom-up connections, reflect partwhole relationships. We analyze a recurrent network with bidirectional synaptic interactions between a layer of neurons representing parts and a layer of neurons representing wholes. Within each layer, there is lateral inhibition. When the network detects a whole, it can rigorously enforce part-whole relationships by ignoring parts that do not belong. The network can complete the whole by filling in missing parts. The network can refuse to recognize a whole, if the activated parts do not conform to a stored part-whole relationship. Parameter regimes in which these behaviors happen are identified using the theory of permitted and forbidden sets [3, 4]. The network behaviors are illustrated by recreating Rumelhart and McClelland's ""interactive activation"" model [7]. In neural network models of visual object recognition [2, 6, 8], patterns of synaptic connectivity often reflect part-whole relationships between the features that are represented by neurons. For example, the connections of Figure 1 reflect the fact that feature B both contains simpler features A1, A2, and A3, and is contained in more complex features C1, C2, and C3. Such connectivity allows neurons to follow the rule that existence of the part is evidence for existence of the whole. By combining synaptic input from multiple sources of evidence for a feature, a neuron can ""decide"" whether that feature is present. 1 The synapses shown in Figure 1 are purely bottom-up, directed from simple to complex features. However, there are also top-down connections in the visual system, and there is little consensus about their function. One possibility is that top-down connections also reflect part-whole relationships. They allow feature detectors to make decisions using the rule that existence of the whole is evidence for existence of its parts. In this paper, we analyze the dynamics of a recurrent network in which part-whole relationships are stored as bidirectional synaptic interactions, rather than the unidirectional interactions of Figure 1. The network has a number of interesting computational capabilities. When the network detects a whole, it can rigorously enforce part-whole relationships Synaptic connectivity may reflect other relationships besides part-whole. For example, invariances can be implemented by connecting detectors of several instances of the same feature to the same target, which is consequently an invariant detector of the feature. 1"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/379a7ba015d8bf1c70b8add2c287c6fa-Abstract.html,Size Regularized Cut for Data Clustering,"Yixin Chen, Ya Zhang, Xiang Ji","We present a novel spectral clustering method that enables users to incorporate prior knowledge of the size of clusters into the clustering process. The cost function, which is named size regularized cut (SRcut), is defined as the sum of the inter-cluster similarity and a regularization term measuring the relative size of two clusters. Finding a partition of the data set to minimize SRcut is proved to be NP-complete. An approximation algorithm is proposed to solve a relaxed version of the optimization problem as an eigenvalue problem. Evaluations over different data sets demonstrate that the method is not sensitive to outliers and performs better than normalized cut."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/3812f9a59b634c2a9c574610eaba5bed-Abstract.html,"How fast to work: Response vigor, motivation and tonic dopamine","Yael Niv, Nathaniel D. Daw, Peter Dayan","Reinforcement learning models have long promised to unify computa- tional, psychological and neural accounts of appetitively conditioned be- havior. However, the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for rein- forcement. Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor. They thus fail to ad- dress the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater produc- tivity even when working for irrelevant outcomes such as water. Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and beneÔ¨Åts of quick responding. Motivational states such as hunger shift these factors, skewing the tradeoff. This accounts normatively for the effects of motivation on response rates, as well as many other classic Ô¨Åndings. Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related ef- fects of pharmacological manipulation of dopamine."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/3953630da28e5181cffca1278517e3cf-Abstract.html,Robust design of biological experiments,"Patrick Flaherty, Adam Arkin, Michael I. Jordan","We address the problem of robust, computationally-efficient design of biological experiments. Classical optimal experiment design methods have not been widely adopted in biological practice, in part because the resulting designs can be very brittle if the nominal parameter estimates for the model are poor, and in part because of computational constraints. We present a method for robust experiment design based on a semidefinite programming relaxation. We present an application of this method to the design of experiments for a complex calcium signal transduction pathway, where we have found that the parameter estimates obtained from the robust design are better than those obtained from an ""optimal"" design."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/39d352b0395ba768e18f042c6e2a8621-Abstract.html,On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?,"Michael Schmitt, Laura Martignon","Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-thebest searches for a sufficiently good ordering of cues (features) in a task where objects are to be compared lexicographically. We investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies. We show that no efficient algorithm can approximate the optimum to within any constant factor, if P = NP. We further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm. This algorithm is proven to perform better than take-the-best."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/3c333aadfc3ee8ecb8d77ee31197d96a-Abstract.html,Assessing Approximations for Gaussian Process Classification,"Malte Kuss, Carl E. Rasmussen","Gaussian processes are attractive models for probabilistic classification but unfortunately exact inference is analytically intractable. We compare Laplace's method and Expectation Propagation (EP) focusing on marginal likelihood estimates and predictive performance. We explain theoretically and corroborate empirically that EP is superior to Laplace. We also compare to a sophisticated MCMC scheme and show that EP is surprisingly accurate. In recent years models based on Gaussian process (GP) priors have attracted much attention in the machine learning community. Whereas inference in the GP regression model with Gaussian noise can be done analytically, probabilistic classification using GPs is analytically intractable. Several approaches to approximate Bayesian inference have been suggested, including Laplace's approximation, Expectation Propagation (EP), variational approximations and Markov chain Monte Carlo (MCMC) sampling, some of these in conjunction with generalisation bounds, online learning schemes and sparse approximations. Despite the abundance of recent work on probabilistic GP classifiers, most experimental studies provide only anecdotal evidence, and no clear picture has yet emerged, as to when and why which algorithm should be preferred. Thus, from a practitioners point of view probabilistic GP classification remains a jungle. In this paper, we set out to understand and compare two of the most wide-spread approximations: Laplace's method and Expectation Propagation (EP). We also compare to a sophisticated, but computationally demanding MCMC scheme to examine how close the approximations are to ground truth. We examine two aspects of the approximation schemes: Firstly the accuracy of approximations to the marginal likelihood which is of central importance for model selection and model comparison. In any practical application of GPs in classification (usually multiple) parameters of the covariance function (hyperparameters) have to be handled. Bayesian model selection provides a consistent framework for setting such parameters. Therefore, it is essential to evaluate the accuracy of the marginal likelihood approximations as a function of the hyperparameters, in order to assess the practical usefulness of the approach Secondly, we need to assess the quality of the approximate probabilistic predictions. In the past, the probabilistic nature of the GP predictions have not received much attention, the focus being mostly on classification error rates. This unfortunate state of affairs is caused primarily by typical benchmarking problems being considered outside of a realistic context. The ability of a classifier to produce class probabilities or confidences, have obvious
relevance in most areas of application, eg. medical diagnosis. We evaluate the predictive distributions of the approximate methods, and compare to the MCMC gold standard.
1
The Gaussian Process Model for Binary Classification
Let y  {-1, 1} denote the class label of an input x. Gaussian process classification (GPC) is discriminative in modelling p(y |x) for given x by a Bernoulli distribution. The probability of success p(y = 1|x) is related to an unconstrained latent function f (x) which is mapped to the unit interval by a sigmoid transformation, eg. the logit or the probit. For reasons of analytic convenience we exclusively use the probit model p(y = 1|x) = (f (x)), where  denotes the cumulative density function of the standard Normal distribution. In the GPC model Bayesian inference is performed about the latent function f in the light of observed data D = {(yi , xi )|i = 1, . . . , m}. Let fi = f (xi ) and f = [f1 , . . . , fm ] e shorthand for the values of the latent function and y = [y1 , . . . , ym ] and X = [x1 , . . . , xm ] collect the class labels and inputs respectively. Given the latent function the class labels are independent Bernoulli variables, so the joint likelihood factories: b"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/3fc2c60b5782f641f76bcefc39fb2392-Abstract.html,Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation,"Jin Yu, Douglas Aberdeen, Nicol N. Schraudolph","Reinforcement learning by direct policy gradient estimation is attractive in theory but in practice leads to notoriously ill-behaved optimization problems. We improve its robustness and speed of convergence with stochastic meta-descent, a gain vector adaptation method that employs fast Hessian-vector products. In our experiments the resulting algorithms outperform previously employed online stochastic, ofÔ¨Çine conjugate, and natural policy gradient methods."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/4191ef5f6c1576762869ac49281130c9-Abstract.html,A Bayes Rule for Density Matrices,Manfred K. Warmuth,"The classical Bayes rule computes the posterior model probability from the prior probability and the data likelihood. We generalize this rule to the case when the prior is a density matrix (symmetric positive definite and trace one) and the data likelihood a covariance matrix. The classical Bayes rule is retained as the special case when the matrices are diagonal. In the classical setting, the calculation of the probability of the data is an expected likelihood, where the expectation is over the prior distribution. In the generalized setting, this is replaced by an expected variance calculation where the variance is computed along the eigenvectors of the prior density matrix and the expectation is over the eigenvalues of the density matrix (which form a probability vector). The variances along any direction is determined by the covariance matrix. Curiously enough this expected variance calculation is a quantum measurement where the co-variance matrix specifies the instrument and the prior density matrix the mixture state of the particle. We motivate both the classical and the generalized Bayes rule with a minimum relative entropy principle, where the Kullbach-Leibler version gives the classical Bayes rule and Umegaki's quantum relative entropy the new Bayes rule for density matrices."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/42853a61b26fef79e2ae788d97356799-Abstract.html,Combining Graph Laplacians for Semi--Supervised Learning,"Andreas Argyriou, Mark Herbster, Massimiliano Pontil",A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the `k ' in nearest neighbors.
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/42a6845a557bef704ad8ac9cb4461d43-Abstract.html,Integrate-and-Fire models with adaptation are good enough,"Renaud Jolivet, Alexander Rauch, Hans-rudolf L√ºscher, Wulfram Gerstner","Integrate-and-Fire-type models are usually criticized because of their simplicity. On the other hand, the Integrate-and-Fire model is the basis of most of the theoretical studies on spiking neuron models. Here, we develop a sequential procedure to quantitatively evaluate an equivalent Integrate-and-Fire-type model based on intracellular recordings of cortical pyramidal neurons. We find that the resulting effective model is sufficient to predict the spike train of the real pyramidal neuron with high accuracy. In in vivo-like regimes, predicted and recorded traces are almost indistinguishable and a significant part of the spikes can be predicted at the correct timing. Slow processes like spike-frequency adaptation are shown to be a key feature in this context since they are necessary for the model to connect between different driving regimes."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/445e1050156c6ae8c082a8422bb7dfc0-Abstract.html,Large-Scale Multiclass Transduction,"Thomas G√§rtner, Quoc V. Le, Simon Burton, Alex J. Smola, Vishy Vishwanathan","We present a method for performing transductive inference on very large datasets. Our algorithm is based on multiclass Gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be computed sufÔ¨Åciently fast. This holds, for instance, for certain graph and string kernels. Transduction is achieved by varia- tional inference over the unlabeled data subject to a balancing constraint."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/4491777b1aa8b5b32c2e8666dbe1a495-Abstract.html,Sparse Gaussian Processes using Pseudo-inputs,"Edward Snelson, Zoubin Ghahramani","We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M 2 N ) training cost and O(M 2 ) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M , i.e. very sparse solutions, and it significantly outperforms other approaches in this regime."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/44ac09ac6a149136a4102ee4b4103ae6-Abstract.html,Fast biped walking with a reflexive controller and real-time policy searching,"Tao Geng, Bernd Porr, Florentin W√∂rg√∂tter","In this paper, we present our design and experiments of a planar biped robot (""RunBot"") under pure reflexive neuronal control. The goal of this study is to combine neuronal mechanisms with biomechanics to obtain very fast speed and the on-line learning of circuit parameters. Our controller is built with biologically inspired sensor- and motor-neuron models, including local reflexes and not employing any kind of position or trajectory-tracking control algorithm. Instead, this reflexive controller allows RunBot to exploit its own natural dynamics during critical stages of its walking gait cycle. To our knowledge, this is the first time that dynamic biped walking is achieved using only a pure reflexive controller. In addition, this structure allows using a policy gradient reinforcement learning algorithm to tune the parameters of the reflexive controller in real-time during walking. This way RunBot can reach a relative speed of 3.5 leg-lengths per second after a few minutes of online learning, which is faster than that of any other biped robot, and is also comparable to the fastest relative speed of human walking. In addition, the stability domain of stable walking is quite large supporting this design strategy."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/465636eb4a7ff4b267f3b765d07a02da-Abstract.html,Learning from Data of Variable Quality,"Koby Crammer, Michael Kearns, Jennifer Wortman","We initiate the study of learning from multiple sources of limited data, each of which may be corrupted at a different rate. We develop a com- plete theory of which data sources should be used for two fundamental problems: estimating the bias of a coin, and learning a classiÔ¨Åer in the presence of label noise. In both cases, efÔ¨Åcient algorithms are provided for computing the optimal subset of data."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/46b2644cbdf489fac0e2d192212d206d-Abstract.html,"Two view learning: SVM-2K, Theory and Practice","Jason Farquhar, David Hardoon, Hongying Meng, John S. Shawe-taylor, S√°ndor Szedm√°k",Kernel methods make it relatively easy to define complex highdimensional feature spaces. This raises the question of how we can identify the relevant subspaces for a particular learning task. When two views of the same phenomenon are available kernel Canonical Correlation Analysis (KCCA) has been shown to be an effective preprocessing step that can improve the performance of classification algorithms such as the Support Vector Machine (SVM). This paper takes this observation to its logical conclusion and proposes a method that combines this two stage learning (KCCA followed by SVM) into a single optimisation termed SVM-2K. We present both experimental and theoretical analysis of the approach showing encouraging results and insights.
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/46f76a4bda9a9579eab38a8f6eabcda1-Abstract.html,Extracting Dynamical Structure Embedded in Neural Activity,"Byron M. Yu, Afsheen Afshar, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, Maneesh Sahani","Spiking activity from neurophysiological experiments often exhibits dy- namics beyond that driven by external stimulation, presumably reÔ¨Çect- ing the extensive recurrence of neural circuitry. Characterizing these dynamics may reveal important features of neural computation, par- ticularly during internally-driven cognitive operations. For example, the activity of premotor cortex (PMd) neurons during an instructed de- lay period separating movement-target speciÔ¨Åcation and a movement- initiation cue is believed to be involved in motor planning. We show that the dynamics underlying this activity can be captured by a low- dimensional non-linear dynamical systems model, with underlying re- current structure and stochastic point-process output. We present and validate latent variable methods that simultaneously estimate the system parameters and the trial-by-trial dynamical trajectories. These meth- ods are applied to characterize the dynamics in PMd data recorded from a chronically-implanted 96-electrode array while monkeys perform delayed-reach tasks."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/488c1e0332065eb80e1129139a67d6e0-Abstract.html,Large-scale biophysical parameter estimation in single neurons via constrained linear regression,"Misha Ahrens, Liam Paninski, Quentin J. Huys","Our understanding of the input-output function of single cells has been substantially advanced by biophysically accurate multi-compartmental models. The large number of parameters needing hand tuning in these models has, however, somewhat hampered their applicability and interpretability. Here we propose a simple and well-founded method for automatic estimation of many of these key parameters: 1) the spatial distribution of channel densities on the cell's membrane; 2) the spatiotemporal pattern of synaptic input; 3) the channels' reversal potentials; 4) the intercompartmental conductances; and 5) the noise level in each compartment. We assume experimental access to: a) the spatiotemporal voltage signal in the dendrite (or some contiguous subpart thereof, e.g. via voltage sensitive imaging techniques), b) an approximate kinetic description of the channels and synapses present in each compartment, and c) the morphology of the part of the neuron under investigation. The key observation is that, given data a)-c), all of the parameters 1)-4) may be simultaneously inferred by a version of constrained linear regression; this regression, in turn, is efficiently solved using standard algorithms, without any ""local minima"" problems despite the large number of parameters and complex dynamics. The noise level 5) may also be estimated by standard techniques. We demonstrate the method's accuracy on several model datasets, and describe techniques for quantifying the uncertainty in our estimates."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/4a5876b450b45371f6cfe5047ac8cd45-Abstract.html,Data-Driven Online to Batch Conversions,"Ofer Dekel, Yoram Singer","Online learning algorithms are typically fast, memory efficient, and simple to implement. However, many common learning problems fit more naturally in the batch learning setting. The power of online learning algorithms can be exploited in batch settings by using online-to-batch conversions techniques which build a new batch algorithm from an existing online algorithm. We first give a unified overview of three existing online-to-batch conversion techniques which do not use training data in the conversion process. We then build upon these data-independent conversions to derive and analyze data-driven conversions. Our conversions find hypotheses with a small risk by explicitly minimizing datadependent generalization bounds. We experimentally demonstrate the usefulness of our approach and in particular show that the data-driven conversions consistently outperform the data-independent conversions."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/4ab52371762b735317125e6446a51e8f-Abstract.html,Learning Rankings via Convex Hull Separation,"Glenn Fung, R√≥mer Rosales, Balaji Krishnapuram","We propose efÔ¨Åcient algorithms for learning ranking functions from or- der constraints between sets‚Äîi.e. classes‚Äîof training samples. Our al- gorithms may be used for maximizing the generalized Wilcoxon Mann Whitney statistic that accounts for the partial ordering of the classes: spe- cial cases include maximizing the area under the ROC curve for binary classiÔ¨Åcation and its generalization for ordinal regression. Experiments on public benchmarks indicate that: (a) the proposed algorithm is at least as accurate as the current state-of-the-art; (b) computationally, it is sev- eral orders of magnitude faster and‚Äîunlike current methods‚Äîit is easily able to handle even large datasets with over 20,000 samples."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/4b21cf96d4cf612f239a6c322b10c8fe-Abstract.html,Interpolating between types and tokens by estimating power-law generators,"Sharon Goldwater, Mark Johnson, Thomas L. Griffiths","Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that generically produce power-laws, augmenting stan- dard generative models with an adaptor that produces the appropriate pattern of token frequencies. We show that taking a particular stochastic process ‚Äì the Pitman-Yor process ‚Äì as an adaptor justiÔ¨Åes the appearance of type frequencies in formal analyses of natural language, and improves the performance of a model for unsupervised learning of morphology."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/4b29fa4efe4fb7bc667c7b301b74d52d-Abstract.html,Response Analysis of Neuronal Population with Synaptic Depression,"Wentao Huang, Licheng Jiao, Shan Tan, Maoguo Gong","In this paper, we aim at analyzing the characteristic of neuronal population responses to instantaneous or time-dependent inputs and the role of synapses in neural information processing. We have derived an evolution equation of the membrane potential density function with synaptic depression, and obtain the formulas for analytic computing the response of instantaneous re rate. Through a technical analysis, we arrive at several signi cant conclusions: The background inputs play an important role in information processing and act as a switch betwee temporal integration and coincidence detection. the role of synapses can be regarded as a spatio-temporal lter; it is important in neural information processing for the spatial distribution of synapses and the spatial and temporal relation of inputs. The instantaneous input frequency can affect the response amplitude and phase delay."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/4e2a6330465c8ffcaa696a5a16639176-Abstract.html,Sequence and Tree Kernels with Statistical Feature Mining,"Jun Suzuki, Hideki Isozaki","This paper proposes a new approach to feature selection based on a sta- tistical feature mining technique for sequence and tree kernels. Since natural language data take discrete structures, convolution kernels, such as sequence and tree kernels, are advantageous for both the concept and accuracy of many natural language processing tasks. However, experi- ments have shown that the best results can only be achieved when lim- ited small sub-structures are dealt with by these kernels. This paper dis- cusses this issue of convolution kernels and then proposes a statistical feature selection that enable us to use larger sub-structures effectively. The proposed method, in order to execute efÔ¨Åciently, can be embedded into an original kernel calculation process by using sub-structure min- ing algorithms. Experiments on real NLP tasks conÔ¨Årm the problem in the conventional method and compare the performance of a conventional method to that of the proposed method."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/4ea6a546c19499318091a9df40a13181-Abstract.html,Faster Rates in Regression via Active Learning,"Rebecca Willett, Robert Nowak, Rui M. Castro","This paper presents a rigorous statistical analysis characterizing regimes in which active learning significantly outperforms classical passive learning. Active learning algorithms are able to make queries or select sample locations in an online fashion, depending on the results of the previous queries. In some regimes, this extra flexibility leads to significantly faster rates of error decay than those possible in classical passive learning settings. The nature of these regimes is explored by studying fundamental performance limits of active and passive learning in two illustrative nonparametric function classes. In addition to examining the theoretical potential of active learning, this paper describes a practical algorithm capable of exploiting the extra flexibility of the active setting and provably improving upon the classical passive techniques. Our active learning theory and methods show promise in a number of applications, including field estimation using wireless sensor networks and fault line detection."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/4f1f29888cabf5d45f866fe457737a23-Abstract.html,Top-Down Control of Visual Attention: A Rational Account,"Michael Shettel, Shaun Vecera, Michael Mozer","Theories of visual attention commonly posit that early parallel processes extract con- spicuous features such as color contrast and motion from the visual field. These features are then combined into a saliency map, and attention is directed to the most salient regions first. Top-down attentional control is achieved by modulating the contribution of different feature types to the saliency map. A key source of data concerning attentional control comes from behavioral studies in which the effect of recent experience is exam- ined as individuals repeatedly perform a perceptual discrimination task (e.g., ‚Äúwhat shape is the odd-colored object?‚Äù). The robust finding is that repetition of features of recent trials (e.g., target color) facilitates performance. We view this facilitation as an adaptation to the statistical structure of the environment. We propose a probabilistic model of the environment that is updated after each trial. Under the assumption that attentional control operates so as to make performance more efficient for more likely environmental states, we obtain parsimonious explanations for data from four different experiments. Further, our model provides a rational explanation for why the influence of past experience on attentional control is short lived."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/564645fbd0332f066cbd9d083ddd077c-Abstract.html,The Role of Top-down and Bottom-up Processes in Guiding Eye Movements during Visual Search,"Gregory Zelinsky, Wei Zhang, Bing Yu, Xin Chen, Dimitris Samaras","To investigate how top-down (TD) and bottom-up (BU) information is weighted in the guidance of human search behavior, we manipulated the proportions of BU and TD components in a saliency-based model. The model is biologically plausible and implements an artiÔ¨Åcial retina and a neuronal population code. The BU component is based on feature- contrast. The TD component is deÔ¨Åned by a feature-template match to a stored target representation. We compared the model‚Äôs behavior at differ- ent mixtures of TD and BU components to the eye movement behavior of human observers performing the identical search task. We found that a purely TD model provides a much closer match to human behavior than any mixture model using BU information. Only when biological con- straints are removed (e.g., eliminating the retina) did a BU/TD mixture model begin to approximate human behavior."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/566a9968b43628588e76be5a85a0f9e8-Abstract.html,A Bayesian Framework for Tilt Perception and Confidence,"Odelia Schwartz, Peter Dayan, Terrence J. Sejnowski","The misjudgement of tilt in images lies at the heart of entertaining visual illusions and rigorous perceptual psychophysics. A wealth of findings has attracted many mechanistic models, but few clear computational principles. We adopt a Bayesian approach to perceptual tilt estimation, showing how a smoothness prior offers a powerful way of addressing much confusing data. In particular, we faithfully model recent results showing that confidence in estimation can be systematically affected by the same aspects of images that affect bias. Confidence is central to Bayesian modeling approaches, and is applicable in many other perceptual domains. Perceptual anomalies and illusions, such as the misjudgements of motion and tilt evident in so many psychophysical experiments, have intrigued researchers for decades.13 A Bayesian view48 has been particularly influential in models of motion processing, treating such anomalies as the normative product of prior information (often statistically codifying Gestalt laws) with likelihood information from the actual scenes presented. Here, we expand the range of statistically normative accounts to tilt estimation, for which there are classes of results (on estimation confidence) that are so far not available for motion. The tilt illusion arises when the perceived tilt of a center target is misjudged (ie bias) in the presence of flankers. Another phenomenon, called Crowding, refers to a loss in the confidence (ie sensitivity) of perceived target tilt in the presence of flankers. Attempts have been made to formalize these phenomena quantitatively. Crowding has been modeled as compulsory feature pooling (ie averaging of orientations), ignoring spatial positions.9, 10 The tilt illusion has been explained by lateral interactions11, 12 in populations of orientationtuned units; and by calibration.13 However, most models of this form cannot explain a number of crucial aspects of the data. First, the geometry of the positional arrangement of the stimuli affects attraction versus repulsion in bias, as emphasized by Kapadia et al14 (figure 1A), and others.15, 16 Second, Solomon et al. recently measured bias and sensitivity simultaneously.11 The rich and surprising range of sensitivities, far from flat as a function of flanker angles (figure 1B), are outside the reach of standard models. Moreover, current explanations do not offer a computational account of tilt perception as the outcome of a normative inference process. Here, we demonstrate that a Bayesian framework for orientation estimation, with a prior favoring smoothness, can naturally explain a range of seemingly puzzling tilt data. We explicitly consider both the geometry of the stimuli, and the issue of confidence in the esti-
(A) 6 5 4 3 2 1 0 -1 -2"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/56c82ccd658e09e829f16bb99457bcbc-Abstract.html,Multiple Instance Boosting for Object Detection,"Cha Zhang, John C. Platt, Paul A. Viola","A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MILBoost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classifier."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/56cb94cb34617aeadff1e79b53f38354-Abstract.html,Generalization to Unseen Cases,"Teemu Roos, Peter Gr√ºnwald, Petri Myllym√§ki, Henry Tirri","We analyze classification error on unseen cases, i.e. cases that are different from those in the training set. Unlike standard generalization error, this off-training-set error may differ significantly from the empirical error with high probability even with large sample sizes. We derive a datadependent bound on the difference between off-training-set and standard generalization error. Our result is based on a new bound on the missing mass, which for small samples is stronger than existing bounds based on Good-Turing estimators. As we demonstrate on UCI data-sets, our bound gives nontrivial generalization guarantees in many practical cases. In light of these results, we show that certain claims made in the No Free Lunch literature are overly pessimistic."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/5a2756a3cb9cde852cad3c97e120b656-Abstract.html,Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity,"Amir Navot, Lavi Shpigelman, Naftali Tishby, Eilon Vaadia","We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target func- tion on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on syn- thetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying fea- ture selection we are able to improve prediction quality and suggest a novel way of exploring neural data."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/5b4130c9e891d39891289001cc97d86b-Abstract.html,Visual Encoding with Jittering Eyes,Michele Rucci,"Under natural viewing conditions, small movements of the eye and body prevent the maintenance of a steady direction of gaze. It is known that stimuli tend to fade when they are stabilized on the retina for several sec- onds. However, it is unclear whether the physiological self-motion of the retinal image serves a visual purpose during the brief periods of natural visual Ô¨Åxation. This study examines the impact of Ô¨Åxational instability on the statistics of visual input to the retina and on the structure of neural activity in the early visual system. Fixational instability introduces Ô¨Çuc- tuations in the retinal input signals that, in the presence of natural images, lack spatial correlations. These input Ô¨Çuctuations strongly inÔ¨Çuence neu- ral activity in a model of the LGN. They decorrelate cell responses, even if the contrast sensitivity functions of simulated cells are not perfectly tuned to counter-balance the power-law spectrum of natural images. A decorrelation of neural activity has been proposed to be beneÔ¨Åcial for discarding statistical redundancies in the input signals. Fixational insta- bility might, therefore, contribute to establishing efÔ¨Åcient representations of natural stimuli."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/5be278a9e02bed9248a4674ff62fea2c-Abstract.html,Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods,"Yaakov Engel, Peter Szabo, Dmitry Volkinshtein","The Octopus arm is a highly versatile and complex limb. How the Octo- pus controls such a hyper-redundant arm (not to mention eight of them!) is as yet unknown. Robotic arms based on the same mechanical prin- ciples may render present day robotic arms obsolete. In this paper, we tackle this control problem using an online reinforcement learning al- gorithm, based on a Bayesian approach to policy evaluation known as Gaussian process temporal difference (GPTD) learning. Our substitute for the real arm is a computer simulation of a 2-dimensional model of an Octopus arm. Even with the simpliÔ¨Åcations inherent to this model, the state space we face is a high-dimensional one. We apply a GPTD- based algorithm to this domain, and demonstrate its operation on several learning tasks of varying degrees of difÔ¨Åculty."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/5d75b942ab4bd730bc2e819df9c9a4b5-Abstract.html,Improved risk tail bounds for on-line algorithms,"Nicol√≤ Cesa-bianchi, Claudio Gentile",We prove the strongest known  bound for the risk of hypotheses selected  from the ensemble generated by running a learning algorithm incremen(cid:173) tally on the training data. Our result is based on proof techniques that are  remarkably  different from  the  standard  risk  analysis  based  on  uniform  convergence arguments.
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/5dc126b503e374b0e08231344a7f493f-Abstract.html,Kernelized Infomax Clustering,"David Barber, Felix V. Agakov","We propose a simple information-theoretic approach to soft clus- tering based on maximizing the mutual information I(x, y) between the unknown cluster labels y and the training patterns x with re- spect to parameters of speciÔ¨Åcally constrained encoding distribu- tions. The constraints are chosen such that patterns are likely to be clustered similarly if they lie close to speciÔ¨Åc unknown vectors in the feature space. The method may be conveniently applied to learning the optimal aÔ¨Énity matrix, which corresponds to learn- ing parameters of the kernelized encoder. The procedure does not require computations of eigenvalues of the Gram matrices, which makes it potentially attractive for clustering large data sets."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/60243f9b1ac2dba11ff8131c8f4431e0-Abstract.html,Temporally changing synaptic plasticity,"Minija Tamosiunaite, Bernd Porr, Florentin W√∂rg√∂tter","Recent experimental results suggest that dendritic and back-propagating spikes can influence synaptic plasticity in different ways [1]. In this study we investigate how these signals could temporally interact at dendrites leading to changing plasticity properties at local synapse clusters. Similar to a previous study [2], we employ a differential Hebbian plasticity rule to emulate spike-timing dependent plasticity. We use dendritic (D-) and back-propagating (BP-) spikes as post-synaptic signals in the learning rule and investigate how their interaction will influence plasticity. We will analyze a situation where synapse plasticity characteristics change in the course of time, depending on the type of post-synaptic activity momentarily elicited. Starting with weak synapses, which only elicit local D-spikes, a slow, unspecific growth process is induced. As soon as the soma begins to spike this process is replaced by fast synaptic changes as the consequence of the much stronger and sharper BP-spike, which now dominates the plasticity rule. This way a winner-take-all-mechanism emerges in a two-stage process, enhancing the best-correlated inputs. These results suggest that synaptic plasticity is a temporal changing process by which the computational properties of dendrites or complete neurons can be substantially augmented."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/6244b2ba957c48bc64582cf2bcec3d04-Abstract.html,Modeling Neural Population Spiking Activity with Gibbs Distributions,"Frank Wood, Stefan Roth, Michael J. Black","Probabilistic modeling of correlated neural population firing activity is central to understanding the neural code and building practical decoding algorithms. No parametric models currently exist for modeling multivariate correlated neural data and the high dimensional nature of the data makes fully non-parametric methods impractical. To address these problems we propose an energy-based model in which the joint probability of neural activity is represented using learned functions of the 1D marginal histograms of the data. The parameters of the model are learned using contrastive divergence and an optimization procedure for finding appropriate marginal directions. We evaluate the method using real data recorded from a population of motor cortical neurons. In particular, we model the joint probability of population spiking times and 2D hand position and show that the likelihood of test data under our model is significantly higher than under other models. These results suggest that our model captures correlations in the firing activity. Our rich probabilistic model of neural population activity is a step towards both measurement of the importance of correlations in neural coding and improved decoding of population activity."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/6271faadeedd7626d661856b7a004e27-Abstract.html,Searching for Character Models,"Jaety Edwards, David Forsyth","We introduce a method to automatically improve character models for a handwritten script without the use of transcriptions and using a minimum of document speciÔ¨Åc training data. We show that we can use searches for the words in a dictionary to identify portions of the document whose transcriptions are unambiguous. Using templates extracted from those regions, we retrain our character prediction model to drastically improve our search retrieval performance for words in the document."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/64c31821603ab476a318839606743bd6-Abstract.html,An aVLSI Cricket Ear Model,"Andre V. Schaik, Richard Reeve, Craig Jin, Tara Hamilton","Female crickets can locate males by phonotaxis to the mating song they produce. The behaviour and underlying physiology has been studied in some depth showing that the cricket auditory system solves this complex problem in a unique manner. We present an analogue very large scale integrated (aVLSI) circuit model of this process and show that results from testing the circuit agree with simulation and what is known from the behaviour and physiology of the cricket auditory system. The aVLSI circuitry is now being extended to use on a robot along with previously modelled neural circuitry to better understand the complete sensorimotor pathway.
1
In t r o d u c t i o n
Understanding how insects carry out complex sensorimotor tasks can help in the design of simple sensory and robotic systems. Often insect sensors have evolved into intricate filters matched to extract highly specific data from the environment which solves a particular problem directly with little or no need for further processing [1]. Examples include head stabilisation in the fly, which uses vision amongst other senses to estimate self-rotation and thus to stabilise its head in flight, and phonotaxis in the cricket. Because of the narrowness of the cricket body (only a few millimetres), the Interaural Time Difference (ITD) for sounds arriving at the two sides of the head is very small (1020s). Even with the tympanal membranes (eardrums) located, as they are, on the forelegs of the cricket, the ITD only reaches about 40s, which is too low to detect directly from timings of neural spikes. Because the wavelength of the cricket calling song is significantly greater than the width of the cricket body the Interaural Intensity Difference (IID) is also very low. In the absence of ITD or IID information, the cricket uses phase to determine direction. This is possible because the male cricket produces an almost pure tone for its calling song. *
+
School of Electrical and Information Engineering, Institute of Perception, Action and Behaviour.
Figure 1: The cricket auditory system. Four acoustic inputs channel sounds directly or through tracheal tubes onto two tympanal membranes. Sound from contralateral inputs has to pass a (double) central membrane (the medial septum), inducing a phase delay and reduction in gain. The sound transmission from the contralateral tympanum is very weak, making each eardrum effectively a 3 input system. The physics of the cricket auditory system is well understood [2]; the system (see Figure 1) uses a pair of sound receivers with four acoustic inputs, two on the forelegs, which are the external surfaces of the tympana, and two on the body, the prothoracic or acoustic spiracles [3]. The connecting tracheal tubes are such that interference occurs as sounds travel inside the cricket, producing a directional response at the tympana to frequencies near to that of the calling song. The amplitude of vibration of the tympana, and hence the firing rate of the auditory afferent neurons attached to them, vary as a sound source is moved around the cricket and the sounds from the different inputs move in and out of phase. The outputs of the two tympana match when the sound is straight ahead, and the inputs are bilaterally symmetric with respect to the sound source. However, when sound at the calling song frequency is off-centre the phase of signals on the closer side comes better into alignment, and the signal increases on that side, and conversely decreases on the other. It is that crossover of tympanal vibration amplitudes which allows the cricket to track a sound source (see Figure 6 for example). A simplified version of the auditory system using only two acoustic inputs was implemented in hardware [4], and a simple 8-neuron network was all that was required to then direct a robot to carry out phonotaxis towards a species-specific calling song [5]. A simple simulator was also created to model the behaviour of the auditory system of Figure 1 at different frequencies [6]. Data from Michelsen et al. [2] (Figures 5 and 6) were digitised, and used together with average and ""typical"" values from the paper to choose gains and delays for the simulation. Figure 2 shows the model of the internal auditory system of the cricket from sound arriving at the acoustic inputs through to transmission down auditory receptor fibres. The simulator implements this model up to the summing of the delayed inputs, as well as modelling the external sound transmission. Results from the simulator were used to check the directionality of the system at different frequencies, and to gain a better understanding of its response. It was impractical to check the effect of leg movements or of complex sounds in the simulator due to the necessity of simulating the sound production and transmission. An aVLSI chip was designed to implement the same model, both allowing more complex experiments, such as leg movements to be run, and experiments to be run in the real world.
Figure 2: A model of the auditory system of the cricket, used to build the simulator and the aVLSI implementation (shown in boxes). These experiments with the simulator and the circuits are being published in [6] and the reader is referred to those papers for more details. In the present paper we present the details of the circuits used for the aVLSI implementation."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/6624b6d8217cf71640993409df58204f-Abstract.html,An Analog Visual Pre-Processing Processor Employing Cyclic Line Access in Only-Nearest-Neighbor-Interconnects Architecture,"Yusuke Nakashita, Yoshio Mita, Tadashi Shibata",An analog focal-plane processor having a 128128 photodiode array has been developed for directional edge Ô¨Åltering. It can perform 44-pixel kernel convolution for entire pixels only with 256 steps of simple ana- log processing. Newly developed cyclic line access and row-parallel processing scheme in conjunction with the ‚Äúonly-nearest-neighbor in- terconnects‚Äù architecture has enabled a very simple implementation. A proof-of-concept chip was fabricated in a 0.35-(cid:0)m 2-poly 3-metal CMOS technology and the edge Ô¨Åltering at a rate of 200 frames/sec. has been experimentally demonstrated.
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/663772ea088360f95bac3dc7ffb841be-Abstract.html,The Curse of Highly Variable Functions for Local Kernel Machines,"Yoshua Bengio, Olivier Delalleau, Nicolas L. Roux","We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior  with similarity between examples expressed with a local kernel  are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semisupervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html,Fast Gaussian Process Regression using KD-Trees,"Yirong Shen, Matthias Seeger, Andrew Y. Ng","1 Introduction
We consider (regression) estimation of a function x  u(x ) from noisy observations. If the data-generating process is not well understood, simple parametric learning algorithms, for example ones from the generalized linear model (GLM) family, may be hard to apply because of the difficulty of choosing good features. In contrast, the nonparametric Gaussian process (GP) model [19] offers a flexible and powerful alternative. However, a major drawback of GP models is that the computational cost of learning is about O(n 3 ), and the cost of making a single prediction is O(n), where n is the number of training examples. This high computational complexity severely limits its scalability to large problems, and we believe has proved a significant barrier to the wider adoption of the GP model. In this paper, we address the scaling issue by recognizing that learning and predictions with a GP regression (GPR) model can be implemented using the matrix-vector multiplication (MVM) primitive z  K z . Here, K  Rn,n is the kernel matrix, and z  Rn is an arbitrary vector. For the wide class of so-called isotropic kernels, MVM can be approximated efficiently by arranging the dataset in a tree-type multiresolution data structure such as kd-trees [13], ball trees [11], or cover trees [1]. This approximation can sometimes be made orders of magnitude faster than the direct computation, without sacrificing much in terms of accuracy. Further, the storage requirements for the tree is O(n), while a direct storage of the kernel matrix would require O(n2 ) spare. We demonstrate the efficiency of the tree approach on several large datasets. In the sequel, for the sake of simplicity we will focus on kd-trees (even though it is known that kd-trees do not scale well to high dimensional data). However, it is also completely straightforward to apply the ideas in this paper to other tree-type data structures, for example ball trees and cover trees, which typically scale significantly better to high dimensional data.
2 The Gaussian Process Regression Model Suppose that we observe some data D = {(xi , yi ) | i = 1, . . . , n}, xi  X , yi  R, sampled independently and identically distributed (i.i.d.) from some unknown distribution.
Our goal is to predict the response y on future test points x with small mean-squared error under the data distribution. Our model consists of a latent (unobserved) function x  u so that yi = ui + i , where ui = u(xi ), and the i are independent Gaussian noise variables with zero mean and variance  2 > 0. Following the Bayesian paradigm, we place a prior distribution P (u()) on the function u() and use the posterior distribution P (u()|D)  N (y |u ,  2 I )P (u()) in order to predict y on new points x . Here, y = [y1 , . . . , yn ]T and u = [u1 , . . . , un ]T are vectors in Rn , and N (|, ) is the density of a Gaussian with mean  and covariance . For a GPR model, the prior distribution is a (zero-mean) Gaussian process defined in terms of a positive definite kernel (or covariance) function K : X 2  R. For the purposes of this paper, a GP can be thought of as a mapping from arbitrary finite subsets ~ {x i }  X of points, to corresponding zero-mean Gaussian distributions with covariance ~ ~ ~~ matrix K = (K (x i , x j ))i,j . (This notation indicates that K is a matrix whose (i, j )~~ element is K (x i , x j ).) In this paper, we focus on the problem of speeding up GPR under the assumption that the kernel is monotonic isotropic. A kernel function K (x , x ) is called isotropic if it depends only on the Euclidean distance r = x - x 2 between the points, and it is monotonic isotropic if it can be written as a monotonic function of r.
3 Fast GPR predictions Since u(x1 ), u(x2 ), . . . , u(xn ) and u(x ) are jointly Gaussian, it is easy to see that the predictive (posterior) distribution P (u |D), u = u(x ) is given by , u (1) P (u |D) = N  | kT M -1 y , K (x , x ) - kT M -1 k   where k = [K (x , x1 ), . . . , K (x , xn )]T  Rn , and M = K +  2 I , K = (K (xi , xj ))i,j . Therefore, if p = M -1 y , the optimal prediction under the model is u = kT p , and the predictive variance (of P (u |D)) can be used to quantify our uncer^  tainty in the prediction. Details can be found in [19]. ([16] also provides a tutorial on GPs.) Once p is determined, making a prediction now requires that we compute in in kT p = K (x , xi )pi = wi p i (2)  which is O(n) since it requires scanning through the entire training set and computing K (x , xi ) for each xi in the training set. When the training set is very large, this becomes prohibitively slow. In such situations, it is desirable to use a fast approximation instead of the exact direct implementation. 3.1 Weighted Sum Approximation The computations in Equation 2 can be thought of as a weighted sum, where w i = K (x , xi ) is the weight on the i-th summand pi . We observe that if the dataset is divided into groups where all data points in a group have similar weights, then it is possible to compute a fast approximation to the above weighted sum. For example, let G be a set of data points that all have weights near some value w. The contribution to the weighted sum by points in G is i i i i i wi p i = w pi + (wi - w)pi = w pi + ipi i i i i i i i Where i = wi - w. Assuming that :xi G pi is known in advance, w i :xi G pi can tihen be computed in constant time and used as an approximation to :xi G wi pi if ipi is small. :xi G"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/6b8b8e3bd6ad94b985c1b1f1b7a94cb2-Abstract.html,Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails,"Nebojsa Jojic, Vladimir Jojic, Christopher Meek, David Heckerman, Brendan J. Frey","We introduce a new model of genetic diversity which summarizes a large input dataset into an epitome, a short sequence or a small set of short sequences of probability distributions capturing many overlapping subsequences from the dataset. The epitome as a representation has already been used in modeling real-valued signals, such as images and audio. The discrete sequence model we introduce in this paper targets applications in genetics, from multiple alignment to recombination and mutation inference. In our experiments, we concentrate on modeling the diversity of HIV where the epitome emerges as a natural model for producing relatively small vaccines covering a large number of immune system targets known as epitopes. Our experiments show that the epitome includes more epitopes than other vaccine designs of similar length, including cocktails of consensus strains, phylogenetic tree centers, and observed strains. We also discuss epitome designs that take into account uncertainty about Tcell cross reactivity and epitope presentation. In our experiments, we find that vaccine optimization is fairly robust to these uncertainties."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/6c990b7aca7bc7058f5e98ea909e924b-Abstract.html,Learning Cue-Invariant Visual Responses,Jarmo Hurri,"Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/6d19c113404cee55b4036fce1a37c058-Abstract.html,Learning Topology with the Generative Gaussian Graph and the EM Algorithm,Micha√´l Aupetit,"Given a set of points and a set of prototypes representing them, how to create a graph of the prototypes whose topology accounts for that of the points? This problem had not yet been explored in the framework of statistical learning theory. In this work, we propose a generative model based on the Delaunay graph of the prototypes and the ExpectationMaximization algorithm to learn the parameters. This work is a first step towards the construction of a topological model of a set of points grounded on statistics."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/6e0e24295e8a86282cb559b860416812-Abstract.html,Optimizing spatio-temporal filters for improving Brain-Computer Interfacing,"Guido Dornhege, Benjamin Blankertz, Matthias Krauledat, Florian Losch, Gabriel Curio, Klaus-Robert M√ºller","Brain-Computer Interface (BCI) systems create a novel communication channel from the brain to an output device by bypassing conventional motor output pathways of nerves and muscles. Therefore they could provide a new communication and control option for paralyzed patients. Modern BCI technology is essentially based on techniques for the clas- siÔ¨Åcation of single-trial brain signals. Here we present a novel technique that allows the simultaneous optimization of a spatial and a spectral Ô¨Ålter enhancing discriminability of multi-channel EEG single-trials. The eval- uation of 60 experiments involving 22 different subjects demonstrates the superiority of the proposed algorithm. Apart from the enhanced clas- siÔ¨Åcation, the spatial and/or the spectral Ô¨Ålter that are determined by the algorithm can also be used for further analysis of the data, e.g., for source localization of the respective brain rhythms."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/6e3197aae95c2ff8fcab35cb730f6a86-Abstract.html,Unbiased Estimator of Shape Parameter for Spiking Irregularities under Changing Environments,"Keiji Miura, Masato Okada, Shun-ichi Amari","We considered a gamma distribution of interspike intervals as a statisti- cal model for neuronal spike generation. The model parameters consist of a time-dependent Ô¨Åring rate and a shape parameter that characterizes spiking irregularities of individual neurons. Because the environment changes with time, observed data are generated from the time-dependent Ô¨Åring rate, which is an unknown function. A statistical model with an unknown function is called a semiparametric model, which is one of the unsolved problem in statistics and is generally very difÔ¨Åcult to solve. We used a novel method of estimating functions in information geometry to estimate the shape parameter without estimating the unknown function. We analytically obtained an optimal estimating function for the shape parameter independent of the functional form of the Ô¨Åring rate. This estimation is efÔ¨Åcient without Fisher information loss and better than maximum likelihood estimation."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/6e82873a32b95af115de1c414a1849cb-Abstract.html,Coarse sample complexity bounds for active learning,Sanjoy Dasgupta,"We characterize the sample complexity of active learning problems in terms of a parameter which takes into account the distribution over the input space, the specific target hypothesis, and the desired accuracy."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/6f1d0705c91c2145201df18a1a0c7345-Abstract.html,A PAC-Bayes approach to the Set Covering Machine,"Fran√ßois Laviolette, Mario Marchand, Mohak Shah",We design a new learning algorithm for the Set Covering Ma- chine from a PAC-Bayes perspective and propose a PAC-Bayes risk bound which is minimized for classiÔ¨Åers achieving a non trivial margin-sparsity trade-oÔ¨Ä.
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/6fe131632103526e3a6e8114c78eb1e1-Abstract.html,Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours,Eric Saund,"This paper presents representation and logic for labeling contrast edges and ridges in visual scenes in terms of both surface occlusion (border ownership) and thinline objects. In natural scenes, thinline objects in- clude sticks and wires, while in human graphical communication thin- lines include connectors, dividers, and other abstract devices. Our analy- sis is directed at both natural and graphical domains. The basic problem is to formulate the logic of the interactions among local image events, speciÔ¨Åcally contrast edges, ridges, junctions, and alignment relations, such as to encode the natural constraints among these events in visual scenes. In a sparse heterogeneous Markov Random Field framework, we deÔ¨Åne a set of interpretation nodes and energy/potential functions among them. The minimum energy conÔ¨Åguration found by Loopy Belief Prop- agation is shown to correspond to preferred human interpretation across a wide range of prototypical examples including important illusory con- tour Ô¨Ågures such as the Kanizsa Triangle, as well as more difÔ¨Åcult ex- amples. In practical terms, the approach delivers correct interpretations of inherently ambiguous hand-drawn box-and-connector diagrams at low computational cost."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/73b817090081cef1bca77232f4532c5d-Abstract.html,Non-iterative Estimation with Perturbed Gaussian Markov Processes,"Yunsong Huang, B. Keith Jenkins","We develop an approach for estimation with Gaussian Markov processes that imposes a smoothness prior while allowing for discontinuities. In- stead of propagating information laterally between neighboring nodes in a graph, we study the posterior distribution of the hidden nodes as a whole‚Äîhow it is perturbed by invoking discontinuities, or weakening the edges, in the graph. We show that the resulting computation amounts to feed-forward fan-in operations reminiscent of V1 neurons. Moreover, using suitable matrix preconditioners, the incurred matrix inverse and determinant can be approximated, without iteration, in the same compu- tational style. Simulation results illustrate the merits of this approach."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/74378afe5e8b20910cf1f939e57f0480-Abstract.html,Products of ``Edge-perts,"Max Welling, Peter V. Gehler","Images represent an important and abundant source of data. Understanding their statistical structure has important applications such as image compression and restoration. In this paper we propose a particular kind of probabilistic model, dubbed the ""products of edge-perts model"" to describe the structure of wavelet transformed images. We develop a practical denoising algorithm based on a single edge-pert and show state-ofthe-art denoising performance on benchmark images."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/765d5fb115a9f6a3e0b23b80a5b2e4c4-Abstract.html,Ideal Observers for Detecting Motion: Correspondence Noise,"Hongjing Lu, Alan L. Yuille","We derive a Bayesian Ideal Observer (BIO) for detecting motion and solving the correspondence problem. We obtain Barlow and Tripathy‚Äôs classic model as an approximation. Our psychophysical experiments show that the trends of human performance are similar to the Bayesian Ideal, but overall human performance is far worse. We investigate ways to degrade the Bayesian Ideal but show that even extreme degradations do not approach human performance. Instead we propose that humans perform motion tasks using generic, general purpose, models of motion. We perform more psychophysical experiments which are consistent with humans using a Slow-and-Smooth model and which rule out an alterna- tive model using Slowness."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/780965ae22ea6aee11935f3fb73da841-Abstract.html,Selecting Landmark Points for Sparse Manifold Learning,"Jorge Silva, Jorge Marques, Jo√£o Lemos","There has been a surge of interest in learning non-linear manifold models to approximate high-dimensional data. Both for computational complexity reasons and for generalization capability, sparsity is a desired feature in such models. This usually means dimensionality reduction, which naturally implies estimating the intrinsic dimension, but it can also mean selecting a subset of the data to use as landmarks, which is especially important because many existing algorithms have quadratic complexity in the number of observations. This paper presents an algorithm for selecting landmarks, based on LASSO regression, which is well known to favor sparse approximations because it uses regularization with an l1 norm. As an added benefit, a continuous manifold parameterization, based on the landmarks, is also found. Experimental results with synthetic and real data illustrate the algorithm."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/7a006957be65e608e863301eb98e1808-Abstract.html,Statistical Convergence of Kernel CCA,"Kenji Fukumizu, Arthur Gretton, Francis R. Bach","While kernel canonical correlation analysis (kernel CCA) has been applied in many problems, the asymptotic convergence of the functions estimated from a finite sample to the true functions has not yet been established. This paper gives a rigorous proof of the statistical convergence of kernel CCA and a related method (NOCCO), which provides a theoretical justification for these methods. The result also gives a sufficient condition on the decay of the regularization coefficient in the methods to ensure convergence."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/7e0a0209b929d097bd3e8ef30567a5c1-Abstract.html,Efficient Estimation of OOMs,"Herbert Jaeger, Mingjie Zhao, Andreas Kolling","A standard method to obtain stochastic models for symbolic time series is to train state-emitting hidden Markov models (SE-HMMs) with the Baum-Welch algorithm. Based on observable operator models (OOMs), in the last few months a number of novel learning algorithms for similar purposes have been developed: (1,2) two versions of an ""efficiency sharpening"" (ES) algorithm, which iteratively improves the statistical efficiency of a sequence of OOM estimators, (3) a constrained gradient descent ML estimator for transition-emitting HMMs (TE-HMMs). We give an overview on these algorithms and compare them with SE-HMM/EM learning on synthetic and real-life data."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/7f018eb7b301a66658931cb8a93fd6e8-Abstract.html,AER Building Blocks for Multi-Layer Multi-Chip Neuromorphic Vision Systems,"R. Serrano-Gotarredona, M. Oster, P. Lichtsteiner, A. Linares-Barranco, R. Paz-Vicente, F. Gomez-Rodriguez, H. Kolle Riis, T. Delbruck, S. C. Liu, S. Zahnd, A. M. Whatley, R. Douglas, P. Hafliger, G. Jimenez-Moreno, A. Civit, T. Serrano-Gotarredona, A. Acosta-Jimenez, B. Linares-Barranco","A 5-layer neuromorphic vision processor whose components communicate spike events asychronously using the address-event- representation (AER) is demonstrated. The system includes a retina chip, two convolution chips, a 2D winner-take-all chip, a delay line chip, a learning classiÔ¨Åer chip, and a set of PCBs for computer interfacing and address space remappings. The components use a mixture of analog and digital computation and will learn to classify trajectories of a moving object. A complete experimental setup and measurements results are shown."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/7f141cf8e7136ce8701dc6636c2a6fe4-Abstract.html,A Hierarchical Compositional System for Rapid Object Detection,"Long Zhu, Alan L. Yuille","We describe a hierarchical compositional system for detecting deformable objects in images. Objects are represented by graphical models. The algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower-level elements of the tree correspond to simpler features. The algorithm proceeds by passing simple messages up and down the tree. The method works rapidly, in under a second, on 320  240 images. We demonstrate the approach on detecting cats, horses, and hands. The method works in the presence of background clutter and occlusions. Our approach is contrasted with more traditional methods such as dynamic programming and belief propagation."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/801fd8c2a4e79c1d24a40dc735c051ae-Abstract.html,Conditional Visual Tracking in Kernel Space,"Cristian Sminchisescu, Atul Kanujia, Zhiguo Li, Dimitris Metaxas","We present a conditional temporal probabilistic framework for recon- structing 3D human motion in monocular video based on descriptors en- coding image silhouette observations. For computational efÔ¨Åciency we restrict visual inference to low-dimensional kernel induced non-linear state spaces. Our methodology (kBME) combines kernel PCA-based non-linear dimensionality reduction (kPCA) and Conditional Bayesian Mixture of Experts (BME) in order to learn complex multivalued pre- dictors between observations and model hidden states. This is necessary for accurate, inverse, visual perception inferences, where several proba- ble, distant 3D solutions exist due to noise or the uncertainty of monoc- ular perspective projection. Low-dimensional models are appropriate because many visual processes exhibit strong non-linear correlations in both the image observations and the target, hidden state variables. The learned predictors are temporally combined within a conditional graphi- cal model in order to allow a principled propagation of uncertainty. We study several predictors and empirically show that the proposed algo- rithm positively compares with techniques based on regression, Kernel Dependency Estimation (KDE) or PCA alone, and gives results competi- tive to those of high-dimensional mixture predictors at a fraction of their computational cost. We show that the method successfully reconstructs the complex 3D motion of humans in real monocular video sequences.
1
Introduction and Related Work
We consider the problem of inferring 3D articulated human motion from monocular video. This research topic has applications for scene understanding including human-computer in- terfaces, markerless human motion capture, entertainment and surveillance. A monocular approach is relevant because in real-world settings the human body parts are rarely com- pletely observed even when using multiple cameras. This is due to occlusions form other people or objects in the scene. A robust system has to necessarily deal with incomplete, ambiguous and uncertain measurements. Methods for 3D human motion reconstruction can be classiÔ¨Åed as generative and discriminative. They both require a state representation, namely a 3D human model with kinematics (joint angles) or shape (surfaces or joint po- sitions) and they both use a set of image features as observations for state inference. The computational goal in both cases is the conditional distribution for the model state given
image observations.
Generative model-based approaches [6, 16, 14, 13] have been demonstrated to Ô¨Çexibly re- construct complex unknown human motions and to naturally handle problem constraints. However it is difÔ¨Åcult to construct reliable observation likelihoods due to the complexity of modeling human appearance. This varies widely due to different clothing and defor- mation, body proportions or lighting conditions. Besides being somewhat indirect, the generative approach further imposes strict conditional independence assumptions on the temporal observations given the states in order to ensure computational tractability. Due to these factors inference is expensive and produces highly multimodal state distributions [6, 16, 13]. Generative inference algorithms require complex annealing schedules [6, 13] or systematic non-linear search for local optima [16] in order to ensure continuing tracking.
These difÔ¨Åculties motivate the advent of a complementary class of discriminative algo- rithms [10, 12, 18, 2], that approximate the state conditional directly, in order to simplify inference. However, inverse, observation-to-state multivalued mappings are difÔ¨Åcult to learn (see e.g. Ô¨Åg. 1a) and a probabilistic temporal setting is necessary. In an earlier paper [15] we introduced a probabilistic discriminative framework for human motion reconstruc- tion. Because the method operates in the originally selected state and observation spaces that can be task generic, therefore redundant and often high-dimensional, inference is more expensive and can be less robust. To summarize, reconstructing 3D human motion in a
Figure 1: (a, Left) Example of 180o ambiguity in predicting 3D human poses from sil- houette image features (center). It is essential that multiple plausible solutions (e.g. F1 and F2) are correctly represented and tracked over time. A single state predictor will either average the distant solutions or zig-zag between them, see also tables 1 and 2. (b, Right) A conditional chain model. The local distributions p(ytjyt(cid:0)1; zt) or p(ytjzt) are learned as in Ô¨Åg. 2. For inference, the predicted local state conditional is recursively combined with the Ô¨Åltered prior c.f . (1).
conditional temporal framework poses the following difÔ¨Åculties: (i) The mapping between temporal observations and states is multivalued (i.e. the local conditional distributions to be learned are multimodal), therefore it cannot be accurately represented using global function approximations. (ii) Human models have multivariate, high-dimensional continuous states of 50 or more human joint angles. The temporal state conditionals are multimodal which makes efÔ¨Åcient Kalman Ô¨Åltering algorithms inapplicable. General inference methods (par- ticle Ô¨Ålters, mixtures) have to be used instead, but these are expensive for high-dimensional models (e.g. when reconstructing the motion of several people that operate in a joint state space). (iii) The components of the human state and of the silhouette observation vector ex- hibit strong correlations, because many repetitive human activities like walking or running have low intrinsic dimensionality. It appears wasteful to work with high-dimensional states of 50+ joint angles. Even if the space were truly high-dimensional, predicting correlated state dimensions independently may still be suboptimal.
In this paper we present a conditional temporal estimation algorithm that restricts visual inference to low-dimensional, kernel induced state spaces. To exploit correlations among observations and among state variables, we model the local, temporal conditional distri- butions using ideas from Kernel PCA [11, 19] and conditional mixture modeling [7, 5], here adapted to produce multiple probabilistic predictions. The corresponding predictor is
referred to as a Conditional Bayesian Mixture of Low-dimensional Kernel-Induced Experts (kBME). By integrating it within a conditional graphical model framework (Ô¨Åg. 1b), we can exploit temporal constraints probabilistically. We demonstrate that this methodology is effective for reconstructing the 3D motion of multiple people in monocular video. Our con- tribution w.r.t. [15] is a probabilistic conditional inference framework that operates over a non-linear, kernel-induced low-dimensional state spaces, and a set of experiments (on both real and artiÔ¨Åcial image sequences) that show how the proposed framework positively com- pares with powerful predictors based on KDE, PCA, or with the high-dimensional models of [15] at a fraction of their cost.
2 Probabilistic Inference in a Kernel Induced State Space
We work with conditional graphical models with a chain structure [9], as shown in Ô¨Åg. 1b, These have continuous temporal states yt, t = 1 : : : T , observations zt. For compactness, we denote joint states Yt = (y1; y2; : : : ; yt) or joint observations Zt = (z1; : : : ; zt). Learning and inference are based on local conditionals: p(ytjzt) and p(ytjyt(cid:0)1; zt), with yt and zt being low-dimensional, kernel induced representations of some initial model having state xt and observation rt. We obtain zt; yt from rt, xt using kernel PCA [11, 19]. Inference is performed in a low-dimensional, non-linear, kernel induced latent state space (see Ô¨Åg. 1b and Ô¨Åg. 2 and (1)). For display or error reporting, we compute the original conditional p(xjr), or a temporally Ô¨Åltered version p(xtjRt); Rt = (r1; r2; : : : ; rt), using a learned pre-image state map [3].
2.1 Density Propagation for Continuous Conditional Chains
For online Ô¨Åltering, we compute the optimal distribution p(ytjZt) for the state yt, con- ditioned by observations Zt up to time t. The Ô¨Åltered density can be recursively derived as:
p(ytjZt) = Zyt(cid:0)1
p(ytjyt(cid:0)1; zt)p(yt(cid:0)1jZt(cid:0)1)
(1)
We compute using a conditional mixture for p(ytjyt(cid:0)1; zt) (a Bayesian mixture of experts c.f . x2.2) and the prior p(yt(cid:0)1jZt(cid:0)1), each having, say M components. We integrate M 2 pairwise products of Gaussians analytically. The means of the expanded posterior are clus- tered and the centers are used to initialize a reduced M-component Kullback-Leibler ap- proximation that is reÔ¨Åned using gradient descent [15]. The propagation rule (1) is similar to the one used for discrete state labels [9], but here we work with multivariate continuous state spaces and represent the local multimodal state conditionals using kBME (Ô¨Åg. 2), and not log-linear models [9] (these would require intractable normalization). This complex continuous model rules out inference based on Kalman Ô¨Åltering or dynamic programming [9].
2.2 Learning Bayesian Mixtures over Kernel Induced State Spaces (kBME)
In order to model conditional mappings between low-dimensional non-linear spaces we rely on kernel dimensionality reduction and conditional mixture predictors. The authors of KDE [19] propose a powerful structured unimodal predictor. This works by decorrelating the output using kernel PCA and learning a ridge regressor between the input and each decorrelated output dimension.
Our procedure is also based on kernel PCA but takes into account the structure of the studied visual problem where both inputs and outputs are likely to be low-dimensional and the mapping between them multivalued. The output variables xi are projected onto the column vectors of the principal space in order to obtain their principal coordinates yi. A"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/85690f81aadc1749175c187784afc9ee-Abstract.html,Fast Krylov Methods for N-Body Learning,"Nando D. Freitas, Yang Wang, Maryam Mahdaviani, Dustin Lang","This paper addresses the issue of numerical computation in machine learning domains based on similarity metrics, such as kernel methods, spectral techniques and Gaussian processes. It presents a general solution strategy based on Krylov subspace iteration and fast N-body learning methods. The experiments show significant gains in computation and storage on datasets arising in image segmentation, object detection and dimensionality reduction. The paper also presents theoretical bounds on the stability of these methods."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/86c4ad52768c511046fea7b2d42b300c-Abstract.html,A Connectionist Model for Constructive Modal Reasoning,"Artur Garcez, Luis C. Lamb, Dov M. Gabbay","We present a new connectionist model for constructive, intuitionistic modal reasoning. We use ensembles of neural networks to represent in- tuitionistic modal theories, and show that for each intuitionistic modal program there exists a corresponding neural network ensemble that com- putes the program. This provides a massively parallel model for intu- itionistic modal reasoning, and sets the scene for integrated reasoning, knowledge representation, and learning of intuitionistic theories in neural networks, since the networks in the ensemble can be trained by examples using standard neural learning algorithms."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/881c6efa917cff1c97a74e03e15f43e8-Abstract.html,Spiking Inputs to a Winner-take-all Network,"Matthias Oster, Shih-Chii Liu","Recurrent networks that perform a winner-take-all computation have been studied extensively. Although some of these studies include spik- ing networks, they consider only analog input rates. We present results of this winner-take-all computation on a network of integrate-and-Ô¨Åre neurons which receives spike trains as inputs. We show how we can con- Ô¨Ågure the connectivity in the network so that the winner is selected after a pre-determined number of input spikes. We discuss spiking inputs with both regular frequencies and Poisson-distributed rates. The robustness of the computation was tested by implementing the winner-take-all network on an analog VLSI array of 64 integrate-and-Ô¨Åre neurons which have an innate variance in their operating parameters."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html,Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization,"Maxim Raginsky, Svetlana Lazebnik","We introduce a technique for dimensionality estimation based on the notion of quantization dimension, which connects the asymptotic optimal quantization error for a probability distribution on a manifold to its intrinsic dimension. The definition of quantization dimension yields a family of estimation algorithms, whose limiting case is equivalent to a recent method based on packing numbers. Using the formalism of high-rate vector quantization, we address issues of statistical consistency and analyze the behavior of our scheme in the presence of noise."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/894db62f7b7a6ed2f2a277dae56a017c-Abstract.html,Generalization error bounds for classifiers trained with interdependent data,"Nicolas Usunier, Massih R. Amini, Patrick Gallinari","In this paper we propose a general framework to study the generalization properties of binary classiÔ¨Åers trained with data which may be depen- dent, but are deterministically generated upon a sample of independent examples. It provides generalization bounds for binary classiÔ¨Åcation and some cases of ranking problems, and clariÔ¨Åes the relationship between these learning tasks."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/8b3bac12926cc1d9fb5d68783376971d-Abstract.html,Bayesian model learning in human visual perception,"Gerg≈ë Orb√°n, Jozsef Fiser, Richard N Aslin, M√°t√© Lengyel",Humans make optimal perceptual decisions in noisy and ambiguous conditions. Computations underlying such optimal behavior have been shown to rely on probabilistic inference according to generative models whose structure is usually taken to be known a priori. We argue that Bayesian model selection is ideal for inferring similar and even more complex model structures from experience. We Ô¨Ånd in experiments that humans learn subtle statistical properties of visual scenes in a completely unsupervised manner. We show that these Ô¨Åndings are well captured by Bayesian model learning within a class of models that seek to explain observed variables by independent hidden causes.
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/8e930496927757aac0dbd2438cb3f4f6-Abstract.html,Active Learning For Identifying Function Threshold Boundaries,"Brent Bryan, Robert C. Nichol, Christopher R Genovese, Jeff Schneider, Christopher J. Miller, Larry Wasserman","We present an efÔ¨Åcient algorithm to actively select queries for learning the boundaries separating a function domain into regions where the func- tion is above and below a given threshold. We develop experiment selec- tion methods based on entropy, misclassiÔ¨Åcation rate, variance, and their combinations, and show how they perform on a number of data sets. We then show how these algorithms are used to determine simultaneously valid 1 ‚àí Œ± conÔ¨Ådence intervals for seven cosmological parameters. Ex- perimentation shows that the algorithm reduces the computation neces- sary for the parameter estimation problem by an order of magnitude."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/8e987cf1b2f1f6ffa6a43066798b4b7f-Abstract.html,Cue Integration for Figure/Ground Labeling,"Xiaofeng Ren, Jitendra Malik, Charless C. Fowlkes","We present a model of edge and region grouping using a conditional random field built over a scale-invariant representation of images to integrate multiple cues. Our model includes potentials that capture low-level similarity, mid-level curvilinear continuity and high-level object shape. Maximum likelihood parameters for the model are learned from human labeled groundtruth on a large collection of horse images using belief propagation. Using held out test data, we quantify the information gained by incorporating generic mid-level cues and high-level shape."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/8fc687aa152e8199fe9e73304d407bca-Abstract.html,Inferring Motor Programs from Images of Handwritten Digits,"Vinod Nair, Geoffrey E. Hinton","We describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program. We show how neural networks can be trained to infer the motor programs required to accurately reconstruct the MNIST digits. The inferred motor programs can be used directly for digit classiÔ¨Åcation, but they can also be used in other ways. By adding noise to the motor program inferred from an MNIST image we can generate a large set of very different images of the same class, thus enlarging the training set available to other methods. We can also use the motor programs as additional, highly informative outputs which reduce overÔ¨Åtting when training a feed-forward classiÔ¨Åer.
1 Overview The idea that patterns can be recognized by Ô¨Åguring out how they were generated has been around for at least half a century [1, 2] and one of the Ô¨Årst proposed applications was the recognition of handwriting using a generative model that involved pairs of opposing springs [3, 4]. The ‚Äúanalysis-by-synthesis‚Äù approach is attractive because the true generative model should provide the most natural way to characterize a class of patterns. The handwritten 2‚Äôs in Ô¨Ågure 1, for example, are very variable when viewed as pixels but they have very similar motor programs. Despite its obvious merits, analysis-by-synthesis has had few successes, partly because it is computationally expensive to invert non-linear generative models and partly because the underlying parameters of the generative model are unknown for most large data sets. For example, the only source of information about how the MNIST digits were drawn is the images themselves.
We describe a simple generative model in which a pen is controlled by two pairs of op- posing springs whose stiffnesses are speciÔ¨Åed by a motor program. If the sequence of stiffnesses is speciÔ¨Åed correctly, the model can produce images which look very like the MNIST digits. Using a separate network for each digit class, we show that backpropaga- tion can be used to learn a ‚Äúrecognition‚Äù network that maps images to the motor programs required to produce them. An interesting aspect of this learning is that the network creates its own training data, so it does not require the training images to be labelled with motor programs. Each recognition network starts with a single example of a motor program and grows an ‚Äúisland of competence‚Äù around this example, progressively extending the region over which it can map small changes in the image to the corresponding small changes in the motor program (see Ô¨Ågure 2).
Figure 1: An MNIST image of a 2 and the additional images that can be generated by infer- ring the motor program and then adding random noise to it. The pixels are very different, but they are all clearly twos.
Fairly good digit recognition can be achieved by using the 10 recognition networks to Ô¨Ånd 10 motor programs for a test image and then scoring each motor program by its squared error in reconstructing the image. The 10 scores are then fed into a softmax classiÔ¨Åer. Recognition can be improved by using PCA to model the distribution of motor trajectories for each class and using the distance of a motor trajectory from the relevant PCA hyperplane as an additional score.
Each recognition network is solving a difÔ¨Åcult global search problem in which the correct motor program must be found by a single, ‚Äúopen-loop‚Äù pass through the network. More accurate recognition can be achieved by using this open-loop global search to initialize an iterative, closed-loop local search which uses the error in the reconstructed image to re- vise the motor program. This requires reconstruction errors in pixel space to be mapped to corrections in the space of spring stiffnesses. We cannot backpropagate errors through the generative model because it is just a hand-coded computer program. So we learn ‚Äúgenera- tive‚Äù networks, one per digit class, that emulate the generator. After learning, backpropa- gation through these generative networks is used to convert pixel reconstruction errors into stiffness corrections.
Our Ô¨Ånal system gives 1.82% error on the MNIST test set which is similar to the 1.7% achieved by a very different generative approach [5] but worse than the 1.53% produced by the best backpropagation networks or the 1.4% produced by support vector machines [6]. It is much worse than the 0.4% produced by convolutional neural networks that use cleverly enhanced training sets [7]. Recognition of test images is quite slow because it uses ten different recognition networks followed by iterative local search. There is, however, a much more efÔ¨Åcient way to make use of our ability to extract motor programs. They can be treated as additional output labels when using backpropagation to train a single, multi- layer, discriminative neural network. These additional labels act as a very informative regularizer that reduces the error rate from 1.53% to 1.27% in a network with two hidden layers of 500 units each. This is a new method of improving performance that can be used in conjunction with other tricks such as preprocessing the images, enhancing the training set or using convolutional neural nets [8, 7].
2 A simple generative model for drawing digits
The generative model uses two pairs of opposing springs at right angles. One end of each spring is attached to a frictionless horizontal or vertical rail that is 39 pixels from the center of the image. The other end is attached to a ‚Äúpen‚Äù that has signiÔ¨Åcant mass. The springs themselves are weightless and have zero rest length. The pen starts at the equilibrium position deÔ¨Åned by the initial stiffnesses of the four springs. It then follows a trajectory that is determined by the stiffness of each spring at each of the 16 subsequent time steps in the motor program. The mass is large compared with the rate at which the stiffnesses change, so the system is typically far from equilibrium as it follows the smooth trajectory. On each time step, the momentum is multiplied by 0.9 to simulate viscosity. A coarse-grain trajectory is computed by using one step of forward integration for each time step in the motor program, so it contains 17 points. The code is at www.cs.toronto.edu/‚àº hinton/code.
Figure 2: The training data for each class-speciÔ¨Åc recognition network is produced by adding noise to motor programs that are inferred from MNIST images using the current parameters of the recognition network. To initiate this process, the biases of the output units are set by hand so that they represent a prototypical motor program for the class.
Given a coarse-grain trajectory, we need a way of assigning an intensity to each pixel. We tried various methods until we hand-evolved one that was able to reproduce the MNIST im- ages fairly accurately, but we suspect that many other methods would be just as good. For each point on the coarse trajectory, we share two units of ink between the the four closest pixels using bilinear interpolation. We also use linear interpolation to add three Ô¨Åne-grain trajectory points between every pair of coarse-grain points. These Ô¨Åne-grain points also contribute ink to the pixels using bilinear interpolation, but the amount of ink they con- tribute is zero if they are less than one pixel apart and rises linearly to the same amount as the coarse-grain points if they are more than two pixels apart. This generates a thin skeleton with a fairly uniform ink density. To Ô¨Çesh-out the skeleton, we use two ‚Äúink parameters‚Äù, a, b, to specify a 3 √ó 3 kernel of the form b(1 + a)[ a 12 ] which is convolved with the image four times. Finally, the pixel intensities are clipped to lie in the interval [0,1]. The matlab code is at www.cs.toronto.edu/‚àº hinton/code. The values of 2a and b/1.5 are additional, logistic outputs of the recognition networks1."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/9087b0efc7c7acd1ef7e153678809c77-Abstract.html,Dual-Tree Fast Gauss Transforms,"Dongryeol Lee, Andrew W. Moore, Alexander G. Gray","In previous work we presented an efÔ¨Åcient approach to computing ker- nel summations which arise in many machine learning methods such as kernel density estimation. This approach, dual-tree recursion with Ô¨Ånite- difference approximation, generalized existing methods for similar prob- lems arising in computational physics in two ways appropriate for sta- tistical problems: toward distribution sensitivity and general dimension, partly by avoiding series expansions. While this proved to be the fastest practical method for multivariate kernel density estimation at the optimal bandwidth, it is much less efÔ¨Åcient at larger-than-optimal bandwidths. In this work, we explore the extent to which the dual-tree approach can be integrated with multipole-like Hermite expansions in order to achieve reasonable efÔ¨Åciency across all bandwidth scales, though only for low di- mensionalities. In the process, we derive and demonstrate the Ô¨Årst truly hierarchical fast Gauss transforms, effectively combining the best tools from discrete algorithms and continuous approximation theory.
1 Fast Gaussian Summation
Kernel summations are fundamental in both statistics/learning and computational physics."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/92bf5e6240737e0326ea59846a83e076-Abstract.html,Describing Visual Scenes using Transformed Dirichlet Processes,"Antonio Torralba, Alan S. Willsky, Erik B. Sudderth, William T. Freeman","Motivated by the problem of learning to detect and recognize objects with minimal supervision, we develop a hierarchical probabilistic model for the spatial structure of visual scenes. In contrast with most existing models, our approach explicitly captures uncertainty in the number of object instances depicted in a given image. Our scene model is based on the transformed Dirichlet process (TDP), a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data. For visual scenes, mixture components describe the spatial structure of visual features in an objectcentered coordinate frame, while transformations model the object positions in a particular image. Learning and inference in the TDP, which has many potential applications beyond computer vision, is based on an empirically effective Gibbs sampler. Applied to a dataset of partially labeled street scenes, we show that the TDP's inclusion of spatial structure improves detection performance, flexibly exploiting partially labeled training images."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/952c3ff98a6acdc36497d839e31aa57c-Abstract.html,Worst-Case Bounds for Gaussian Process Models,"Sham M. Kakade, Matthias W. Seeger, Dean P. Foster","We present a competitive analysis of some non-parametric Bayesian al- gorithms in a worst-case online learning setting, where no probabilistic assumptions about the generation of the data are made. We consider models which use a Gaussian process prior (over the space of all func- tions) and provide bounds on the regret (under the log loss) for com- monly used non-parametric Bayesian algorithms ‚Äî including Gaussian regression and logistic regression ‚Äî which show how these algorithms can perform favorably under rather general conditions. These bounds ex- plicitly handle the inÔ¨Ånite dimensionality of these non-parametric classes in a natural way. We also make formal connections to the minimax and minimum description length (MDL) framework. Here, we show precisely how Bayesian Gaussian regression is a minimax strategy."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/9719a00ed0c5709d80dfef33795dcef3-Abstract.html,Analyzing Auditory Neurons by Learning Distance Functions,"Inna Weiner, Tomer Hertz, Israel Nelken, Daphna Weinshall","We present a novel approach to the characterization of complex sensory neurons. One of the main goals of characterizing sensory neurons is to characterize dimensions in stimulus space to which the neurons are highly sensitive (causing large gradients in the neural responses) or al- ternatively dimensions in stimulus space to which the neuronal response are invariant (deÔ¨Åning iso-response manifolds). We formulate this prob- lem as that of learning a geometry on stimulus space that is compatible with the neural responses: the distance between stimuli should be large when the responses they evoke are very different, and small when the re- sponses they evoke are similar. Here we show how to successfully train such distance functions using rather limited amount of information. The data consisted of the responses of neurons in primary auditory cortex (A1) of anesthetized cats to 32 stimuli derived from natural sounds. For each neuron, a subset of all pairs of stimuli was selected such that the responses of the two stimuli in a pair were either very similar or very dissimilar. The distance function was trained to Ô¨Åt these constraints. The resulting distance functions generalized to predict the distances between the responses of a test stimulus and the trained stimuli."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/9752d873fa71c19dc602bf2a0696f9b5-Abstract.html,Cyclic Equilibria in Markov Games,"Martin Zinkevich, Amy Greenwald, Michael L. Littman","Although variants of value iteration have been proposed for Ô¨Ånding Nash or correlated equilibria in general-sum Markov games, these variants have not been shown to be effective in general. In this paper, we demon- strate by construction that existing variants of value iteration cannot Ô¨Ånd stationary equilibrium policies in arbitrary general-sum Markov games. Instead, we propose an alternative interpretation of the output of value it- eration based on a new (non-stationary) equilibrium concept that we call ‚Äúcyclic equilibria.‚Äù We prove that value iteration identiÔ¨Åes cyclic equi- libria in a class of games in which it fails to Ô¨Ånd stationary equilibria. We also demonstrate empirically that value iteration Ô¨Ånds cyclic equilibria in nearly all examples drawn from a random distribution of Markov games."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/98d8a23fd60826a2a474c5b4f5811707-Abstract.html,The Information-Form Data Association Filter,"Brad Schumitsch, Sebastian Thrun, Gary Bradski, Kunle Olukotun","This paper presents a new Ô¨Ålter for online data association problems in high-dimensional spaces. The key innovation is a representation of the data association posterior in information form, in which the ‚Äúproxim- ity‚Äù of objects and tracks are expressed by numerical links. Updating these links requires linear time, compared to exponential time required for computing the exact posterior probabilities. The paper derives the algorithm formally and provides comparative results using data obtained by a real-world camera array and by a large-scale sensor network simu- lation."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/9a11883317fde3aef2e2432a58c86779-Abstract.html,Consistency of one-class SVM and related algorithms,"R√©gis Vert, Jean-philippe Vert","We determine the asymptotic limit of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to infinity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held fixed. Non-asymptotic convergence bounds to this limit in the L2 sense are provided, together with upper bounds on the classification error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the first time to be a consistent density level set estimator."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/9dc372713683fd865d366d5d9ee810ba-Abstract.html,Nested sampling for Potts models,"Iain Murray, David MacKay, Zoubin Ghahramani, John Skilling","Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior sub ject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/9e82757e9a1c12cb710ad680db11f6f1-Abstract.html,Correlated Topic Models,"John D. Lafferty, David M. Blei","Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [1]. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/a226e450e214f350856e2980b6e55ac9-Abstract.html,Learning in Silicon: Timing is Everything,"John V. Arthur, Kwabena Boahen","We describe a neuromorphic chip that uses binary synapses with spike timing-dependent plasticity (STDP) to learn stimulated patterns of activ- ity and to compensate for variability in excitability. SpeciÔ¨Åcally, STDP preferentially potentiates (turns on) synapses that project from excitable neurons, which spike early, to lethargic neurons, which spike late. The additional excitatory synaptic current makes lethargic neurons spike ear- lier, thereby causing neurons that belong to the same pattern to spike in synchrony. Once learned, an entire pattern can be recalled by stimulating a subset.
1 Variability in Neural Systems
Evidence suggests precise spike timing is important in neural coding, speciÔ¨Åcally, in the hippocampus. The hippocampus uses timing in the spike activity of place cells (in addition to rate) to encode location in space [1]. Place cells employ a phase code: the timing at which a neuron spikes relative to the phase of the inhibitory theta rhythm (5-12Hz) conveys information. As an animal approaches a place cell‚Äôs preferred location, the place cell not only increases its spike rate, but also spikes at earlier phases in the theta cycle.
To implement a phase code, the theta rhythm is thought to prevent spiking until the input synaptic current exceeds the sum of the neuron threshold and the decreasing inhibition on the downward phase of the cycle [2]. However, even with identical inputs and common theta inhibition, neurons do not spike in synchrony. Variability in excitability spreads the activity in phase. Lethargic neurons (such as those with high thresholds) spike late in the theta cycle, since their input exceeds the sum of the neuron threshold and theta inhibition only after the theta inhibition has had time to decrease. Conversely, excitable neurons (such as those with low thresholds) spike early in the theta cycle. Consequently, variability in excitability translates into variability in timing.
We hypothesize that the hippocampus achieves its precise spike timing (about 10ms) through plasticity enhanced phase-coding (PEP). The source of hippocampal timing preci- sion in the presence of variability (and noise) remains unexplained. Synaptic plasticity can compensate for variability in excitability if it increases excitatory synaptic input to neurons in inverse proportion to their excitabilities. Recasting this in a phase-coding framework, we desire a learning rule that increases excitatory synaptic input to neurons directly related to their phases. Neurons that lag require additional synaptic input, whereas neurons that lead"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/a36b0dcd1e6384abc0e1867860ad3ee3-Abstract.html,Correcting sample selection bias in maximum entropy density estimation,"Miroslav Dud√≠k, Steven J. Phillips, Robert E. Schapire","We study the problem of maximum entropy density estimation in the presence of known sample selection bias. We propose three bias cor- rection approaches. The Ô¨Årst one takes advantage of unbiased sufÔ¨Åcient statistics which can be obtained from biased samples. The second one es- timates the biased distribution and then factors the bias out. The third one approximates the second by only using samples from the sampling distri- bution. We provide guarantees for the Ô¨Årst two approaches and evaluate the performance of all three approaches in synthetic experiments and on real data from species habitat modeling, where maxent has been success- fully applied and where sample selection bias is a signiÔ¨Åcant problem."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/a3788c8c64fd65c470e23e7534c3ebc8-Abstract.html,Rate Distortion Codes in Sensor Networks: A System-level Analysis,"Tatsuto Murayama, Peter Davis","This paper provides a system-level analysis of a scalable distributed sens- ing model for networked sensors. In our system model, a data center ac- quires data from a bunch of L sensors which each independently encode their noisy observations of an original binary sequence, and transmit their encoded data sequences to the data center at a combined rate R, which is limited. Supposing that the sensors use independent LDGM rate dis- tortion codes, we show that the system performance can be evaluated for any given Ô¨Ånite R when the number of sensors L goes to inÔ¨Ånity. The analysis shows how the optimal strategy for the distributed sensing prob- lem changes at critical values of the data rate R or the noise level."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/a4666cd9e1ab0e4abf05a0fb232f4ad3-Abstract.html,Beyond Pair-Based STDP: a Phenomenological Rule for Spike Triplet and Frequency Effects,"Jean-pascal Pfister, Wulfram Gerstner","While classical experiments on spike-timing dependent plasticity analyzed synaptic changes as a function of the timing of pairs of pre- and postsynaptic spikes, more recent experiments also point to the effect of spike triplets. Here we develop a mathematical framework that allows us to characterize timing based learning rules. Moreover, we identify a candidate learning rule with five variables (and 5 free parameters) that captures a variety of experimental data, including the dependence of potentiation and depression upon pre- and postsynaptic firing frequencies. The relation to the Bienenstock-Cooper-Munro rule as well as to some timing-based rules is discussed."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/a57e8915461b83adefb011530b711704-Abstract.html,Is Early Vision Optimized for Extracting Higher-order Dependencies?,"Yan Karklin, Michael S. Lewicki","Linear implementations of the efficient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive fields in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6, 7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been fixed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and find that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and filters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive fields and collectively span a broad range of spatial scales. Our work unifies several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/a6db4ed04f1621a119799fd3d7545d3d-Abstract.html,An Approximate Inference Approach for the PCA Reconstruction Error,Manfred Opper,"The problem of computing a resample estimate for the reconstruction error in PCA is reformulated as an inference problem with the help of the replica method. Using the expectation consistent (EC) approximation, the intractable inference problem can be solved efficiently using only two variational parameters. A perturbative correction to the result is computed and an alternative simplified derivation is also presented."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/a78482ce76496fcf49085f2190e675b4-Abstract.html,Active Bidirectional Coupling in a Cochlear Chip,"Bo Wen, Kwabena A. Boahen","We present a novel cochlear model implemented in analog very large scale integration (VLSI) technology that emulates nonlinear active cochlear behavior. This silicon cochlea includes outer hair cell (OHC) electromotility through active bidirectional coupling (ABC), a mech- anism we proposed in which OHC motile forces, through the mi- croanatomical organization of the organ of Corti, realize the cochlear ampliÔ¨Åer. Our chip measurements demonstrate that frequency responses become larger and more sharply tuned when ABC is turned on; the de- gree of the enhancement decreases with input intensity as ABC includes saturation of OHC forces.
1 Silicon Cochleae
Cochlear models, mathematical and physical, with the shared goal of emulating nonlinear active cochlear behavior, shed light on how the cochlea works if based on cochlear mi- cromechanics. Among the modeling efforts, silicon cochleae have promise in meeting the need for real-time performance and low power consumption. Lyon and Mead developed the Ô¨Årst analog electronic cochlea [1], which employed a cascade of second-order Ô¨Ålters with exponentially decreasing resonant frequencies. However, the cascade structure suf- fers from delay and noise accumulation and lacks fault-tolerance. Modeling the cochlea more faithfully, Watts built a two-dimensional (2D) passive cochlea that addressed these shortcomings by incorporating the cochlear Ô¨Çuid using a resistive network [2]. This par- allel structure, however, has its own problem: response gain is diminished by interference among the second-order sections‚Äô outputs due to the large phase change at resonance [3].
Listening more to biology, our silicon cochlea aims to overcome the shortcomings of exist- ing architectures by mimicking the cochlear micromechanics while including outer hair cell (OHC) electromotility. Although how exactly OHC motile forces boost the basilar mem- brane‚Äôs (BM) vibration remains a mystery, cochlear microanatomy provides clues. Based on these clues, we previously proposed a novel mechanism, active bidirectional coupling (ABC), for the cochlear ampliÔ¨Åer [4]. Here, we report an analog VLSI chip that implements this mechanism. In essence, our implementation is the Ô¨Årst silicon cochlea that employs stimulus enhancement (i.e., active behavior) instead of undamping (i.e., high Ô¨Ålter Q [5]).
The paper is organized as follows. In Section 2, we present the hypothesized mechanism (ABC), Ô¨Årst described in [4]. In Section 3, we provide a mathematical formulation of the"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/a7a3d70c6d17a73140918996d03c014f-Abstract.html,Affine Structure From Sound,Sebastian Thrun,"We consider the problem of localizing a set of microphones together with a set of external acoustic events (e.g., hand claps), emitted at un- known times and unknown locations. We propose a solution that ap- proximates this problem under a far Ô¨Åeld approximation deÔ¨Åned in the calculus of afÔ¨Åne geometry, and that relies on singular value decompo- sition (SVD) to recover the afÔ¨Åne structure of the problem. We then deÔ¨Åne low-dimensional optimization techniques for embedding the solu- tion into Euclidean geometry, and further techniques for recovering the locations and emission times of the acoustic events. The approach is use- ful for the calibration of ad-hoc microphone arrays and sensor networks."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/a7f592cef8b130a6967a90617db5681b-Abstract.html,Distance Metric Learning for Large Margin Nearest Neighbor Classification,"Kilian Q. Weinberger, John Blitzer, Lawrence K. Saul","We show how to learn a Mahanalobis distance metric for k -nearest neighbor (kNN) classification by semidefinite programming. The metric is trained with the goal that the k -nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification--for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modification or extension for problems in multiway (as opposed to binary) classification."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/ac34ae1fda29b8fe781ac8d6d32a6bc7-Abstract.html,Estimating the wrong Markov random field: Benefits in the computation-limited setting,Martin J. Wainwright,"Consider the problem of joint parameter estimation and prediction in a Markov random field: i.e., the model parameters are estimated on the basis of an initial set of data, and then the fitted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working in the computation-limited setting, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for fitting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the ""wrong"" model even in the infinite data limit) is provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of variational methods. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. 1 Keywords: Markov random fields; variational method; message-passing algorithms; sum-product; belief propagation; parameter estimation; learning."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/ad82140cafe816c41a9c9974e9240b7a-Abstract.html,Gaussian Processes for Multiuser Detection in CDMA receivers,"Juan J. Murillo-fuentes, Sebastian Caro, Fernando P√©rez-Cruz","In this paper we propose a new receiver for digital communications. We focus on the application of Gaussian Processes (GPs) to the multiuser detection (MUD) in code division multiple access (CDMA) systems to solve the near-far problem. Hence, we aim to reduce the interference from other users sharing the same frequency band. While usual approaches minimize the mean square error (MMSE) to linearly retrieve the user of interest, we exploit the same criteria but in the design of a nonlinear MUD. Since the optimal solution is known to be nonlinear, the performance of this novel method clearly improves that of the MMSE detectors. Furthermore, the GP based MUD achieves excellent interference suppression even for short training sequences. We also include some experiments to illustrate that other nonlinear detectors such as those based on Support Vector Machines (SVMs) exhibit a worse performance."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/ad8e88c0f76fa4fc8e5474384142a00a-Abstract.html,Metric Learning by Collapsing Classes,"Amir Globerson, Sam T. Roweis","We present an algorithm for learning a quadratic Gaussian metric (Maha- lanobis distance) for use in classiÔ¨Åcation tasks. Our method relies on the simple geometric intuition that a good metric is one under which points in the same class are simultaneously near each other and far from points in the other classes. We construct a convex optimization problem whose solution generates such a metric by trying to collapse all examples in the same class to a single point and push examples in other classes inÔ¨Ånitely far away. We show that when the metric we learn is used in simple clas- siÔ¨Åers, it yields substantial improvements over standard alternatives on a variety of problems. We also discuss how the learned metric may be used to obtain a compact low dimensional feature representation of the original input space, allowing more efÔ¨Åcient classiÔ¨Åcation with very little reduction in performance.
1 Supervised Learning of Metrics
The problem of learning a distance measure (metric) over an input space is of fundamental importance in machine learning [10, 9], both supervised and unsupervised. When such measures are learned directly from the available data, they can be used to improve learn- ing algorithms which rely on distance computations such as nearest neighbour classiÔ¨Å- cation [5], supervised kernel machines (such as GPs or SVMs) and even unsupervised clustering algorithms [10]. Good similarity measures may also provide insight into the inter-protein distances), and may aid in building bet- underlying structure of data (e.g. ter data visualizations via embedding. In fact, there is a close link between distance
learning and feature extraction since whenever we construct a featurefx for an input spaceX, we can measure distances betweenx1;x22X using a simple distance func- tion (e.g. Euclidean)d[fx1;fx2‚ÑÑ in feature space. Thus by Ô¨Åxingd, any feature illustration of this approach is when thefx is a linear projection ofx2< fx=Wx. The Euclidean distance betweenfx1 andfx2 is then the Mahalanobis distancekfx1fx2k2=x1x2TAx1x2, whereA=WTW is a positive learning the matrixA. This is also the goal of the
semideÔ¨Ånite matrix. Much of the recent work on metric learning has indeed focused on learning Mahalanobis distances, i.e. current work.
extraction algorithm may be considered a metric learning method. Perhaps the simplest so that
A common approach to learning metrics is to assume some knowledge in the form of equiv-
alence relations, i.e. which points should be close and which should be far (without speci- fying their exact distances). In the classiÔ¨Åcation setting there is a natural equivalence rela- tion, namely whether two points are in the same class or not. One of the classical statistical methods which uses this idea for the Mahalanobis distance is Fisher‚Äôs Linear Discriminant Analysis (see e.g. [6]). Other more recent methods are [10, 9, 5] which seek to minimize various separation criteria between the classes under the new metric.
2 The Approach of Collapsing Classes"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/ade55409d1224074754035a5a937d2e0-Abstract.html,Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation,"Dmitry Malioutov, Alan S. Willsky, Jason K. Johnson",This paper presents a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose correlations between variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlations. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. This perspective leads to a better understanding of Gaussian belief propagation and of its convergence in loopy graphs.
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/ae587cfeea5ac21a8f1c1ea51027fef0-Abstract.html,Analyzing Coupled Brain Sources: Distinguishing True from Spurious Interaction,"Guido Nolte, Andreas Ziehe, Frank Meinecke, Klaus-Robert M√ºller","When trying to understand the brain, it is of fundamental importance to analyse (e.g. from EEG/MEG measurements) what parts of the cortex interact with each other in order to infer more accurate models of brain activity. Common techniques like Blind Source Separation (BSS) can estimate brain sources and single out artifacts by using the underlying assumption of source signal independence. However, physiologically interesting brain sources typically interact, so BSS will--by construction-- fail to characterize them properly. Noting that there are truly interacting sources and signals that only seemingly interact due to effects of volume conduction, this work aims to contribute by distinguishing these effects. For this a new BSS technique is proposed that uses anti-symmetrized cross-correlation matrices and subsequent diagonalization. The resulting decomposition consists of the truly interacting brain sources and suppresses any spurious interaction stemming from volume conduction. Our new concept of interacting source analysis (ISA) is successfully demonstrated on MEG data."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/aee92f16efd522b9326c25cc3237ac15-Abstract.html,Group and Topic Discovery from Relations and Their Attributes,"Xuerui Wang, Natasha Mohanty, Andrew McCallum","We present a probabilistic generative model of entity relationships and their attributes that simultaneously discovers groups among the entities and topics among the corresponding textual attributes. Block-models of relationship data have been studied in social network analysis for some time. Here we simultaneously cluster in several modalities at once, incor- porating the attributes (here, words) associated with certain relationships. SigniÔ¨Åcantly, joint inference allows the discovery of topics to be guided by the emerging groups, and vice-versa. We present experimental results on two large data sets: sixteen years of bills put before the U.S. Sen- ate, comprising their corresponding text and voting records, and thirteen years of similar data from the United Nations. We show that in compari- son with traditional, separate latent-variable models for words, or Block- structures for votes, the Group-Topic model‚Äôs joint inference discovers more cohesive groups and improved topics."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/aeefb050911334869a7a5d9e4d0e1689-Abstract.html,Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care,"Christopher Williams, John Quinn, Neil Mcintosh","The observed physiological dynamics of an infant receiving intensive care are affected by many possible factors, including interventions to the baby, the operation of the monitoring equipment and the state of health. The Factorial Switching Kalman Filter can be used to infer the presence of such factors from a sequence of observations, and to estimate the true values where these observations have been corrupted. We apply this model to clinical time series data and show it to be effective in identifying a number of artifactual and physiological patterns."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b030afbb3a8af8fb0759241c97466ee4-Abstract.html,A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity,"Robert A. Legenstein, Wolfgang Maass","We investigate under what conditions a neuron can learn by experimen- tally supported rules for spike timing dependent plasticity (STDP) to pre- dict the arrival times of strong ‚Äúteacher inputs‚Äù to the same neuron. It turns out that in contrast to the famous Perceptron Convergence Theo- rem, which predicts convergence of the perceptron learning rule for a simpliÔ¨Åed neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP. But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron. This criterion is reminiscent of the linear separability criterion of the Percep- tron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs. In addition we show through computer simulations for more realistic neuron models that the resulting analyti- cally predicted positive learning results not only hold for the common interpretation of STDP where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b0bef4c9a6e50d43880191492d4fc827-Abstract.html,Q-Clustering,"Mukund Narasimhan, Nebojsa Jojic, Jeff A. Bilmes","We show that Queyranne's algorithm for minimizing symmetric submodular functions can be used for clustering with a variety of different objective functions. Two specific criteria that we consider in this paper are the single linkage and the minimum description length criteria. The first criterion tries to maximize the minimum distance between elements of different clusters, and is inherently ""discriminative"". It is known that optimal clusterings into k clusters for any given k in polynomial time for this criterion can be computed. The second criterion seeks to minimize the description length of the clusters given a probabilistic generative model. We show that the optimal partitioning into 2 clusters, and approximate partitioning (guaranteed to be within a factor of 2 of the the optimal) for more clusters can be computed. To the best of our knowledge, this is the first time that a tractable algorithm for finding the optimal clustering with respect to the MDL criterion for 2 clusters has been given. Besides the optimality result for the MDL criterion, the chief contribution of this paper is to show that the same algorithm can be used to optimize a broad class of criteria, and hence can be used for many application specific criterion for which efficient algorithm are not known."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b1300291698eadedb559786c809cc592-Abstract.html,Generalization Error Bounds for Aggregation by Mirror Descent with Averaging,"Anatoli Juditsky, Alexander Nazin, Alexandre Tsybakov, Nicolas Vayatis","We consider the problem of constructing an aggregated estimator from a Ô¨Ånite class of base functions which approximately minimizes a con- vex risk functional under the ‚Ñì1 constraint. For this purpose, we propose a stochastic procedure, the mirror descent, which performs gradient de- scent in the dual space. The generated estimates are additionally aver- aged in a recursive fashion with speciÔ¨Åc weights. Mirror descent algo- rithms have been developed in different contexts and they are known to be particularly efÔ¨Åcient in high dimensional problems. Moreover their implementation is adapted to the online setting. The main result of the paper is the upper bound on the convergence rate for the generalization error."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b139aeda1c2914e3b579aafd3ceeb1bd-Abstract.html,Asymptotics of Gaussian Regularized Least Squares,"Ross Lippert, Ryan Rifkin","We consider regularized least-squares (RLS) with a Gaussian kernel. We prove that if we let the Gaussian bandwidth œÉ ‚Üí ‚àû while letting the regularization parameter Œª ‚Üí 0, the RLS solution tends to a polynomial whose order is controlled by the rielative rates of decay of 1 œÉ2 and Œª: if Œª = œÉ‚àí(2k+1), then, as œÉ ‚Üí ‚àû, the RLS solution tends to the kth order polynomial with minimal empirical error. We illustrate the result with an example."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b19aa25ff58940d974234b48391b9549-Abstract.html,Large scale networks fingerprinting and visualization using the k-core decomposition,"J. I. Alvarez-hamelin, Luca Dall'asta, Alain Barrat, Alessandro Vespignani","We use the k-core decomposition to develop algorithms for the analysis of large scale complex networks. This decomposition, based on a re- cursive pruning of the least connected vertices, allows to disentangle the hierarchical structure of networks by progressively focusing on their cen- tral cores. By using this strategy we develop a general visualization algo- rithm that can be used to compare the structural properties of various net- works and highlight their hierarchical structure. The low computational complexity of the algorithm, O(n + e), where n is the size of the net- work, and e is the number of edges, makes it suitable for the visualization of very large sparse networks. We show how the proposed visualization tool allows to Ô¨Ånd speciÔ¨Åc structural Ô¨Ångerprints of networks."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b2ea5e977c5fc1ccfa74171a9723dd61-Abstract.html,Norepinephrine and Neural Interrupts,"Peter Dayan, Angela J. Yu","Angela J. Yu
Center for Brain, Mind & Behavior Green Hall, Princeton University
Princeton, NJ 08540, USA ajyu@princeton.edu
Experimental data indicate that norepinephrine is critically involved in aspects of vigilance and attention. Previously, we considered the func- tion of this neuromodulatory system on a time scale of minutes and longer, and suggested that it signals global uncertainty arising from gross changes in environmental contingencies. However, norepinephrine is also known to be activated phasically by familiar stimuli in well- learned tasks. Here, we extend our uncertainty-based treatment of nore- pinephrine to this phasic mode, proposing that it is involved in the de- tection and reaction to state uncertainty within a task. This role of nore- pinephrine can be understood through the metaphor of neural interrupts."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b31df16a88ce00fed951f24b46e08649-Abstract.html,Consensus Propagation,"Benjamin V. Roy, Ciamac C. Moallemi","We propose consensus propagation, an asynchronous distributed protocol for averaging numbers across a network. We establish convergence, characterize the convergence rate for regular graphs, and demonstrate that the protocol exhibits better scaling properties than pairwise averaging, an alternative that has received much recent attention. Consensus propagation can be viewed as a special case of belief propagation, and our results contribute to the belief propagation literature. In particular, beyond singly-connected graphs, there are very few classes of relevant problems for which belief propagation is known to converge."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b44afe91b8a427a6be2078cc89bd6f9b-Abstract.html,CMOL CrossNets: Possible Neuromorphic Nanoelectronic Circuits,"Jung Hoon Lee, Xiaolong Ma, Konstantin K. Likharev","Hybrid  ‚ÄúCMOL‚Äù  integrated  circuits,  combining  CMOS  subsystem  with  nanowire  crossbars  and  simple  two-terminal  nanodevices,  promise  to  extend  the  exponential  Moore-Law  development  of  microelectronics  into  the  sub-10-nm  range.  We  are  developing  neuromorphic  network  (‚ÄúCrossNet‚Äù)  architectures  for  this  future  technology, in which neural cell bodies are implemented in CMOS,  nanowires  are  used  as  axons  and  dendrites,  while  nanodevices  (bistable  latching  switches)  are  used  as  elementary  synapses.  We  have  shown  how  CrossNets  may  be  trained  to  perform  pattern  recovery  and  classification  despite  the  limitations  imposed  by  the  CMOL  hardware.    Preliminary  estimates  have  shown  that  CMOL  CrossNets may be extremely dense (~107 cells per cm2) and operate  approximately a million times faster than biological neural networks,  at  manageable  power  consumption.  In  Conclusion,  we  discuss  in  brief possible short-term and long-term applications of the emerging  technology. 
1  Introduction: CMOL Circuits 
Recent  results  [1,  2]  indicate  that  the  current  VLSI  paradigm  based  on  CMOS  technology  can  be  hardly  extended  beyond  the  10-nm  frontier:  in  this  range  the  sensitivity  of  parameters  (most  importantly,  the  gate  voltage  threshold)  of  silicon  field-effect  transistors  to  inevitable  fabrication  spreads  grows  exponentially.  This  sensitivity will probably send the fabrication facilities costs skyrocketing, and may  lead to the end of Moore‚Äôs Law some time during the next decade.   There  is  a  growing  consensus  that  the  impending  Moore‚Äôs  Law  crisis  may  be  preempted by a radical paradigm shift from the purely CMOS technology to hybrid  CMOS/nanodevice circuits, e.g., those of ‚ÄúCMOL‚Äù variety (Fig. 1). Such circuits (see,  e.g., Ref. 3 for their recent review) would combine a level of advanced CMOS devices  fabricated by the lithographic patterning, and two-layer nanowire crossbar formed,  e.g.,  by  nanoimprint,  with  nanowires  connected  by  simple,  similar,  two-terminal  nanodevices at each crosspoint. For such devices, molecular single-electron latching  switches [4] are presently the leading candidates, in particular because they may be  fabricated using the self-assembled monolayer (SAM) technique which already gave  reproducible results for simpler molecular devices [5]."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b4944963b5c83d545c3d3022bcf03282-Abstract.html,A General and Efficient Multiple Kernel Learning Algorithm,"S√∂ren Sonnenburg, Gunnar R√§tsch, Christin Sch√§fer","While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lankriet et al. (2004) considered conic combinations of kernel matrices for classi- Ô¨Åcation, leading to a convex quadratically constraint quadratic program. We show that it can be rewritten as a semi-inÔ¨Ånite linear program that can be efÔ¨Åciently solved by recycling the standard SVM implementa- tions. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classiÔ¨Åca- tion. Experimental results show that the proposed algorithm helps for automatic model selection, improving the interpretability of the learn- ing result and works for hundred thousands of examples or hundreds of kernels to be combined.
f (x) = sign N Xi=1
Œ±iyik(xi, x) + b! ,"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b4a721cfb62f5d19ec61575114d8a2d1-Abstract.html,Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity,"Kenneth Hild, Kensuke Sekihara, Hagai T. Attias, Srikantan S. Nagarajan","This paper presents a novel technique for analyzing electromagnetic imaging data obtained using the stimulus evoked experimental paradigm. The technique is based on a probabilistic graphical model, which describes the data in terms of underlying evoked and interference sources, and explicitly models the stimulus evoked paradigm. A variational Bayesian EM algorithm infers the model from data, suppresses interference sources, and reconstructs the activity of separated individual brain sources. The new algorithm outperforms existing techniques on two real datasets, as well as on simulated data."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b4fd1d2cb085390fbbadae65e07876a7-Abstract.html,Augmented Rescorla-Wagner and Maximum Likelihood Estimation,Alan L. Yuille,"We show that linear generalizations of Rescorla-Wagner can perform Maximum Likelihood estimation of the parameters of all generative models for causal reasoning. Our approach involves augmenting variables to deal with conjunctions of causes, similar to the agumented model of Rescorla. Our results involve genericity assumptions on the distributions of causes. If these assumptions are violated, for example for the Cheng causal power theory, then we show that a linear Rescorla-Wagner can estimate the parameters of the model up to a nonlinear transformtion. Moreover, a nonlinear Rescorla-Wagner is able to estimate the parameters directly to within arbitrary accuracy. Previous results can be used to determine convergence and to estimate convergence rates."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b5b03f06271f8917685d14cea7c6c50a-Abstract.html,Laplacian Score for Feature Selection,"Xiaofei He, Deng Cai, Partha Niyogi","In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning scenarios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of previous unsupervised feature selection methods are ""wrapper"" techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a ""filter"" method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classification problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demonstrate the effectiveness and efficiency of our algorithm."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b6617980ce90f637e68c3ebe8b9be745-Abstract.html,Variational Bayesian Stochastic Complexity of Mixture Models,"Kazuho Watanabe, Sumio Watanabe","The Variational Bayesian framework has been widely used to approximate the Bayesian learning. In various applications, it has provided computational tractability and good generalization performance. In this paper, we discuss the Variational Bayesian learning of the mixture of exponential families and provide some additional theoretical support by deriving the asymptotic form of the stochastic complexity. The stochastic complexity, which corresponds to the minimum free energy and a lower bound of the marginal likelihood, is a key quantity for model selection. It also enables us to discuss the effect of hyperparameters and the accuracy of the Variational Bayesian approach as an approximation of the true Bayesian learning."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/b8b9c74ac526fffbeb2d39ab038d1cd7-Abstract.html,Hierarchical Linear/Constant Time SLAM Using Particle Filters for Dense Maps,"Austin I. Eliazar, Ronald Parr","We present an improvement to the DP-SLAM algorithm for simultane- ous localization and mapping (SLAM) that maintains multiple hypothe- ses about densely populated maps (one full map per particle in a par- ticle Ô¨Ålter) in time that is linear in all signiÔ¨Åcant algorithm parameters and takes constant (amortized) time per iteration. This means that the asymptotic complexity of the algorithm is no greater than that of a pure localization algorithm using a single map and the same number of parti- cles. We also present a hierarchical extension of DP-SLAM that uses a two level particle Ô¨Ålter which models drift in the particle Ô¨Åltering process itself. The hierarchical approach enables recovery from the inevitable drift that results from using a Ô¨Ånite number of particles in a particle Ô¨Ålter and permits the use of DP-SLAM in more challenging domains, while maintaining linear time asymptotic complexity."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/bc4e356fee1972242c8f7eabf4dff517-Abstract.html,An exploration-exploitation model based on norepinepherine and dopamine activity,"Samuel M. McClure, Mark S. Gilzenrat, Jonathan D. Cohen","We propose a model by which dopamine (DA) and norepinepherine  (NE) combine to alternate behavior between relatively exploratory  and  exploitative  modes.  The  model  is  developed  for  a  target  detection  task  for  which  there  is  extant  single  neuron  recording  data  available  from  locus  coeruleus  (LC)  NE  neurons.  An  exploration-exploitation trade-off is elicited by regularly switching  which  of  the  two  stimuli  are  rewarded.  DA  functions  within  the  model  to  change  synaptic  weights  according  to  a  reinforcement  learning  algorithm.  Exploration  is  mediated  by  the  state  of  LC  firing,  with  higher  tonic  and  lower  phasic  activity  producing  greater  response  variability.  The  opposite  state  of  LC  function,  with lower baseline firing rate and greater phasic responses, favors  exploitative  behavior.  Changes  in  LC  firing  mode  result  from  combined  measures  of  response  conflict  and  reward  rate,  where  response  conflict  is  monitored  using  models  of  anterior  cingulate  cortex (ACC). Increased long-term response conflict and decreased  reward  rate,  which  occurs  following  reward  contingency  switch,  favors  the  higher  tonic  state  of  LC  function  and  NE  release.  This  increases exploration, and facilitates discovery of the new target."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/bd1354624fbae3b2149878941c60df99-Abstract.html,Kernels for gene regulatory regions,"Jean-philippe Vert, Robert Thurman, William S. Noble","We describe a hierarchy of motif-based kernels for multiple alignments of biological sequences, particularly suitable to process regulatory regions of genes. The kernels incorporate progressively more information, with the most complex kernel accounting for a multiple alignment of orthologous regions, the phylogenetic tree relating the species, and the prior knowledge that relevant sequence patterns occur in conserved motif blocks. These kernels can be used in the presence of a library of known transcription factor binding sites, or de novo by iterating over all k -mers of a given length. In the latter mode, a discriminative classifier built from such a kernel not only recognizes a given class of promoter regions, but as a side effect simultaneously identifies a collection of relevant, discriminative sequence motifs. We demonstrate the utility of the motif-based multiple alignment kernels by using a collection of aligned promoter regions from five yeast species to recognize classes of cell-cycle regulated genes. Supplementary data is available at http://noble.gs.washington.edu/proj/pkernel."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/bf2fb7d1825a1df3ca308ad0bf48591e-Abstract.html,Transfer learning for text classification,"Chuong B. Do, Andrew Y. Ng","Linear text classiÔ¨Åcation algorithms work by computing an inner prod- uct between a test document vector and a parameter vector. In many such algorithms, including naive Bayes and most TFIDF variants, the parame- ters are determined by some simple, closed-form, function of training set statistics; we call this mapping mapping from statistics to parameters, the parameter function. Much research in text classiÔ¨Åcation over the last few decades has consisted of manual efforts to identify better parameter func- tions. In this paper, we propose an algorithm for automatically learning this function from related classiÔ¨Åcation problems. The parameter func- tion found by our algorithm then deÔ¨Ånes a new learning algorithm for text classiÔ¨Åcation, which we can apply to novel classiÔ¨Åcation tasks. We Ô¨Ånd that our learned classiÔ¨Åer outperforms existing methods on a variety of multiclass text classiÔ¨Åcation tasks.
arg maxk2f1;:::;KgPn"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/c0f971d8cd24364f2029fcb9ac7b71f5-Abstract.html,The Forgetron: A Kernel-Based Perceptron on a Fixed Budget,"Ofer Dekel, Shai Shalev-shwartz, Yoram Singer","The Perceptron algorithm, despite its simplicity, often performs well on online classification tasks. The Perceptron becomes especially effective when it is used in conjunction with kernels. However, a common difficulty encountered when implementing kernel-based online algorithms is the amount of memory required to store the online hypothesis, which may grow unboundedly. In this paper we present and analyze the Forgetron algorithm for kernel-based online learning on a fixed memory budget. To our knowledge, this is the first online learning algorithm which, on one hand, maintains a strict limit on the number of examples it stores while, on the other hand, entertains a relative mistake bound. In addition to the formal results, we also present experiments with real datasets which underscore the merits of our approach."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/c3008b2c6f5370b744850a98a95b73ad-Abstract.html,Online Discovery and Learning of Predictive State Representations,"Peter Mccracken, Michael Bowling","Predictive state representations (PSRs) are a method of modeling dynamical systems using only observable data, such as actions and observations, to describe their model. PSRs use predictions about the outcome of future tests to summarize the system state. The best existing techniques for discovery and learning of PSRs use a Monte Carlo approach to explicitly estimate these outcome probabilities. In this paper, we present a new algorithm for discovery and learning of PSRs that uses a gradient descent approach to compute the predictions for the current state. The algorithm takes advantage of the large amount of structure inherent in a valid prediction matrix to constrain its predictions. Furthermore, the algorithm can be used online by an agent to constantly improve its prediction quality; something that current state of the art discovery and learning algorithms are unable to do. We give empirical results to show that our constrained gradient algorithm is able to discover core tests using very small amounts of data, and with larger amounts of data can compute accurate predictions of the system dynamics."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/c429429bf1f2af051f2021dc92a8ebea-Abstract.html,Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification,"Ashish Kapoor, Hyungil Ahn, Yuan Qi, Rosalind W. Picard","There have been many graph-based approaches for semi-supervised clas- siÔ¨Åcation. One problem is that of hyperparameter learning: performance depends greatly on the hyperparameters of the similarity graph, trans- formation of the graph Laplacian and the noise model. We present a Bayesian framework for learning hyperparameters for graph-based semi- supervised classiÔ¨Åcation. Given some labeled data, which can contain inaccurate labels, we pose the semi-supervised classiÔ¨Åcation as an in- ference problem over the unknown labels. Expectation Propagation is used for approximate inference and the mean of the posterior is used for classiÔ¨Åcation. The hyperparameters are learned using EM for evidence maximization. We also show that the posterior mean can be written in terms of the kernel matrix, providing a Bayesian classiÔ¨Åer to classify new points. Tests on synthetic and real datasets show cases where there are signiÔ¨Åcant improvements in performance over the existing approaches."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/c61f571dbd2fb949d3fe5ae1608dd48b-Abstract.html,Fusion of Similarity Data in Clustering,"Tilman Lange, Joachim M. Buhmann","Fusing multiple information sources can yield signiÔ¨Åcant beneÔ¨Åts to suc- cessfully accomplish learning tasks. Many studies have focussed on fus- ing information in supervised learning contexts. We present an approach to utilize multiple information sources in the form of similarity data for unsupervised learning. Based on similarity information, the clustering task is phrased as a non-negative matrix factorization problem of a mix- ture of similarity measurements. The tradeoff between the informative- ness of data sources and the sparseness of their mixture is controlled by an entropy-based weighting mechanism. For the purpose of model se- lection, a stability-based approach is employed to ensure the selection of the most self-consistent hypothesis. The experiments demonstrate the performance of the method on toy as well as real world data sets."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/c68c9c8258ea7d85472dd6fd0015f047-Abstract.html,Sensory Adaptation within a Bayesian Framework for Perception,"Alan Stocker, Eero P. Simoncelli","We extend a previously developed Bayesian framework for perception to account for sensory adaptation. We Ô¨Årst note that the perceptual ef- fects of adaptation seems inconsistent with an adjustment of the inter- nally represented prior distribution. Instead, we postulate that adaptation increases the signal-to-noise ratio of the measurements by adapting the operational range of the measurement stage to the input range. We show that this changes the likelihood function in such a way that the Bayesian estimator model can account for reported perceptual behavior. In particu- lar, we compare the model‚Äôs predictions to human motion discrimination data and demonstrate that the model accounts for the commonly observed perceptual adaptation effects of repulsion and enhanced discriminability."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/c7558e9d1f956b016d1fdba7ea132378-Abstract.html,Radial Basis Function Network for Multi-task Learning,"Xuejun Liao, Lawrence Carin","We extend radial basis function (RBF) networks to the scenario in which multiple correlated tasks are learned simultaneously, and present the cor- responding learning algorithms. We develop the algorithms for learn- ing the network structure, in either a supervised or unsupervised manner. Training data may also be actively selected to improve the network‚Äôs gen- eralization to test data. Experimental results based on real data demon- strate the advantage of the proposed algorithms and support our conclu- sions."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/c9efe5f26cd17ba6216bbe2a7d26d490-Abstract.html,Prediction and Change Detection,"Mark Steyvers, Scott Brown","We measure the ability of human observers to predict the next datum  in a sequence that is generated by a simple statistical process  undergoing change at random points in time. Accurate performance  in this task requires the identification of changepoints. We assess  individual differences between observers both empirically, and  using two kinds of models: a Bayesian approach for change detection  and a family of cognitively plausible fast and frugal models. Some  individuals detect  too many changes and hence perform  sub-optimally due to excess variability. Other individuals do not  detect enough changes, and perform sub-optimally because they fail  to notice short-term temporal trends. 
1 I n t r o d u c t i o n 
Decision-making often requires a rapid response to change. For example, stock  analysts need to quickly detect changes in the market in order to adjust investment  strategies. Coaches need to track changes in a player‚Äôs performance in order to adjust  strategy. When tracking changes, there are costs involved when either more or less  changes are observed than actually occurred. For example, when using an overly  conservative change detection criterion, a stock analyst might miss important  short-term trends and interpret them as random fluctuations instead. On the other  hand, a change may also be detected too readily. For example, in basketball, a player  who makes a series of consecutive baskets is often identified as a ‚Äúhot hand‚Äù player  whose underlying ability is perceived to have suddenly increased [1,2]. This might  lead to sub-optimal passing strategies, based on random fluctuations. 
We are interested in explaining individual differences in a sequential prediction task.  Observers are shown stimuli generated from a simple statistical process with the task  of predicting the next datum in the sequence. The latent parameters of the statistical  process change discretely at random points in time. Performance in this task depends  on the accurate detection of those changepoints, as well as inference about future  outcomes based on the outcomes that followed the most recent inferred changepoint.  There is much prior research in statistics on the problem of identifying changepoints  [3,4,5]. In this paper, we adopt a Bayesian approach to the changepoint identification  problem and develop a simple inference procedure to predict the next datum in a  sequence. The Bayesian model serves as an ideal observer model and is useful to  characterize the ways in which individuals deviate from optimality. 
The plan of the paper is as follows. We first introduce the sequential prediction task  and discuss a Bayesian analysis of this prediction problem. We then discuss the results  from a few individuals in this prediction task and show how the Bayesian approach  can capture individual differences with a single ‚Äútwitchiness‚Äù parameter that  describes how readily changes are perceived in random sequences. We will show that  some individuals are too twitchy: their performance is too variable because they base  their predictions on too little of the recent data. Other individuals are not twitchy  enough, and they fail to capture fast changes in the data. We also show how behavior  can be explained with a set of fast and frugal models [6]. These are cognitively  realistic models that operate under plausible computational constraints. 
2 A p r e d i c t i o n t a s k w i t h m u l t i p l e c h a n g e p o i n t s 
In the prediction task, stimuli are presented sequentially and the task is to predict the  next stimulus in the sequence. After t trials, the observer has been presented with  stimuli y1, y2, ‚Ä¶, yt and the task is to make a prediction about yt+1. After the prediction  is made, the actual outcome yt+1 is revealed and the next trial proceeds to the  prediction of yt+2. This procedure starts with y1 and is repeated for T trials. 
The observations yt are D-dimensional vectors with elements sampled from binomial  distributions. The parameters of those distributions change discretely at random  points in time such that the mean increases or decreases after a change point. This  generates a sequence of observation vectors, y1, y2, ‚Ä¶, yT, where each yt = {yt,1 ‚Ä¶  yt,D}. Each of the yt,d is sampled from a binomial distribution Bin(Œ∏t,d,K), so 0 ‚â§ yt,d ‚â§  K. The parameter vector Œ∏t ={Œ∏t,1 ‚Ä¶ Œ∏t,D} changes depending on the locations of the  changepoints. At each time step,  x is a binary indicator for the occurrence of a 
t
changepoint occurring at time t+1. The parameter Œ± determines the probability of a  change occurring in the sequence. The generative model is specified by the following  algorithm: 

For d=1..D sample Œ∏1,d from a Uniform(0,1) distribution"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/ccd45007df44dd0f12098f486e7e8a0f-Abstract.html,Gaussian Process Dynamical Models,"Jack Wang, Aaron Hertzmann, David J Fleet","This paper introduces Gaussian Process Dynamical Models (GPDM) for nonlinear time series analysis. A GPDM comprises a low-dimensional latent space with associated dynamics, and a map from the latent space to an observation space. We marginalize out the model parameters in closed-form, using Gaussian Process (GP) priors for both the dynamics and the observation mappings. This results in a nonparametric model for dynamical systems that accounts for uncertainty in the model. We demonstrate the approach on human motion capture data in which each pose is 62-dimensional. Despite the use of small data sets, the GPDM learns an effective representation of the nonlinear dynamics in these spaces. Webpage: http://www.dgp.toronto.edu/"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/d0010a6f34908640a4a6da2389772a78-Abstract.html,Rodeo: Sparse Nonparametric Regression in High Dimensions,"Larry Wasserman, John D. Lafferty","We present a method for nonparametric regression that performs bandwidth selection and variable selection simultaneously. The approach is based on the technique of incrementally decreasing the bandwidth in directions where the gradient of the estimator with respect to bandwidth is large. When the unknown function satisfies a sparsity condition, our approach avoids the curse of dimensionality, achieving the optimal minimax rate of convergence, up to logarithmic factors, as if the relevant variables were known in advance. The method--called rodeo (regularization of derivative expectation operator)--conducts a sequence of hypothesis tests, and is easy to implement. A modified version that replaces hard with soft thresholding effectively solves a sequence of lasso problems."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/d0bb8259d8fe3c7df4554dab9d7da3c9-Abstract.html,Pattern Recognition from One Example by Chopping,"Francois Fleuret, Gilles Blanchard","We investigate the learning of the appearance of an object from a single image of it. Instead of using a large number of pictures of the object to recognize, we use a labeled reference database of pictures of other ob- jects to learn invariance to noise and variations in pose and illumination. This acquired knowledge is then used to predict if two pictures of new objects, which do not appear on the training pictures, actually display the same object. We propose a generic scheme called chopping to address this task. It relies on hundreds of random binary splits of the training set chosen to keep together the images of any given object. Those splits are extended to the complete image space with a simple learning algorithm. Given two images, the responses of the split predictors are combined with a Bayesian rule into a posterior probability of similarity. Experiments with the COIL-100 database and with a database of 150 de- graded LATEX symbols compare our method to a classical learning with several examples of the positive class and to a direct learning of the sim- ilarity."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/d210cf373cf002a04ec72ee395f66306-Abstract.html,Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs,"Firas Hamze, Nando de Freitas","This paper presents a new sampling algorithm for approximating func- tions of variables representable as undirected graphical models of arbi- trary connectivity with pairwise potentials, as well as for estimating the notoriously dif(cid:2)cult partition function of the graph. The algorithm (cid:2)ts into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of in- termediate distributions which get closer to the desired one. While the idea of using (cid:147)tempered(cid:148) proposals is known, we construct a novel se- quence of target distributions where, rather than dropping a global tem- perature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the parti- tion function for sparse and densely-connected graphs."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/d3aeec875c479e55d1cdeea161842ec6-Abstract.html,Separation of Music Signals by Harmonic Structure Modeling,"Yun-gang Zhang, Chang-shui Zhang","Separation of music signals is an interesting but difficult problem. It is helpful for many other music researches such as audio content analysis. In this paper, a new music signal separation method is proposed, which is based on harmonic structure modeling. The main idea of harmonic structure modeling is that the harmonic structure of a music signal is stable, so a music signal can be represented by a harmonic structure model. Accordingly, a corresponding separation algorithm is proposed. The main idea is to learn a harmonic structure model for each music signal in the mixture, and then separate signals by using these models to distinguish harmonic structures of different signals. Experimental results show that the algorithm can separate signals and obtain not only a very high Signalto-Noise Ratio (SNR) but also a rather good subjective audio quality."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/d3d80b656929a5bc0fa34381bf42fbdd-Abstract.html,Learning Minimum Volume Sets,"Clayton Scott, Robert Nowak","Given a probability measure P and a reference measure ¬µ, one is often interested in the minimum ¬µ-measure set with P -measure at least Œ±. Minimum volume sets of this type summarize the regions of greatest probability mass of P , and are useful for detecting anoma- lies and constructing conÔ¨Ådence regions. This paper addresses the problem of estimating minimum volume sets based on independent samples distributed according to P . Other than these samples, no other information is available regarding P , but the reference mea- sure ¬µ is assumed to be known. We introduce rules for estimating minimum volume sets that parallel the empirical risk minimization and structural risk minimization principles in classiÔ¨Åcation. As in classiÔ¨Åcation, we show that the performances of our estimators are controlled by the rate of uniform convergence of empirical to true probabilities over the class from which the estimator is drawn. Thus we obtain Ô¨Ånite sample size performance bounds in terms of VC dimension and related quantities. We also demonstrate strong universal consistency and an oracle inequality. Estimators based on histograms and dyadic partitions illustrate the proposed rules."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/d47844673f2db74d78da8687d794523d-Abstract.html,Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms,"Baback Moghaddam, Yair Weiss, Shai Avidan","Sparse PCA seeks approximate sparse ""eigenvectors"" whose projections capture the maximal variance of data. As a cardinality-constrained and non-convex optimization problem, it is NP-hard and is encountered in a wide range of applied fields, from bio-informatics to finance. Recent progress has focused mainly on continuous approximation and convex relaxation of the hard cardinality constraint. In contrast, we consider an alternative discrete spectral formulation based on variational eigenvalue bounds and provide an effective greedy strategy as well as provably optimal solutions using branch-and-bound search. Moreover, the exact methodology used reveals a simple renormalization step that improves approximate solutions obtained by any continuous method. The resulting performance gain of discrete algorithms is demonstrated on real-world benchmark data and in extensive Monte Carlo evaluation trials."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/d53697441ef12a45422f6660202f9840-Abstract.html,A Domain Decomposition Method for Fast Manifold Learning,"Zhenyue Zhang, Hongyuan Zha","We propose a fast manifold learning algorithm based on the methodology of domain decomposition. Starting with the set of sample points partitioned into two subdomains, we develop the solution of the interface problem that can glue the embeddings on the two subdomains into an embedding on the whole domain. We provide a detailed analysis to assess the errors produced by the gluing process using matrix perturbation theory. Numerical examples are given to illustrate the efficiency and effectiveness of the proposed methods."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/d58e2f077670f4de9cd7963c857f2534-Abstract.html,Generalized Nonnegative Matrix Approximations with Bregman Divergences,"Suvrit Sra, Inderjit S. Dhillon","Nonnegative matrix approximation (NNMA) is a recent technique for dimensionality reduction and data analysis that yields a parts based, sparse nonnegative representation for nonnegative input data. NNMA has found a wide variety of applications, including text analysis, document clustering, face/image recognition, language modeling, speech processing and many others. Despite these numerous applications, the algorithmic development for computing the NNMA factors has been relatively deficient. This paper makes algorithmic progress by modeling and solving (using multiplicative updates) new generalized NNMA problems that minimize Bregman divergences between the input matrix and its lowrank approximation. The multiplicative update formulae in the pioneering work by Lee and Seung [11] arise as a special case of our algorithms. In addition, the paper shows how to use penalty functions for incorporating constraints other than nonnegativity into the problem. Further, some interesting extensions to the use of ""link"" functions for modeling nonlinear relationships are also discussed."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/d71fa38b648d86602d14ac610f2e6194-Abstract.html,Predicting EMG Data from M1 Neurons with Variational Bayesian Least Squares,"Jo-anne Ting, Aaron D'souza, Kenji Yamamoto, Toshinori Yoshioka, Donna Hoffman, Shinji Kakei, Lauren Sergio, John Kalaska, Mitsuo Kawato","An increasing number of projects in neuroscience requires the sta- tistical analysis of high dimensional data sets, as, for instance, in predicting behavior from neural Ô¨Åring or in operating artiÔ¨Åcial de- vices from brain recordings in brain-machine interfaces. Linear analysis techniques remain prevalent in such cases, but classical linear regression approaches are often numerically too fragile in high dimensions. In this paper, we address the question of whether EMG data collected from arm movements of monkeys can be faith- fully reconstructed with linear approaches from neural activity in primary motor cortex (M1). To achieve robust data analysis, we develop a full Bayesian approach to linear regression that auto- matically detects and excludes irrelevant features in the data, reg- ularizing against overÔ¨Åtting. In comparison with ordinary least squares, stepwise regression, partial least squares, LASSO regres- sion and a brute force combinatorial search for the most predictive input features in the data, we demonstrate that the new Bayesian method oÔ¨Äers a superior mixture of characteristics in terms of reg- ularization against overÔ¨Åtting, computational eÔ¨Éciency and ease of use, demonstrating its potential as a drop-in replacement for other linear regression techniques. As neuroscientiÔ¨Åc results, our anal- yses demonstrate that EMG data can be well predicted from M1 neurons, further opening the path for possible real-time interfaces between brains and machines."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/d8e1344e27a5b08cdfd5d027d9b8d6de-Abstract.html,Comparing the Effects of Different Weight Distributions on Finding Sparse Representations,"Bhaskar D. Rao, David P. Wipf","Given a redundant dictionary of basis vectors (or atoms), our goal is to Ô¨Ånd maximally sparse representations of signals. Previously, we have argued that a sparse Bayesian learning (SBL) framework is particularly well-suited for this task, showing that it has far fewer local minima than other Bayesian-inspired strategies. In this paper, we provide further evi- dence for this claim by proving a restricted equivalence condition, based on the distribution of the nonzero generating model weights, whereby the SBL solution will equal the maximally sparse representation. We also prove that if these nonzero weights are drawn from an approximate Jef- freys prior, then with probability approaching one, our equivalence con- dition is satisÔ¨Åed. Finally, we motivate the worst-case scenario for SBL and demonstrate that it is still better than the most widely used sparse rep- resentation algorithms. These include Basis Pursuit (BP), which is based on a convex relaxation of the ‚Ñì0 (quasi)-norm, and Orthogonal Match- ing Pursuit (OMP), a simple greedy strategy that iteratively selects basis vectors most aligned with the current residual."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/db5cea26ca37aa09e5365f3e7f5dd9eb-Abstract.html,Goal-Based Imitation as Probabilistic Inference over Graphical Models,"Deepak Verma, Rajesh P. Rao","Humans are extremely adept at learning new skills by imitating the actions of others. A progression of imitative abilities has been observed in children, ranging from imitation of simple body movements to goalbased imitation based on inferring intent. In this paper, we show that the problem of goal-based imitation can be formulated as one of inferring goals and selecting actions using a learned probabilistic graphical model of the environment. We first describe algorithms for planning actions to achieve a goal state using probabilistic inference. We then describe how planning can be used to bootstrap the learning of goal-dependent policies by utilizing feedback from the environment. The resulting graphical model is then shown to be powerful enough to allow goal-based imitation. Using a simple maze navigation task, we illustrate how an agent can infer the goals of an observed teacher and imitate the teacher even when the goals are uncertain and the demonstration is incomplete."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/dba31bb5c75992690f20c2d3b370ec7c-Abstract.html,Policy-Gradient Methods for Planning,Douglas Aberdeen,"Probabilistic temporal planning attempts to Ô¨Ånd good policies for acting in domains with concurrent durative tasks, multiple uncertain outcomes, and limited resources. These domains are typically modelled as Markov decision problems and solved using dynamic programming methods. This paper demonstrates the application of reinforcement learning ‚Äî in the form of a policy-gradient method ‚Äî to these domains. Our emphasis is large domains that are infeasible for dynamic programming. Our ap- proach is to construct simple policies, or agents, for each planning task. The result is a general probabilistic temporal planner, named the Factored Policy-Gradient Planner (FPG-Planner), which can handle hundreds of tasks, optimising for probability of success, duration, and resource use."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/dc20d1211f3e7a99d775b26052e0163e-Abstract.html,Message passing for task redistribution on sparse graphs,"K. Y. Michael Wong, David Saad, Zhuo Gao","The problem of resource allocation in sparse graphs with real variables is studied using methods of statistical physics. An efÔ¨Åcient distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulations, showing excellent performance and full agreement with the theoretical results."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/dfb84a11f431c62436cfb760e30a34fe-Abstract.html,Neuronal Fiber Delineation in Area of Edema from Diffusion Weighted MRI,"Ofer Pasternak, Nathan Intrator, Nir Sochen, Yaniv Assaf","Diffusion Tensor Magnetic Resonance Imaging (DT-MRI) is a non inva- sive method for brain neuronal Ô¨Åbers delineation. Here we show a mod- iÔ¨Åcation for DT-MRI that allows delineation of neuronal Ô¨Åbers which are inÔ¨Åltrated by edema. We use the Muliple Tensor Variational (MTV) framework which replaces the diffusion model of DT-MRI with a mul- tiple component model and Ô¨Åts it to the signal attenuation with a vari- ational regularization mechanism. In order to reduce free water con- tamination we estimate the free water compartment volume fraction in each voxel, remove it, and then calculate the anisotropy of the remaining compartment. The variational framework was applied on data collected with conventional clinical parameters, containing only six diffusion di- rections. By using the variational framework we were able to overcome the highly ill posed Ô¨Åtting. The results show that we were able to Ô¨Ånd Ô¨Åbers that were not found by DT-MRI."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/e07bceab69529b0f0b43625953fbf2a0-Abstract.html,A Computational Model of Eye Movements during Object Class Detection,"Wei Zhang, Hyejin Yang, Dimitris Samaras, Gregory J. Zelinsky","We present a computational model of human eye movements in an ob- ject class detection task. The model combines state-of-the-art computer vision object class detection methods (SIFT features trained using Ad- aBoost) with a biologically plausible model of human eye movement to produce a sequence of simulated Ô¨Åxations, culminating with the acqui- sition of a target. We validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task (looking for a teddy bear among visually complex non- target objects). We found considerable agreement between the model and human data in multiple eye movement measures, including number of Ô¨Åxations, cumulative probability of Ô¨Åxating the target, and scanpath distance."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/e27a949795bbe863f31c3b79a2686770-Abstract.html,Efficient Unsupervised Learning for Localization and Detection in Object Categories,"Nicolas Loeff, Himanshu Arora, Alexander Sorokin, David Forsyth","We describe a novel method for learning templates for recognition and localization of objects drawn from categories. A generative model repre- sents the conÔ¨Åguration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features. The com- plexity of the model in the number of features is low, meaning our model is much more efÔ¨Åcient to train than comparative methods. Moreover, a variational approximation is introduced that allows learning to be or- ders of magnitude faster than previous approaches while incorporating many more features. This results in both accuracy and localization im- provements. Our model has been carefully tested on standard datasets; we compare with a number of recent template models. In particular, we demonstrate state-of-the-art results for detection and localization."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/e2c61965b5e23b47b77d7c51611b6d7f-Abstract.html,Beyond Gaussian Processes: On the Distributions of Infinite Networks,"Ricky Der, Daniel D. Lee","A general analysis of the limiting distribution of neural network functions is performed, with emphasis on non-Gaussian limits. We show that with i.i.d. symmetric stable output weights, and more generally with weights distributed from the normal domain of attraction of a stable variable, that the neural functions converge in distribution to stable processes. Conditions are also investigated under which Gaussian limits do occur when the weights are independent but not identically distributed. Some particularly tractable classes of stable distributions are examined, and the possibility of learning with such processes."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/e2f9247929b404b2fe98ba6f32301e3b-Abstract.html,Preconditioner Approximations for Probabilistic Graphical Models,"John D. Lafferty, Pradeep K. Ravikumar","We present a family of approximation techniques for probabilistic graphical models, based on the use of graphical preconditioners developed in the scientific computing literature. Our framework yields rigorous upper and lower bounds on event probabilities and the log partition function of undirected graphical models, using non-iterative procedures that have low time complexity. As in mean field approaches, the approximations are built upon tractable subgraphs; however, we recast the problem of optimizing the tractable distribution parameters and approximate inference in terms of the well-studied linear systems problem of obtaining a good matrix preconditioner. Experiments are presented that compare the new approximation schemes to variational methods."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/e465ae46b07058f4ab5e96b98f101756-Abstract.html,Structured Prediction via the Extragradient Method,"Ben Taskar, Simon Lacoste-Julien, Michael I. Jordan","We present a simple and scalable algorithm for large-margin estima- tion of structured models, including an important class of Markov net- works and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem and apply the extragradient method, yielding an algorithm with linear convergence using simple gra- dient and projection calculations. The projection step can be solved us- ing combinatorial algorithms for min-cost quadratic Ô¨Çow. This makes the approach an efÔ¨Åcient alternative to formulations based on reductions to a quadratic program (QP). We present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/e6af401c28c1790eaef7d55c92ab6ab6-Abstract.html,Noise and the two-thirds power Law,"Uri Maoz, Elon Portugaly, Tamar Flash, Yair Weiss","The two-thirds power law, an empirical law stating an inverse non-linear relationship between the tangential hand speed and the curvature of its trajectory during curved motion, is widely acknowledged to be an invariant of upper-limb movement. It has also been shown to exist in eyemotion, locomotion and was even demonstrated in motion perception and prediction. This ubiquity has fostered various attempts to uncover the origins of this empirical relationship. In these it was generally attributed either to smoothness in hand- or joint-space or to the result of mechanisms that damp noise inherent in the motor system to produce the smooth trajectories evident in healthy human motion. We show here that white Gaussian noise also obeys this power-law. Analysis of signal and noise combinations shows that trajectories that were synthetically created not to comply with the power-law are transformed to power-law compliant ones after combination with low levels of noise. Furthermore, there exist colored noise types that drive non-power-law trajectories to power-law compliance and are not affected by smoothing. These results suggest caution when running experiments aimed at verifying the power-law or assuming its underlying existence without proper analysis of the noise. Our results could also suggest that the power-law might be derived not from smoothness or smoothness-inducing mechanisms operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/e6cbc650cd5798a05dfd0f51d14cde5c-Abstract.html,From Lasso regression to Feature vector machine,"Fan Li, Yiming Yang, Eric P. Xing","Lasso regression tends to assign zero weights to most irrelevant or redundant features, and hence is a promising technique for feature selection. Its limitation, however, is that it only offers solutions to linear models. Kernel machines with feature scaling techniques have been studied for feature selection with non-linear models. However, such approaches require to solve hard non-convex optimization problems. This paper proposes a new approach named the Feature Vector Machine (FVM). It reformulates the standard Lasso regression into a form isomorphic to SVM, and this form can be easily extended for feature selection with non-linear models by introducing kernels defined on feature vectors. FVM generates sparse solutions in the nonlinear feature space and it is much more tractable compared to feature scaling kernel machines. Our experiments with FVM on simulated data show encouraging results in identifying the small number of dominating features that are non-linearly correlated to the response, a task the standard Lasso fails to complete."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/e833e042f509c996b1b25324d56659fb-Abstract.html,Maximum Margin Semi-Supervised Learning for Structured Variables,"Y. Altun, D. McAllester, M. Belkin","Many real-world classiÔ¨Åcation problems involve the prediction of multiple inter-dependent variables forming some structural depen- dency. Recent progress in machine learning has mainly focused on supervised classiÔ¨Åcation of such structured variables. In this paper, we investigate structured classiÔ¨Åcation in a semi-supervised setting. We present a discriminative approach that utilizes the intrinsic ge- ometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Unlike transductive algorithms, our for- mulation naturally extends to new test points."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/e8f2779682fd11fa2067beffc27a9192-Abstract.html,Generalization in Clustering with Unobserved Features,"Eyal Krupka, Naftali Tishby","We argue that when objects are characterized by many attributes, clustering them on the basis of a relatively small random subset of these attributes can capture information on the unobserved attributes as well. Moreover, we show that under mild technical conditions, clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set. We prove a finite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting. The scheme is demonstrated for collaborative filtering of users with movies rating as attributes."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/e924517087669cf201ea91bd737a4ff4-Abstract.html,Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery,"Jeremy Kubica, Joseph Masiero, Robert Jedicke, Andrew Connolly, Andrew W. Moore","In this paper we consider the problem of finding sets of points that conform to a given underlying model from within a dense, noisy set of observations. This problem is motivated by the task of efficiently linking faint asteroid detections, but is applicable to a range of spatial queries. We survey current tree-based approaches, showing a trade-off exists between single tree and multiple tree algorithms. To this end, we present a new type of multiple tree algorithm that uses a variable number of trees to exploit the advantages of both approaches. We empirically show that this algorithm performs well using both simulated and astronomical data."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/e98741479a7b998f88b8f8c9f0b6b6f1-Abstract.html,Neural mechanisms of contrast dependent receptive field size in V1,"Jim Wielaard, Paul Sajda","Based on a large scale spiking neuron model of the input layers 4C and  of macaque, we identify neural mechanisms for the observed contrast dependent receptive field size of V1 cells. We observe a rich variety of mechanisms for the phenomenon and analyze them based on the relative gain of excitatory and inhibitory synaptic inputs. We observe an average growth in the spatial extent of excitation and inhibition for low contrast, as predicted from phenomenological models. However, contrary to phenomenological models, our simulation results suggest this is neither sufficient nor necessary to explain the phenomenon."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/eb9fc349601c69352c859c1faa287874-Abstract.html,Benchmarking Non-Parametric Statistical Tests,"Mikaela Keller, Samy Bengio, Siew Y. Wong","Although non-parametric tests have already been proposed for that purpose, statistical significance tests for non-standard measures (different from the classification error) are less often used in the literature. This paper is an attempt at empirically verifying how these tests compare with more classical tests, on various conditions. More precisely, using a very large dataset to estimate the whole ""population"", we analyzed the behavior of several statistical test, varying the class unbalance, the compared models, the performance measure, and the sample size. The main result is that providing big enough evaluation sets non-parametric tests are relatively reliable in all conditions."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/ec0bfd000f253eff3acb1043e1c06979-Abstract.html,Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations,"Aurelie C. Lozano, Sanjeev R. Kulkarni, Robert E. Schapire","We study the statistical convergence and consistency of regularized Boosting methods, where the samples are not independent and identi- cally distributed (i.i.d.) but come from empirical processes of stationary Œ≤-mixing sequences. Utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples, we prove the consistency of the composite classiÔ¨Åers resulting from a regulariza- tion achieved by restricting the 1-norm of the base classiÔ¨Åers‚Äô weights. When compared to the i.i.d. case, the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/ec7f346604f518906d35ef0492709f78-Abstract.html,A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects,David Arathorn,"Recent neurophysiological evidence suggests the ability to  interpret  biological  motion  is  facilitated  by  a  neuronal  ""mirror  system""  which  maps  visual  inputs  to  the  pre-motor  cortex.  If the  common  architecture  and  circuitry  of  the  cortices  is  taken  to  imply  a  common  computation  across  multiple  perceptual  and  cognitive  modalities, this visual-motor interaction  might be  expected to  have  a unified computational basis.  Two  essential tasks underlying such  visual-motor  cooperation  are  shown  here  to  be  simply  expressed  and  directly  solved  as  transformation-discovery  inverse  problems:  (a)  discriminating  and  determining  the  pose  of a  primed  3D  object  in  a  real-world  scene,  and  (b)  interpreting  the  3D  configuration  of  an  articulated kinematic object in  an  image.  The recently developed  map-seeking  method  provides  tractable,  cortically-plausible  solution  to  these  and  a  variety  of other  inverse  problems  which  can be  posed  as  the  discovery  of a  composition of  transformations  between  two  patterns.  The  method  relies  on  an  ordering  property  of superpositions  and  on  decomposition  of the  transformation  spaces  inherent  in  the  generating  processes  of the  problem. 
a  mathematically"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/ec8b57b0be908301f5748fb04b0714c7-Abstract.html,Dynamic Social Network Analysis using Latent Space Models,"Purnamrita Sarkar, Andrew W. Moore","This paper explores two aspects of social network modeling. First, we generalize a successful static model of relationships into a dynamic model that accounts for friendships drifting over time. Second, we show how to make it tractable to learn such models from data, even as the number of entities n gets large. The generalized model associates each entity with a point in p-dimensional Euclidian latent space. The points can move as time progresses but large moves in latent space are improb- able. Observed links between entities are more likely if the entities are close in latent space. We show how to make such a model tractable (sub- quadratic in the number of entities) by the use of appropriate kernel func- tions for similarity in latent space; the use of low dimensional kd-trees; a new ef(cid:2)cient dynamic adaptation of multidimensional scaling for a (cid:2)rst pass of approximate projection of entities into latent space; and an ef(cid:2)- cient conjugate gradient update rule for non-linear local optimization in which amortized time per entity during an update is O(log n). We use both synthetic and real-world data on upto 11,000 entities which indicate linear scaling in computation time and improved performance over four alternative approaches. We also illustrate the system operating on twelve years of NIPS co-publication data. We present a detailed version of this work in [1]."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/ef0d17b3bdb4ee2aa741ba28c7255c53-Abstract.html,Principles of real-time computing with feedback applied to cortical microcircuit models,"Wolfgang Maass, Prashant Joshi, Eduardo D. Sontag","The network topology of neurons in the brain exhibits an abundance of feedback connections, but the computational function of these feedback connections is largely unknown. We present a computational theory that characterizes the gain in computational power achieved through feedback in dynamical systems with fading memory. It implies that many such systems acquire through feedback universal computational capabilities for analog computing with a non-fading memory. In particular, we show that feedback enables such systems to process time-varying input streams in diverse ways according to rules that are implemented through internal states of the dynamical system. In contrast to previous attractor-based computational models for neural networks, these Ô¨Çexible internal states are high-dimensional attractors of the circuit dynamics, that still allow the circuit state to absorb new information from online input streams. In this way one arrives at novel models for working memory, integration of evidence, and reward expectation in cortical circuits. We show that they are applicable to circuits of conductance-based Hodgkin-Huxley (HH) neurons with high levels of noise that reÔ¨Çect experimental data on in- vivo conditions."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/ef67f7c2d86352c2c42e19d20f881f53-Abstract.html,Location-based activity recognition,"Lin Liao, Dieter Fox, Henry Kautz","Learning patterns of human behavior from sensor data is extremely important for high-level activity inference. We show how to extract and label a person's activities and significant places from traces of GPS data. In contrast to existing techniques, our approach simultaneously detects and classifies the significant locations of a person and takes the highlevel context into account. Our system uses relational Markov networks to represent the hierarchical activity model that encodes the complex relations among GPS readings, activities and significant places. We apply FFT-based message passing to perform efficient summation over large numbers of nodes in the networks. We present experiments that show significant improvements over existing techniques."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/efd7e9ed0e5e694ba6df444d84dfa37d-Abstract.html,Modeling Memory Transfer and Saving in Cerebellar Motor Learning,"Naoki Masuda, Shun-ichi Amari","There is a long-standing controversy on the site of the cerebellar motor learning. Different theories and experimental results suggest that either the cerebellar flocculus or the brainstem learns the task and stores the memory. With a dynamical system approach, we clarify the mechanism of transferring the memory generated in the flocculus to the brainstem and that of so-called savings phenomena. The brainstem learning must comply with a sort of Hebbian rule depending on Purkinje-cell activities. In contrast to earlier numerical models, our model is simple but it accommodates explanations and predictions of experimental situations as qualitative features of trajectories in the phase space of synaptic weights, without fine parameter tuning."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/f076073b2082f8741a9cd07b789c77a0-Abstract.html,TD(0) Leads to Better Policies than Approximate Value Iteration,Benjamin V. Roy,We consider approximate value iteration with a parameterized approximator in which the state space is partitioned and the optimal cost-to-go function over each partition is approximated by a constant. We establish performance loss bounds for policies derived from approximations associated with fixed points. These bounds identify benefits to having projection weights equal to the invariant distribution of the resulting policy. Such projection weighting leads to the same fixed points as TD(0). Our analysis also leads to the first performance loss bound for approximate value iteration with an average cost objective.
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/f3b7e5d3eb074cde5b76e26bc0fb5776-Abstract.html,Gradient Flow Independent Component Analysis in Micropower VLSI,"Abdullah Celik, Milutin Stanacevic, Gert Cauwenberghs","We present micropower mixed-signal VLSI hardware for real-time blind separation and localization of acoustic sources. Gradient flow representation of the traveling wave signals acquired over a miniature (1cm diameter) array of four microphones yields linearly mixed instantaneous observations of the time-differentiated sources, separated and localized by independent component analysis (ICA). The gradient flow and ICA processors each measure 3mm  3mm in 0.5 m CMOS, and consume 54 W and 180 W power, respectively, from a 3 V supply at 16 ks/s sampling rate. Experiments demonstrate perceptually clear (12dB) separation and precise localization of two speech sources presented through speakers positioned at 1.5m from the array on a conference room table. Analysis of the multipath residuals shows that they are spectrally diffuse, and void of the direct path."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/f499d34bd87b42948b3960b8f6b82e74-Abstract.html,An Alternative Infinite Mixture Of Gaussian Process Experts,"Edward Meeds, Simon Osindero","We present an inÔ¨Ånite mixture model in which each component com- prises a multivariate Gaussian distribution over an input space, and a Gaussian Process model over an output space. Our model is neatly able to deal with non-stationary covariance functions, discontinuities, multi- modality and overlapping output signals. The work is similar to that by Rasmussen and Ghahramani [1]; however, we use a full generative model over input and output space rather than just a conditional model. This al- lows us to deal with incomplete data, to perform inference over inverse functional mappings as well as for regression, and also leads to a more powerful and consistent Bayesian speciÔ¨Åcation of the effective ‚Äògating network‚Äô for the different experts."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/f565bb9efccaf6986443db0bf01018bc-Abstract.html,Silicon growth cones map silicon retina,"Brian Taba, Kwabena Boahen","We demonstrate the Ô¨Årst fully hardware implementation of retinotopic self-organization, from photon transduction to neural map formation. A silicon retina transduces patterned illumination into correlated spike trains that drive a population of silicon growth cones to automatically wire a topographic mapping by migrating toward sources of a diffusible guidance cue that is released by postsynaptic spikes. We varied the pat- tern of illumination to steer growth cones projected by different retinal ganglion cell types to self-organize segregated or coordinated retinotopic maps."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/f5b1b89d98b7286673128a5fb112cb9a-Abstract.html,Bayesian models of human action understanding,"Chris Baker, Rebecca Saxe, Joshua B. Tenenbaum","We present a Bayesian framework for explaining how people reason about and predict the actions of an intentional agent, based on observ- ing its behavior. Action-understanding is cast as a problem of inverting a probabilistic generative model, which assumes that agents tend to act rationally in order to achieve their goals given the constraints of their en- vironment. Working in a simple sprite-world domain, we show how this model can be used to infer the goal of an agent and predict how the agent will act in novel situations or when environmental constraints change. The model provides a qualitative account of several kinds of inferences that preverbal infants have been shown to perform, and also Ô¨Åts quantita- tive predictions that adult observers make in a new experiment."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/f6c9dc70ecfd8f90ba8598aa2401cd1a-Abstract.html,Learning Influence among Interacting Markov Chains,"Dong Zhang, Daniel Gatica-perez, Samy Bengio, Deb Roy","We present a model that learns the influence of interacting Markov chains within a team. The proposed model is a dynamic Bayesian network (DBN) with a two-level structure: individual-level and group-level. Individual level models actions of each player, and the group-level models actions of the team as a whole. Experiments on synthetic multi-player games and a multi-party meeting corpus show the effectiveness of the proposed model."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/f75526659f31040afeb61cb7133e4e6d-Abstract.html,Off-policy Learning with Options and Recognizers,"Doina Precup, Cosmin Paduraru, Anna Koop, Richard S. Sutton, Satinder P. Singh","We introduce a new algorithm for off-policy temporal-difference learning with function approximation that has lower variance and requires less knowledge of the behavior policy than prior methods. We develop the notion of a recognizer, a filter on actions that distorts the behavior policy to produce a related target policy with low-variance importance-sampling corrections. We also consider target policies that are deviations from the state distribution of the behavior policy, such as potential temporally abstract options, which further reduces variance. This paper introduces recognizers and their potential advantages, then develops a full algorithm for linear function approximation and proves that its updates are in the same direction as on-policy TD updates, which implies asymptotic convergence. Even though our algorithm is based on importance sampling, we prove that it requires absolutely no knowledge of the behavior policy for the case of state-aggregation function approximators.
Off-policy learning is learning about one way of behaving while actually behaving in another way. For example, Q-learning is an off- policy learning method because it learns about the optimal policy while taking actions in a more exploratory fashion, e.g., according to an -greedy policy. Off-policy learning is of interest because only one way of selecting actions can be used at any time, but we would like to learn about many different ways of behaving from the single resultant stream of experience. For example, the options framework for temporal abstraction involves considering a variety of different ways of selecting actions. For each such option one would like to learn a model of its possible outcomes suitable for planning and other uses. Such option models have been proposed as fundamental building blocks of grounded world knowledge (Sutton, Precup & Singh, 1999; Sutton, Rafols & Koop, 2005). Using off-policy learning, one would be able to learn predictive models for many options at the same time from a single stream of experience. Unfortunately, off-policy learning using temporal-difference methods has proven problematic when used in conjunction with function approximation. Function approximation is essential in order to handle the large state spaces that are inherent in many problem do-
mains. Q-learning, for example, has been proven to converge to an optimal policy in the tabular case, but is unsound and may diverge in the case of linear function approximation (Baird, 1996). Precup, Sutton, and Dasgupta (2001) introduced and proved convergence for the first off-policy learning algorithm with linear function approximation. They addressed the problem of learning the expected value of a target policy based on experience generated using a different behavior policy. They used importance sampling techniques to reduce the off-policy case to the on-policy case, where existing convergence theorems apply (Tsitsiklis & Van Roy, 1997; Tadic, 2001). There are two important difficulties with that approach. First, the behavior policy needs to be stationary and known, because it is needed to compute the importance sampling corrections. Second, the importance sampling weights are often ill-conditioned. In the worst case, the variance could be infinite and convergence would not occur. The conditions required to prevent this were somewhat awkward and, even when they applied and asymptotic convergence was assured, the variance could still be high and convergence could be slow. In this paper we address both of these problems in the context of off-policy learning for options. We introduce the notion of a recognizer. Rather than specifying an explicit target policy (for instance, the policy of an option), about which we want to make predictions, a recognizer specifies a condition on the actions that are selected. For example, a recognizer for the temporally extended action of picking up a cup would not specify which hand is to be used, or what the motion should be at all different positions of the cup. The recognizer would recognize a whole variety of directions of motion and poses as part of picking the cup. The advantage of this strategy is not that one might prefer a multitude of different behaviors, but that the behavior may be based on a variety of different strategies, all of which are relevant, and we would like to learn from any of them. In general, a recognizer is a function that recognizes or accepts a space of different ways of behaving and thus, can learn from a wider range of data. Recognizers have two advantages over direct specification of a target policy: 1) they are a natural and easy way to specify a target policy for which importance sampling will be well conditioned, and 2) they do not require the behavior policy to be known. The latter is important because in many cases we may have little knowledge of the behavior policy, or a stationary behavior policy may not even exist. We show that for the case of state aggregation, even if the behavior policy is unknown, convergence to a good model is achieved.
1
Non-sequential example
The benefits of using recognizers in off-policy learning can be most easily seen in a nonsequential context with a single continuous action. Suppose you are given a sequence of sample actions ai  [0, 1], selected i.i.d. according to probability density b : [0, 1]  + (the behavior density). For example, suppose the behavior density is of the oscillatory form shown as a red line in Figure 1. For each each action, ai , we observe a corresponding outcome, zi  , a random variable whose distribution depends only on ai . Thus the behavior density induces an outcome density. The on-policy problem is to estimate the mean mb of the outcome density. This problem can be solved simply by averaging the sample outcomes: mb = (1/n) n=1 zi . The off-policy problem is to use this same data to learn what ^ i the mean would be if actions were selected in some way other than b, for example, if the actions were restricted to a designated range, such as between 0.7 and 0.9. There are two natural ways to pose this off-policy problem. The most straightforward way is to be equally interested in all actions within the designated region. One professes to be interested in actions selected according to a target density  : [0, 1]  + , which in the example would be 5.0 between 0.7 and 0.9, and zero elsewhere, as in the dashed line in
12
Probability density functions
Target policy with recognizer Target policy w/o recognizer
1.5
Empirical variances (average of 200 sample variances)
1 without recognizer .5 with recognizer 100 200 300 400 500"
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/f804d21145597e42851fa736e221da3f-Abstract.html,A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification,"Yves Grandvalet, Johnny Mariethoz, Samy Bengio","In this paper, we show that the hinge loss can be interpreted as the neg-log-likelihood of a semi-parametric model of posterior probabilities. From this point of view, SVMs represent the parametric component of a semi-parametric model Ô¨Åtted by a maximum a posteriori estimation pro- cedure. This connection enables to derive a mapping from SVM scores to estimated posterior probabilities. Unlike previous proposals, the sug- gested mapping is interval-valued, providing a set of posterior probabil- ities compatible with each SVM score. This framework offers a new way to adapt the SVM optimization problem to unbalanced classiÔ¨Åca- tion, when decisions result in unequal (asymmetric) losses. Experiments show improvements over state-of-the-art procedures."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/f9fd2624beefbc7808e4e405d73f57ab-Abstract.html,Nonparametric inference of prior probabilities from Bayes-optimal behavior,Liam Paninski,"We discuss a method for obtaining a subject‚Äôs a priori beliefs from his/her behavior in a psychophysics context, under the assumption that the behavior is (nearly) optimal from a Bayesian perspective. The method is nonparametric in the sense that we do not assume that the prior belongs to any Ô¨Åxed class of distributions (e.g., Gaussian). Despite this increased generality, the method is relatively simple to implement, being based in the simplest case on a linear programming algorithm, and more generally on a straightforward maximum likelihood or maximum a posteriori formulation, which turns out to be a convex optimization problem (with no non-global local maxima) in many important cases. In addition, we develop methods for analyzing the uncertainty of these esti- mates. We demonstrate the accuracy of the method in a simple simulated coin-Ô¨Çipping setting; in particular, the method is able to precisely track the evolution of the subject‚Äôs posterior distribution as more and more data are observed. We close by brieÔ¨Çy discussing an interesting connection to recent models of neural population coding."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/fb3f76858cb38e5b7fd113e0bc1c0721-Abstract.html,Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games,"Gabriel Y. Weintraub, Lanier Benkard, Benjamin Van Roy",We propose a mean-Ô¨Åeld approximation that dramatically reduces the computational complexity of solving stochastic dynamic games. We pro- vide conditions that guarantee our method approximates an equilibrium as the number of agents grow. We then derive a performance bound to assess how well the approximation performs for any given number of agents. We apply our method to an important class of problems in ap- plied microeconomics. We show with numerical experiments that we are able to greatly expand the set of economic problems that can be analyzed computationally.
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/fc192b0c0d270dbf41870a63a8c76c2f-Abstract.html,Dynamical Synapses Give Rise to a Power-Law Distribution of Neuronal Avalanches,"Anna Levina, Michael Herrmann",There is experimental evidence that cortical neurons show avalanche activity with the intensity of firing events being distributed as a power-law. We present a biologically plausible extension of a neural network which exhibits a power-law avalanche distribution for a wide range of connectivity parameters.
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/fd4f21f2556dad0ea8b7a5c04eabebda-Abstract.html,From Weighted Classification to Policy Search,"Doron Blatt, Alfred O. Hero","This paper proposes an algorithm to convert a T -stage stochastic decision problem with a continuous state space to a sequence of supervised learning problems. The optimization problem associated with the trajectory tree and random trajectory methods of Kearns, Mansour, and Ng, 2000, is solved using the Gauss-Seidel method. The algorithm breaks a multistage reinforcement learning problem into a sequence of single-stage reinforcement learning subproblems, each of which is solved via an exact reduction to a weighted-classification problem that can be solved using off-the-self methods. Thus the algorithm converts a reinforcement learning problem into simpler supervised learning subproblems. It is shown that the method converges in a finite number of steps to a solution that cannot be further improved by componentwise optimization. The implication of the proposed algorithm is that a plethora of classification methods can be applied to find policies in the reinforcement learning problem."
2005,https://papers.nips.cc/paper_files/paper/2005,https://papers.nips.cc/paper_files/paper/2005/hash/fdf1bc5669e8ff5ba45d02fded729feb-Abstract.html,Off-Road Obstacle Avoidance through End-to-End Learning,"Urs Muller, Jan Ben, Eric Cosatto, Beat Flepp, Yann L. Cun","We describe a vision-based obstacle avoidance system for off-road mobile robots. The system is trained from end to end to map raw in put images to steering angles. It is trained in supervised mode to predict the steering angles provided by a human driver during training r uns collected in a wide variety of terrains, weather conditions, lighting conditions, and obstacle types. The robot is a 50cm off-road truck, with two f orwardpointing wireless color cameras. A remote computer process es the video and controls the robot via radio. The learning system is a lar ge 6-layer convolutional network whose input is a single left/right pair of unprocessed low-resolution images. The robot exhibits an excellent ability to detect obstacles and navigate around them in real time at speeds of 2 m/s."
